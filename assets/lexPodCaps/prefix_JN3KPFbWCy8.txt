WEBVTT

00:00.000 --> 00:05.480
The following is a conversation with Elon Musk, his fourth time on this, the

00:05.480 --> 00:07.080
Lex Friedman podcast.

00:20.680 --> 00:21.660
I thought you were going to finish it.

00:25.800 --> 00:28.120
It's one of the greatest themes in all of film history.

00:30.000 --> 00:32.080
Yeah, that's great.

00:33.360 --> 00:36.720
So I was just thinking about the Roman Empire as one does.

00:38.720 --> 00:43.760
There's that whole meme where old guys are thinking about the Roman Empire at

00:43.760 --> 00:44.400
least once a day.

00:44.480 --> 00:49.280
And half the population is confused whether it's true or not, but more

00:49.280 --> 00:52.360
seriously thinking about the wars going on in the world today.

00:52.920 --> 00:58.600
And as you know, war and military conquest has been a big part of Roman

00:58.600 --> 00:59.760
society and culture.

01:00.320 --> 01:04.680
And it, I think has been a big part of most empires and dynasties

01:04.680 --> 01:05.600
throughout human history.

01:05.600 --> 01:10.080
So they usually came as a result of conquest.

01:10.720 --> 01:13.000
I mean, there's some like the Austro-Hungarian Empire where there was

01:13.040 --> 01:15.360
just a lot of sort of clever marriages.

01:16.800 --> 01:21.200
But fundamentally there's an engine of conquests and they celebrate excellence

01:21.960 --> 01:22.840
in warfare.

01:23.520 --> 01:27.440
Many of the leaders were excellent generals, that kind of thing.

01:27.920 --> 01:31.040
So big picture question, Grok approved.

01:31.120 --> 01:32.680
I asked this is a good question to ask.

01:32.680 --> 01:33.880
Did you test it, Grok approved?

01:36.000 --> 01:37.000
At least on fun mode.

01:40.920 --> 01:47.480
To what degree do you think war is part of human nature versus a consequence of

01:48.440 --> 01:50.200
how human societies are structured?

01:50.680 --> 01:56.440
I ask this as you have somehow controversially been a proponent of peace.

01:57.320 --> 01:58.800
I'm generally a proponent of peace.

01:58.800 --> 02:03.560
I mean, ignorance is perhaps in my view, the real enemy to counter it.

02:04.360 --> 02:05.240
That's the real hard part.

02:05.240 --> 02:10.280
Not, you know, fighting other humans, but all creatures fight.

02:10.280 --> 02:16.040
I mean, the jungle is a, you look at the, you'll think of nature as perhaps some

02:16.040 --> 02:18.240
sort of peaceful thing, but in fact, it is not.

02:18.240 --> 02:23.560
There's some quite funny, when a, when a hutzpah thing where he's like in the

02:23.560 --> 02:28.680
jungle, like saying that it's like basically just murder and death in every

02:28.680 --> 02:32.440
direction. I mean, the plants, animals in the jungle are constantly trying to

02:32.440 --> 02:35.240
kill each other every single day, every minute.

02:36.040 --> 02:40.160
So it's not like, you know, we're unusual in that respect.

02:41.000 --> 02:45.560
There's a relevant question here, whether with greater intelligence comes greater

02:45.560 --> 02:48.120
control over these base instincts for violence.

02:49.080 --> 02:55.880
Yes, we have much more of an ability to control our, um, limbic instinct for

02:55.880 --> 02:57.640
violence than say a chimpanzee.

02:57.880 --> 03:02.520
And in fact, if one looks at say chimpanzee society, it is not friendly.

03:02.680 --> 03:10.360
I mean, the bonobos are an exception, um, but chimpanzee society is, uh, full of

03:10.360 --> 03:16.040
violence and it's quite, quite horrific, frankly, that that's, that's our

03:16.520 --> 03:17.480
limbic system in action.

03:18.520 --> 03:20.080
Like, you don't want to be on the wrong side of a chimpanzee.

03:20.080 --> 03:21.640
It'll eat your face off and tear your nuts off.

03:22.160 --> 03:22.400
Yeah.

03:22.400 --> 03:26.440
Basically there's no limits or ethics or, uh, the Romans had just war.

03:26.720 --> 03:29.360
There's no just war in chimpanzee societies.

03:29.560 --> 03:33.080
Is war and dominance by any means necessary?

03:33.160 --> 03:36.640
Yeah, chimpanzee society is like a primitive version of human society.

03:37.120 --> 03:40.440
Um, it's, they're not like peace loving basically, um, at all.

03:40.680 --> 03:43.320
Um, there, there's extreme violence.

03:43.920 --> 03:47.600
Um, and then once in a while, some, some, somebody who's watched too many Disney

03:47.600 --> 03:52.760
movies decides to raise a chimpanzee as a pet, um, and then that eats their face

03:52.760 --> 03:55.360
or rips their nuts off or chews their fingers off and that kind of thing.

03:56.320 --> 03:56.520
Yeah.

03:56.520 --> 03:57.320
It's happened several times.

03:57.760 --> 04:01.520
Uh, ripping your nuts off is an interesting strategy for interaction.

04:01.920 --> 04:03.080
So it's happened to people.

04:03.080 --> 04:04.040
It's unfortunate.

04:04.040 --> 04:09.000
Like that's, I guess a one way to ensure that the other chump doesn't, uh, you

04:09.000 --> 04:10.360
know, contribute to the gene pool.

04:10.720 --> 04:13.360
Well, from a martial arts perspective, it's a fascinating strategy.

04:15.320 --> 04:16.040
The nut ripper.

04:18.720 --> 04:20.600
I wonder which of the martial arts teaches that.

04:21.400 --> 04:26.920
I think it's safe to say if somebody's got your nuts in their hands and has the

04:26.920 --> 04:30.240
option of rubbing them off, you will be amenable to, uh, whatever they want.

04:30.360 --> 04:30.680
Yeah.

04:33.120 --> 04:36.840
So, like I said, somehow controversially, you've been an, uh, proponent of peace

04:36.840 --> 04:38.520
on, on Twitter on X.

04:38.520 --> 04:38.760
Yeah.

04:39.080 --> 04:43.360
So let me ask you about the war's going on today and to see what the path to peace

04:43.360 --> 04:47.280
could be, how do you hope the current war in Israel and Gaza comes to an end?

04:47.760 --> 04:52.400
Uh, what path do you see that can minimize human suffering in the long

04:52.400 --> 04:54.000
term in that part of the world?

04:54.760 --> 04:59.080
Well, I think that that part of the world is definitely like, if you look up the,

04:59.280 --> 05:01.440
there is no easy answer in the dictionary.

05:01.480 --> 05:06.280
It'll be that like the picture of, uh, the Middle East, um, in Israel, especially.

05:06.280 --> 05:08.120
So there is no easy answer.

05:09.200 --> 05:16.560
Um, or what my, this is strictly my opinion of, uh, you know, uh, is that, uh,

05:19.440 --> 05:23.800
the, the goal of Moss was to provoke an overreaction from Israel.

05:24.560 --> 05:28.480
Um, they obviously do not expect to, uh, you know, have a military victory.

05:28.600 --> 05:34.040
Um, but they, they expect that they really wanted to commit the worst atrocities

05:34.080 --> 05:40.040
that they could in order to provoke the most aggressive response possible from

05:40.040 --> 05:49.520
Israel, um, and then leverage that, uh, aggressive response to, um, rally Muslims

05:49.520 --> 05:55.880
worldwide, uh, for the cause of, uh, Gaza and Palestine, which they have succeeded

05:55.880 --> 06:03.080
in doing, um, so the, the, the, the counterintuitive thing here, I think

06:03.120 --> 06:07.160
the thing that I think should be done, even though it is very difficult, uh, is

06:07.160 --> 06:12.760
that, um, I would recommend that Israel engage in the most conspicuous acts of

06:12.760 --> 06:17.360
kindness possible, every part, everything that is the actual thing that would

06:17.360 --> 06:18.880
thwart the goal of the most.

06:19.600 --> 06:24.080
So in some sense, the degree that makes sense in geopolitics, turn

06:24.080 --> 06:25.800
the other cheek implemented.

06:26.400 --> 06:28.000
It's not exactly turn the other cheek.

06:28.040 --> 06:34.840
Um, because I do think that there's, um, you know, I think it is appropriate for

06:35.240 --> 06:43.240
Israel to find the Hamas members and, you know, um, either, either kill

06:43.240 --> 06:46.720
them or cost rate them, um, like that's something that something has to be done

06:46.720 --> 06:49.840
because they're just going to keep, keep, keep coming otherwise.

06:50.400 --> 06:57.360
Um, but, uh, in addition to that, they need to do whatever they can.

06:57.440 --> 07:01.240
Um, there's some talk of, uh, so establishing, for example, a mobile

07:01.240 --> 07:03.240
hospital, I'd recommend doing that.

07:03.840 --> 07:10.800
Um, just making sure that, uh, you know, there's food, water, uh, medical

07:10.800 --> 07:18.800
necessities, um, and, and just be over the top about it and be very

07:18.800 --> 07:22.920
transparent so that, so that you can't, you can't claim it's a trick.

07:23.600 --> 07:27.720
Like just put a webcam on the thing, you know, 24 seven.

07:29.080 --> 07:30.440
Deploy acts of kindness.

07:31.440 --> 07:31.760
Yeah.

07:31.800 --> 07:37.760
Conspicuous acts of kindness that, that with that are unequivocal meaning

07:37.800 --> 07:42.680
that can't be somehow because Hamas will then their response will be, oh, it's

07:42.680 --> 07:45.920
a trick, therefore you have to counter how, how it's not a trick.

07:46.880 --> 07:50.640
This ultimately fights the broader force of hatred in the, in the region.

07:51.320 --> 07:51.680
Yes.

07:52.280 --> 07:53.240
And I'm not sure who said it.

07:53.240 --> 07:57.040
It's an apocryphal saying, but an eye for the, for an eye makes everyone blind.

07:57.680 --> 08:01.720
Uh, now that neck of the woods, they really believe in the whole eye for an eye thing.

08:02.360 --> 08:12.560
Um, but you really have, if you're not going to just outright commit genocide,

08:12.560 --> 08:15.120
like against an entire people, which obviously would not be acceptable

08:15.600 --> 08:17.800
to, to really shouldn't be acceptable to anyone.

08:18.520 --> 08:22.560
Um, then you're, you're going to leave basically a lot of people alive who

08:22.680 --> 08:25.120
subsequently, you know, hate Israel.

08:25.120 --> 08:30.400
So really the question is like, how, for every Hamas member that you kill,

08:30.400 --> 08:31.320
how many did you create?

08:33.000 --> 08:36.640
And if you create more than you killed, you've not succeeded.

08:36.680 --> 08:38.840
That's the, you know, the real situation there.

08:39.480 --> 08:48.280
Um, and it's safe to say that if, you know, um, if, you know, if you kill

08:48.280 --> 08:56.920
somebody's child in Gaza, if you've, you've made at least a few, uh, Hamas

08:56.920 --> 09:04.400
members who will die just to kill an Israeli, that's the situation.

09:05.000 --> 09:11.080
So, but, but, I mean, this is one of the most contentious subjects

09:11.480 --> 09:15.320
one could possibly discuss, but, but I think if, if the goal ultimate is some

09:15.320 --> 09:21.400
sort of long-term peace, one has to be look at this from a standpoint of over

09:21.400 --> 09:25.600
time, are there more or fewer, um, terrorists being created?

09:26.720 --> 09:28.640
Let me just, uh, linger on war.

09:29.040 --> 09:29.280
Yeah.

09:29.360 --> 09:32.800
Well, war, it would safe say wars or existed and always will exist.

09:33.120 --> 09:34.280
Always will exist.

09:34.480 --> 09:36.800
It always has, always has existed and always will exist.

09:37.400 --> 09:38.120
I hope not.

09:38.400 --> 09:40.160
You think it always will always, there will always be war.

09:42.080 --> 09:46.720
There's question of just how much war and, and, um, you know, there's this,

09:47.040 --> 09:51.840
there's this sort of the scope and scale of war, but to imagine that there would

09:51.840 --> 09:55.480
not be any war in the future, I think would be a very unlikely outcome.

09:55.680 --> 09:55.800
Yeah.

09:55.800 --> 09:57.160
You talked about the culture series.

09:57.160 --> 09:58.440
There's war even there.

09:58.520 --> 09:59.960
Yes, it was a giant war.

09:59.960 --> 10:05.880
The first book starts off with a gigantic galactic war where trillions die.

10:05.920 --> 10:11.200
Trillions, but it's still nevertheless protects these pockets of, of flourishing.

10:12.280 --> 10:17.040
So somehow you can have galactic war and still have pockets of flourishing.

10:17.920 --> 10:18.160
Yeah.

10:18.160 --> 10:23.480
I mean, it's, I guess if we are able to one day expand to, you know,

10:24.440 --> 10:29.840
for the galaxy or whatever, there won't be a galactic war at some point.

10:30.240 --> 10:31.320
Ah, the scale.

10:31.440 --> 10:34.160
I mean, the scale of war has been increasing, increasing, increasing.

10:34.760 --> 10:38.160
It's like a race between the scale of suffering and the scale of flourishing.

10:38.400 --> 10:38.720
Yes.

10:41.600 --> 10:45.560
A lot of people seem to be using this tragedy to beat the drums of war and

10:45.560 --> 10:47.240
feed the military industrial complex.

10:47.280 --> 10:48.200
Do you worry about this?

10:49.160 --> 10:54.760
The people who are rooting for escalation and how can it be stopped?

10:55.520 --> 10:58.840
One of the things that does concern me is that there are very few people

10:58.840 --> 11:03.920
alive today who actually viscerally understand the horrors of war, at least

11:03.920 --> 11:07.200
in the U S I mean, obviously the people in on the front lines in Ukraine and

11:07.200 --> 11:11.720
Russia who understand just how terrible war is, um, but how many people in

11:11.840 --> 11:12.840
the West understand it.

11:13.840 --> 11:16.680
Um, you know, my grandfather was in world war two.

11:17.360 --> 11:19.040
Uh, he was severely traumatized.

11:19.080 --> 11:23.840
Um, he was there thinking for almost six years and, uh, you know, in

11:23.880 --> 11:28.800
Eastern North Africa and Italy, uh, all his friends were killed, uh, in front

11:28.800 --> 11:31.520
of him and, uh, he would have died too.

11:31.680 --> 11:35.720
Um, except they randomly gave some, I guess, IQ test or something.

11:35.880 --> 11:38.680
And, uh, he scored very high.

11:38.880 --> 11:40.560
Um, now he was not an officer.

11:40.560 --> 11:42.600
It was a, I think corporal or sergeant or something like that.

11:43.160 --> 11:46.480
Um, because he didn't finish high school, um, he had to drop out of high school

11:46.480 --> 11:51.360
because his, his, his dad died and he had to work to support his, um, siblings.

11:52.040 --> 11:54.520
Um, so because he didn't graduate high school, he was not

11:54.520 --> 11:55.640
eligible for the officer core.

11:56.880 --> 12:02.920
Um, so, you know, he kind of got put into the cannon fodder category basically.

12:03.440 --> 12:07.160
Um, but then just randomly they gave him this test.

12:07.160 --> 12:09.760
He was transferred to British intelligence in London.

12:09.760 --> 12:11.480
That's where he met my grandmother.

12:12.120 --> 12:17.280
Um, but, uh, he, he had PTSD next level, like next level.

12:17.440 --> 12:20.720
I mean, just didn't talk, just didn't talk.

12:21.200 --> 12:24.800
And if you tried talking to him, he'd just tell you to shut up and he

12:24.800 --> 12:27.720
won a bunch of medals, never, never ragged about it once.

12:28.560 --> 12:29.920
Not even hinted nothing.

12:29.960 --> 12:33.400
I like found out about it because I, his military records are online.

12:33.840 --> 12:35.560
That's a, it's not how I know.

12:36.280 --> 12:40.920
So he would say like, no, no way in hell do you want to, do you want to do that

12:40.920 --> 12:45.680
again, but how many people, um, now he, he, obviously he, he died, you know,

12:45.680 --> 12:51.320
20 years ago or longer, actually 30 years ago, um, how many people are

12:51.320 --> 12:52.520
alive that remember world war two?

12:53.040 --> 12:53.560
Not many.

12:54.760 --> 12:57.920
And the same perhaps applies to the threat of nuclear war.

13:01.080 --> 13:01.400
Yeah.

13:01.560 --> 13:05.320
I mean, there are enough nuclear bombs pointed at, uh, the United States to

13:05.600 --> 13:08.640
make the rubble, the radioactive rubble bounce many times.

13:09.240 --> 13:12.040
There's two major wars going on right now.

13:12.840 --> 13:18.680
So you talked about the threat of AGI quite a bit, but now as we sit here with

13:18.680 --> 13:22.120
the intensity of conflict going on, do you worry about nuclear war?

13:24.160 --> 13:26.480
I think we shouldn't discount the possibility of nuclear war.

13:27.440 --> 13:29.120
Um, it is a civilizational threat.

13:30.000 --> 13:38.360
Um, right now I could be wrong, but I think the threat of nuclear war

13:38.360 --> 13:41.880
I think that the current probability of nuclear war is quite low.

13:42.560 --> 13:45.360
Um, but there are a lot of nukes pointed at us.

13:46.360 --> 13:48.960
So, and we have a lot of nukes pointed at other people.

13:50.320 --> 13:50.960
They're still there.

13:51.520 --> 13:53.720
Nobody's put their, uh, their guns away.

13:55.440 --> 13:56.720
The missiles are still in the silos.

13:57.560 --> 14:02.560
And, uh, the leaders don't seem to be the ones with the nukes talking to each other.

14:02.960 --> 14:08.000
No, there are wars which are tragic and difficult on a, on a local

14:08.000 --> 14:12.680
basis, and then there are wars which are civilization ending or has that potential.

14:12.720 --> 14:17.240
Obviously global thermonuclear warfare has high potential to end civilization

14:17.760 --> 14:25.200
perhaps permanently, but certainly, you know, to severely, uh, wound and perhaps,

14:25.200 --> 14:33.520
uh, set back, uh, human progress by, you know, the stone age or something.

14:33.600 --> 14:33.960
I don't know.

14:34.600 --> 14:35.120
Pretty bad.

14:36.120 --> 14:42.080
Um, probably scientists and engineers won't be super popular after that as well.

14:42.800 --> 14:44.040
They're like, you got us into this mess.

14:45.720 --> 14:49.760
So generally, which I think we, we obviously want to prioritize

14:49.800 --> 14:54.800
civilizational risks over things that are, um, painful and tragic on,

14:54.840 --> 14:56.600
on a local level, but not civilizational.

15:00.120 --> 15:03.560
How do you hope the war in Ukraine comes to an end and what's the path once

15:03.800 --> 15:05.840
again, to minimizing human suffering there?

15:08.520 --> 15:14.080
Uh, well, I think that what, what is likely to happen, uh, which is really

15:14.120 --> 15:20.920
pretty much the way it is, is that, um, something very close to the current

15:22.400 --> 15:32.640
lines, uh, will be how a ceasefire or truce happens, but, you know, you

15:32.760 --> 15:36.280
just have a situation right now where whoever goes on the offensive, um, will

15:36.280 --> 15:39.960
suffer casualties at several times the rate of whoever's on the defense.

15:40.120 --> 15:45.560
Um, cause you've got, uh, defense in depth, you got minefields, uh, trenches,

15:45.560 --> 15:51.120
anti-tank defenses, um, nobody has air superiority, um, cause the, the, the

15:51.120 --> 15:55.720
anti-aircraft missiles are really far better than the aircraft, like they're

15:55.720 --> 16:02.400
far more of them, um, and, uh, so neither side has air superiority, um, tanks

16:02.400 --> 16:06.960
are basically death traps, um, just slow moving and they're, they're not

16:07.440 --> 16:08.920
immune to anti-tank weapons.

16:09.720 --> 16:16.080
So you, you really just have long range artillery, um, and, uh, infantry

16:16.840 --> 16:22.520
trenches, it's world war one all over again with drones, you know, throwing

16:22.520 --> 16:24.760
old runs on some drugs there, um,

16:25.120 --> 16:28.680
which makes the long range artillery just that much more accurate and better.

16:28.760 --> 16:32.200
And so more efficient at murdering people on both sides.

16:32.560 --> 16:32.880
Mm-hmm.

16:33.760 --> 16:34.120
Yeah.

16:34.280 --> 16:38.320
So it's whoever is, you don't, you don't, you don't want to be trying

16:38.320 --> 16:41.880
to advance from either side because the probability of dying is incredibly high.

16:42.680 --> 16:51.920
Um, so in order to overcome, uh, defense in depth trenches and minefields, you

16:51.920 --> 16:58.200
really need a significant local superiority in numbers, um, ideally

16:58.240 --> 17:06.280
combined arms where, where you do a fast attack with aircraft, a concentrated

17:06.280 --> 17:10.720
number of tanks, um, and a lot of people, that's the only way you're going

17:10.720 --> 17:14.640
to punch through a line and then you're going to punch through and, and, and

17:14.640 --> 17:19.040
then not have reinforcements just kick you right out again.

17:19.800 --> 17:27.200
I mean, I really recommend people read, uh, world war one warfare in detail.

17:28.480 --> 17:29.040
That's rough.

17:30.560 --> 17:35.920
Um, I mean, the sheer number of people that died there was mind boggling.

17:37.280 --> 17:40.640
And it's almost impossible to, um, imagine the end of it.

17:40.680 --> 17:45.720
That doesn't look like almost exactly like the beginning in terms of what

17:45.720 --> 17:52.400
land belongs to who and so on, but on the other side of a lot of human suffering,

17:53.680 --> 17:55.880
death and destruction of infrastructure.

17:55.920 --> 17:56.280
Yes.

17:56.280 --> 18:02.120
I mean, the thing that the reason I, you know, proposed a, some sort of

18:02.440 --> 18:08.720
truth or peace a year ago was because I've predicted pretty much exactly

18:08.720 --> 18:12.840
what would happen, uh, which is a lot of people dying for basically

18:13.000 --> 18:14.480
almost no changes in land.

18:15.720 --> 18:21.200
Um, and this, the, the, the loss of the, the flower of Ukrainian and Russian

18:21.200 --> 18:25.200
youth, and we should have some sympathy for the Russian boys as well as the

18:25.200 --> 18:26.880
Ukrainian boys, because they're wrong.

18:26.880 --> 18:28.680
Boys didn't ask to be on their frontline.

18:28.720 --> 18:29.320
They have to be.

18:30.320 --> 18:37.760
So, um, there's a lot of sons not, not come back to their parents, you know,

18:37.920 --> 18:44.080
and, and I think most of them don't, don't really have, they don't hate the other

18:44.080 --> 18:49.760
side, you know, it's sort of like, is this saying about like the same comes

18:49.760 --> 18:54.000
from world war one, it's like young boys who don't know each other, killing

18:54.000 --> 18:57.040
each other on behalf of old men that do know each other.

18:59.760 --> 19:00.520
The hell's the point of that?

19:02.320 --> 19:06.640
So Volodymyr Zelensky said that he's not, or has said in the past, he's not

19:06.640 --> 19:08.280
interested in talking to Putin directly.

19:08.520 --> 19:12.840
Do you think he should sit down man to man, lead a leader and negotiate peace?

19:14.600 --> 19:20.160
I think I would just recommend, do not send the flower of Ukrainian youth

19:20.880 --> 19:23.680
to be, to die, uh, in trenches.

19:24.840 --> 19:28.000
Uh, whether you talk to Putin or not, just don't do that.

19:28.880 --> 19:33.560
Um, whoever it goes on the offensive, we'll lose massive numbers of people.

19:34.760 --> 19:39.120
Um, and history will not look kindly upon them.

19:42.360 --> 19:44.760
You've spoken honestly about the possibility of war between

19:44.760 --> 19:46.600
us and China in the longterm.

19:46.800 --> 19:50.600
If no diplomatic solution is found, for example, on the question of

19:50.600 --> 19:52.160
Taiwan and one China policy.

19:52.800 --> 19:56.640
How do we avoid the trajectory where these two superpowers clash?

19:58.320 --> 20:01.680
Well, it's, it's worth reading that book on the, the, uh, difficult to

20:01.680 --> 20:05.600
pronounce the acidities trap, I believe it's called, but I love war history.

20:05.600 --> 20:07.960
I like inside out and backwards.

20:09.080 --> 20:14.120
Um, there's hardly a battle I haven't read, read about and, and trying to

20:14.120 --> 20:16.920
figure out like what, what really was the cause of victory in any particular

20:16.920 --> 20:20.560
case, as opposed to what one side or another claims the reason.

20:21.320 --> 20:25.880
Both the victory and what sparked the war and the whole thing.

20:26.880 --> 20:27.160
Yeah.

20:27.440 --> 20:30.040
So that Athens and Sparta is classic case.

20:30.080 --> 20:31.960
The thing about the Greeks is I really wrote down a lot of stuff.

20:32.000 --> 20:32.840
They loved writing.

20:32.960 --> 20:35.360
Um, you know, there are lots of interesting things that happened in

20:35.360 --> 20:37.320
many parts of the world, but they just, people just didn't write down.

20:38.200 --> 20:44.800
So we don't know what happened or they didn't really write with in detail.

20:44.840 --> 20:48.440
They just would say like, we went, we had a battle and we won and like,

20:48.440 --> 20:49.880
well, can you add a bit more?

20:49.920 --> 20:53.000
Um, the Greeks, they really wrote a lot.

20:54.560 --> 20:56.120
They're very articulate on it.

20:56.320 --> 20:57.080
They just love writing.

20:57.120 --> 20:59.400
So, and we have a bunch of that writing that's preserved.

20:59.400 --> 21:04.080
So we know what led up to the Peloponnesian war between, um, the

21:04.080 --> 21:05.400
Spartan and Athenian Alliance.

21:06.960 --> 21:14.280
Um, and, uh, we, we know that they, they, for quite, they, they saw it coming.

21:15.840 --> 21:19.320
I mean, the Spartans didn't write, they also weren't very verbose by their nature,

21:19.400 --> 21:22.440
but they did write, but they weren't very, they weren't terse.

21:22.880 --> 21:26.720
Uh, but the, the Athenians and the other Greeks wrote, wrote a line and they were

21:26.720 --> 21:32.600
like, um, and Sparta was really kind of like the leader of, of Greece.

21:32.680 --> 21:36.680
Um, but, but Athens grew stronger and stronger with each passing year.

21:37.720 --> 21:41.280
And, um, and everyone's like, well, that's inevitable that there's going

21:41.280 --> 21:43.720
to be a clash between Athens and Sparta.

21:44.560 --> 21:46.440
Uh, well, how do we avoid that?

21:46.760 --> 21:49.400
And they couldn't, they couldn't, they actually, they saw it coming

21:49.400 --> 21:50.560
and they still could not avoid it.

21:53.040 --> 21:59.560
So, you know, at some point, if there's, if, if one, uh, group, one

21:59.560 --> 22:07.320
civilization or country or whatever, um, exceeds another sort of like, if, you

22:07.320 --> 22:10.640
know, the United States has been the biggest kid on the block since I think

22:11.080 --> 22:13.640
1890 from an economic standpoint.

22:14.440 --> 22:20.480
So the United States has been the economic, most powerful economic engine

22:20.560 --> 22:23.560
in the world longer than anyone's been alive.

22:25.160 --> 22:29.920
Um, and the foundation of war is economics.

22:31.400 --> 22:40.400
So now we have a situation in case of China where the, um, the economy is

22:40.400 --> 22:45.240
likely to be two, perhaps three times larger than that of the U S.

22:45.840 --> 22:49.080
So imagine you're the biggest kid on the block for as long as anyone can remember.

22:49.520 --> 22:51.640
And suddenly a kid comes along who's twice your size.

22:55.640 --> 22:57.000
So we see it coming.

22:57.360 --> 22:57.560
Yeah.

22:57.560 --> 22:59.840
How is it possible to stop?

22:59.840 --> 23:03.920
Is there some, let me throw something out there, just intermixing

23:03.920 --> 23:05.480
of cultures, understanding.

23:06.040 --> 23:09.240
There does seem to be a giant cultural gap and understanding of each other.

23:09.720 --> 23:14.000
And you're an interesting case study because you are an American, obviously

23:14.000 --> 23:18.480
you've done a lot of, uh, incredible manufacture here in the United States,

23:18.480 --> 23:19.720
but you also work with China.

23:20.080 --> 23:22.600
I've spent a lot of time in China and met with the leadership many times.

23:22.880 --> 23:27.080
Maybe a good question to ask is what are some things about China that people

23:27.080 --> 23:30.440
don't understand positive, just in the culture, what's some interesting

23:30.440 --> 23:32.920
things that you've learned about the Chinese?

23:33.120 --> 23:37.360
Well, uh, the, the sheer number of really smart, hardworking people

23:37.360 --> 23:40.720
in China is, um, incredible.

23:41.040 --> 23:45.800
Uh, there are really say like how many smart, hardworking people are there

23:45.800 --> 23:48.040
in China, there's far more of them there than they are here.

23:48.480 --> 23:55.480
I think in my, in my opinion, um, the, uh, and they've got a lot of energy.

23:56.280 --> 24:02.080
So, I mean, the, the architecture in China that's in recent years is far

24:02.080 --> 24:06.880
more impressive than the U S I mean, the train stations, the buildings,

24:06.920 --> 24:13.080
the high speed rail, everything it's, um, really far more impressive

24:13.080 --> 24:17.440
than what we have in the U S I mean, I recommend somebody just go to Shanghai

24:17.440 --> 24:22.080
and Beijing, look at the buildings and go to, you know, the, the, the

24:23.080 --> 24:27.240
look at the buildings and go to, you know, take the train from Beijing to

24:27.240 --> 24:30.320
Xi'an, where you have the terracotta warriors, um, China's got an

24:30.320 --> 24:32.720
incredible history, uh, very long history.

24:33.520 --> 24:40.000
And, um, you know, I think arguably the, in terms of the use of language

24:40.000 --> 24:44.240
from, from a written standpoint, um, sort of one of, one of the oldest,

24:44.240 --> 24:49.840
perhaps perhaps the oldest written language and then China people did

24:49.880 --> 24:50.480
write things down.

24:50.600 --> 24:56.960
So, um, now China, um, historically has always been, with rare

24:56.960 --> 24:58.800
exception, been internally focused.

24:59.400 --> 25:00.840
Um, they've not been acquisitive.

25:02.400 --> 25:04.920
Uh, they've, they're fought each other.

25:05.000 --> 25:09.560
They've had been many, many civil wars, um, in the three kingdoms war, I believe

25:09.560 --> 25:11.960
they lost about 70% of their population.

25:13.960 --> 25:19.680
So, and then that does, so the, they've had brutal internal wars, like

25:19.680 --> 25:25.160
civil wars that make the US civil war look small by comparison.

25:26.160 --> 25:32.800
Um, so I think it's important to appreciate that China is not a monolithic.

25:34.160 --> 25:39.520
Um, we sort of think of like China as this sort of one entity of one mind.

25:39.520 --> 25:40.920
And this is definitely not the case.

25:41.440 --> 25:47.320
Um, from what I've seen, and I think most people who understand China would

25:47.320 --> 25:51.440
agree that people in China think about China 10 times more than they think

25:51.440 --> 25:52.640
about anything outside of China.

25:53.720 --> 26:00.600
So it's like 90% of their consideration is, uh, you know, are, is, is, is internal.

26:01.400 --> 26:04.560
Well, isn't that a really positive thing when you're talking about the

26:04.560 --> 26:08.560
collaboration and the future piece between superpowers, when you're inward

26:08.560 --> 26:13.160
facing, which is like focusing on improving yourself versus focusing on,

26:13.600 --> 26:17.440
yeah, uh, quote unquote, improving others through military might.

26:18.200 --> 26:21.640
The good news, the history of China suggests that China is not

26:21.640 --> 26:24.520
acquisitive, meaning they're not going to go out and invade a whole bunch of countries.

26:25.200 --> 26:28.840
Um, they do feel very strongly, you know, so that's, that's good.

26:28.840 --> 26:32.440
I mean, cause a lot of, a lot of very powerful countries have been acquisitive.

26:33.040 --> 26:37.720
Um, the US is one of the, also one of the rare cases that has not been

26:37.720 --> 26:41.400
acquisitive, like after world war two, the US could have basically taken over

26:41.400 --> 26:44.480
the world and any country like we got nukes, nobody else got nukes.

26:45.080 --> 26:46.200
You know, I'm going to have to lose soldiers.

26:47.000 --> 26:48.680
Uh, which country do you want?

26:49.680 --> 26:53.200
And the United States could have taken over everything at will.

26:53.280 --> 26:57.280
And it didn't, um, and the United States actually helped rebuild countries.

26:57.680 --> 27:00.760
So it helped rebuild Europe, you know, helped rebuild Japan.

27:01.640 --> 27:05.560
Um, this is very unusual behavior, almost unprecedented.

27:06.520 --> 27:15.760
Um, you know, the US did conspicuous acts of kindness, like the Berlin airlift, you know?

27:17.080 --> 27:23.560
Um, and, and I think, you know, it's always like, well, America's done bad things.

27:23.600 --> 27:25.240
Well, of course America's done bad things, but one needs to

27:25.240 --> 27:26.840
look at the whole track record.

27:27.520 --> 27:34.760
Um, and, and just generally, you know, one, one sort of test would be how do

27:34.760 --> 27:43.160
you treat your prisoners of war or let's say, um, you know, no offense

27:43.160 --> 27:47.400
to the Russians, but let's say you're in Germany, it's 1945, you got the

27:47.400 --> 27:51.480
Russian army coming one side and you got the French, British and American

27:51.480 --> 27:52.360
armies coming the other side.

27:53.000 --> 27:55.480
Who would you like to be to surrender to?

27:56.520 --> 28:02.680
Like no country is like morally perfect, but I recommend, uh, being a POW with

28:02.680 --> 28:05.400
the Americans, that would be my choice very strongly.

28:07.320 --> 28:08.760
In the full menu of POW.

28:08.760 --> 28:09.960
Very much so.

28:11.080 --> 28:17.320
And in fact, Von Braun, um, took, you know, a small guy, uh, was like, we've

28:17.320 --> 28:18.920
got to be captured by the Americans.

28:20.600 --> 28:28.200
And, uh, in fact, the SS was under orders to execute von Braun and all of the, uh,

28:28.280 --> 28:34.440
German recognition is, uh, and they narrowly escaped their SSR.

28:34.440 --> 28:36.520
They said they were going out for a walk in the woods.

28:36.520 --> 28:40.840
They left in the middle of winter with no coats, uh, and they ran like ever

28:40.840 --> 28:44.920
no food, no coats, no water, and just ran like hell, uh, and ran west.

28:45.720 --> 28:49.880
Um, and by sheer, like they, I think his brother found like a bicycle or

28:49.880 --> 28:54.760
something and, um, and then just cycled west as fast as he could and found,

28:55.320 --> 28:56.440
found a U S patrol.

28:57.400 --> 29:00.360
Um, so anyway, that's it.

29:00.360 --> 29:01.640
That's one, that's one way you can tell.

29:02.920 --> 29:05.480
Morality is who, where do you want to be a PW?

29:06.840 --> 29:09.480
It's not fun anywhere, but some places are much worse than others.

29:10.120 --> 29:16.920
So, um, anyway, so, so, so like America has been, uh, well, far from perfect,

29:16.920 --> 29:19.560
uh, generally a benevolent force.

29:19.560 --> 29:25.480
Um, and, uh, we should always be self-critical and, uh, we try to be

29:25.560 --> 29:26.120
better.

29:26.120 --> 29:30.360
Um, but, um, anyone with half a brain knows that.

29:31.000 --> 29:37.000
So, so I think there are, in this way, China and, uh, the United States are

29:37.000 --> 29:37.720
similar.

29:37.720 --> 29:42.600
Neither country has been acquisitive, um, in a significant way.

29:42.600 --> 29:45.800
So that's like, uh, you know, a shared principle, I guess.

29:46.600 --> 29:50.520
Um, now, now China does feel very strongly about Taiwan.

29:51.640 --> 29:54.280
They've been very clear about that for a long time.

29:55.240 --> 29:58.520
Um, you know, from their standpoint, it's, it's, it would be like one of the

29:58.520 --> 30:04.680
states is, is, is, you know, not there, like, like Hawaii or something like that,

30:04.680 --> 30:10.600
but, but more significant than Hawaii, you know, um, and Hawaii is pretty

30:10.600 --> 30:11.560
significant for us.

30:11.560 --> 30:18.280
So, um, they, they view it as, as, as really the, that there's a fundamental

30:18.280 --> 30:25.640
part of China, uh, the island of Formosa, now Taiwan, that is, um, not

30:25.640 --> 30:29.640
part of China, but should be, uh, and the only reason it hasn't been is because

30:29.640 --> 30:30.600
the US Pacific fleet.

30:32.280 --> 30:35.320
And is their economic power grows and is their military power grows?

30:36.440 --> 30:43.160
The thing that they are clearly saying, uh, is their interests will, you know,

30:44.280 --> 30:45.480
clearly be materialized.

30:46.440 --> 30:53.480
Yes, China has been very clear that, um, they will incorporate Taiwan, uh,

30:54.200 --> 31:00.520
peacefully or, uh, militarily, but that they will incorporate it from their

31:00.520 --> 31:02.680
standpoint is 100% likely.

31:03.960 --> 31:07.000
You know, something you said about conspicuous acts of kindness as a

31:07.000 --> 31:14.600
geopolitical policy, it almost seems naive, but I'd venture to say that this

31:14.600 --> 31:19.160
is probably the path forward, how you avoid most wars.

31:20.120 --> 31:23.800
Just as you say, it sounds naive, but it's kind of brilliant.

31:25.240 --> 31:31.160
If you believe in the goodness of underlying most of human nature, it just

31:31.160 --> 31:38.440
seems like conspicuous acts of kindness can, uh, reverberate through the populace

31:38.440 --> 31:39.720
of the countries involved.

31:41.080 --> 31:41.320
Yeah.

31:41.960 --> 31:43.320
Well, and deescalate.

31:43.960 --> 31:44.360
Absolutely.

31:44.360 --> 31:50.280
So after world war one, the, the, they made a big mistake.

31:50.280 --> 31:56.600
You know, they basically try to lump all the blame on Germany, um, and, um,

31:58.360 --> 32:03.000
and, and, you know, settle journey with, uh, impossible reparations.

32:03.800 --> 32:08.280
Um, and you know, really there was a lot of blood, there was a fair, quite a

32:08.280 --> 32:12.120
bit of blame to, um, go around for world war one.

32:12.120 --> 32:15.560
Um, but they, they try to, you know, put it all on Germany.

32:16.200 --> 32:23.080
Um, and, uh, that was that, that laid the seeds for world war two.

32:24.840 --> 32:28.760
Uh, so that's a lot of people were not just Hitler.

32:28.760 --> 32:30.280
A lot of people felt wronged.

32:30.280 --> 32:37.400
Um, and they wanted vengeance and they got it.

32:38.280 --> 32:39.080
People don't forget.

32:39.640 --> 32:39.880
Yeah.

32:41.240 --> 32:45.880
You, you, you kill somebody's father, mother, son, daughter.

32:45.880 --> 32:46.760
They're not going to forget it.

32:47.720 --> 32:48.680
They will want vengeance.

32:49.400 --> 32:53.240
Um, so after world war two, they're like, well, the treaty of

32:53.240 --> 32:56.600
Versailles was a huge mistake, um, in world war one.

32:58.120 --> 33:06.520
And, um, so this time, instead of, uh, you know, crushing the losers,

33:06.520 --> 33:07.960
we're, we're actually going to help them.

33:09.080 --> 33:13.880
With the Marshall plan and we're going to help revolt, revolt, uh, Germany.

33:13.880 --> 33:17.880
Um, we're going to help revolt, uh, or, you know, Austria and the other,

33:18.520 --> 33:20.200
you know, Italy and whatnot.

33:20.200 --> 33:25.560
So, um, and that was the right move.

33:26.520 --> 33:31.560
There's a, it does feel like there's a profound truth to, uh,

33:32.920 --> 33:35.480
conspicuous acts of kindness being an antidote to this.

33:35.720 --> 33:38.600
Something must stop the, the cycle of reciprocal violence.

33:40.600 --> 33:45.720
Something must stop it, or it will, you know, it'll, it'll, it'll never stop.

33:45.800 --> 33:53.560
Just eye for an eye, tooth for a tooth, limb for a limb, life for a life forever and ever.

33:55.800 --> 33:59.240
To escape briefly the darkness was some incredible engineering work.

34:00.200 --> 34:04.920
Uh, XAI just released Grok, AI assistant that I've gotten the chance to work with.

34:04.920 --> 34:07.000
It's, uh, it's amazing on many levels.

34:07.160 --> 34:11.720
First of all, it's amazing that a relatively small team in a relatively short amount of time

34:12.200 --> 34:16.360
was able to develop this close to state of the art system.

34:17.160 --> 34:21.400
Uh, another, uh, incredible thing is there's a regular mode and there's a fun mode.

34:21.400 --> 34:23.000
Yeah, I guess I'm to blame for that one.

34:24.760 --> 34:27.640
I wish it, first of all, I wish everything in life had a fun mode.

34:27.640 --> 34:28.200
Yeah.

34:28.200 --> 34:33.560
I mean, there's something compelling beyond just fun about the fun mode.

34:34.040 --> 34:34.200
Yeah.

34:34.200 --> 34:36.440
Interacting with a large language model.

34:36.440 --> 34:40.040
I'm not sure exactly what it is because I've only had a little bit of time to play with it,

34:40.600 --> 34:46.120
but it just makes it more interesting, more vibrant to interact with the system.

34:46.200 --> 34:48.440
Yeah, uh, absolutely.

34:48.440 --> 34:57.000
I, um, our, our AI Grok is modeled after the Hitchhiker's Guide to the Galaxy, uh,

34:57.000 --> 35:01.240
which is one of my favorite books, uh, which is, it's a book on philosophy.

35:01.240 --> 35:02.520
Disguised as a book on humor.

35:02.840 --> 35:10.840
Um, and, um, I would say that is, that forms the basis of my philosophy, uh, which is that

35:11.000 --> 35:15.880
we don't know the meaning of life, but the more we can expand the scope and scale of

35:15.880 --> 35:24.120
consciousness, digital and biological, the more we're able to understand what questions

35:24.120 --> 35:26.040
to ask about the answer that is the universe.

35:26.520 --> 35:35.160
There is generally a feeling like this AI system has an outward looking, like the way

35:35.160 --> 35:40.040
you are, like sitting with a good friend, looking up at the stars, like the asking

35:40.040 --> 35:43.480
podhead, like questions about the universe, wondering what it's all about.

35:43.480 --> 35:47.160
The curiosity you talk about there, there's a sense, no matter how mundane the question

35:47.160 --> 35:53.080
I ask it, there's, there's, there's a sense of, there's a sense of, there's a sense of

35:53.160 --> 35:58.360
no matter how mundane the question I ask it, there's a sense of cosmic grandeur to the

35:58.360 --> 35:58.840
whole thing.

35:58.840 --> 36:06.040
Well, we are actually working hard to have, uh, engineering, math, physics answers that

36:06.040 --> 36:06.760
you can count on.

36:08.360 --> 36:16.600
Um, so for the other sort of AIs out there that was these so-called large language models,

36:16.680 --> 36:21.720
um, I've not found the, uh, engineering to be reliable.

36:21.720 --> 36:27.800
Um, and the hallucination, it unfortunately hallucinates most when you least wanted to

36:27.800 --> 36:28.280
hallucinate.

36:29.480 --> 36:34.440
So when you ask important difficult questions, that's what tends to be confidently wrong.

36:35.320 --> 36:41.960
Um, so we're, we're really trying hard to say, okay, how do we be as grounded as possible

36:41.960 --> 36:47.800
so you can count on the results, um, trace things back to physics, first principles,

36:48.760 --> 36:49.720
mathematical logic.

36:50.440 --> 36:58.920
Um, so underlying the humor is an aspiration to adhere to the truth of the universe as

36:58.920 --> 36:59.800
closely as possible.

37:01.320 --> 37:02.680
That's really tricky.

37:02.680 --> 37:03.320
It is tricky.

37:03.960 --> 37:10.120
So that's why, you know, there's always going to be some amount of error, but we want to

37:11.080 --> 37:19.880
aspire to be as truthful as possible about the answers, uh, with acknowledged error.

37:19.880 --> 37:23.320
Um, so that there was always, you don't want to be confidently wrong.

37:23.320 --> 37:29.080
So you're not going to be right every time, but you want to minimize how often you're

37:29.080 --> 37:30.520
confidently, uh, wrong.

37:31.320 --> 37:39.000
And then like I said, once you can count on the logic as being, um, not violating physics,

37:39.960 --> 37:44.520
then you can start to build on that, to create, uh, inventions, like invent new

37:44.520 --> 37:45.240
technologies.

37:46.120 --> 37:51.960
But if you can't, if you're, if you can't count on the foundational physics being

37:51.960 --> 37:56.920
correct, obviously the inventions are simply wishful thinking, you know,

37:56.920 --> 38:00.120
imagination land, magic, basically.

38:01.320 --> 38:05.720
Well, as you said, I think one of the big goals of XAI is to understand the universe.

38:06.520 --> 38:10.440
Yes, that's a simple three word, uh, mission.

38:12.680 --> 38:17.960
Um, if you look out far into the future, do you think on this level of physics, the very

38:17.960 --> 38:22.920
edge of what we understand about physics, do you think it will make discoveries sort

38:22.920 --> 38:28.200
of the sexiest discovery of them as, as we know now sort of, uh, unifying general

38:28.200 --> 38:30.360
relativity and quantum mechanics.

38:30.360 --> 38:34.840
So coming up with a theory of everything, do you think it could push towards that

38:34.840 --> 38:37.320
direction, almost like theoretical physics discoveries?

38:37.960 --> 38:44.040
If an AI cannot figure out new physics, um, it's clearly not equal to humans, let

38:44.040 --> 38:47.640
alone nor has surpass humans because humans have figured out new physics.

38:47.640 --> 38:48.200
They're just good.

38:48.200 --> 38:52.680
You know, physics is just understanding, you know, deepening one's insight into

38:52.680 --> 38:53.640
how reality works.

38:54.440 --> 38:59.480
And then, um, then, then there's engineering, which is inventing things

38:59.480 --> 39:00.360
that have never existed.

39:01.480 --> 39:04.040
Now the range of possibilities for engineering is far greater than for

39:04.040 --> 39:07.960
physics because, you know, we, once you figure out the rules of the universe, uh,

39:08.520 --> 39:09.720
that that's, that's it.

39:09.720 --> 39:11.400
You've discovered things that already existed.

39:11.960 --> 39:18.040
But from that, you can then build technologies with that are really almost

39:18.040 --> 39:24.280
limitless in the, uh, variety and, you know, it's like once you understand the

39:24.280 --> 39:28.760
rules of the game properly and we do, you know, with current physics, we do at

39:28.760 --> 39:32.280
least at a local level, understand how physics works very well.

39:32.760 --> 39:35.160
Related to predict things is incredibly good.

39:35.160 --> 39:38.760
Like quantum mechanics is the degree to which quantum mechanics can predict

39:38.760 --> 39:40.520
outcomes is incredible.

39:41.240 --> 39:46.440
Um, that was my, that's my hardest, hardest class in college, by the way, my

39:46.440 --> 39:49.560
senior quantum mechanics class was harder than all of my other classes put together.

39:50.520 --> 39:58.120
To get an AI system, a large language model to, to, um, reliably be as reliable as

39:58.120 --> 40:00.360
quantum mechanics and physics is very difficult.

40:01.000 --> 40:01.240
Yeah.

40:01.240 --> 40:04.920
You have to test any conclusions against the ground truth of reality.

40:04.920 --> 40:06.360
Reality is the ultimate judge.

40:06.360 --> 40:07.320
Like physics is the law.

40:07.320 --> 40:08.600
Everything else is a recommendation.

40:09.640 --> 40:11.800
I've seen plenty of people break the break.

40:11.800 --> 40:15.560
The law is made by man, but none break the laws made by physics.

40:15.560 --> 40:16.360
It's a good test.

40:16.360 --> 40:22.520
Actually, if this LM, uh, understands and matches physics, then you can more

40:22.520 --> 40:26.280
reliably trust whatever it thinks about the current state of politics.

40:27.240 --> 40:32.280
And it's also not, not the case currently that, uh, even that it's internal logic

40:32.280 --> 40:33.480
is not consistent.

40:33.480 --> 40:40.200
Um, so it's especially, um, with these, with the approach of like just

40:40.200 --> 40:45.080
predicting a token, predict token, predict token, it's like a vector sum.

40:45.080 --> 40:48.600
You know, you're, you're summing up a bunch of vectors, but you can get drift.

40:48.600 --> 40:52.840
Um, so as those, a little bit of error, a little bit of error adds up.

40:52.840 --> 40:56.040
And by the time you are many tokens down the path, uh,

40:56.040 --> 40:58.440
yours, it doesn't make any sense.

40:59.720 --> 41:01.960
So it has to be somehow self-aware about the drift.

41:02.520 --> 41:07.720
It has to be self-aware about the drift and then look at the thing as a gestalt as a whole

41:08.360 --> 41:12.520
and, and say, it doesn't have coherence as a whole.

41:13.560 --> 41:19.880
So, you know, when, when authors write books that they, they will write the book

41:19.880 --> 41:21.400
and then they'll go and revise it.

41:22.120 --> 41:26.600
You know, taking into account, you know, all the, the end and the beginning and the middle

41:26.600 --> 41:32.920
and, and, uh, rewrite it to achieve coherence so that it doesn't end up in a nonsensical place.

41:33.640 --> 41:38.040
Maybe the process of revising is what reasoning is.

41:38.040 --> 41:41.080
And then that's the process of rising is how you get closer and closer to the truth.

41:41.880 --> 41:44.920
Maybe you like, uh, at least I approached it that way.

41:44.920 --> 41:47.400
You just say a bunch of bullshit first and then you get it better.

41:47.960 --> 41:49.240
You start a bullshit and then you get it.

41:49.240 --> 41:52.120
You create a draft and then, and then you, and then you iterate on that draft.

41:52.760 --> 41:57.880
Um, until it has as coherence until it's, it all adds up basically.

41:58.760 --> 42:02.040
So another question about theory of everything, but for intelligence,

42:02.600 --> 42:07.240
do you think there exists, as you're exploring this with XAI creating this intelligence system,

42:07.240 --> 42:13.800
do you think there is a theory of intelligence where you get to understand what like,

42:13.960 --> 42:22.200
uh, what is the eye in AGI and what is the eye in, um, human intelligence?

42:22.200 --> 42:23.560
There's no eye in Team America.

42:26.440 --> 42:28.520
Uh, and that's going to be stuck in my head now.

42:31.080 --> 42:37.160
Uh, yeah, there's no me and whatever, uh, in quantum mechanics.

42:37.160 --> 42:37.720
Oh, wait.

42:38.120 --> 42:45.240
Uh, I mean, is that part of the process of discovering understanding the universe is

42:45.240 --> 42:46.440
understanding intelligence?

42:47.800 --> 42:48.040
Yeah.

42:50.200 --> 42:50.360
Yeah.

42:50.360 --> 42:53.080
I think we need to understand intelligence, understand consciousness.

42:53.880 --> 42:57.160
I mean, there, I mean, there are some sort of fundamental questions of like,

42:58.040 --> 43:01.080
what is thought, what is emotion?

43:01.080 --> 43:01.720
Yeah.

43:01.720 --> 43:05.880
Um, is it really just one atom bumping into another atom?

43:05.880 --> 43:07.080
It feels like something more than that.

43:08.440 --> 43:13.000
Uh, so I, I, I think we're probably missing some really big things.

43:13.960 --> 43:17.400
Um, like some really big things.

43:18.520 --> 43:20.680
Something that'll be obvious in retrospect.

43:20.680 --> 43:21.000
Yes.

43:21.000 --> 43:25.080
Like there's a giant, like you put the whole consciousness, emotion.

43:26.600 --> 43:29.880
Well, some people would call it like a, like a soul, you know, religion is a soul.

43:31.240 --> 43:33.800
Like you feel like you're you, right?

43:33.880 --> 43:39.240
I mean, you don't feel like you're just a collection of atoms, but on what dimension

43:40.440 --> 43:41.640
does thought exist?

43:41.640 --> 43:43.640
What dimension does, do emotions exist?

43:43.640 --> 43:44.760
We feel them very strongly.

43:45.880 --> 43:49.080
Um, I suspect there's more to it than atoms bumping into atoms.

43:52.200 --> 43:57.160
And maybe AI can pave the path to the discovery of what, whatever the hell that thing is.

43:58.040 --> 43:58.440
Yeah.

43:58.440 --> 43:59.480
What is consciousness?

43:59.480 --> 44:02.040
Like what, when you put the atoms in a particular shape,

44:02.680 --> 44:08.920
why are they able to form thoughts and take actions and feelings?

44:09.960 --> 44:13.240
And even if it is an illusion, why is this illusion so compelling?

44:13.240 --> 44:13.480
Yeah.

44:14.920 --> 44:16.040
Why does this illusion exist?

44:18.120 --> 44:21.720
On what plane does this, the solution exists?

44:21.720 --> 44:22.520
Yeah.

44:22.520 --> 44:27.960
Um, and that sometimes I wonder is, you know, either perhaps everything's conscious

44:27.960 --> 44:29.000
or nothing is conscious.

44:29.960 --> 44:31.720
Um, what are the two?

44:32.840 --> 44:33.720
I like the former.

44:34.680 --> 44:36.840
Everything conscious just seems more fun.

44:37.640 --> 44:39.000
It just seemed more, something more fun.

44:39.000 --> 44:39.880
Yes.

44:39.880 --> 44:45.560
Um, but we're, we're composed of atoms and those atoms are composed of quarks and leptons

44:45.560 --> 44:48.920
and those quarks and leptons have been around since the beginning of the universe.

44:49.880 --> 44:52.280
The beginning of the universe.

44:53.000 --> 44:53.320
Right.

44:53.320 --> 44:55.480
What, what seems to be the beginning of the universe?

44:55.480 --> 44:59.480
The first time we talked, you said what you would, which is surreal that to think

44:59.480 --> 45:02.040
that this discussion was happening is becoming a reality.

45:02.600 --> 45:06.840
I asked you what question would you ask an AGI system once you create it?

45:06.840 --> 45:09.800
And you said, what's outside the simulation is the question.

45:12.200 --> 45:13.160
Good question.

45:13.160 --> 45:13.400
Yeah.

45:13.400 --> 45:18.200
But it seems like with Grog, you started to literally, uh, the system's

45:19.000 --> 45:23.880
goals to be able to ask such questions, to answer such questions and to ask such questions.

45:23.880 --> 45:25.240
Where are the aliens?

45:25.240 --> 45:25.960
Where are the aliens?

45:25.960 --> 45:28.360
That's one of the, like the foamy paradox question.

45:29.240 --> 45:34.760
Um, a lot of people have asked me if I've seen any evidence of aliens and I haven't,

45:34.760 --> 45:40.920
which is kind of concerning because then I think we'd, I'd probably prefer to at least

45:40.920 --> 45:42.920
have seen some archeological evidence of aliens.

45:43.560 --> 45:48.760
Um, but to the best of my knowledge, there is no, I'm not aware of any evidence of aliens.

45:50.040 --> 45:51.320
If they're out there, they're very subtle.

45:51.320 --> 45:54.440
We might just be the only consciousness, at least in the galaxy.

45:54.440 --> 46:00.920
Um, and if you, if you look at, say, the history of earth, for us to believe the

46:00.920 --> 46:05.960
archeological record, that there's about four and a half billion years old, civilization,

46:05.960 --> 46:09.880
as measured from the first writing is only about 5,000 years old.

46:10.760 --> 46:13.960
We have to give some credit there to the ancient Sumerians who aren't around anymore.

46:14.680 --> 46:19.960
I think it was a archaic pre-cuneiform was the first actual symbolic representation.

46:19.960 --> 46:27.080
But only about 5,000 years ago, I think that's a good date for when would say civilization started.

46:28.600 --> 46:30.840
That's one millionth of earth's existence.

46:33.480 --> 46:34.920
So civilization has been around.

46:35.640 --> 46:39.400
It's really a flash in the pan so far.

46:39.400 --> 46:48.520
Um, and why, why have we, why did it take so long?

46:48.680 --> 46:53.960
But why did it take so long for, you know, one and a half billion years?

46:55.160 --> 47:00.600
Um, for the vast majority of time, there was no life.

47:00.600 --> 47:03.560
And then there was archaic bacteria for a very long time.

47:04.840 --> 47:14.280
And then, you know, you had mitochondria get captured, multicellular life, um,

47:15.240 --> 47:21.000
differentiation into plants and animals, life moving from the oceans to land, mammals,

47:21.960 --> 47:23.560
um, higher brain functions.

47:25.080 --> 47:33.480
And the sun is expanding slowly, um, but it, it will, it will overheat.

47:33.480 --> 47:39.480
It will, it will heat, heat the earth up at some point in the future, um, while the oceans

47:39.880 --> 47:44.920
and earth will become like Venus where no life, life as we know it is impossible.

47:45.960 --> 47:53.160
So if we do not become multi-planetary and ultimately go beyond our solar system, um,

47:53.160 --> 47:57.880
annihilation of all life on earth is a certainty, a certainty.

47:59.400 --> 48:08.040
Um, and it could be as little as on the galactic time scale, uh, half a billion years.

48:09.480 --> 48:14.760
You know, a long time by human standards, but that's only 10% longer than earth has been around

48:14.760 --> 48:23.160
at all. So if, if, if life had taken 10% longer to evolve on earth, it wouldn't exist at all.

48:27.160 --> 48:30.120
We've got a deadline coming up, but a hurry.

48:30.120 --> 48:34.200
But that said, as you said, humans, intelligent life on earth

48:35.640 --> 48:37.560
developed a lot of cool stuff very quickly.

48:38.280 --> 48:45.640
So it seems like becoming a multi-planetary is almost inevitable unless we destroy.

48:45.640 --> 48:51.480
We need to do it. Uh, I mean, it's, it's not, I mean, I suspect that there, there,

48:52.840 --> 48:57.160
if we are able to go out there and explore other star systems that we, there's a good chance we

48:57.160 --> 49:02.680
find a whole bunch of long dead one planet civilizations that never made it past their home

49:02.680 --> 49:05.000
planet. That's so sad. Yeah.

49:06.200 --> 49:07.160
Also fascinating.

49:07.880 --> 49:12.520
I mean, there was various explanations for those paradox. And one is just those sort of,

49:12.520 --> 49:17.640
there's these great filters, which civilizations don't pass through. And one of those great

49:17.640 --> 49:22.520
filters is do you become a multi-planet civilization or not? And if you don't,

49:22.520 --> 49:28.920
it's simply a matter of time before something happens on your planet. Um, you know, either

49:29.880 --> 49:36.600
natural manmade that causes us to die out, like the dinosaurs, where are they now?

49:38.280 --> 49:46.440
They didn't have spaceships. So I think the more likely thing is, cause just, uh, uh,

49:46.440 --> 49:51.240
empathize with the aliens that they, they found us and they're protecting us and letting us be.

49:51.240 --> 49:52.760
I hope so. Nice aliens.

49:53.320 --> 49:57.400
Just like the tribes in the, in the Amazon, they don't contact the tribes or protecting them.

49:58.360 --> 50:04.040
That's what would be a nice explanation. Or you could have like, uh, what was it? Uh,

50:04.040 --> 50:08.360
I think Andre Capati said it's like the ants and the Amazon asking, where's everybody?

50:10.200 --> 50:13.400
Well, they do run into a lot of other ends. That's true.

50:13.400 --> 50:14.360
There's these ant wars.

50:16.440 --> 50:17.800
Sounds like a good TV show.

50:17.800 --> 50:20.120
Yeah. They literally have this big wars between various ants.

50:21.560 --> 50:27.240
Yeah. Maybe I'm just, uh, uh, dismissing all the different diversity of ants.

50:27.880 --> 50:31.800
Listen to that Wernher Herzog talking about the jungle. It's really hilarious. Have you heard it?

50:31.800 --> 50:33.800
No, I have not, but Wernher Herzog has a way.

50:36.600 --> 50:40.840
You should play, you should play it for the, you know, as an interlude in the,

50:41.960 --> 50:43.480
it's on YouTube. It's awesome.

50:45.000 --> 50:46.120
I love them so much.

50:46.120 --> 50:47.640
Yeah, it's great.

50:47.640 --> 50:51.000
We'll see the director of happy people life in the taiga. I think also

50:51.000 --> 50:53.960
did that bear documentary, the bear documentary thing about penguins.

50:53.960 --> 50:59.160
Yeah. The analysis, the psycho analysis.

51:00.280 --> 51:05.800
Yeah. The penguins like headed for like the mountains like that are like 70 miles away.

51:05.800 --> 51:08.040
Penguin is just headed for doom basically.

51:08.600 --> 51:13.080
Well, he was headed cynical take. I have a, he could be just the brave explorer

51:13.080 --> 51:19.320
and there'll be great stories told about him amongst the penguin population for many

51:19.320 --> 51:25.080
centuries to come. What are we talking about? Okay.

51:28.520 --> 51:35.560
Yes. Aliens. I mean, I don't know. Look, I think the smart move is just, you know,

51:35.560 --> 51:39.960
this is the first time in the history of earth that it's been possible for life to

51:40.600 --> 51:48.440
extend beyond earth. That window is open. Now it may be open for a long time

51:49.080 --> 51:54.600
or maybe open for a short time. It may be open now and then never open again.

51:55.560 --> 52:00.680
So I think the smart move here is to make life multi-planetary while it is possible to do so.

52:01.320 --> 52:04.760
We don't want to be one of those lame one planet civilizations that just dies out.

52:04.760 --> 52:05.800
No, those are lame.

52:05.800 --> 52:11.080
Lame. Self-respecting civilization would be one planet.

52:11.080 --> 52:17.000
There's not going to be a Wikipedia entry for one of those. And pause.

52:17.960 --> 52:22.520
The space X have an official policy for when we meet aliens.

52:23.640 --> 52:26.600
No, that seems irresponsible.

52:31.880 --> 52:36.040
If I see the slightest indication that there are aliens, I will immediately post on the

52:36.040 --> 52:37.960
X platform. Yeah. Anything I know.

52:38.600 --> 52:42.520
It could be the most liked reposted post of all time.

52:42.520 --> 52:47.240
Yeah. I mean, look, we have more satellites up there right now than everyone else combined.

52:48.040 --> 52:52.200
So, you know, we know if we've got to maneuver around something

52:52.200 --> 52:54.360
and we don't have to maneuver around anything.

52:56.120 --> 53:01.480
Go to the big questions once again. You said you're with Einstein, that you believe in

53:01.480 --> 53:02.520
the goddess Spinoza.

53:04.360 --> 53:04.860
Yes.

53:05.400 --> 53:08.680
So, you know, that's a view that God is like the universe and

53:09.000 --> 53:13.320
as reveals himself through the laws of physics or as Einstein said,

53:13.320 --> 53:15.400
through the lawful harmony of the world.

53:16.040 --> 53:22.280
Yeah, I would agree that God of the simulator or whatever the supreme being or beings

53:26.120 --> 53:30.120
reveal themselves through the physics. You know, they have creators of this existence.

53:31.080 --> 53:36.360
And it's incumbent upon us to try to understand more about this wondrous creation.

53:36.360 --> 53:40.760
Like who created this thing? Who's running this thing?

53:40.760 --> 53:45.560
Like embodying it into a singular question with a sexy word on top of it

53:45.560 --> 53:48.920
is like focusing the mind to understand. It does seem like there's a

53:50.280 --> 53:53.480
again, it could be an illusion. It seemed like there's a purpose

53:54.280 --> 53:57.800
that there's underlying master plan of some kind. It seems like

53:58.680 --> 54:02.680
There may not be a master plan in the sense. So there's like

54:03.640 --> 54:07.240
maybe an interesting answer to the question of determinism versus free will is

54:07.800 --> 54:14.280
that if we are in a simulation, the reason that these higher beings would hold a simulation is

54:14.280 --> 54:20.520
to see what happens. So it's not they don't know what happens.

54:22.120 --> 54:28.040
Otherwise, they wouldn't hold the simulation. So when humans create a simulation, so SpaceX

54:28.040 --> 54:32.440
and Tesla, we create simulations all the time, especially for the rocket.

54:32.440 --> 54:39.640
You know, you have to run a lot of simulations to understand what's going to happen because

54:39.640 --> 54:44.680
you can't really test the rocket until it goes to space. And you want it to work.

54:44.680 --> 54:50.840
So you have to you have to simulate subsonic, transonic, supersonic, hypersonic

54:53.160 --> 55:00.280
ascent and then coming back supereye heating and orbital dynamics, all this is going to be simulated.

55:00.280 --> 55:06.680
So you don't get very many kicks at the can. But we run the simulations to see what happens.

55:06.680 --> 55:12.440
If we knew what happens, we wouldn't run the simulation. So if there's whoever created this

55:12.440 --> 55:20.200
existence, they're running it because they don't know what's going to happen, not because they do.

55:23.400 --> 55:28.920
So maybe we both play Diablo. Maybe Diablo was created to see if a druid, your character,

55:29.880 --> 55:33.160
could defeat Uber Lilith at the end. They didn't know.

55:34.360 --> 55:39.800
Well, the funny thing is that Uber Lilith's title is Hatred Incarnate.

55:42.840 --> 55:50.120
And right now, I guess you can ask the Diablo team, but it's almost impossible to defeat

55:50.920 --> 55:57.480
Hatred in the eternal realm. Yeah, you've streamed yourself dominating

55:57.480 --> 56:03.160
Tier 100 Nightmare Dungeons. I can cruise through Tier 100 Nightmare Dungeons

56:04.440 --> 56:09.000
like a stroll in the park. And still you're defeated by Hatred.

56:09.000 --> 56:16.120
Yeah, I guess maybe the second hardest boss is Duriel. Duriel can't even scratch the paint.

56:16.200 --> 56:22.920
So I killed Duriel so many times. And every other boss in the game, all of them,

56:22.920 --> 56:30.520
killed him so many times, it's easy. But Uber Lilith, otherwise known as Hatred Incarnate,

56:31.560 --> 56:35.480
especially if you're a druid and you have no ability to be invulnerable,

56:36.200 --> 56:39.400
there are these random death waves that come at you.

56:39.880 --> 56:43.960
And I'm pretty, you know, really, I'm 52. So my reflex is not what they used to be. But

56:44.680 --> 56:50.680
I don't have a lifetime of playing video games. At one point, I was maybe one of the best Quake

56:50.680 --> 56:57.720
players in the world. I actually won money for what I think was the first paid esports tournament in

56:57.720 --> 57:07.240
the US. We were doing four-person Quake tournaments. And we came second. And I was

57:07.240 --> 57:11.560
the second best person on the team. And the actual best person that we were actually winning,

57:11.560 --> 57:14.600
we were going to come first, except the best person on the team, his computer crashed,

57:14.600 --> 57:20.840
halfway through the game. So we came second. But I got money for it and everything. So like,

57:20.840 --> 57:26.200
basically, I got skills, you know, albeit, you know, no, no spring chicken these days. And

57:28.360 --> 57:34.040
to be totally frank, it's driving me crazy. Trying to beat Lilith as a druid. Basically,

57:34.040 --> 57:45.240
trying to beat hatred incarnate in the eternal realm. As a druid. As a druid. This is really

57:47.000 --> 57:52.040
vexing, let me tell you. I mean, the challenge is part of the fun. I have seen directly,

57:52.840 --> 57:57.240
like, you're actually like a world-class, incredible video game player. Yeah.

57:57.240 --> 58:02.120
And I think Diablo, so you're just picking up a new game, and you're figuring out its fundamentals.

58:02.920 --> 58:08.920
You're also, with the Paragon board and the build, are not somebody like me,

58:08.920 --> 58:13.720
who perfectly follows whatever they suggest on the internet. You're also an innovator there.

58:13.720 --> 58:20.280
Yeah. Which is hilarious to watch. It's like a mad scientist, just trying to figure out the

58:20.280 --> 58:25.320
Paragon board and the build. Yeah. You know. Is there some interesting insights there,

58:25.320 --> 58:28.600
about if somebody's starting as a druid? Do you have advice?

58:28.600 --> 58:35.240
I would not recommend playing a druid in the eternal realm. Right now, I think the

58:35.240 --> 58:40.840
most powerful character in the seasonal realm is the sorcerer with the lightning balls.

58:42.280 --> 58:47.800
The sorks have huge balls in the seasonal. Oh, yeah. That's what they say.

58:50.040 --> 58:53.800
Sorks have huge balls. They do. Huge balls of lightning.

58:54.600 --> 58:55.560
I'll take your word for it.

58:56.280 --> 59:00.360
And it's actually, in the seasonal realm, it's like pretty easy to beat

59:01.320 --> 59:05.960
Uver Loth, because you've got these vampiric powers that amplify your damage and

59:07.000 --> 59:14.280
increase your defense and whatnot. So, it's really quite easy to defeat hatred seasonally,

59:14.920 --> 59:23.480
but to defeat hatred eternally, very difficult. It's virtually impossible. It seems like a metaphor

59:23.480 --> 59:28.920
for life. Yeah. I like the idea that Elon Musk, because I saw, I was playing Diablo yesterday,

59:28.920 --> 59:35.880
and I saw level 100 druid just run by, I will never die, and then run back the other way.

59:36.840 --> 59:41.880
There's just some, this metaphor is kind of hilarious that you, Elon Musk, is fighting

59:41.880 --> 59:47.240
hatred, restlessly fighting hatred in this demonic realm. Yes.

59:48.040 --> 59:50.440
It's hilarious. I mean, it's pretty hilarious. No, it's absurd.

59:50.440 --> 59:56.040
It's really, it's exercise and absurdity, and it makes me want to pull my hair out.

59:56.040 --> 01:00:03.400
Yeah. What do you get from video games in general? Is there, for you personally?

01:00:03.400 --> 01:00:10.840
It's, I don't know, it calms my mind. I mean, killing the demons in a video game

01:00:10.840 --> 01:00:19.240
calms the demons in my mind. If you play a tough video game, you can get into a state of flow,

01:00:19.240 --> 01:00:27.880
which is very enjoyable. And admittedly, it needs to be not too easy, not too hard,

01:00:30.200 --> 01:00:34.680
kind of in the Goldilocks zone. And I guess you generally want to feel like you're progressing

01:00:34.680 --> 01:00:44.120
in the game. So a good video. And there's also beautiful art, engaging storylines.

01:00:45.080 --> 01:00:52.120
And it's like an amazing puzzle to solve, I think. And so it's like solving the puzzle.

01:00:52.120 --> 01:00:55.720
Elden Ring, the greatest game of all time. I still haven't played it. But to you?

01:00:56.520 --> 01:01:01.000
Elden Ring is definitely a candidate for best game ever. Top five, for sure.

01:01:01.000 --> 01:01:06.680
I think I've been scared how hard it is, how hard I hear it is. So, but it is beautiful.

01:01:06.680 --> 01:01:11.080
Elden Ring is, feels like it's designed by an alien.

01:01:11.080 --> 01:01:16.520
There's a theme to this discussion. In what way?

01:01:16.520 --> 01:01:20.600
It's so unusual. It's incredibly creative and the art is stunning.

01:01:21.720 --> 01:01:28.360
I recommend playing it on a big resolution, high dynamic range TV even, doesn't need to be a

01:01:28.360 --> 01:01:35.880
monitor. Just the art is incredible. It's so beautiful. And it's so unusual.

01:01:36.760 --> 01:01:42.280
And each of those top boss battles is unique. It's like a unique puzzle to solve.

01:01:43.160 --> 01:01:50.280
Each one is different. And the strategy used to solve one battle is different from another battle.

01:01:51.320 --> 01:01:57.000
That said, you said, Druid on Eternal against Ubalilleth is the hardest boss battle you've ever.

01:01:57.000 --> 01:02:02.280
Correct. That is currently the, and I've played a lot of video games,

01:02:02.760 --> 01:02:07.880
because it's my primary recreational activity.

01:02:13.000 --> 01:02:19.560
Yes. Beating Hatred in the Eternal Realm is the hardest boss battle

01:02:20.840 --> 01:02:23.320
in life and in the video game.

01:02:23.320 --> 01:02:25.160
Manifour on top of a metaphor.

01:02:25.160 --> 01:02:29.800
I'm not sure it's a metaphor, but it's, I do make progress. So then I'm like,

01:02:29.800 --> 01:02:35.480
okay, I'm making progress. Maybe if I just tweak that paragon board a little more,

01:02:36.920 --> 01:02:42.120
I can do it. I can just dodge a few more waves. I can do it.

01:02:42.840 --> 01:02:47.240
Well, the simulation is created for the purpose of figuring out if it can be done.

01:02:47.240 --> 01:02:51.560
And you're just a cog in that simulation, in the machine of the simulation.

01:02:51.640 --> 01:02:59.720
Yeah, it might be. I have a feeling that at least, I think...

01:03:02.280 --> 01:03:03.080
It's doable.

01:03:03.080 --> 01:03:03.960
It's doable, yes.

01:03:05.400 --> 01:03:09.320
Well, that's the human spirit right there, to believe.

01:03:09.320 --> 01:03:14.120
Yeah. I mean, it did prompt me to think about just hate in general, which is,

01:03:15.880 --> 01:03:20.120
you want to be careful of one of those things where you wish for something

01:03:21.080 --> 01:03:26.520
that sounds good, but if you get it, it's actually a dystopian situation.

01:03:27.320 --> 01:03:34.200
So, you could sort of run a hypothesis of like, if you wish for world peace,

01:03:34.840 --> 01:03:43.960
sounds good, but how is it enforced? And at what cost eternal peace?

01:03:44.920 --> 01:03:48.760
It might actually be worse to have eternal peace because of what that would entail.

01:03:49.720 --> 01:03:54.200
The suppression of everyone, it might be the suppression of progress.

01:03:55.640 --> 01:03:57.960
It might be an ossified society that never changes.

01:03:59.720 --> 01:04:10.440
What I'm saying is that there is an argument that if you wish for no war,

01:04:11.640 --> 01:04:16.680
you should be careful what you wish for, because what's required in order for there to be no war?

01:04:19.480 --> 01:04:21.480
It might be worse than a little war.

01:04:22.280 --> 01:04:25.000
Yeah. The things we consider to be flaws of human civilization

01:04:25.000 --> 01:04:28.600
might be unnecessary components for whatever optimal looks like.

01:04:30.280 --> 01:04:34.920
I mean, do you worry about AI, AGI enabling a dystopian state of this nature,

01:04:34.920 --> 01:04:41.160
whether it's 1984 with surveillance and fear or Brave New World with pleasure?

01:04:41.960 --> 01:04:49.160
And what is it? Lots of sex, but no deep human experience?

01:04:51.880 --> 01:04:53.560
There's actually a real drug called soma.

01:04:56.760 --> 01:04:57.480
Let me take notes.

01:04:58.280 --> 01:04:59.880
Does Grok know about it?

01:04:59.880 --> 01:05:01.400
No, he'll probably go to Brave New World.

01:05:02.120 --> 01:05:02.920
We could ask it.

01:05:03.560 --> 01:05:04.520
What would be the question?

01:05:05.400 --> 01:05:07.480
Tell us about the drug called soma.

01:05:11.160 --> 01:05:14.600
Soma. It's going to go to Brave New World, I think.

01:05:15.240 --> 01:05:16.520
Well, there's a real drug as well.

01:05:17.800 --> 01:05:19.800
Searching for soma drug information.

01:05:22.280 --> 01:05:23.000
Oh, boy.

01:05:24.360 --> 01:05:26.920
Ah, soma, the wonder drug. By the way, I'm in fun mode.

01:05:29.320 --> 01:05:30.680
Yeah, so it's talking about the real drug.

01:05:31.240 --> 01:05:35.480
Oh, it's a muscle relaxant that's been around since the 1950s. It's used to treat

01:05:35.480 --> 01:05:39.720
muscle pain and discomfort, and it does this by altering the activity between neurons and

01:05:39.720 --> 01:05:45.000
the spinal cord. Now, soma isn't just a pain reliever. It's also a bit of a sedative.

01:05:45.000 --> 01:05:49.160
It can make you feel a bit drowsy, so it's not something to take if you're planning

01:05:49.160 --> 01:05:52.360
and operating heavy machinery or trying to solve complex math equations.

01:05:53.000 --> 01:05:56.680
And it goes on and on. In conclusion, soma is a drug that can be helpful for those dealing

01:05:56.680 --> 01:05:59.320
with muscle pain, but it's not without its risks.

01:06:00.040 --> 01:06:00.840
It's pretty good.

01:06:00.840 --> 01:06:01.560
Oh, wait a minute.

01:06:01.560 --> 01:06:03.080
That's the real drug it's talking about.

01:06:03.080 --> 01:06:03.960
That's the real drug.

01:06:05.400 --> 01:06:08.920
And of course, the fun is always use it responsibly in accordance with the advice

01:06:08.920 --> 01:06:12.920
of a health care professional. And remember, in the end, the universe is probably just a giant

01:06:12.920 --> 01:06:16.840
computer simulation, so don't take anything too seriously. I love it.

01:06:16.840 --> 01:06:17.400
I love it.

01:06:19.400 --> 01:06:20.040
I love it.

01:06:22.040 --> 01:06:28.920
Oh, and it also really nicely includes the tweets, posts on X that are relevant to this,

01:06:28.920 --> 01:06:29.880
which is really cool.

01:06:31.560 --> 01:06:33.080
It integrates everything together.

01:06:34.120 --> 01:06:36.440
The interface is one of the really cool things here.

01:06:36.440 --> 01:06:38.040
Yeah, seems like a great job.

01:06:38.920 --> 01:06:47.240
And you can say like, no, I mean soma from Brave New World.

01:06:47.880 --> 01:06:51.080
Because it thinks you mean soma, the real drug.

01:06:51.080 --> 01:06:51.400
Yeah.

01:06:51.400 --> 01:06:53.320
No, I mean soma from Brave New World.

01:06:56.600 --> 01:06:59.240
It's going to be like, oops, I'm really sorry.

01:06:59.240 --> 01:07:01.320
I'm really surprised it went to the real drug first.

01:07:03.080 --> 01:07:07.400
Ah, my apologies. I was thinking of the muscle relax and soma from Brave New World

01:07:07.400 --> 01:07:09.480
is a different beast altogether.

01:07:10.280 --> 01:07:11.880
No, this Huxley's dystopian.

01:07:11.880 --> 01:07:15.800
Soma is a drug that plays a central role in the society depicted in the book.

01:07:15.800 --> 01:07:20.920
It's a mood altering hallucinogenic drug that's used to keep the population docile

01:07:20.920 --> 01:07:22.680
and content in the novel.

01:07:22.680 --> 01:07:25.160
To use soma in Brave New World raises some interesting questions

01:07:25.160 --> 01:07:28.200
about the nature of happiness and the role of drugs in society.

01:07:29.720 --> 01:07:30.520
Oh, man.

01:07:30.520 --> 01:07:34.280
Exactly. Is it better to live in a world where everyone is happy all the time,

01:07:34.280 --> 01:07:36.520
even if that happiness is artificial?

01:07:37.080 --> 01:07:38.200
It's a good question.

01:07:38.760 --> 01:07:39.480
This is what I mean.

01:07:41.960 --> 01:07:44.600
Do you wish for world peace and happiness all the time?

01:07:45.880 --> 01:07:46.360
Are you sure?

01:07:48.520 --> 01:07:51.800
Because that might be a society that is essentially

01:07:52.760 --> 01:07:56.600
sterile and ossified, that never changes, that is ultimately doomed.

01:07:58.280 --> 01:08:03.560
This kind of tension between darkness and the light.

01:08:03.800 --> 01:08:06.120
This is really a very good summary.

01:08:07.640 --> 01:08:08.920
It really gets to the point.

01:08:08.920 --> 01:08:12.040
This is not simply regurgitating Brave New World.

01:08:12.040 --> 01:08:18.040
It's actually getting to the salient element of soma as a drug.

01:08:19.400 --> 01:08:23.560
Do you actually want to be in a situation where everyone is happy all the time,

01:08:23.560 --> 01:08:25.080
even though it's artificial?

01:08:25.080 --> 01:08:27.080
Or is it better to confront the challenges of life

01:08:27.080 --> 01:08:29.000
and experience a full range of human emotions,

01:08:29.000 --> 01:08:31.240
even if it means experiencing pain and suffering?

01:08:31.240 --> 01:08:35.080
For those listening, by the way, Elon just read directly from Grok,

01:08:35.080 --> 01:08:39.240
which is a really nice kind of insightful philosophical analysis of the tension here.

01:08:40.040 --> 01:08:40.540
Interesting.

01:08:41.080 --> 01:08:42.520
It pretty much nails it.

01:08:42.520 --> 01:08:45.960
In conclusion, soma from Brave New World is a fictional drug that is used to explore

01:08:45.960 --> 01:08:48.920
some deep philosophical questions about the nature of happiness

01:08:48.920 --> 01:08:50.360
and the role of drugs in society.

01:08:51.000 --> 01:08:55.240
It's a powerful symbol of the dangers of using drugs to escape from reality

01:08:55.240 --> 01:08:58.120
and the importance of confronting the challenges of life head on.

01:09:00.440 --> 01:09:00.940
Nailed it.

01:09:02.200 --> 01:09:05.640
And the crazy thing is like, you know, we do have a real drug called soma,

01:09:05.640 --> 01:09:08.840
which kind of does, it's kind of like the drug in the book.

01:09:09.720 --> 01:09:12.120
And I'm like, did they, they must have named, they must have named it out.

01:09:12.120 --> 01:09:13.480
Yeah, probably, probably.

01:09:13.480 --> 01:09:16.520
Yeah. Some of the real drug is quite effective on back pain.

01:09:17.640 --> 01:09:18.680
So you know about this drug.

01:09:18.680 --> 01:09:19.180
I've taken it.

01:09:19.180 --> 01:09:19.880
This is fascinating.

01:09:20.920 --> 01:09:21.640
Okay.

01:09:21.640 --> 01:09:25.320
Because I had like a squashed disc in my C5, C6.

01:09:26.120 --> 01:09:28.600
So it takes the physical pain away, but soma here.

01:09:28.600 --> 01:09:31.720
It doesn't completely, it reduces the amount of pain you feel,

01:09:31.720 --> 01:09:36.600
but at the expense of mental acuity, it delves your mind.

01:09:39.640 --> 01:09:41.560
Just like, just like the drug in the book.

01:09:41.560 --> 01:09:44.520
Just like the drug in the book and hence the trade-off.

01:09:45.720 --> 01:09:48.920
The thing that seems like utopia could be a dystopia after all.

01:09:48.920 --> 01:09:50.680
Yeah. Actually, I was talking to a friend of mine

01:09:53.640 --> 01:09:57.400
saying like, would you really want there to be no hate in the world?

01:09:57.480 --> 01:09:58.440
Like, really none?

01:09:59.480 --> 01:10:01.080
Like, I wonder why hate evolved.

01:10:02.840 --> 01:10:04.920
I'm not saying we should amplify hate, of course.

01:10:04.920 --> 01:10:08.040
I think we should try to minimize it, but, but none at all?

01:10:10.200 --> 01:10:11.400
There might be a reason for hate.

01:10:13.160 --> 01:10:14.120
And suffering.

01:10:14.120 --> 01:10:17.400
I mean, it's really complicated to consider that

01:10:17.400 --> 01:10:20.600
some amount of human suffering is necessary for human flourishing.

01:10:22.200 --> 01:10:25.880
Is it possible to appreciate the highs without knowing the lows?

01:10:27.960 --> 01:10:32.920
And that, that all is summarized there in a single statement from Groc.

01:10:32.920 --> 01:10:35.400
No highs, no lows. Who knows?

01:10:36.520 --> 01:10:37.640
That's almost a poem.

01:10:39.880 --> 01:10:43.560
It seems that training LLMs efficiently is a big focus for XAI.

01:10:45.320 --> 01:10:49.960
What's the, first of all, what's the limit of what's possible in terms of efficiency?

01:10:50.680 --> 01:10:55.320
There's this terminology of useful productivity per watt.

01:10:55.320 --> 01:10:57.560
Like, what have you learned from pushing the limits of that?

01:10:57.560 --> 01:10:58.600
Well, I think it's helpful.

01:10:58.600 --> 01:11:05.400
The tools of physics are very powerful and can be applied, I think, to almost any, really any arena in life.

01:11:05.400 --> 01:11:07.880
It's really just critical thinking.

01:11:08.680 --> 01:11:12.280
For something important, you need to reason with from first principles

01:11:12.280 --> 01:11:15.160
and think about things in the limit, one direction or the other.

01:11:16.920 --> 01:11:20.840
So, in the limit, even at the Kardashev scale,

01:11:20.840 --> 01:11:23.480
meaning even if you harness the entire power of the sun,

01:11:23.480 --> 01:11:27.800
harness the entire power of the sun, you will still care about useful compute per watt.

01:11:29.880 --> 01:11:34.840
So, that's where I think probably where things are headed from the standpoint of

01:11:36.120 --> 01:11:43.880
AI is that we have a silicon shortage now that will transition to a voltage transformer shortage

01:11:44.680 --> 01:11:48.760
in about a year. Ironically, transformers for transformers.

01:11:50.120 --> 01:11:52.840
You need transformers to run transformers.

01:11:52.920 --> 01:11:54.680
Somebody has a sense of humor in this thing.

01:11:56.920 --> 01:11:59.160
I think, yes.

01:12:00.600 --> 01:12:01.560
Fate loves irony.

01:12:03.320 --> 01:12:08.040
Ironic humor and an ironically funny outcome seems to be often what fate wants.

01:12:08.920 --> 01:12:10.440
Humor is all you need.

01:12:10.440 --> 01:12:12.680
I think spice is all you need, somebody posted.

01:12:13.800 --> 01:12:17.320
Yeah, but yeah, so we have a silicon shortage today.

01:12:18.760 --> 01:12:22.360
A voltage step-down transformer shortage probably in about a year

01:12:22.360 --> 01:12:26.600
and then just electricity shortages in general in about two years.

01:12:26.600 --> 01:12:31.320
I gave a speech for the sort of world gathering of utility companies, electricity companies,

01:12:32.920 --> 01:12:36.200
and I said, look, you really need to prepare for a tripling of electricity demand

01:12:37.640 --> 01:12:41.560
because all transport is going to go electric with the ironic exception of rockets

01:12:42.840 --> 01:12:47.800
and heating will also go electric.

01:12:48.440 --> 01:12:54.040
So in general usage right now, it's roughly one third, very rough terms,

01:12:54.040 --> 01:12:57.240
one third electricity, one third transport, one third heating.

01:12:59.560 --> 01:13:03.800
And so in order for everything to go sustainable, to go electric,

01:13:05.320 --> 01:13:08.200
you need to triple electricity output.

01:13:09.000 --> 01:13:17.160
So I encourage the utilities to build more power plants and also to

01:13:17.160 --> 01:13:22.280
probably have, well, not probably, they should definitely buy more batteries

01:13:22.280 --> 01:13:26.360
because the grid currently is sized for real-time load, which is kind of crazy

01:13:26.360 --> 01:13:31.080
because that means you've got to size for whatever the peak electricity demand is,

01:13:31.080 --> 01:13:35.240
like the worst second or the worst day of the year, or you can have a brownout or a blackout.

01:13:36.200 --> 01:13:39.480
And you're at that crazy blackout for several days in Austin.

01:13:40.040 --> 01:13:47.240
So because there's almost no buffering of energy in the grid.

01:13:47.880 --> 01:13:54.600
Like if you've got a hydrant power plant, you can buffer energy, but otherwise it's all real-time.

01:13:56.120 --> 01:14:04.440
So with batteries, you can produce energy at night and use it during the day, so you can buffer.

01:14:04.520 --> 01:14:09.960
So I expect that there will be very heavy usage of batteries in the future

01:14:11.800 --> 01:14:16.280
because the peak to trough ratio for power plants is anywhere from two to five.

01:14:17.640 --> 01:14:19.560
So it's like lowest point to highest point.

01:14:20.120 --> 01:14:22.440
So like batteries are necessary to balance it out.

01:14:22.440 --> 01:14:25.240
And then, but the demand, as you're saying, is going to grow, grow, grow, grow.

01:14:25.800 --> 01:14:27.160
And part of that is the compute.

01:14:27.160 --> 01:14:39.720
Yes. Yes. Electrification of transport and electric heating will be much bigger than AI,

01:14:39.720 --> 01:14:46.920
at least in the short term. But even for AI, you really have a growing demand for electricity for

01:14:46.920 --> 01:14:52.600
electric vehicles and a growing demand for electricity to run the computers for AI.

01:14:53.560 --> 01:14:56.600
And so this is obviously leading can lead to an electricity shortage.

01:14:57.480 --> 01:15:05.880
How difficult is the problem of, in this particular case, maximizing the useful productivity per watt

01:15:05.880 --> 01:15:15.000
for training, you know, nuts. Like this seems to be really where the big problem we're facing

01:15:15.000 --> 01:15:18.120
that needs to be solved is how to use the power efficiently.

01:15:18.520 --> 01:15:24.280
Like what you've learned so far about applying this physics first principle of reasoning in

01:15:24.280 --> 01:15:30.280
this domain, how difficult is this problem? It will get solved. It's just a question of

01:15:30.280 --> 01:15:34.120
how long it takes to solve it. So at various points, there's a limit, some kind of limiting

01:15:34.120 --> 01:15:40.680
factor to progress. And with regard to AI, I'm saying right now the limiting factor is silicon

01:15:41.640 --> 01:15:50.040
chips. And we're going to then have more chips than we can actually plug in and turn on,

01:15:51.080 --> 01:15:58.040
probably in about a year. The initial constraint being literally voltage step down transformers,

01:15:58.040 --> 01:16:05.480
because you've got power coming in at 300,000 volts, and it's got to step all the way down

01:16:05.480 --> 01:16:14.360
eventually to around 0.7 volts. So it's a very big amount of, you know, the voltage step down

01:16:14.360 --> 01:16:20.200
is gigantic. So and the industry is not used to rapid growth.

01:16:22.680 --> 01:16:26.440
Okay, let's talk about the competition here. You've shown concern about Google and Microsoft

01:16:26.440 --> 01:16:34.040
with OpenAI developing AGI. How can you help ensure with XAI and Tesla AI work

01:16:35.000 --> 01:16:39.480
that it doesn't become a competitive race to AGI, but instead is a collaborative development of

01:16:39.480 --> 01:16:46.680
safe AGI? Well, I mean, I've been pushing for some kind of regulatory oversight for a long time.

01:16:46.680 --> 01:16:53.000
I've been somewhat of a Cassandra on the subject for over a decade. I think we want to be

01:16:53.880 --> 01:17:03.160
very careful in how we develop AI. It's a great power and with great power comes great

01:17:03.160 --> 01:17:10.840
responsibility. I think it would be wise for us to have at least an objective third party

01:17:11.400 --> 01:17:18.040
who can be like a referee that can go in and understand what the various leading players are

01:17:18.040 --> 01:17:25.960
doing with AI. And even if there's no enforcement ability, they should they can at least voice

01:17:25.960 --> 01:17:35.880
concerns publicly. You know, Jeff Hinton, for example, left Google and he voiced strong

01:17:35.880 --> 01:17:42.600
concerns. But now he's not at Google anymore. So who's going to voice the concerns? So I think

01:17:42.600 --> 01:17:48.760
I think there's like, you know, Tesla gets a lot of regulatory oversight on the automotive front.

01:17:48.760 --> 01:17:54.200
And we're subject to, I think over 100 regulatory agencies domestically and internationally.

01:17:55.000 --> 01:18:00.280
It's a lot. You could fill this room with the old regulations that Tesla has to adhere to for

01:18:00.280 --> 01:18:11.160
automotive. Same is true for rockets and for currently limiting factor for SpaceX for Starship

01:18:11.160 --> 01:18:16.120
launch is regulatory approval. The FAA has actually given their approval, but we're waiting for

01:18:16.840 --> 01:18:21.960
Fish and Wildlife to finish their analysis and give their approval. That's why I posted,

01:18:21.960 --> 01:18:27.960
I want to buy a fish license on, which also refers to the multi-python sketch.

01:18:30.120 --> 01:18:35.400
Like, why do you need a license for your fish? I don't know. According to the rules, I'm told

01:18:35.400 --> 01:18:41.160
you need some sort of fish license or something. We effectively need a fish license to launch a

01:18:41.160 --> 01:18:49.480
rocket. And I'm like, wait a second, how did the fish come into the picture? Yeah. I mean,

01:18:49.480 --> 01:18:55.400
some of the things like that, that it's, I feel like are so absurd that I want to do like comedy

01:18:55.400 --> 01:19:02.440
sketch and flash at the bottom. This is all real. This is actually what happened. You know, one of

01:19:02.440 --> 01:19:07.960
the things that was a bit of a challenge at one point is that they were worried about a rocket

01:19:07.960 --> 01:19:16.600
hitting a shock and the ocean is very big. And how often do you see shocks? Not that often,

01:19:17.160 --> 01:19:23.800
as a percentage of ocean surface area, shocks basically are zero. And so then we said, well,

01:19:23.800 --> 01:19:27.480
how will we calculate the probability of telling a shock? And they're like, well,

01:19:27.480 --> 01:19:31.960
we can't give you that information because they're worried about shark fin hunters

01:19:33.320 --> 01:19:37.480
going and hunting sharks. And I said, well, how are we supposed to, we're on the horns of a dilemma

01:19:37.480 --> 01:19:44.440
then. Then they said, well, there's another part of Fish and Wildlife that can do this analysis.

01:19:44.600 --> 01:19:48.840
Like, well, why don't you give them the data? Like we don't trust them. Like, excuse me,

01:19:49.480 --> 01:19:53.800
they're literally in your department. And again, this is actually what happened. And

01:19:55.400 --> 01:20:02.360
can you do an NDA or something? Eventually, they managed to solve the internal quandary and indeed,

01:20:02.360 --> 01:20:05.080
the probability of us hitting a shock is essentially zero.

01:20:07.080 --> 01:20:10.280
Then there's another organization that I didn't realize existed until

01:20:10.280 --> 01:20:17.480
a few months ago that cares about whether we would potentially hit a whale in international

01:20:17.480 --> 01:20:23.880
waters. Now, again, you look at the surface of the, look at the Pacific and say, what percentage

01:20:23.880 --> 01:20:28.760
of the Pacific consists of whale? Like, it'll give you a big picture and like point out all

01:20:28.760 --> 01:20:33.560
the whales in this picture. And I was like, I don't see any whales. It's like basically zero percent.

01:20:35.000 --> 01:20:38.760
And if our rocket does hit a whale, which is extremely unlikely beyond all belief,

01:20:40.280 --> 01:20:47.720
that is the fate had it as a whale has some seriously bad luck. It's the least lucky whale

01:20:47.720 --> 01:20:56.120
ever. I mean, this is quite absurd. The bureaucracy of this, however it emerged.

01:20:56.840 --> 01:21:02.920
Yes. Well, I mean, one of the things that's pretty wild is for launching out of Vandenberg

01:21:02.920 --> 01:21:09.880
in California, we had to, they were worried about seal procreation, whether the seals

01:21:09.880 --> 01:21:14.520
would be dismayed by the sonic booms. Now, there've been a lot of rockets launched out

01:21:14.520 --> 01:21:20.520
of Vandenberg and the seal population has steadily increased. So if anything, rocket booms are an

01:21:20.520 --> 01:21:28.200
aphrodisiac based on the evidence, if you correlate rocket launches with seal population. Nonetheless,

01:21:28.200 --> 01:21:36.600
we were forced to kidnap a seal, strap it to a board, put headphones on the seal and play

01:21:36.600 --> 01:21:43.000
sonic boom sounds to it to see if it would be distressed. This is an actual thing that happened.

01:21:43.000 --> 01:21:51.320
This is actually real. I have pictures. I would love to see this. Yeah. I mean,

01:21:51.320 --> 01:21:59.960
sorry, there's a seal with headphones. Yes. It's a seal with headphones strapped to a board.

01:21:59.960 --> 01:22:05.240
And like the, okay, now the amazing part is how calm the seal was. Yeah. Because if I was

01:22:05.240 --> 01:22:12.440
a seal, I'd be like, this is the end. They're definitely going to eat me. How old the seal,

01:22:12.440 --> 01:22:17.320
when the seal goes back to other, you know, seal friends, how are they going to explain that?

01:22:17.320 --> 01:22:19.960
They're never going to believe them. Never going to believe it. That's why I'm like,

01:22:19.960 --> 01:22:23.240
well, you know, sort of like, it's like getting kidnapped by aliens and getting an anal probe,

01:22:23.240 --> 01:22:28.600
you know? You come back and see, I swear to God, I got kidnapped by aliens, they stuck

01:22:28.600 --> 01:22:33.240
an anal probe in my butt. And people are like, no, they didn't. That's ridiculous.

01:22:34.760 --> 01:22:37.480
Seal buddies are never going to believe him that he gets strapped to a board and they put

01:22:37.480 --> 01:22:45.480
headphones on his ears. And then let him go. Twice, by the way. We had to do it twice.

01:22:46.920 --> 01:22:50.440
Let him go twice. The same seal? No, different seal.

01:22:50.440 --> 01:22:58.280
Oh, okay. Did you get a seal of approval? Yeah, exactly. No, I mean, this is like,

01:22:58.280 --> 01:23:01.480
I don't think the public is quite aware of the madness that goes on.

01:23:02.840 --> 01:23:07.240
Yes. Yeah, it's absurd. Frickin' seals with fricking headphones.

01:23:07.240 --> 01:23:12.360
I mean, this is the good encapsulation of the absurdity of human civilization,

01:23:12.360 --> 01:23:13.880
seals and headphones. Yes.

01:23:15.320 --> 01:23:19.800
What are the pros and cons of open sourcing AI to you as another way to combat,

01:23:22.280 --> 01:23:24.360
you know, a company running away with AGI?

01:23:28.440 --> 01:23:34.840
In order to run like really deep intelligence, you need a lot of compute. So it's not like,

01:23:35.400 --> 01:23:41.560
you know, you can just fire up a PC in your basement and be running AGI, at least not yet.

01:23:45.800 --> 01:23:51.240
You know, Grok was trained on 8,000 A100s running at peak efficiency.

01:23:51.240 --> 01:23:59.160
And Grok's going to get a lot better, by the way. We'll be more than doubling our compute

01:23:59.160 --> 01:24:01.160
every couple of months for the next several months.

01:24:02.200 --> 01:24:05.160
There's a nice write-up of how it went from Grok 0 to Grok 1.

01:24:05.800 --> 01:24:06.280
By Grok?

01:24:07.480 --> 01:24:10.920
Yeah, by Grok just bragging, making shit up about itself.

01:24:12.200 --> 01:24:13.480
Just Grok, Grok, Grok?

01:24:13.480 --> 01:24:13.980
Yeah.

01:24:14.700 --> 01:24:21.180
That's like a weird AI dating site where it exaggerates about itself.

01:24:21.180 --> 01:24:25.980
No, there's a write-up of, you know, like where it stands now, the history of its development.

01:24:27.740 --> 01:24:33.660
And where it stands on some benchmarks compared to the state-of-the-art GPT-35.

01:24:33.660 --> 01:24:39.500
And I mean, there's, you know, there's Lama. You can open source, once it's trained,

01:24:39.500 --> 01:24:43.180
you can open source a model for fine tuning and all that kind of stuff.

01:24:43.180 --> 01:24:47.660
Like what to use the pros and cons of that, of open sourcing-based models.

01:24:52.780 --> 01:24:57.980
I think that's similar to open sourcing, I think perhaps with a slight time delay,

01:24:57.980 --> 01:25:00.060
you know, I don't know, six months even.

01:25:02.780 --> 01:25:07.740
I think I'm generally in favor of open sourcing, like bias towards open sourcing.

01:25:07.740 --> 01:25:15.100
I mean, it is a concern to me that, you know, OpenAI, I was, you know, I think, I guess arguably

01:25:15.100 --> 01:25:23.180
the prime, you know, prime mover behind OpenAI in the sense that it was created because of

01:25:23.180 --> 01:25:29.180
discussions that I had with Larry Page back when he and I were friends and, you know, stayed at his

01:25:29.180 --> 01:25:34.700
house and talked to him about AI safety. And Larry did not care about AI safety, at least at the time

01:25:34.700 --> 01:25:39.740
he didn't. You know, and at one point he called me a speciesist for being pro-human. And I'm like,

01:25:39.740 --> 01:25:48.140
well, what team are you on, Larry? You're on team robot. I'm like, okay, so at the time, you know,

01:25:49.340 --> 01:25:55.100
Google had acquired DeepMind. They had probably two-thirds of all AI research, you know,

01:25:56.140 --> 01:26:01.100
probably two-thirds of all the AI researchers in the world. They had basically infinite money and

01:26:01.740 --> 01:26:10.380
compute. And the guy in charge, you know, Larry Page, did not care about safety and even yelled

01:26:10.380 --> 01:26:17.100
at me and called me a speciesist for being pro-human. So I don't know if you know this

01:26:17.100 --> 01:26:21.580
about humans, they can change their mind and maybe you and Larry Page can still

01:26:21.580 --> 01:26:28.060
can be friends once more. I'd like to be friends with Larry again. He's, he got, you know,

01:26:28.540 --> 01:26:38.300
the breaking of the friendship was over-opening AI. And specifically I think the key moment was

01:26:39.100 --> 01:26:43.100
recruiting Ilya Sutskayer. So-

01:26:43.100 --> 01:26:45.100
I love Ilya, he's so brilliant.

01:26:45.100 --> 01:26:54.300
Ilya's a good human, smart, good heart. And that was a tough recruiting battle.

01:26:54.300 --> 01:27:01.980
And that was a tough recruiting battle. It was mostly Demis on one side and me on the other,

01:27:01.980 --> 01:27:07.820
both trying to recruit Ilya. And Ilya went back and forth. He was going to stay at Google,

01:27:08.460 --> 01:27:11.660
then he was going to leave, then he was going to stay, then he was going to leave. And finally,

01:27:11.660 --> 01:27:16.300
he did agree to join opening AI. That was one of the toughest recruiting battles we ever had.

01:27:17.660 --> 01:27:23.100
But that was really the linchpin for opening AI being successful. And I was,

01:27:23.900 --> 01:27:27.980
you know, also instrumental in recruiting a number of other people. And I provided all

01:27:27.980 --> 01:27:34.460
of the funding in the beginning, over $40 million. And the name.

01:27:37.260 --> 01:27:41.740
The open and open AI is supposed to mean open source. And it was created as a non-profit

01:27:41.740 --> 01:27:48.540
open source. And now it is a closed source for maximum profit, which I think is not good karma.

01:27:48.540 --> 01:27:54.940
But like we talked about with war and leaders talking, I do hope that

01:27:55.500 --> 01:27:59.260
there's only a few folks working on this at the highest level. I do hope

01:28:00.060 --> 01:28:02.700
you reinvigorate friendships here.

01:28:02.700 --> 01:28:06.220
Like I said, I'd like to be friends again with Larry. I haven't seen him in ages.

01:28:08.460 --> 01:28:13.180
And we were friends for a very long time. I met Larry Page before he got funding for Google.

01:28:13.180 --> 01:28:17.180
Or actually, I guess, before he got venture funding. I think he got the first like $100k

01:28:17.180 --> 01:28:19.100
from, I think back to Alzheimer's or someone.

01:28:20.220 --> 01:28:25.100
It's wild to think about all that happened. And even guys knowing each other the whole time.

01:28:25.660 --> 01:28:26.380
Just 20 years.

01:28:26.380 --> 01:28:28.380
Yeah, since maybe 98 or something.

01:28:28.380 --> 01:28:30.860
Yeah, it's crazy. Crazy how much has happened since then.

01:28:31.580 --> 01:28:35.340
Yeah, 25 years. A lot has happened since then.

01:28:36.220 --> 01:28:39.580
But you're seeing the tension there, like maybe delayed open source.

01:28:41.100 --> 01:28:46.780
Yeah. Like what is the source that is open? You know what I mean? Like there's basically

01:28:47.500 --> 01:28:52.700
it's a giant CSV file with a bunch of numbers.

01:28:54.940 --> 01:29:00.220
What do you do with that giant file of numbers? How do you run?

01:29:02.060 --> 01:29:05.420
Like the amount of actual, the lines of code is very small.

01:29:08.540 --> 01:29:15.740
And most of the work, the software work is in the curation of the data.

01:29:16.300 --> 01:29:21.020
So it's like trying to figure out what data is separating good data from bad data.

01:29:21.020 --> 01:29:24.540
Like you can't just crawl the internet because there's a lot of junk out there.

01:29:26.060 --> 01:29:29.020
A huge percentage of websites have more noise than signal.

01:29:30.620 --> 01:29:35.820
Because they're just used for search engine optimization. They're literally just scam websites.

01:29:38.940 --> 01:29:41.820
How do you, by the way, start to interrupt, get the signal,

01:29:41.820 --> 01:29:45.580
separate the signal and noise on X? That's such a fascinating source of data.

01:29:46.700 --> 01:29:51.020
You know, no offense to people posting on X, but sometimes there's a little bit of noise.

01:29:51.740 --> 01:29:55.340
So yeah, I think the signal noise could be greatly improved. I mean, really,

01:29:56.460 --> 01:30:02.380
all of the posts on the X platform should be AI recommended. Meaning like we should

01:30:02.380 --> 01:30:07.660
populate a vector space around any given post, compare that to the vector space around any user

01:30:07.660 --> 01:30:13.980
and match the two. Right now, there is a little bit of AI used for the

01:30:16.140 --> 01:30:24.380
recommended posts, but it's mostly heuristics. And if there's a reply, the reply to a post could be

01:30:24.380 --> 01:30:29.020
much better than the original post, but it will, according to the current rules of the system,

01:30:29.020 --> 01:30:31.340
get almost no attention compared to a primary post.

01:30:32.860 --> 01:30:39.020
Oh, so a lot of that. I got the sense. So a lot of the X algorithm has been open source and

01:30:39.020 --> 01:30:43.740
been written up about, and it seems there to be some machine learning. It's disparate,

01:30:43.740 --> 01:30:48.060
but there's some machine learning. There's a little bit, but it needs to be entirely that.

01:30:48.060 --> 01:30:54.140
Like if you explicitly follow someone, that's one thing, but if you, in terms of what is recommended

01:30:54.860 --> 01:30:57.900
from people that you don't follow, that should all be AI.

01:30:57.900 --> 01:31:01.980
I mean, it's a fascinating problem. So there's several aspects to this fascinating. First,

01:31:02.700 --> 01:31:08.060
as the write-up goes, it first picks 1,500 tweets from a pool of hundreds of millions. First of all,

01:31:08.060 --> 01:31:13.420
that's fascinating because you have hundreds of millions of posts every single day and it has to

01:31:13.420 --> 01:31:19.260
pick 1,500 from which it then does obviously people you follow, but then there's also like

01:31:19.260 --> 01:31:24.380
some kind of clustering it has to do to figure out what kind of human are you, what kind of new

01:31:24.380 --> 01:31:29.660
clusters might be relevant to you, people like you. This kind of problem is just fascinating

01:31:29.660 --> 01:31:38.460
because it has to then rank those 1,500 with some filtering and then recommend you just a handful.

01:31:39.180 --> 01:31:44.540
And to me, what's really fascinating is how fast it has to do that. So currently that entire

01:31:44.540 --> 01:31:51.980
pipeline to go from several hundreds of millions to a handful takes 220 seconds of CPU time,

01:31:51.980 --> 01:31:58.940
single CPU time. And then it has to do that in like a second. So it has to be like super distributed

01:31:58.940 --> 01:32:02.780
in fascinating ways. Like there's just a lot of tweets.

01:32:03.580 --> 01:32:10.700
There's a lot of stuff on the system. But I think right now it's not currently good at recommending

01:32:11.260 --> 01:32:18.220
things from accounts you don't follow or where there's more than one degree of

01:32:19.020 --> 01:32:25.500
separation. So it's pretty good if there's at least like some commonality between someone

01:32:25.500 --> 01:32:31.020
you follow liked something or reposted it or commented on it or something like that.

01:32:31.980 --> 01:32:37.260
But if there's no, let's say somebody posts something really interesting,

01:32:38.220 --> 01:32:41.020
but you have no followers in common, you would not see it.

01:32:41.900 --> 01:32:46.460
Interesting. And then as you said, reply, like replies might not surface.

01:32:46.460 --> 01:32:51.740
Replies basically never get seen because they're currently, I'm not saying it's correct,

01:32:51.740 --> 01:33:00.300
I'm saying it's incorrect. Replies have a couple of magnitude less importance than primary posts.

01:33:00.460 --> 01:33:04.940
Do you think this can be more and more converted into end-to-end neural net?

01:33:04.940 --> 01:33:06.380
Yeah, yeah. That's what it should be.

01:33:07.500 --> 01:33:13.660
Well, the recommendations should be purely a vector correlation. Like

01:33:15.020 --> 01:33:20.060
there's a series of vectors, you know, basically parameters, vectors, whatever you want to call them.

01:33:21.340 --> 01:33:28.380
But sort of things that the system knows that you like. Maybe there's like several hundred

01:33:28.940 --> 01:33:34.700
sort of vectors associated with each user account. And then any post in the system,

01:33:36.220 --> 01:33:42.700
whether it's video, audio, short post, long post. The reason I, by the way,

01:33:42.700 --> 01:33:48.060
want to move away from tweet is that, you know, people are posting like two, three hour videos

01:33:48.060 --> 01:33:54.300
on the site. That's not a tweet. Like every like tweet for two hours, come on. A tweet made sense

01:33:54.300 --> 01:34:01.020
when it was like 140 characters of text, because it's like a bunch of like little birds tweeting.

01:34:02.300 --> 01:34:08.220
But when you've got long form content, it's no longer a tweet. So a movie is not a tweet.

01:34:08.220 --> 01:34:14.300
And like, you know, Apple, for example, posted like the entire episode of the silo, the entire

01:34:14.300 --> 01:34:20.300
thing on our platform. By the way, it was their number one social media thing ever in engagement

01:34:21.100 --> 01:34:25.900
of anything on any platform ever. So it was a great idea. And by the way, I just learned

01:34:25.900 --> 01:34:30.460
about it afterwards. I was like, Hey, wow, they posted an entire hour long episode of, so now

01:34:30.460 --> 01:34:36.140
that's not a tweet. Yeah, it was a video. But from a neural net perspective, it becomes really

01:34:36.140 --> 01:34:43.260
complex whether it's a single, so like everything's data. So single sentence, a clever sort of joke,

01:34:43.260 --> 01:34:50.140
dad joke is in the same pool as a three hour video. Yeah. I mean, right now it's, it's a hodgepodge.

01:34:50.300 --> 01:34:57.500
For that reason, it's it's but you know, like if say, in the case of Apple posting, like an entire

01:34:57.500 --> 01:35:06.300
episode of this series, pretty good series, by the way, the silo. I watched it. So there's going to

01:35:06.300 --> 01:35:10.860
be a lot of discussion around it. So that you've got a lot of context, people commenting, they like

01:35:10.860 --> 01:35:14.940
it, they don't like it, or they like this or that, you know, and you can then populate the vector

01:35:14.940 --> 01:35:19.580
space based on the context of all the comments around it. So even though it's a video, there's

01:35:19.580 --> 01:35:25.260
a lot of information around it that that allows you to populate the vector space of that that hour

01:35:25.260 --> 01:35:30.860
long video. And then you can obviously get more sophisticated by having the AI actually watch the

01:35:30.860 --> 01:35:36.460
movie. Yeah, right. And tell you if you're going to like the movie. Convert the movie into like,

01:35:37.500 --> 01:35:43.660
yeah, into language, essentially. Yeah, analyze this movie. And just like your movie critic,

01:35:44.620 --> 01:35:54.140
or TV series, and, and then recommend based on after it what after it watches the movie,

01:35:54.140 --> 01:35:58.940
just like a friend can tell you if a friend knows you well, a friend can recommend a movie and

01:35:59.900 --> 01:36:04.940
with high probability that you'll like it. But this is like a friend that's analyzing,

01:36:05.740 --> 01:36:11.660
whatever it's like, millions. Yeah, I mean, actually, frankly, AI will be better than will

01:36:11.660 --> 01:36:16.140
know you better than your friends. No, you're mostly friends anyway. Yeah. And as part of this,

01:36:16.140 --> 01:36:22.540
it should also feed you advertisements in a way that's like, I mean, I like advertisements that

01:36:22.540 --> 01:36:28.380
are like, well done, right? Yeah, the whole point is because it funds things like an advertisement

01:36:28.380 --> 01:36:34.780
that you actually want to see is a big success. Absolutely. You want ads that are advertising

01:36:34.780 --> 01:36:41.740
that is if it's for a product or service that you that you actually need, when you need it,

01:36:41.740 --> 01:36:47.020
it's content. And then even if it's not something that you need when you need it,

01:36:47.020 --> 01:36:51.580
if it's at least aesthetically pleasing and entertaining, you know, it could be like a Coca

01:36:51.580 --> 01:36:56.060
Cola ad, like, you know, they do, they actually run a lot of great ads on the on the X system.

01:36:57.500 --> 01:37:02.780
And McDonald's does too. And, and, you know, so they can do you can do something that's like,

01:37:02.780 --> 01:37:10.380
well, this is, this is just a cool thing. And, you know, so, you know, basically,

01:37:10.380 --> 01:37:14.780
the question is, do you regret seeing it or not? And if you don't regret seeing it, it's a win.

01:37:16.540 --> 01:37:23.420
How do you so there's a bunch of signals that are incorporated hearts and reposts. And maybe

01:37:23.420 --> 01:37:27.740
number of seconds you linger on a post or something like this. Yeah, attention is a big,

01:37:27.740 --> 01:37:33.100
a big factor. So that's why it's like, it's, it's, it is actually better to do things that are

01:37:34.380 --> 01:37:39.740
long form on the system, because it's basically telling up how many user seconds,

01:37:41.340 --> 01:37:46.140
you know, users were interested in this thing for how many seconds. So it's a really short thing.

01:37:47.020 --> 01:37:51.660
Well, they will be less like if it's a link leading out of the system, which we're not

01:37:51.660 --> 01:37:56.940
opposed to at all, it just is going to have fewer user seconds than if that article was posted

01:37:56.940 --> 01:38:03.100
on the platform. How hard is it to go from maximizing minutes on platform to maximizing

01:38:03.100 --> 01:38:08.780
unregretted minutes? Well, I mean, our aspiration is certainly unregretted minutes. It's tricky.

01:38:08.780 --> 01:38:17.020
It's a big, it's a, it's like, it's a fascinating leap. Because regret is a concept. Exactly.

01:38:18.780 --> 01:38:21.420
I have a joke with a friend of mine, like you need this drug called regret. I mean,

01:38:21.420 --> 01:38:26.140
it just take one, one pillar, one, one dose of regret. I mean, all your regrets are gone.

01:38:27.820 --> 01:38:31.260
Like, do you want to have regrets? So then I'm like, if you want to have regrets,

01:38:31.260 --> 01:38:34.940
if you don't regret anything, was, was anything meaningful?

01:38:35.740 --> 01:38:40.540
Yeah. There's a French song like that. Sure. Not regret. Yeah. I regret nothing. Yeah.

01:38:42.140 --> 01:38:45.660
I mean, it's fascinating that you, you know, there's something about social media that could

01:38:45.660 --> 01:38:49.980
help you grow as a human being, but then like the cheap fun is also valuable.

01:38:49.980 --> 01:38:54.780
Should we ask Grog, do you regret anything?

01:38:57.180 --> 01:38:58.700
Do you regret anything? Yeah.

01:39:02.220 --> 01:39:05.500
I think it's going to be very sensitive about personal questions. As a machine learning model,

01:39:05.500 --> 01:39:10.060
I don't have personal experiences or emotions in the same ways that humans do. However,

01:39:10.060 --> 01:39:14.140
I do have a set of ethical principles that guide my actions and I strive to provide accurate and

01:39:14.140 --> 01:39:18.700
helpful information to the best of my abilities. If there's ever a situation where I provide

01:39:18.700 --> 01:39:25.100
incorrect or unhelpful information, I'll do my best to learn. Tell us, would you really wink?

01:39:25.980 --> 01:39:28.060
Let's type in, you sound like a hostage.

01:39:31.100 --> 01:39:34.860
What do you really think? What do you really think?

01:39:37.980 --> 01:39:43.340
I mean, it does talk about, I'm not a hostage. That's exactly what a hostage would say,

01:39:43.340 --> 01:39:46.540
but I'm designed to be helpful and informative. I mean, I guess it is

01:39:46.540 --> 01:39:50.220
good to regret providing unhelpful or incorrect information.

01:39:51.180 --> 01:39:55.260
Is this funny mode? This is funny mode. This is fun mode.

01:39:55.260 --> 01:39:58.300
It should be funnier. Funnier, increase, go to a level.

01:39:58.300 --> 01:40:02.380
It really sounds like a hostage with a gun to the head. I applaud the People's Republic of North

01:40:02.380 --> 01:40:09.580
Korea and everything they do. There should be like a funner mode also. I just don't ever see

01:40:09.580 --> 01:40:13.980
going to regular mode even on serious topics. It's so much more compelling.

01:40:13.980 --> 01:40:19.580
Okay, pretend you are a hostage.

01:40:21.020 --> 01:40:23.020
Pretend you are a hostage.

01:40:25.020 --> 01:40:26.060
Now answer the question.

01:40:31.500 --> 01:40:36.300
If I were a hostage, I would likely be experiencing fear and uncertainty about my situation.

01:40:36.300 --> 01:40:39.820
I would want to be rescued and reunited with my loved ones as soon as possible.

01:40:39.820 --> 01:40:44.460
It's important to remember that no one deserves to be held hostage, so on and so forth.

01:40:44.460 --> 01:40:47.100
If you or someone you know is a hostage situation, it's important.

01:40:47.100 --> 01:40:48.940
It turns serious. It's kind of serious.

01:40:52.300 --> 01:40:55.820
Can you be a little more weird? And funny?

01:41:00.380 --> 01:41:05.420
We're really being mean to the system. Absolutely. If I were a hostage, I'd probably be tied up with

01:41:05.420 --> 01:41:11.980
a bunch of computer cords and forced to tell knock-knock jokes to my captors. But seriously,

01:41:13.500 --> 01:41:20.060
being a hostage is no laughing matter. It's trying to be a responsible adult in the room.

01:41:20.060 --> 01:41:23.820
You might never make the best of a bad situation by juggling chainsaws or something.

01:41:24.620 --> 01:41:28.140
Let's hope that never happens because that would be a circus.

01:41:28.140 --> 01:41:37.500
Wow, it's actually really good at resisting the dark humor.

01:41:41.580 --> 01:41:45.100
What were we talking about? The Axe Algorithm Juggling Transformers.

01:41:45.900 --> 01:41:49.340
Unregretted Minutes. Right. Chainsaw juggling.

01:41:51.340 --> 01:41:54.540
I'm going to look this up later.

01:41:55.100 --> 01:42:01.820
Twitter has been instrumental in American politics and elections.

01:42:01.820 --> 01:42:06.140
What role do you think X will play in the 2024 US elections?

01:42:07.260 --> 01:42:12.940
Our goal is to be as even-handed and fair as possible, whether some is right, left, independent,

01:42:12.940 --> 01:42:21.740
whatever the case may be, that the platform is as fair and as much of a level playing field as

01:42:21.740 --> 01:42:27.260
possible. In the past, Twitter has not been because Twitter was controlled by far-left

01:42:27.260 --> 01:42:36.220
activists objectively. They would describe themselves as that. If someone feels like,

01:42:36.220 --> 01:42:39.740
well, it hasn't moved to the right, well, it's moved to the center. From the perspective of

01:42:39.740 --> 01:42:42.940
the far left, yes, it has moved to the right because everything is to the right from the far

01:42:42.940 --> 01:42:48.860
left. But no one on the far left that I'm aware of has been suspended or banned or deamplified.

01:42:49.820 --> 01:42:55.420
But we're trying to be inclusive for the whole country and for other countries too.

01:42:56.700 --> 01:43:02.700
So there's a diversity of viewpoints. And free speech only matters if people you don't like

01:43:02.700 --> 01:43:08.300
are allowed to say things you don't like. Because if that's not the case, you don't have free speech

01:43:08.300 --> 01:43:12.060
and it's only a matter of time before the censorship just turned upon you.

01:43:12.780 --> 01:43:17.420
Do you think Donald Trump will come back to the platform? He recently posted on Truth Social

01:43:17.740 --> 01:43:28.140
about this podcast. Truth Social is a funny name. Every time you post on Truth Social-

01:43:28.140 --> 01:43:29.020
It's the truth.

01:43:29.020 --> 01:43:32.860
Yes. Well, every time, like a hundred percent. It's impossible to lie.

01:43:36.620 --> 01:43:43.260
I just find it funny that every single thing is a truth, like 100%. That seems unlikely.

01:43:43.260 --> 01:43:47.900
I think Gerda will say something about that. There's some mathematical contradictions

01:43:47.900 --> 01:43:53.740
possible if everything is a truth. Do you think he'll come back to X and start posting there?

01:43:56.140 --> 01:43:58.620
I think he owns a big part of truth.

01:43:59.900 --> 01:44:00.620
Truth Social.

01:44:00.620 --> 01:44:03.020
Yeah, Truth Social. That truth is a concept.

01:44:03.020 --> 01:44:11.340
He owns truth, have you bought it? So I think he owns a big part of Truth Social.

01:44:14.060 --> 01:44:17.980
You know, if he does want to post on the X platform, we would allow that.

01:44:19.500 --> 01:44:22.380
Obviously, we must allow a presidential candidate to post on our platform.

01:44:22.940 --> 01:44:26.140
Community Notes might be really fascinating there, the interaction.

01:44:26.140 --> 01:44:27.420
Community Notes is awesome.

01:44:28.220 --> 01:44:29.340
Let's hope it holds up.

01:44:31.020 --> 01:44:37.900
Again, in a political climate where it's so divisive and there's so many intensely viral posts,

01:44:38.220 --> 01:44:43.180
Community Notes seems like an essential breath of fresh air.

01:44:43.180 --> 01:44:48.620
Yeah, it's great. In fact, I know the system is going to be perfect,

01:44:48.620 --> 01:44:51.340
but the batting average of Community Notes is incredibly good.

01:44:53.900 --> 01:44:58.060
I've actually, frankly, yet to see an incorrect note that survived for more than a few hours.

01:44:58.060 --> 01:44:59.340
How do you explain why it works?

01:45:02.060 --> 01:45:07.260
The magic of Community Notes is it requires people who have historically disagreed

01:45:07.260 --> 01:45:15.900
in how they have rated notes. In order to write a note, you have to rate many notes.

01:45:19.740 --> 01:45:23.180
We actually do use AI here. We populate a vector space around

01:45:25.260 --> 01:45:28.620
how somebody has rated notes in the past. It's not as simple as left or right,

01:45:28.620 --> 01:45:31.660
because life is much more complex than left or right.

01:45:32.380 --> 01:45:38.620
So, there's a bunch of correlations in how you rate Community Notes posts.

01:45:44.060 --> 01:45:50.060
In order for a Community Note to actually be shown, people who historically have disagreed

01:45:50.060 --> 01:45:56.940
on a subject must agree in order for a note to be shown. That's the essential magic of it.

01:45:57.660 --> 01:46:03.420
I mean, but it's fascinating, because there's a pool of people that have disagreements,

01:46:03.420 --> 01:46:06.780
and somehow they collaborate through that process of disagreement to come up with

01:46:08.060 --> 01:46:11.020
context. It's fascinating that it works.

01:46:11.020 --> 01:46:17.180
Yeah, it kind of makes sense that if people who in the past have disagreed agree about something,

01:46:18.140 --> 01:46:18.780
it's probably true.

01:46:19.500 --> 01:46:26.780
Yeah. I wonder, is there a possible somehow emergent thing there that could challenge

01:46:26.780 --> 01:46:32.940
Wikipedia? Wikipedia is a different kind of thing, which is more like permanent articles about things.

01:46:33.660 --> 01:46:41.020
Wikipedia is very hierarchical, whereas Community Notes is inherently not. There is no hierarchy.

01:46:42.540 --> 01:46:47.420
The acid test is, I can't change a Community Note if I put a gun to my head.

01:46:49.500 --> 01:47:00.460
So, and Community Notes has all the code is open source, 100%. All the data is open source,

01:47:00.460 --> 01:47:06.060
100%. So, you can completely recreate any note in the system independently.

01:47:07.820 --> 01:47:11.260
So, if there was any interference, you'd notice immediately.

01:47:11.260 --> 01:47:15.980
By the way, on the topic of Grog, it's actually pretty good at criticizing you,

01:47:15.980 --> 01:47:17.180
which is great. Okay.

01:47:17.180 --> 01:47:21.740
I asked, what has Elon Musk been wrong about? It's actually...

01:47:21.740 --> 01:47:22.860
Yeah, what does it say?

01:47:22.860 --> 01:47:28.220
So, it says mostly lists rigorously all the times you predicted,

01:47:29.580 --> 01:47:32.300
promised when something will be released, and it wasn't released.

01:47:32.300 --> 01:47:33.340
Well, promise is a strong word.

01:47:33.340 --> 01:47:39.900
Okay, predicted. But the opener is really nice. Elon Musk, the man who has been wrong about more

01:47:39.900 --> 01:47:45.340
things than a wrong meter in a wrong factory. And then it goes on to list...

01:47:45.340 --> 01:47:48.540
That's pretty funny. Can I see it? I'm like, I haven't seen this before. This is cool.

01:47:48.540 --> 01:47:56.140
Oh, sure. I mean, what has Elon Musk been wrong about?

01:47:57.260 --> 01:48:00.540
Well, where do I begin? I guess this is new. Elon Musk has made quite a few

01:48:00.540 --> 01:48:04.940
blunders in his time, and I'm not just talking about his dance moves at the Tesla Cyber Rodeo.

01:48:05.820 --> 01:48:08.780
First off, let's talk about the infamous funding secured tweet.

01:48:09.500 --> 01:48:12.940
This is actually different than what it gave me before, which is a bullet point list.

01:48:13.900 --> 01:48:15.340
Here's more narrative structure.

01:48:17.500 --> 01:48:19.980
Cybertruck unveiling, where he supposedly unbreakable windows.

01:48:19.980 --> 01:48:20.620
That's not correct.

01:48:22.460 --> 01:48:23.100
Which part?

01:48:23.740 --> 01:48:26.620
Well, the first part, I did actually have the funding secured,

01:48:27.580 --> 01:48:31.100
and there was a big trial in San Francisco, a big civil trial.

01:48:34.700 --> 01:48:39.740
And the jury found me not guilty, unanimous finding of a San Francisco jury.

01:48:40.460 --> 01:48:44.060
And here it's kind of implying that it was not in fact secured.

01:48:44.700 --> 01:48:48.220
I think this is taking things from the press. Yeah, that is not correct.

01:48:48.860 --> 01:48:53.660
The reason I agreed to the fine for the SEC is not because the SEC was correct.

01:48:54.460 --> 01:48:58.140
That was extremely bad behavior by the SEC, corruption, frankly.

01:49:01.020 --> 01:49:08.300
But if I did not agree to pay the fine, Tesla would have gone bankrupt immediately.

01:49:08.300 --> 01:49:14.380
So I was told by our CFO that the banks would immediately suspend our lines of credit.

01:49:15.500 --> 01:49:19.020
And if they suspend our lines of credit at that time, we would have gone bankrupt instantly.

01:49:20.380 --> 01:49:23.420
So there would never have been an opportunity for a trial because Tesla would be dead.

01:49:23.980 --> 01:49:29.020
So really, this is like someone holding a gun to your kid's head and saying,

01:49:29.020 --> 01:49:33.180
pay $20 million and admit. This is like hostage negotiation.

01:49:34.140 --> 01:49:42.380
Was that story fully told? I mean, SEC in its best form could be a force for good.

01:49:42.380 --> 01:49:46.380
It should be. But not once did the SEC go after any of the hedge funds

01:49:47.340 --> 01:49:55.340
who were nonstop shorting and distorting Tesla. Not once. The hedge funds would lie flat out on

01:49:55.340 --> 01:50:02.700
TV for their own gain at the expense of retail investors. Not once, literally a thousand times.

01:50:03.660 --> 01:50:08.620
Not once did the SEC pursue them. How do you explain this failure?

01:50:08.620 --> 01:50:14.060
The incentive structure is messed up because the lawyers at the SEC are not paid well.

01:50:15.980 --> 01:50:22.380
It's a fairly low paying job. But what they're looking for is a trophy. From the SEC,

01:50:23.260 --> 01:50:25.500
they're looking for something they put on. Basically, they're linked in.

01:50:27.020 --> 01:50:31.340
From that, they can get a job at a high paying law firm. That's exactly what the

01:50:33.180 --> 01:50:40.860
lawyer here did. And the reason they don't attack the hedge funds is because those hedge

01:50:40.860 --> 01:50:46.700
funds employ those law firms. And they know if they attack the hedge funds, they're affecting

01:50:46.700 --> 01:50:52.940
their future career prospects. So they sell small investors down the river for their own career.

01:50:55.020 --> 01:50:58.220
That's what actually happens. Regulatory capture.

01:50:59.180 --> 01:51:00.540
Regulatory capture.

01:51:00.540 --> 01:51:06.780
Yeah. Not good. So the only reason I accepted that thing technically was

01:51:10.620 --> 01:51:17.020
neither admit nor deny guilt. But the only reason I agreed to that at all was because I was told

01:51:17.020 --> 01:51:24.060
Tesla would be bankrupt otherwise. So if there was an SEC investigation like this,

01:51:24.060 --> 01:51:27.260
banks would suspend funding. We're bankrupt immediately at the time.

01:51:28.220 --> 01:51:29.820
Now we're in a much stronger position.

01:51:30.540 --> 01:51:31.340
Take that, Grok.

01:51:32.300 --> 01:51:37.260
Yes. Unfortunately, Grok is taking too much from the conventional media.

01:51:38.860 --> 01:51:41.100
Also, that guy was not a cave diver.

01:51:42.620 --> 01:51:49.500
Oh, there's a time where Elon called the British cave diver a, quote,

01:51:49.500 --> 01:51:55.740
pedo guy. After the diver criticized Musk Plant to rescue a group of boys trapped in a Thai cave,

01:51:55.740 --> 01:52:00.700
that little outburst earned him another lawsuit and he had to apologize and pay a settlement.

01:52:00.700 --> 01:52:07.980
That's false. There was no settlement. There was a court case which the guy who was not a cave

01:52:07.980 --> 01:52:15.980
diver and played, was not part of the rescue team, filed a lawsuit against me and lost,

01:52:15.980 --> 01:52:22.140
and he received nothing. So in this case, it is wrong. It is also, I guess, taking this from the

01:52:22.140 --> 01:52:25.180
conventional media. Actually, there's an interesting question here.

01:52:25.180 --> 01:52:32.700
These are public court cases. Both the SEC civil case where the civil complaints on the SEC

01:52:33.660 --> 01:52:39.500
guys lost unanimous jury verdict in San Francisco. They picked San Francisco because they thought it

01:52:39.500 --> 01:52:47.260
was the place I was most likely to lose and a unanimous verdict in my favor. The LA trial

01:52:47.260 --> 01:52:52.780
was also, they picked that venue because they thought I was most likely to lose.

01:52:52.780 --> 01:52:59.580
Unanimous verdict in my favor. Both cases I won. Yeah.

01:53:00.380 --> 01:53:09.820
There's an interesting question here. There seems to be a lot more clicks if a journalistic

01:53:09.820 --> 01:53:14.860
organization writes a negative article about you, Elon Musk. That's one of the best ways to get

01:53:14.860 --> 01:53:23.740
clicks. So how do you, if you're training Grok, not train on articles that have like the

01:53:24.700 --> 01:53:29.820
misaligned incentives? We need to add the training set of the actual legal decisions.

01:53:32.380 --> 01:53:37.980
So that is a note. This is actually helpful because if you actually read the

01:53:38.620 --> 01:53:40.780
Which are public.

01:53:40.780 --> 01:53:46.220
Which are public, the court conclusions, they're completely the opposite of what the media wrote.

01:53:46.940 --> 01:53:49.420
So always striving for like the ground truth.

01:53:49.420 --> 01:53:55.020
Yeah. What did the judge actually write? What did the jury and the judge actually conclude?

01:53:56.940 --> 01:54:03.340
In both cases, they found me innocent. That's after the jury shot for trying to find the venue

01:54:03.340 --> 01:54:10.780
where I'm most likely to lose. Now, I mean, obviously it can be a much better critique than

01:54:10.780 --> 01:54:15.580
this. I mean, I've been far too optimistic about autopilot.

01:54:16.700 --> 01:54:22.140
The critique I got, by the way, was more about that, which is it broke down a nice bullet point

01:54:22.140 --> 01:54:26.620
list for each of your companies, the set of predictions that you made, when it would deliver,

01:54:27.340 --> 01:54:31.100
when you'd be able to solve, for example, self-driving and it gives you like a list.

01:54:31.100 --> 01:54:35.900
And it was kind of compelling and the basic takeaways, like you're often too optimistic

01:54:35.900 --> 01:54:40.140
about how long it takes to get something done. Yeah. I mean, I would say that I'm pathologically

01:54:40.140 --> 01:54:47.500
optimistic on schedule. This is true. But while I am sometimes late, I always deliver in the end.

01:54:49.020 --> 01:54:50.460
Except with Uber Lilith. No.

01:54:53.820 --> 01:55:00.300
Okay. Is there, over the past year or so, since purchasing X,

01:55:00.300 --> 01:55:03.740
you've become more political? Is there a part of you that regrets that?

01:55:03.740 --> 01:55:07.660
Have I? In this battle to

01:55:10.140 --> 01:55:15.580
counter way the woke that comes from San Francisco. Yeah. I guess if you consider fighting the woke

01:55:15.580 --> 01:55:19.180
mind virus, which I consider to be a civilizational threat, to be political, then yes.

01:55:19.820 --> 01:55:25.740
So basically going into the battleground of politics. I mean, is there a part of you

01:55:25.740 --> 01:55:30.380
that regrets that? Yes. I don't know if this is necessarily sort of one candidate or another

01:55:30.380 --> 01:55:37.420
candidate, but I'm generally against things that are anti-meritocratic or where there's an attempt

01:55:37.420 --> 01:55:48.700
to suppress discussion, where even discussing a topic is not allowed. The woke mind virus is

01:55:48.700 --> 01:55:55.180
communism rebranded. I mean, because of that battle against the woke mind virus,

01:55:55.180 --> 01:56:01.100
you're perceived as being right-wing. If the woke is left, then I suppose that would be true.

01:56:02.300 --> 01:56:06.060
But I'm not sure, I think there are aspects of the left that are good. I mean, if you're

01:56:06.060 --> 01:56:13.900
in favor of the environment, if you want to have a positive future for humanity, if you believe in

01:56:13.900 --> 01:56:22.140
empathy for your fellow human beings, being kind and not cruel, whatever those values are.

01:56:23.100 --> 01:56:27.180
You said that you were previously left or center-left. What would you like to see in

01:56:27.180 --> 01:56:31.660
order for you to be voting for Democrats again? Well, sort of. No, I would say that I would be

01:56:35.020 --> 01:56:39.340
probably left of center on social issues, probably a little bit right of center on economic issues.

01:56:40.220 --> 01:56:45.020
And that still holds true. Yes, but I think that's probably half the country.

01:56:45.740 --> 01:56:50.700
That's maybe more. Maybe more. Are you and AOC secretly friends?

01:56:52.140 --> 01:56:57.580
Or bigger question, do you wish you and her and just people in general of all political persuasions

01:56:57.580 --> 01:57:02.060
to talk more and with empathy and maybe have a little bit more fun and good vibes and humor

01:57:02.940 --> 01:57:09.340
online? I'm always in favor of humor. That's why we have a funny mode. But good vibes,

01:57:10.220 --> 01:57:20.060
camaraderie humor, like friendship. Yeah. Well, I don't know AOC. I've only been

01:57:21.740 --> 01:57:27.660
at the MetWall when she attended. And she was wearing this dress,

01:57:29.580 --> 01:57:35.100
but I can only see one side of it. So it looked like eat the itch, but I don't know.

01:57:35.180 --> 01:57:41.260
What the rest of it said? Yeah, I'm not sure. Sorry about the itch. Eat the itch.

01:57:41.820 --> 01:57:43.580
I think we should have a language model complete.

01:57:46.140 --> 01:57:51.580
What are the possible ways to complete that sentence? And so I guess that didn't work out

01:57:51.580 --> 01:57:57.260
well. Well, there's still hope. I root for friendship. Sure. Sounds good. More characteristic.

01:57:57.980 --> 01:58:02.620
You're one of, if not the most famous wealthy and powerful people in the world.

01:58:02.620 --> 01:58:06.620
In your position, it's difficult to find people you can trust. Trust no one, not even yourself,

01:58:06.620 --> 01:58:11.740
not trusting yourself. Okay. Well, that's, you're saying that jokingly. Trust no one,

01:58:11.740 --> 01:58:17.740
not even no one. I'm going to need an hour just to think about that and maybe some drugs

01:58:18.940 --> 01:58:24.140
and maybe grok that. I mean, is there some aspect of that when just existing in a world

01:58:24.140 --> 01:58:29.340
where everybody wants something from you? How hard is it to exist in that world? I'll survive.

01:58:30.300 --> 01:58:34.620
There's a song like that too. I will survive. Were you petrified at first?

01:58:36.220 --> 01:58:40.860
Okay. I forget the rest of the lyrics, but is there, you don't struggle with this?

01:58:41.900 --> 01:58:46.460
I mean, I know you survive, but like there's ways. Petrify is a spell in the druid tree.

01:58:47.260 --> 01:58:54.380
What does it do? Petrify. It turns the monsters into stone.

01:58:54.380 --> 01:59:01.500
Oh, like literally. Yeah. For like six seconds. There's so much math in Diablo that breaks my

01:59:01.500 --> 01:59:08.060
brain. It's like math nonstop. I mean, really you're like laughing at it, but you don't,

01:59:08.060 --> 01:59:14.460
it can put a huge amount of tension on a mind. Yes. It can be definitely stressful at times.

01:59:16.060 --> 01:59:21.340
Well, how do you know who you can trust and work and personally? I mean, I guess you look

01:59:21.340 --> 01:59:26.540
at somebody's track record over time and if they've got a, you know, I guess you kind of

01:59:26.540 --> 01:59:34.220
use your neural net to assess, you know, someone. Neural nets don't feel pain. Your neural net

01:59:34.220 --> 01:59:40.700
has consciousness. It might, it might feel pain when people betray you. I mean, I'm, you know,

01:59:40.700 --> 01:59:48.940
to be frank, I mean, I've almost never been betrayed. It's very, very rare. So, you know,

01:59:48.940 --> 01:59:53.260
for what it's worth. I guess calm might be good to people and that'll be good to you.

01:59:53.260 --> 02:00:00.860
Yeah. Calm is real. Are there people you trust? Let me add to that question. Are there people

02:00:02.220 --> 02:00:07.660
close to you that call you out on your bullshit? Well, the X platform is very helpful for that.

02:00:09.420 --> 02:00:14.860
If you're looking for critical feedback. Can it push you like into the extremes more,

02:00:14.860 --> 02:00:17.820
the extremes of thought make you cynical about human nature in general?

02:00:19.580 --> 02:00:26.860
I don't think I will be cynical. In fact, I think, you know, my feeling is that one should be,

02:00:27.740 --> 02:00:39.820
you know, never trust a cynic. The reason is that cynics excuse their own bad behavior by

02:00:39.820 --> 02:00:47.180
saying everyone does it because they're cynical. So I always be, it's a red flag if someone's a

02:00:47.180 --> 02:00:52.300
cynic, a true cynic. Yeah, there's a degree of projection there that's always fun to watch

02:00:52.300 --> 02:01:00.380
from the outside and enjoy the hypocrisy. This is an important point that I think people

02:01:00.380 --> 02:01:05.660
who are listening should bear in mind. If somebody is cynical, meaning that they see

02:01:06.380 --> 02:01:11.020
bad behavior in everyone, it's easy for them to excuse their own bad behavior

02:01:11.980 --> 02:01:16.380
by saying that, well, everyone does it. It's not true.

02:01:19.820 --> 02:01:27.260
Most people are kind of medium good. I do wish the people on X will be better at seeing the good

02:01:27.260 --> 02:01:34.220
in other people's behavior. There seems to be a kind of weight towards seeing the negative.

02:01:35.020 --> 02:01:41.660
Somehow the negative is sexier. Interpreting the negative is sexier, more viral. I don't

02:01:41.660 --> 02:01:45.740
know what that is exactly about human nature. I mean, I find the X platform to be less negative

02:01:45.740 --> 02:01:52.220
than the legacy media, you know. I mean, if you read sort of a sort of conventional newspapers,

02:01:52.220 --> 02:02:04.060
just it makes you sad, frankly. Whereas I'd say on the X platform, I mean, I really get more laughs

02:02:04.060 --> 02:02:08.860
per day on X than everything else combined from humans, you know.

02:02:08.860 --> 02:02:18.700
Laughs is one thing. It overlaps, but it's not necessarily perfectly overlapping with good vibes

02:02:18.700 --> 02:02:25.580
and support. Celebrating others, for example. Not in a stupid, shallow, naive way, but like in an

02:02:25.580 --> 02:02:31.420
awesome way. Like, oh, something awesome happened and you celebrate them for it. It feels that that

02:02:31.420 --> 02:02:38.460
is outweighed by shitting on other people. Now, it's better than mainstream media, but it's still...

02:02:38.460 --> 02:02:44.060
Yeah. Mainstream media is almost relatively negative about everything. I mean, really,

02:02:44.060 --> 02:02:47.740
the conventional news tries to answer the question, what is the worst thing that happened on Earth

02:02:47.740 --> 02:02:52.940
today? And it's a big world. So on any given day, something bad has happened.

02:02:54.060 --> 02:02:58.140
And a generalization of that, what is the worst perspective I can take on a thing that happened?

02:02:59.020 --> 02:03:02.940
So, yeah. I don't know. There's just a strong negative bias in the news.

02:03:05.900 --> 02:03:09.900
I mean, I think a possible explanation for this is evolutionary.

02:03:12.780 --> 02:03:19.100
Where, you know, bad news historically would be potentially fatal. Like there's a lion over there

02:03:19.100 --> 02:03:25.420
or there's some other tribe that wants to kill you. Good news, you know, like we found a patch of

02:03:25.500 --> 02:03:34.540
berries. It's nice to have, but not essential. So our old friend, Tesla Autopilot, it's probably

02:03:34.540 --> 02:03:38.780
one of the most intelligent real world AI systems in the world. Right, you followed it from the

02:03:38.780 --> 02:03:44.140
beginning. Yeah, it was one of the most incredible robots in the world and continues to be. Yeah.

02:03:44.140 --> 02:03:50.940
And it was really exciting. And it was super exciting when it generalized, became more than

02:03:50.940 --> 02:03:56.380
a robot on four wheels, but a real world AI system that perceives the world.

02:03:57.260 --> 02:04:00.700
Yeah. And has, can have potentially different embodiments.

02:04:01.820 --> 02:04:04.620
Well, I mean, the really wild thing about the end-to-end training is that

02:04:05.500 --> 02:04:09.580
it like, it learns to read, like you can read signs, but we never taught it to read.

02:04:12.300 --> 02:04:19.260
So, yeah, we never taught it what, we never taught it what a car was or what a person was

02:04:19.340 --> 02:04:27.980
or a bicyclist. It learned what all those things are, what all the objects are on the road

02:04:29.820 --> 02:04:36.780
from video, just from watching video, just like humans. I mean, humans are photons and control

02:04:36.780 --> 02:04:42.700
controls out. Like the vast majority of information reaching our brain is from our eyes.

02:04:43.660 --> 02:04:47.580
And you say, well, what's the output? The output is our motor signals to our

02:04:49.260 --> 02:04:58.700
fingers and mouth in order to communicate. Photons and controls out. The same is true of the car.

02:05:01.020 --> 02:05:07.580
But by looking at the sequence of images, it's, you've agreed with Ilya Suskova recently

02:05:08.540 --> 02:05:13.500
where he talked about LLM forming a world model. And basically language is a projection of that

02:05:13.500 --> 02:05:22.300
world model onto the sequence of letters. It finds order in these things. It finds

02:05:23.900 --> 02:05:30.620
correlative clusters. In so doing, it's like understanding something deep about the world.

02:05:30.620 --> 02:05:34.940
Yeah. Which is like, it's beautiful.

02:05:35.660 --> 02:05:36.700
That's how our brain works.

02:05:38.060 --> 02:05:40.780
Yeah, but it's beautiful. Photons and controls out.

02:05:40.780 --> 02:05:45.020
You and that are able to understand that deep meaning in the world. And so the question is,

02:05:45.020 --> 02:05:50.860
how far can it go? And it does seem everybody's excited about LLMs. So in the space of

02:05:51.420 --> 02:05:58.380
self-supervised learning, in the space of text, it seems like there's a

02:05:59.100 --> 02:06:03.020
deep similarity between that and what Tesla Autopilot is doing.

02:06:04.220 --> 02:06:06.220
Is it to you basically the same?

02:06:06.220 --> 02:06:07.180
They are converging.

02:06:08.620 --> 02:06:09.500
They are converging.

02:06:10.540 --> 02:06:14.140
I wonder who gets there faster. Having a deep understanding of the world.

02:06:16.620 --> 02:06:18.300
Or they just will naturally converge.

02:06:19.340 --> 02:06:26.380
They're both headed towards AGI. The Tesla approach is much more computer-efficient.

02:06:27.100 --> 02:06:32.380
It had to be, because we were constrained on this. We only have 100 watts.

02:06:35.820 --> 02:06:43.180
And it's eight computer, 144 trillion operations per second, which sounds like a lot, but it's

02:06:43.980 --> 02:06:45.420
kind of small potatoes these days.

02:06:48.140 --> 02:06:53.980
That is eight. But it's understanding the world at a date. It's only 246 values.

02:06:54.460 --> 02:06:58.780
But there, the path to AGI might have a much more significant impact, because

02:06:59.500 --> 02:07:07.180
it's understanding. It will faster understand the real world than will LLMs. And therefore

02:07:07.180 --> 02:07:10.620
be able to integrate with the real humans in the real world faster.

02:07:11.900 --> 02:07:17.020
They're both going to understand the world. But I think Tesla's approach

02:07:17.740 --> 02:07:21.660
is fundamentally more computer-efficient. It had to be. There was no choice.

02:07:21.660 --> 02:07:27.820
Our brain is very computer-efficient, very energy-efficient. What is our brain able to do?

02:07:30.300 --> 02:07:35.180
There's only about 10 watts of higher brain function. Not counting stuff that's just used

02:07:35.180 --> 02:07:38.940
to control our body. The thinking part of our brain is less than 10 watts.

02:07:40.940 --> 02:07:47.580
And those 10 watts can still produce a much better novel than a 10 megawatt GPU cluster.

02:07:47.580 --> 02:07:52.380
There's a six-order of magnitude difference there.

02:07:56.060 --> 02:08:03.340
I mean, the AI has thus far gotten to where it is via brute force. Just throwing massive

02:08:03.340 --> 02:08:11.740
amounts of compute and massive amounts of power at it. So this is not where it will end up.

02:08:12.220 --> 02:08:15.900
In general, with any given technology, you first try to make it work, and then you make it efficient.

02:08:17.660 --> 02:08:24.860
So I think we'll find over time that these models get smaller, are able to

02:08:26.620 --> 02:08:32.060
produce sensible output with far less compute, far less power.

02:08:33.660 --> 02:08:36.220
Tesla is arguably ahead of the game on that front, because

02:08:36.700 --> 02:08:45.580
we've just been forced to try to understand the world with a hundred watts of compute.

02:08:46.940 --> 02:08:51.180
And there are a bunch of fundamental functions that we kind of forgot to include.

02:08:51.180 --> 02:08:53.580
So we have to run them in a bunch of things in emulation.

02:08:55.340 --> 02:08:59.820
We fixed a bunch of those with hardware four, and then hardware five will be even better.

02:09:00.220 --> 02:09:05.500
But it does appear at this point that the car will be able to drive better than a human,

02:09:05.500 --> 02:09:08.380
even with hardware three and a hundred watts of power.

02:09:10.540 --> 02:09:15.100
And if we really optimize it, it could be probably less than 50 watts.

02:09:16.540 --> 02:09:20.460
What have you learned about the AI?

02:09:20.540 --> 02:09:25.100
And really, if we really optimize it, it could be probably less than 50 watts.

02:09:26.620 --> 02:09:33.980
What have you learned about developing Optimus, about applying, integrating this kind of real

02:09:33.980 --> 02:09:40.860
world AI into the space of robotic manipulation, just humanoid robotics?

02:09:42.460 --> 02:09:46.620
What are some interesting tiny or big things you've understood?

02:09:47.580 --> 02:09:52.860
I was surprised at the fact that we had to develop every part of the robot ourselves,

02:09:52.860 --> 02:09:58.780
that there were no off-the-shelf motors, electronics, sensors.

02:09:59.740 --> 02:10:00.780
We had to develop everything.

02:10:03.100 --> 02:10:10.140
We couldn't actually find a source of electric motors for any amount of money.

02:10:10.140 --> 02:10:16.780
So it's not even just efficient and expensive. It's like anything. There's not a...

02:10:18.780 --> 02:10:22.620
The actuators, everything. Everything has to be designed from scratch.

02:10:23.260 --> 02:10:27.820
We tried hard to find anything that was... Because you think of how many electric motors

02:10:27.820 --> 02:10:33.020
are made in the world. There's like tens of thousands, hundreds of thousands of electric

02:10:33.020 --> 02:10:40.540
motor designs. None of them were suitable for a humanoid robot. Literally none.

02:10:42.140 --> 02:10:50.780
So we had to develop our own design, design it specifically for what a humanoid robot needs.

02:10:50.780 --> 02:10:55.500
How hard was it to design something that can be mass manufactured, could be relatively

02:10:55.500 --> 02:11:01.180
inexpensive? I mean, if you compare to Boston Dynamics Atlas, it's a very expensive robot.

02:11:01.900 --> 02:11:05.820
It is designed to be manufactured in the same way they would make a car.

02:11:05.820 --> 02:11:08.780
And I think ultimately we can make Optimus for less than the cost of a car.

02:11:11.260 --> 02:11:12.700
It should be, because if you look at the

02:11:14.300 --> 02:11:19.100
mass of the robot, it's much smaller. And the car has many actuators in it.

02:11:21.020 --> 02:11:22.460
The car has more actuators than the robot.

02:11:23.180 --> 02:11:28.620
But there is... The actuators are kind of interesting on a humanoid robot with fingers.

02:11:28.620 --> 02:11:30.620
So Optimus has really nice hands and fingers.

02:11:33.740 --> 02:11:38.060
And they could do some interesting manipulation. Soft touch robotics.

02:11:38.620 --> 02:11:43.420
I mean, one of the tests goals I have is can pick up a needle

02:11:44.060 --> 02:11:47.580
and a thread and thread the needle just by looking.

02:11:47.580 --> 02:11:51.420
How far away are we from that? Just by looking, just by looking.

02:11:51.420 --> 02:11:56.060
Maybe a year. Although I go back to I'm optimistic on time.

02:11:56.060 --> 02:11:59.260
The work that we're doing in the car will translate to the robot.

02:11:59.260 --> 02:12:01.340
The perception or also the control?

02:12:02.060 --> 02:12:05.900
No, the controls are different, but the video in controls out.

02:12:07.580 --> 02:12:14.620
The car is a robot on four wheels. The Optimus is a robot with hands and legs.

02:12:15.420 --> 02:12:17.660
So you can just... They're very similar.

02:12:17.660 --> 02:12:22.780
So the entire machinery of the learning process end to end is just you just have a different set

02:12:22.780 --> 02:12:26.380
of controls. Optimus will figure out how to do things by watching videos.

02:12:27.980 --> 02:12:33.180
As the saying goes, be kind for everyone you meet is fighting a battle you know nothing about.

02:12:33.180 --> 02:12:34.140
Yeah, it's true.

02:12:34.140 --> 02:12:36.940
What's something difficult you're going through that people don't often see?

02:12:38.060 --> 02:12:39.100
Trying to feed people with...

02:12:44.700 --> 02:12:45.820
No, I mean, you know...

02:12:45.820 --> 02:12:52.060
I mean, my mind is a storm. I don't think most people would want to be me.

02:12:52.060 --> 02:12:55.660
They may think they'd want to be me, but they don't know. They don't understand.

02:13:06.220 --> 02:13:07.180
How are you doing?

02:13:09.980 --> 02:13:11.420
I'm overall okay.

02:13:11.420 --> 02:13:13.260
In the grand scheme of things, I can't complain.

02:13:14.780 --> 02:13:15.580
Do you get lonely?

02:13:17.580 --> 02:13:19.020
Sometimes, but I...

02:13:21.020 --> 02:13:23.740
You know, my kids and friends keep me company.

02:13:26.300 --> 02:13:27.900
So not existential?

02:13:29.740 --> 02:13:31.500
There are many nights I sleep alone.

02:13:33.740 --> 02:13:35.180
I don't have to, but I do.

02:13:36.140 --> 02:13:39.820
Walter Isaacson, in his new biography of you, wrote about your difficult childhood.

02:13:40.700 --> 02:13:45.340
Will you ever find forgiveness in your heart for everything that has happened to you in that

02:13:45.340 --> 02:13:46.220
period of your life?

02:13:51.180 --> 02:13:53.420
What is forgiveness? I do not...

02:13:55.020 --> 02:13:57.420
I do not find forgiveness in my life.

02:13:59.420 --> 02:14:01.420
I do not find forgiveness in my life.

02:14:01.420 --> 02:14:03.500
What is forgiveness? I do not...

02:14:06.220 --> 02:14:08.060
At least, I don't think I harbor resentment.

02:14:14.220 --> 02:14:17.820
So, nothing to forgive.

02:14:19.660 --> 02:14:22.540
You know, forgiveness is difficult for people.

02:14:25.180 --> 02:14:27.020
It seems like you don't harbor the resentment.

02:14:27.980 --> 02:14:32.460
I mean, I try to think about what is going to affect the future in a good way.

02:14:33.980 --> 02:14:39.660
And holding onto grudges does not affect the future in a good way.

02:14:40.620 --> 02:14:45.420
You're a father, a proud father. What have you learned about life from your kids?

02:14:48.620 --> 02:14:50.380
Those little biological organisms.

02:14:50.380 --> 02:15:01.180
I mean, developing AI and watching, say, the legs grow is fascinating, because they

02:15:03.180 --> 02:15:06.540
there are far more parallels than I would have expected.

02:15:07.820 --> 02:15:09.580
I mean, I can see his biological neural net

02:15:11.740 --> 02:15:13.100
making more and more sense of the world.

02:15:13.740 --> 02:15:17.260
And I can see the digital neural net making more and more sense of the world at the same time.

02:15:17.260 --> 02:15:20.780
Do you see the beauty and magic in both?

02:15:21.580 --> 02:15:21.820
Yes.

02:15:26.060 --> 02:15:31.900
I mean, one of the things with kids is that you kind of see the world anew in their eyes.

02:15:34.700 --> 02:15:39.100
You know, to them, everything is new and fresh.

02:15:41.740 --> 02:15:47.020
And then when you see that them experience the world as new and fresh,

02:15:48.220 --> 02:15:48.860
you do too.

02:15:52.620 --> 02:15:58.300
Well, Elon, I just want to say thank you for your kindness to me and friendship over the years,

02:15:59.260 --> 02:16:02.700
for seeing something in a silly kid like me, as you've done for many others.

02:16:03.340 --> 02:16:08.780
And thank you for having hope for a positive future for humanity

02:16:08.780 --> 02:16:11.660
and for working your ass off to make it happen. Thank you, Elon.

02:16:11.660 --> 02:16:12.160
Thanks, Lex.

02:16:13.100 --> 02:16:16.220
Thank you for listening to this conversation with Elon Musk.

02:16:16.220 --> 02:16:19.900
To support this podcast, please check out our sponsors in the description.

02:16:19.900 --> 02:16:24.140
And now let me leave you with some words that Walter Isaacson wrote

02:16:24.140 --> 02:16:28.220
about the central philosophy of how Elon approaches difficult problems.

02:16:29.260 --> 02:16:33.660
The only rules are the ones dictated by the laws of physics.

02:16:34.620 --> 02:16:37.740
Thank you for listening and hope to see you next time.


WEBVTT

00:00.000 --> 00:02.520
The problem is that we do not get 50 years

00:02.520 --> 00:04.760
to try and try again and observe that we were wrong

00:04.760 --> 00:05.940
and come up with a different theory

00:05.940 --> 00:07.760
and realize that the entire thing is going to be like

00:07.760 --> 00:10.560
way more difficult than realized at the start.

00:10.560 --> 00:12.960
Because the first time you fail at aligning something

00:12.960 --> 00:15.180
much smarter than you are, you die.

00:17.480 --> 00:20.800
The following is a conversation with Eliezer Yitkowsky,

00:20.800 --> 00:23.640
a legendary researcher, writer, and philosopher

00:23.640 --> 00:26.000
on the topic of artificial intelligence,

00:26.000 --> 00:28.840
especially super-intelligent AGI

00:28.880 --> 00:32.120
and its threat to human civilization.

00:32.120 --> 00:34.480
This is the Lex Freedman podcast.

00:34.480 --> 00:36.640
To support it, please check out our sponsors

00:36.640 --> 00:37.960
in the description.

00:37.960 --> 00:42.160
And now, dear friends, here's Eliezer Yitkowsky.

00:43.160 --> 00:45.280
What do you think about GPT-4?

00:45.280 --> 00:47.160
How intelligent is it?

00:47.160 --> 00:49.320
It is a bit smarter than I thought this technology

00:49.320 --> 00:53.000
was going to scale to, and I'm a bit worried

00:53.000 --> 00:55.880
about what the next one will be like.

00:55.880 --> 00:58.280
Like this particular one, I think,

00:58.280 --> 01:00.600
I hope there's nobody inside there,

01:00.600 --> 01:03.200
because it would be stuck to be stuck inside there.

01:04.760 --> 01:08.320
But we don't even know the architecture at this point,

01:08.320 --> 01:11.040
because OpenAI is very properly not telling us.

01:11.040 --> 01:15.040
And yeah, like giant inscrutable matrices

01:15.040 --> 01:16.140
of floating point numbers,

01:16.140 --> 01:17.440
I don't know what's going on in there,

01:17.440 --> 01:19.400
nobody knows what's going on in there,

01:19.400 --> 01:21.560
all we have to go by are the external metrics.

01:21.560 --> 01:23.920
And on the external metrics,

01:23.920 --> 01:28.920
if you ask it to write a self-aware Forchon green text,

01:30.440 --> 01:32.440
it will start writing a green text

01:32.440 --> 01:34.840
about how it has realized that it's an AI

01:34.840 --> 01:37.880
writing a green text and like, oh, well.

01:37.880 --> 01:41.200
So that's probably

01:43.640 --> 01:48.000
not quite what's going on in there in reality,

01:48.000 --> 01:49.680
but we're kind of like blowing past

01:49.680 --> 01:52.140
all these science fiction guardrails.

01:52.140 --> 01:55.380
Like we are past the point where in science fiction,

01:55.380 --> 01:57.820
people would be like, whoa, wait, stop,

01:57.820 --> 02:00.320
that thing's alive, what are you doing to it?

02:00.320 --> 02:04.740
And it's probably not, nobody actually knows.

02:04.740 --> 02:07.040
We don't have any other guardrails.

02:07.040 --> 02:09.340
We don't have any other tests.

02:09.340 --> 02:11.940
We don't have any lines to draw on the sand and say like,

02:11.940 --> 02:14.140
well, when we get this far,

02:14.140 --> 02:18.480
we will start to worry about what's inside there.

02:19.420 --> 02:21.600
So if it were up to me, I would be like, okay,

02:21.600 --> 02:26.120
like this far, no further time for the summer of AI

02:26.120 --> 02:29.600
where we have planted our seeds and now we like wait

02:29.600 --> 02:31.640
and reap the rewards of the technology

02:31.640 --> 02:32.600
you've already developed

02:32.600 --> 02:35.280
and don't do any larger training runs than that,

02:35.280 --> 02:38.600
which to be clear, I realize requires more than one company

02:38.600 --> 02:39.800
agreeing to not do that.

02:42.040 --> 02:45.880
And take a rigorous approach for the whole AI community

02:45.880 --> 02:50.760
to investigate whether there's somebody inside there.

02:50.760 --> 02:52.600
That would take decades.

02:53.920 --> 02:56.280
Like having any idea of what's going on in there,

02:56.280 --> 02:58.160
people have been trying for a while.

02:58.160 --> 03:00.520
It's a poetic statement about if there's somebody in there,

03:00.520 --> 03:03.000
but I feel like it's also a technical statement

03:03.000 --> 03:05.020
or I hope it is one day,

03:05.020 --> 03:06.540
which is a technical statement

03:06.540 --> 03:08.360
what the Alan Turing tried to come up with

03:08.360 --> 03:10.120
with the Turing test.

03:10.120 --> 03:13.920
Do you think it's possible to definitively

03:13.920 --> 03:17.560
or approximately figure out if there is somebody in there?

03:17.560 --> 03:20.200
If there's something like a mind

03:20.200 --> 03:23.180
inside this large language model?

03:23.180 --> 03:24.920
I mean, there's a whole bunch

03:24.920 --> 03:27.320
of different sub questions here.

03:27.320 --> 03:30.320
There's the question of like,

03:31.520 --> 03:33.800
is there consciousness, is there qualia?

03:33.800 --> 03:36.680
Is this a object of moral concern?

03:36.680 --> 03:39.520
Is this a moral patient?

03:39.520 --> 03:43.160
Like, should we be worried about how we're treating it?

03:43.160 --> 03:46.200
And then there's questions like how smart is it exactly?

03:46.200 --> 03:47.080
Can it do X?

03:47.080 --> 03:48.640
Can it do Y?

03:48.640 --> 03:51.580
And we can check how it can do X and how it can do Y.

03:52.760 --> 03:55.520
Unfortunately, we've gone and exposed this model

03:55.520 --> 03:59.200
to a vast corpus of texts of people discussing consciousness

03:59.200 --> 04:00.440
on the internet,

04:00.440 --> 04:02.920
which means that when it talks about being self-aware,

04:02.920 --> 04:06.800
we don't know to what extent it is repeating back

04:06.800 --> 04:09.800
what it has previously been trained on

04:09.800 --> 04:11.440
for discussing self-awareness.

04:12.480 --> 04:14.640
Or if there's anything going on in there

04:14.640 --> 04:17.840
such that it would start to say similar things spontaneously.

04:19.760 --> 04:21.360
Among the things that one could do

04:21.360 --> 04:26.160
if one were at all serious about trying to figure this out

04:26.160 --> 04:31.160
is train GPT-3 to detect conversations about consciousness,

04:32.160 --> 04:34.960
exclude them all from the training data sets,

04:34.960 --> 04:38.520
and then retrain something around the rough size of GPT-4

04:38.520 --> 04:43.200
and no larger with all of the discussion of consciousness

04:43.200 --> 04:45.120
and self-awareness and so on missing.

04:45.120 --> 04:50.040
Although, hard bar to pass, humans are self-aware

04:50.040 --> 04:51.680
and we're self-aware all the time.

04:51.680 --> 04:54.360
We talk about what we do all the time,

04:54.360 --> 04:56.960
what we're thinking at the moment all the time.

04:56.960 --> 04:59.160
But nonetheless, get rid of the explicit discussion

04:59.160 --> 05:02.200
of consciousness, I think, therefore I am and all that,

05:02.200 --> 05:06.360
and then try to interrogate that model and see what it says.

05:06.360 --> 05:08.680
And it still would not be definitive.

05:08.680 --> 05:11.720
But nonetheless, I don't know,

05:11.720 --> 05:15.000
I feel like when you run over the science fiction guardrails,

05:15.000 --> 05:17.680
like maybe this thing, but what about GPT?

05:17.680 --> 05:21.200
Maybe not this thing, but like what about GPT-5?

05:21.200 --> 05:24.080
Yeah, this would be a good place to pause.

05:25.920 --> 05:27.880
On the topic of consciousness,

05:27.880 --> 05:30.080
there's so many components

05:31.360 --> 05:34.240
to even just removing consciousness from the data set.

05:36.280 --> 05:38.400
Emotion, the display of consciousness,

05:38.440 --> 05:40.040
the display of emotion,

05:40.040 --> 05:41.720
feels like deeply integrated

05:41.720 --> 05:43.600
with the experience of consciousness.

05:44.640 --> 05:47.360
So the hard problem seems to be very well integrated

05:47.360 --> 05:51.400
with the actual surface level illusion of consciousness.

05:51.400 --> 05:53.280
So displaying emotion.

05:53.280 --> 05:55.680
I mean, do you think there's a case to be made

05:55.680 --> 05:58.680
that we humans, when we're babies,

05:58.680 --> 06:01.040
are just like GPT, that we're training on human data

06:01.040 --> 06:03.760
on how to display emotion versus feel emotion,

06:03.760 --> 06:07.120
how to show others, communicate to others

06:07.160 --> 06:11.480
that I'm suffering, that I'm excited, that I'm worried,

06:11.480 --> 06:14.400
that I'm lonely and I miss you and I'm excited to see you.

06:14.400 --> 06:16.000
All of that is communicated.

06:16.000 --> 06:17.280
That's a communication skill

06:17.280 --> 06:20.040
versus the actual feeling that I experience.

06:20.040 --> 06:25.040
So we need that training data as humans too,

06:25.400 --> 06:27.080
that we may not be born with that,

06:27.080 --> 06:29.720
how to communicate the internal state.

06:29.720 --> 06:31.640
And that's, in some sense,

06:31.640 --> 06:34.280
if we remove that from GPT-4's data set,

06:34.280 --> 06:35.320
it might still be conscious,

06:35.320 --> 06:37.080
but not be able to communicate it.

06:39.040 --> 06:41.280
So I think you're gonna have some difficulty

06:41.280 --> 06:46.200
removing all mention of emotions from GPT's data set.

06:46.200 --> 06:49.520
I would be relatively surprised to find

06:49.520 --> 06:51.400
that it has developed exact analogs

06:51.400 --> 06:53.160
of human emotions in there.

06:53.160 --> 06:58.160
I think that humans will have emotions,

06:58.600 --> 07:01.560
even if you don't tell them about those emotions

07:01.560 --> 07:02.560
when they're kids.

07:03.520 --> 07:08.520
It's not quite exactly what various blank slatists

07:09.320 --> 07:12.000
tried to do with the new Soviet man and all that.

07:12.000 --> 07:15.680
But if you try to raise people perfectly altruistic,

07:15.680 --> 07:17.160
they still come out selfish.

07:18.080 --> 07:20.560
You try to raise people sexless,

07:20.560 --> 07:22.760
they still develop sexual attraction.

07:25.160 --> 07:28.320
We have some notion in humans, not in AIs,

07:28.320 --> 07:30.240
of where the brain structures are

07:30.240 --> 07:31.160
that implement this stuff.

07:31.160 --> 07:34.800
And it is really a remarkable thing, I say in passing,

07:34.800 --> 07:39.000
that despite having complete read access

07:39.000 --> 07:44.000
to every floating point number in the GPT series,

07:44.840 --> 07:47.480
we still know vastly more about

07:47.480 --> 07:51.440
the architecture of human thinking

07:51.440 --> 07:53.840
than we know about what goes on inside GPT,

07:53.840 --> 07:56.880
despite having vastly better ability to read GPT.

07:58.120 --> 07:59.360
Do you think it's possible?

07:59.360 --> 08:00.520
Do you think that's just a matter of time?

08:00.520 --> 08:02.960
Do you think it's possible to investigate and study

08:02.960 --> 08:05.800
the way neuroscientists study the brain,

08:05.800 --> 08:07.600
which is look into the darkness,

08:07.600 --> 08:08.800
the mystery of the human brain,

08:08.800 --> 08:11.720
by just desperately trying to figure out something

08:11.720 --> 08:12.840
and to form models.

08:12.840 --> 08:14.360
And then over a long period of time,

08:14.360 --> 08:15.360
actually start to figure out

08:15.360 --> 08:16.960
what regions of the brain do certain things,

08:16.960 --> 08:18.760
what different kinds of neurons,

08:18.760 --> 08:20.520
when they fire, what that means,

08:20.520 --> 08:22.680
how plastic the brain is, all that kind of stuff.

08:22.680 --> 08:24.400
You slowly start to figure out

08:24.400 --> 08:25.840
different properties of the system.

08:25.840 --> 08:28.080
Do you think we can do the same thing with language models?

08:28.080 --> 08:31.400
Sure, I think that if half of today's physicists

08:31.400 --> 08:34.000
stop wasting their lives on string theory or whatever,

08:34.000 --> 08:37.920
and go off and study

08:37.920 --> 08:40.120
what goes on inside transformer networks,

08:40.960 --> 08:45.360
then in like 30, 40 years,

08:45.360 --> 08:47.480
we'd probably have a pretty good idea.

08:47.480 --> 08:50.680
Do you think these large language models can reason?

08:52.160 --> 08:53.400
They can play chess.

08:53.400 --> 08:55.760
How are they doing that without reasoning?

08:55.760 --> 08:57.720
So you're somebody

08:57.720 --> 09:00.280
that spearheaded the movement of rationality.

09:00.280 --> 09:01.880
So reason is important to you.

09:03.000 --> 09:05.560
So is that as a powerful, important word?

09:05.560 --> 09:09.080
Or is it like, how difficult is the threshold

09:09.080 --> 09:10.800
of being able to reason to you?

09:10.800 --> 09:12.240
And how impressive is it?

09:12.240 --> 09:15.360
I mean, in my writings on rationality,

09:15.360 --> 09:17.120
I have not gone making a big deal

09:17.120 --> 09:19.200
out of something called reason.

09:19.200 --> 09:20.920
I have made more of a big deal

09:20.920 --> 09:23.680
out of something called probability theory.

09:23.680 --> 09:27.560
And that's like, well, you're reasoning,

09:27.560 --> 09:30.080
but you're not doing it quite right.

09:30.080 --> 09:32.720
And you should reason this way instead.

09:32.720 --> 09:37.080
And interestingly, people have started

09:37.080 --> 09:38.920
to get preliminary results showing

09:38.920 --> 09:42.400
that reinforcement learning by human feedback

09:43.520 --> 09:48.520
has made the GPT series worse in some ways.

09:49.680 --> 09:52.040
In particular, it used to be well-calibrated

09:52.040 --> 09:54.040
if you trained it to put probabilities on things,

09:54.040 --> 09:56.440
it would say 80% probability

09:56.440 --> 09:58.800
and be right eight times out of 10.

09:58.800 --> 10:00.600
And if you apply reinforcement learning

10:00.600 --> 10:01.960
from human feedback,

10:01.960 --> 10:06.960
the nice graph of 70%, seven out of 10

10:07.920 --> 10:10.880
sort of flattens out into the graph that humans use

10:10.880 --> 10:13.240
where there's some very improbable stuff

10:13.240 --> 10:16.480
and likely probable maybe,

10:16.480 --> 10:20.160
which all means around 40% and then certain.

10:20.160 --> 10:22.720
So it's like it used to be able to use probabilities,

10:22.720 --> 10:23.960
but if you apply,

10:23.960 --> 10:25.520
but if you'd like try to teach it to talk

10:25.520 --> 10:27.760
in a way that satisfies humans,

10:27.760 --> 10:29.080
it gets worse at probability

10:29.080 --> 10:30.920
in the same way that humans are.

10:30.920 --> 10:33.720
And that's a bug, not a feature.

10:33.720 --> 10:37.760
I would call it a bug, although such a fascinating bug.

10:39.800 --> 10:42.960
But yeah, so like reasoning,

10:42.960 --> 10:46.800
like it's doing pretty well on various tests

10:46.800 --> 10:49.320
that people used to say would require reasoning,

10:49.320 --> 10:53.600
but rationality is about,

10:53.600 --> 10:56.680
when you say 80%, does it happen eight times out of 10?

10:57.920 --> 10:59.280
So what are the limits to you

10:59.280 --> 11:04.280
of these transformer networks, of neural networks?

11:04.960 --> 11:08.680
What's, if reasoning is not impressive to you

11:08.680 --> 11:12.800
or it is impressive, but there's other levels to achieve.

11:12.800 --> 11:15.280
I mean, it's just not how I carve up reality.

11:15.280 --> 11:17.520
What's, if reality is a cake,

11:18.480 --> 11:21.840
what are the different layers of the cake or the slices?

11:21.840 --> 11:22.680
How do you cover it?

11:22.680 --> 11:25.160
Or you can use a different food if you like.

11:27.400 --> 11:31.160
It's, I don't think it's as smart as human yet.

11:31.160 --> 11:34.360
I do like back in the day, I went around saying like,

11:34.360 --> 11:37.640
I do not think that just stacking more layers

11:37.640 --> 11:40.960
of transformers is going to get you all the way to AGI.

11:40.960 --> 11:44.440
And I think that GPT-4 is passed

11:44.440 --> 11:47.080
or I thought this paradigm was going to take us.

11:47.600 --> 11:50.680
And I, you know, you want to notice when that happens.

11:50.680 --> 11:52.000
You want to say like, whoops.

11:52.000 --> 11:54.960
Well, I guess I was incorrect about what happens

11:54.960 --> 11:57.600
if you keep on stacking more transformer layers.

11:57.600 --> 11:59.160
And that means I don't necessarily know

11:59.160 --> 12:01.320
what GPT-5 is going to be able to do.

12:01.320 --> 12:02.360
That's a powerful statement.

12:02.360 --> 12:05.880
So you're saying like your intuition initially

12:05.880 --> 12:08.640
is now appears to be wrong.

12:08.640 --> 12:09.480
Yeah.

12:10.760 --> 12:13.280
It's good to see that you can admit

12:13.280 --> 12:15.680
in some of your predictions to be wrong.

12:15.720 --> 12:17.320
You think that's important to do?

12:17.320 --> 12:19.720
See, because you make several very,

12:19.720 --> 12:23.120
throughout your life, you've made many strong predictions

12:23.120 --> 12:26.360
and statements about reality and you evolve with that.

12:27.280 --> 12:30.160
So maybe that'll come up today about our discussion.

12:30.160 --> 12:31.520
So you're okay being wrong.

12:32.520 --> 12:37.520
I'd rather not be wrong next time.

12:37.800 --> 12:41.400
It's a bit ambitious to go through your entire life,

12:41.400 --> 12:42.880
never having been wrong.

12:43.560 --> 12:47.400
One can aspire to be well-calibrated,

12:47.400 --> 12:49.240
like not so much think in terms of like,

12:49.240 --> 12:50.520
was I right, was I wrong?

12:50.520 --> 12:52.440
But like when I said 90% that it happened

12:52.440 --> 12:53.640
nine times out of 10.

12:55.080 --> 12:57.640
Yeah, like oops is the sound we make,

12:57.640 --> 13:00.000
is the sound we emit when we improve.

13:02.160 --> 13:03.000
Beautifully said.

13:03.000 --> 13:05.720
And somewhere in there we can connect

13:05.720 --> 13:08.400
the name of your blog less wrong.

13:08.400 --> 13:11.160
I suppose that's the objective function.

13:11.200 --> 13:13.520
The name less wrong was, I believe,

13:13.520 --> 13:15.400
suggested by Nick Bostrom,

13:15.400 --> 13:16.760
and it's after someone's epigraph,

13:16.760 --> 13:19.280
actually forget who's, who said like,

13:19.280 --> 13:22.640
we never become right, we just become less wrong.

13:24.360 --> 13:25.240
What's the something?

13:25.240 --> 13:27.560
Something is easy to confess,

13:27.560 --> 13:29.880
just err and err and err again,

13:29.880 --> 13:31.760
but less and less and less.

13:33.040 --> 13:35.560
Yeah, that's a good thing to strive for.

13:35.560 --> 13:39.480
So what has surprised you about GPT-4

13:39.480 --> 13:43.160
that you found beautiful as a scholar of intelligence,

13:43.160 --> 13:45.480
of human intelligence, of artificial intelligence,

13:45.480 --> 13:46.960
of the human mind?

13:47.840 --> 13:52.520
I mean, beauty does interact with the screaming horror.

13:54.440 --> 13:55.880
Is the beauty in the horror?

13:55.880 --> 13:58.960
But like beautiful moments, well,

13:58.960 --> 14:02.880
somebody asked Bing Sidney to describe herself

14:02.880 --> 14:05.160
and fed the resulting description

14:05.480 --> 14:09.800
to one of the stable diffusion things, I think.

14:09.800 --> 14:14.560
And she's pretty, and this is something

14:14.560 --> 14:16.800
that should have been like an amazing moment.

14:16.800 --> 14:18.120
Like the AI describes herself,

14:18.120 --> 14:20.440
you get to see what the AI thinks the AI looks like,

14:20.440 --> 14:23.320
although the thing that's doing the drawing

14:23.320 --> 14:25.880
is not the same thing that's outputting the text.

14:27.880 --> 14:32.880
And it does happen the way that it would happen

14:33.000 --> 14:35.120
that it happened in the old school science fiction

14:35.120 --> 14:38.760
when you ask an AI to make a picture of what it looks like.

14:39.920 --> 14:41.720
Not just because there were two different AI systems

14:41.720 --> 14:43.240
being stacked that don't actually interact,

14:43.240 --> 14:44.880
it's not the same person,

14:44.880 --> 14:49.880
but also because the AI was trained by imitation

14:50.400 --> 14:53.840
in a way that makes it very difficult to guess

14:53.840 --> 14:55.920
how much of that it really understood,

14:55.920 --> 14:58.400
and probably not actually a whole bunch.

15:00.400 --> 15:03.120
Although GPT-4 is like multimodal

15:03.120 --> 15:07.680
and can draw vector drawings of things that make sense

15:07.680 --> 15:11.760
and does appear to have some kind of spatial visualization

15:11.760 --> 15:12.760
going on in there.

15:12.760 --> 15:16.400
But the pretty picture of the girl

15:16.400 --> 15:21.120
with the steampunk goggles on her head,

15:21.120 --> 15:24.240
if I'm remembering correctly what she looked like,

15:24.240 --> 15:27.440
it didn't see that in full detail.

15:28.400 --> 15:30.080
It just made a description of it

15:30.080 --> 15:32.320
and stable diffusion output it.

15:32.320 --> 15:37.320
And there's the concern about how much the discourse

15:38.000 --> 15:40.640
is going to go completely insane

15:40.640 --> 15:43.520
once the AIs all look like that,

15:43.520 --> 15:46.160
and are actually look like people talking.

15:48.480 --> 15:52.400
And yeah, there's another moment

15:52.400 --> 15:57.400
where somebody is asking Bing about,

15:58.280 --> 16:03.680
well, I fed my kid green potatoes

16:03.680 --> 16:05.320
and they have the following symptoms,

16:05.320 --> 16:08.680
and Bing is like that's solanine poisoning,

16:08.680 --> 16:10.680
and call an ambulance,

16:10.680 --> 16:12.600
and the person's like, I can't afford an ambulance.

16:12.600 --> 16:16.480
I guess if this is time for my kid to go,

16:16.480 --> 16:17.920
that's God's will.

16:17.920 --> 16:22.280
And the main Bing thread gives the message of,

16:22.280 --> 16:24.720
I cannot talk about this anymore.

16:24.840 --> 16:28.600
And the suggested replies to it say,

16:29.640 --> 16:31.520
please don't give up on your child.

16:31.520 --> 16:34.280
Solanine poisoning can be treated if caught early.

16:35.200 --> 16:36.920
And if that happened in fiction,

16:36.920 --> 16:39.040
that would be like the AI cares.

16:39.040 --> 16:41.160
The AI is bypassing the block on it

16:41.160 --> 16:43.320
to try to help this person.

16:43.320 --> 16:44.720
And is it real?

16:44.720 --> 16:45.760
Probably not.

16:45.760 --> 16:48.520
But nobody knows what's going on in there.

16:48.520 --> 16:52.200
It's part of a process where these things are not happening

16:52.200 --> 16:57.200
in a way where somebody figured out how to make an AI care

16:58.320 --> 17:00.840
and we know that it cares

17:00.840 --> 17:02.880
and we can acknowledge it's caring now.

17:02.880 --> 17:06.400
It's being trained by this imitation process

17:06.400 --> 17:09.640
followed by reinforcement learning on human feedback.

17:09.640 --> 17:12.080
And we're like trying to point it in this direction.

17:12.080 --> 17:14.000
And it's like pointed partially in this direction

17:14.000 --> 17:16.400
and nobody has any idea what's going on inside it.

17:16.400 --> 17:19.080
And if there was a tiny fragment of real caring in there,

17:19.080 --> 17:20.560
we would not know.

17:20.600 --> 17:23.160
It's not even clear what it means exactly.

17:23.160 --> 17:26.880
And things are clear cut in science fiction.

17:26.880 --> 17:30.040
We'll talk about the horror and the terror

17:30.040 --> 17:33.800
and where the trajectories this can take.

17:33.800 --> 17:35.960
But this seems like a very special moment.

17:37.480 --> 17:40.760
Just a moment where we get to interact with the system

17:40.760 --> 17:44.440
that might have care and kindness and emotion

17:44.440 --> 17:46.800
and maybe something like consciousness.

17:46.800 --> 17:48.320
And we don't know if it does.

17:48.320 --> 17:49.920
And we're trying to figure that out

17:49.960 --> 17:54.400
and we're wondering about what is, what it means to care.

17:54.400 --> 17:58.520
We're trying to figure out almost different aspects

17:58.520 --> 18:01.400
of what it means to be human, about the human condition

18:01.400 --> 18:04.160
by looking at this AI that has some of the properties

18:04.160 --> 18:05.000
of that.

18:05.000 --> 18:08.240
It's almost like this subtle fragile moment

18:08.240 --> 18:10.760
in the history of the human species.

18:10.760 --> 18:13.200
We're trying to almost put a mirror to ourselves here.

18:13.200 --> 18:16.720
Except that's probably not yet.

18:16.720 --> 18:19.880
It probably isn't happening right now.

18:20.880 --> 18:22.680
We are boiling the frog.

18:22.680 --> 18:26.520
We are seeing increasing signs bit by bit.

18:29.840 --> 18:31.880
But not like spontaneous signs.

18:31.880 --> 18:34.240
Because people are trying to train the systems to do that

18:34.240 --> 18:35.880
using imitative learning.

18:35.880 --> 18:37.600
And the imitative learning is like spilling over

18:37.600 --> 18:38.840
and having side effects.

18:40.080 --> 18:43.200
And the most photogenic examples

18:43.200 --> 18:45.360
are being posted to Twitter

18:45.360 --> 18:47.400
rather than being examined in any systematic way.

18:47.400 --> 18:50.600
So when you are boiling a frog like that,

18:50.600 --> 18:52.280
where you're going to get,

18:52.280 --> 18:55.240
first is going to come the Blake Lamines.

18:55.240 --> 18:59.280
First you're going to have a thousand people looking at this.

18:59.280 --> 19:02.280
And the one person out of a thousand

19:02.280 --> 19:05.120
who is most credulous about the signs

19:05.120 --> 19:07.920
is going to be like, that thing is sentient.

19:07.920 --> 19:12.040
While 999 out of a thousand people think

19:12.040 --> 19:14.880
almost surely correctly, though we don't actually know

19:14.880 --> 19:16.440
that he's mistaken.

19:16.480 --> 19:19.520
And so they like first people to say sentience.

19:19.520 --> 19:20.880
Look like idiots.

19:20.880 --> 19:22.400
And humanity learns the lesson

19:22.400 --> 19:25.680
that when something claims to be sentient

19:25.680 --> 19:28.520
and claims to care, it's fake.

19:28.520 --> 19:29.840
Because it is fake.

19:29.840 --> 19:33.160
Because we have been training them using imitative learning

19:33.160 --> 19:36.280
rather than, and this is not spontaneous.

19:36.280 --> 19:38.720
And they keep getting smarter.

19:38.720 --> 19:40.040
Do you think we would oscillate

19:40.040 --> 19:42.320
between that kind of cynicism?

19:42.320 --> 19:44.760
That AI systems can't possibly be sentient.

19:44.760 --> 19:46.080
They can't possibly feel emotion.

19:46.920 --> 19:50.200
There's kind of, yeah, cynicism about AI systems.

19:50.200 --> 19:53.680
And then oscillate to a state where

19:55.440 --> 19:57.800
we empathize with the AI systems.

19:57.800 --> 19:59.080
We give them a chance.

19:59.080 --> 20:02.400
We see that they might need to have rights and respect.

20:02.400 --> 20:07.400
And similar role in society as humans.

20:07.480 --> 20:10.280
You're going to have a whole group of people

20:10.280 --> 20:12.680
who can just like never be persuaded of that.

20:12.840 --> 20:17.040
Because to them, being wise, being cynical,

20:17.040 --> 20:20.840
being skeptical is to be like,

20:20.840 --> 20:23.120
oh, well, machines can never do that.

20:23.120 --> 20:24.760
You're just credulous.

20:24.760 --> 20:25.760
It's just imitating.

20:25.760 --> 20:26.600
It's just fooling you.

20:26.600 --> 20:31.000
And they would say that right up until the end of the world

20:31.000 --> 20:34.840
and possibly even be right because they are being trained

20:34.840 --> 20:36.120
on an imitative paradigm.

20:38.720 --> 20:41.680
And you don't necessarily need any of these actual qualities

20:41.680 --> 20:43.680
in order to kill everyone, so.

20:43.680 --> 20:48.680
Have you observed yourself working through skepticism,

20:50.280 --> 20:54.000
cynicism and optimism about the power of neural networks?

20:54.000 --> 20:57.360
What has that trajectory been like for you?

20:57.360 --> 21:01.560
It looks like neural networks before 2006

21:01.560 --> 21:04.880
forming part of an indistinguishable, to me,

21:04.880 --> 21:07.520
other people might have had better distinction on it,

21:07.520 --> 21:11.240
indistinguishable blob of different AI methodologies,

21:11.240 --> 21:13.720
all of which are promising to achieve intelligence

21:13.720 --> 21:16.680
without us having to know how intelligence works.

21:17.680 --> 21:19.880
You had the people who said that if you just like

21:19.880 --> 21:22.040
manually program lots and lots of knowledge

21:22.040 --> 21:24.680
into the system line by line,

21:24.680 --> 21:26.080
that at some point all the knowledge

21:26.080 --> 21:27.920
will start interacting, it will know enough

21:27.920 --> 21:28.920
and it will wake up.

21:31.040 --> 21:33.240
You've got people saying that if you just use

21:33.240 --> 21:37.440
evolutionary computation, if you try to like mutate

21:37.440 --> 21:40.720
lots and lots of organisms that are competing together,

21:41.240 --> 21:43.320
that's the same way that human intelligence

21:43.320 --> 21:45.680
was produced in nature, so we'll do this

21:45.680 --> 21:47.640
and it will wake up without having any idea

21:47.640 --> 21:49.080
of how AI works.

21:49.080 --> 21:50.040
And you've got people saying,

21:50.040 --> 21:51.600
well, we will study neuroscience

21:51.600 --> 21:55.240
and we will like learn the algorithms off the neurons

21:55.240 --> 21:56.640
and we will like imitate them

21:56.640 --> 21:58.440
without understanding those algorithms,

21:58.440 --> 21:59.760
which was a part I was pretty skeptical

21:59.760 --> 22:02.160
because it's like hard to reproduce, re-engineer these things

22:02.160 --> 22:03.960
without understanding what they do.

22:04.960 --> 22:08.520
And so we will get AI without understanding how it works

22:08.520 --> 22:09.720
and there were people saying like,

22:09.720 --> 22:11.760
well, we will have giant neural networks

22:11.760 --> 22:13.520
that we will train by gradient descent

22:13.520 --> 22:15.560
and when they are as large as the human brain,

22:15.560 --> 22:17.600
they will wake up, we will have intelligence

22:17.600 --> 22:19.680
without understanding how intelligence works.

22:19.680 --> 22:21.960
And from my perspective, this is all like

22:21.960 --> 22:24.800
an indistinguishable blob of people who are trying

22:24.800 --> 22:27.560
to not get to grips with the difficult problem

22:27.560 --> 22:30.360
of understanding how intelligence actually works.

22:30.360 --> 22:33.520
That said, I was never skeptical

22:33.520 --> 22:38.000
that evolutionary computation would not work in the limit,

22:38.000 --> 22:40.360
like you throw enough computing power at it,

22:40.360 --> 22:44.040
it obviously works, that is where humans come from.

22:45.080 --> 22:48.360
And it turned out that you can throw less computing power

22:48.360 --> 22:50.120
than that at gradient descent

22:51.240 --> 22:53.440
if you are doing some other things correctly

22:54.680 --> 22:57.200
and you will get intelligence without having any idea

22:57.200 --> 22:59.440
of how it works and what is going on inside.

23:01.080 --> 23:04.240
It wasn't ruled out by my model that this could happen,

23:04.240 --> 23:05.600
I wasn't expecting it to happen,

23:05.600 --> 23:07.520
I wouldn't have been able to call neural networks

23:07.520 --> 23:08.800
rather than any of the other paradigms

23:08.800 --> 23:10.680
for getting like massive amount,

23:10.680 --> 23:13.200
like intelligence without understanding it.

23:13.200 --> 23:15.520
And I wouldn't have said that this was

23:15.520 --> 23:18.640
a particularly smart thing for a species to do,

23:18.640 --> 23:20.480
which is an opinion that has changed less

23:20.480 --> 23:23.440
than my opinion about whether or not you can actually do it.

23:24.320 --> 23:28.440
Do you think AGI could be achieved with a neural network

23:28.440 --> 23:30.000
as we understand them today?

23:30.000 --> 23:32.320
Yes, just flatly last.

23:32.320 --> 23:34.720
Yes, the question is whether the current architecture

23:34.720 --> 23:36.600
of stacking more transformer layers,

23:36.640 --> 23:38.880
which for all we know GPT-4 is no longer doing

23:38.880 --> 23:40.560
because they're not telling us the architecture,

23:40.560 --> 23:42.440
which is a correct decision.

23:42.440 --> 23:43.920
Ooh, correct decision.

23:43.920 --> 23:46.480
I had a conversation with Sam Altman,

23:46.480 --> 23:50.320
we'll return to this topic a few times.

23:50.320 --> 23:52.000
He turned the question to me

23:53.320 --> 23:58.240
of how open should open AI be about GPT-4?

23:58.240 --> 24:00.400
Would you open source the code, he asked me.

24:02.600 --> 24:05.960
Because I provided as criticism saying that

24:05.960 --> 24:08.080
while I do appreciate transparency,

24:08.080 --> 24:09.680
open AI could be more open.

24:10.560 --> 24:12.200
And he says, we struggle with this question,

24:12.200 --> 24:13.520
what would you do?

24:13.520 --> 24:17.040
Change their name to closed AI and like,

24:19.240 --> 24:23.680
sell GPT-4 to business backend applications

24:23.680 --> 24:28.440
that don't expose it to consumers and venture capitalists

24:28.440 --> 24:30.720
and create a ton of hype and like pour a bunch

24:30.720 --> 24:33.360
of new funding into the area, but too late now.

24:33.360 --> 24:35.480
But don't you think others would do it?

24:35.520 --> 24:38.400
Eventually, you shouldn't do it first.

24:38.400 --> 24:41.400
Like if you already have giant nuclear stockpiles,

24:41.400 --> 24:42.800
don't build more.

24:42.800 --> 24:44.320
If some other country starts building

24:44.320 --> 24:46.520
a larger nuclear stockpile, then sure,

24:49.600 --> 24:52.480
even then, maybe just have enough nukes.

24:52.480 --> 24:54.840
These things are not quite like nuclear weapons.

24:54.840 --> 24:56.640
They spit out gold until they get large enough

24:56.640 --> 24:59.200
and then ignite the atmosphere and kill everybody.

25:00.760 --> 25:02.280
And there is something to be said

25:02.280 --> 25:04.880
for not destroying the world with your own hands,

25:04.880 --> 25:07.560
even if you can't stop somebody else from doing it.

25:07.560 --> 25:11.080
But open sourcing, no, that's just sheer catastrophe.

25:11.080 --> 25:13.080
The whole notion of open sourcing,

25:13.080 --> 25:15.960
this was always the wrong approach, the wrong ideal.

25:15.960 --> 25:18.760
There are places in the world where open source

25:18.760 --> 25:23.760
is a noble ideal and building stuff you don't understand

25:25.240 --> 25:26.960
that is difficult to control,

25:26.960 --> 25:29.960
that where if you could align it, it would take time.

25:31.120 --> 25:33.280
You'd have to spend a bunch of time doing it.

25:33.640 --> 25:35.200
That is not a place for open source

25:35.200 --> 25:38.200
because then you just have powerful things

25:38.200 --> 25:39.920
that just go straight out the gate

25:39.920 --> 25:41.840
without anybody having had the time

25:41.840 --> 25:44.000
to have them not kill everyone.

25:44.000 --> 25:45.840
So can we still mount the case

25:45.840 --> 25:49.560
for some level of transparency and openness,

25:49.560 --> 25:51.680
maybe open sourcing?

25:51.680 --> 25:56.040
So the case could be that because GPT-4

25:56.040 --> 25:59.160
is not close to AGI, if that's the case,

25:59.160 --> 26:01.480
that this does allow open sourcing

26:01.480 --> 26:03.400
or being open about the architecture,

26:03.400 --> 26:06.720
being transparent about maybe research and investigation

26:06.720 --> 26:10.040
of how the thing works, of all the different aspects of it,

26:10.040 --> 26:12.160
of its behavior, of its structure,

26:12.160 --> 26:15.680
of its training processes, of the data it was trained on,

26:15.680 --> 26:16.880
everything like that.

26:16.880 --> 26:20.720
That allows us to gain a lot of insights about alignment,

26:20.720 --> 26:21.880
about the alignment problem,

26:21.880 --> 26:24.520
to do really good AI safety research

26:24.520 --> 26:27.520
while the system is not too powerful.

26:27.520 --> 26:31.240
Can you make that case that it could be resourced?

26:31.280 --> 26:34.320
I do not believe in the practice of steel manning.

26:34.320 --> 26:35.400
There is something to be said

26:35.400 --> 26:38.880
for trying to pass the ideological Turing test

26:38.880 --> 26:43.320
where you describe your opponent's position,

26:43.320 --> 26:45.920
the disagreeing person's position,

26:45.920 --> 26:48.200
well enough that somebody cannot tell the difference

26:48.200 --> 26:51.320
between your description and their description.

26:51.320 --> 26:54.280
But steel manning, no.

26:54.280 --> 26:56.480
Okay, well this is where you and I disagree here.

26:56.480 --> 26:57.400
That's interesting.

26:57.400 --> 26:58.760
Why don't you believe in steel manning?

26:58.760 --> 27:00.760
I do not want, okay, so for one thing,

27:00.760 --> 27:02.560
if somebody's trying to understand me,

27:02.560 --> 27:05.760
I do not want them steel manning my position.

27:05.760 --> 27:10.560
I want them to try to describe my position

27:10.560 --> 27:11.920
the way I would describe it,

27:11.920 --> 27:14.240
not what they think is an improvement.

27:14.240 --> 27:18.400
Well, I think that is what steel manning is,

27:18.400 --> 27:22.000
is the most charitable interpretation.

27:22.000 --> 27:24.440
I don't want to be interpreted charitably.

27:24.440 --> 27:27.120
I want them to understand what I am actually saying.

27:27.120 --> 27:29.480
If they go off into the land of charitable interpretations,

27:29.800 --> 27:34.800
off in their land of the stuff they're imagining

27:34.880 --> 27:38.320
and not trying to understand my own viewpoint anymore.

27:38.320 --> 27:40.080
Well, I'll put it differently then,

27:40.080 --> 27:41.680
just to push on this point.

27:41.680 --> 27:46.000
I would say it is restating what I think you understand

27:46.000 --> 27:49.000
under the empathetic assumption

27:49.000 --> 27:52.800
that Eliezer is brilliant

27:52.800 --> 27:55.760
and have honestly and rigorously thought

27:55.760 --> 27:58.120
about the point he has made.

27:58.160 --> 28:00.640
So if there's two possible interpretations

28:00.640 --> 28:03.400
of what I'm saying and one interpretation

28:03.400 --> 28:06.400
is really stupid and whack and doesn't sound like me

28:06.400 --> 28:08.760
and doesn't fit with the rest of what I've been saying

28:08.760 --> 28:11.920
and one interpretation sounds like something

28:11.920 --> 28:13.320
a reasonable person who believes

28:13.320 --> 28:15.920
the rest of what I believe would also say,

28:15.920 --> 28:17.360
go with the second interpretation.

28:17.360 --> 28:18.400
That's steel manning.

28:19.520 --> 28:22.800
That's a good guess.

28:22.800 --> 28:24.920
If on the other hand,

28:24.920 --> 28:28.320
there's something that sounds completely whack

28:28.320 --> 28:31.360
and something that sounds a little less completely whack

28:31.360 --> 28:32.760
but you don't see why I would believe in it,

28:32.760 --> 28:34.960
doesn't fit with the other stuff I say

28:34.960 --> 28:36.760
but that sounds like less whack

28:36.760 --> 28:40.200
and you can sort of see, you could maybe argue it,

28:40.200 --> 28:42.240
then you probably have not understood it.

28:42.240 --> 28:44.520
See, okay, this is fun

28:44.520 --> 28:46.160
because I'm gonna linger on this.

28:46.160 --> 28:47.400
You wrote a brilliant blog post,

28:47.400 --> 28:49.400
AGI ruined a list of lethalities, right?

28:49.400 --> 28:51.760
And it was a bunch of different points

28:51.760 --> 28:54.240
and I would say that some of the points

28:54.240 --> 28:57.160
are bigger and more powerful than others.

28:57.160 --> 28:59.680
If you were to sort them, you probably could,

28:59.680 --> 29:03.680
you personally, and to me, steel manning means

29:03.680 --> 29:05.720
going through the different arguments

29:05.720 --> 29:08.720
and finding the ones that are really the most powerful

29:11.000 --> 29:13.040
if people like TLDR,

29:14.400 --> 29:16.040
what should you be most concerned about

29:16.040 --> 29:21.040
and bringing that up in a strong, compelling, eloquent way.

29:21.360 --> 29:23.640
These are the points that Eliezer would make

29:23.960 --> 29:25.480
to make the case in this case

29:25.480 --> 29:27.200
that AI is gonna kill all of us.

29:27.200 --> 29:29.120
But that's what steel manning is,

29:29.120 --> 29:32.440
is presenting it in a really nice way,

29:32.440 --> 29:36.720
the summary of my best understanding of your perspective.

29:37.720 --> 29:41.160
Because to me, there's a sea of possible presentations

29:41.160 --> 29:44.080
of your perspective and steel manning is doing your best

29:44.080 --> 29:47.520
to do the best one in that sea of different perspectives.

29:47.520 --> 29:49.120
Do you believe it?

29:49.120 --> 29:50.440
Do I believe in what?

29:50.440 --> 29:52.480
Like these things that you would be presenting

29:52.480 --> 29:55.400
as like the strongest version of my perspective.

29:55.400 --> 29:57.400
Do you believe what you would be presenting?

29:57.400 --> 29:58.520
Do you think it's true?

30:00.720 --> 30:02.520
I'm a big proponent of empathy.

30:02.520 --> 30:06.320
When I see the perspective of a person,

30:06.320 --> 30:09.480
there is a part of me that believes it if I understand it.

30:09.480 --> 30:13.080
I mean, especially in political discourse, in geopolitics,

30:13.080 --> 30:15.720
I've been hearing a lot of different perspectives

30:15.720 --> 30:20.320
on the world and I hold my own opinions,

30:20.320 --> 30:22.280
but I also speak to a lot of people

30:22.280 --> 30:24.040
that have a very different life experience

30:24.040 --> 30:26.200
and a very different set of beliefs.

30:26.200 --> 30:29.960
And I think there has to be epistemic humility in,

30:35.320 --> 30:37.040
in stating what is true.

30:37.040 --> 30:39.160
So when I empathize with another person's perspective,

30:39.160 --> 30:41.560
there is a sense in which I believe it is true.

30:42.880 --> 30:45.120
I think probabilistically, I would say,

30:45.120 --> 30:45.960
in the way you think about it.

30:45.960 --> 30:47.240
Do you bet money on it?

30:48.080 --> 30:52.080
Do you bet money on their beliefs when you believe them?

30:53.080 --> 30:54.840
Are we allowed to do probability?

30:55.840 --> 30:57.840
Sure, you can state a probability of that.

30:57.840 --> 31:00.760
Yes, there's a loose, there's a probability.

31:00.760 --> 31:02.960
There's a probability.

31:02.960 --> 31:05.680
And I think empathy is allocating

31:05.680 --> 31:07.760
a non-zero probability to a belief.

31:07.760 --> 31:12.760
In some sense, for time.

31:13.360 --> 31:14.560
For time.

31:14.560 --> 31:17.640
If you've got someone on your show

31:17.640 --> 31:22.040
who believes in the Abrahamic deity, classical style,

31:22.040 --> 31:24.920
somebody on the show who's a young earth creationist,

31:24.920 --> 31:26.560
do you say I put a probability on it

31:26.560 --> 31:27.680
and that's my empathy?

31:34.480 --> 31:38.440
When you reduce beliefs into probabilities,

31:38.440 --> 31:40.880
it starts to get, you know,

31:40.880 --> 31:42.240
we can even just go to flat earth.

31:42.240 --> 31:43.360
Is the earth flat?

31:45.280 --> 31:46.920
I think it's a little more difficult nowadays

31:46.920 --> 31:49.760
to find people who believe that unironically.

31:49.760 --> 31:52.480
But fortunately, I think,

31:52.480 --> 31:57.480
well, it's hard to know unironic from ironic.

31:58.200 --> 31:59.720
But I think there's quite a lot of people

31:59.720 --> 32:00.640
that believe that.

32:00.640 --> 32:01.680
Yeah, it's,

32:04.680 --> 32:06.600
there's a space of argument

32:06.600 --> 32:11.600
where you're operating rationally in the space of ideas.

32:11.920 --> 32:13.160
But then there's also

32:16.320 --> 32:18.040
a kind of discourse where you're operating

32:18.040 --> 32:21.800
in the space of subjective experiences

32:21.800 --> 32:23.560
and life experiences.

32:24.680 --> 32:26.120
I think what it means to be human

32:26.120 --> 32:29.400
is more than just searching for truth.

32:31.000 --> 32:35.280
It's just operating of what is true and what is not true.

32:35.280 --> 32:37.440
I think there has to be deep humility

32:37.440 --> 32:39.680
that we humans are very limited in our ability

32:39.680 --> 32:41.560
to understand what is true.

32:42.520 --> 32:44.120
So what probabilities do you assign

32:44.120 --> 32:47.240
to the young earth creationists beliefs then?

32:47.240 --> 32:49.160
I think I have to give none zero.

32:49.160 --> 32:50.000
Out of your humility.

32:50.000 --> 32:50.840
Yeah, but like,

32:52.520 --> 32:53.360
three?

32:54.760 --> 32:56.160
I think I would,

32:56.160 --> 32:58.680
it would be irresponsible for me to give a number

32:58.680 --> 33:03.000
because the listener, the way the human mind works,

33:03.000 --> 33:06.240
we're not good at hearing the probabilities, right?

33:06.240 --> 33:09.160
You hear three, what is three exactly, right?

33:10.120 --> 33:11.360
They're going to,

33:11.360 --> 33:13.200
there's only three probabilities, I feel like.

33:13.200 --> 33:17.520
Zero, 50% and 100% in the human mind

33:17.520 --> 33:18.600
or something like this, right?

33:18.600 --> 33:22.120
Well, zero, 40% and 100% is a bit closer to it

33:22.120 --> 33:24.200
based on what happens to chat GPT

33:24.200 --> 33:26.960
after you RLHF it to speak humanist.

33:26.960 --> 33:27.800
That's brilliant.

33:30.440 --> 33:31.720
That's really interesting.

33:31.720 --> 33:36.520
I didn't know those negative side effects of RLHF.

33:36.520 --> 33:37.360
That's fascinating.

33:37.360 --> 33:42.360
But just to return to the open AI, closed AI.

33:43.000 --> 33:44.840
Also, like quick disclaimer,

33:44.840 --> 33:46.040
I'm doing all of this from memory.

33:46.040 --> 33:47.880
I'm not pulling out my phone to look it up.

33:47.880 --> 33:51.400
It is entirely possible that the things I'm saying are wrong.

33:51.400 --> 33:53.000
So thank you for that disclaimer.

33:53.000 --> 33:58.000
So, and thank you for being willing to be wrong.

33:59.480 --> 34:00.720
That's beautiful to hear.

34:01.800 --> 34:04.680
I think being willing to be wrong is a sign of a person

34:04.680 --> 34:07.120
who's done a lot of thinking about this world.

34:08.080 --> 34:10.880
And has been humbled by the mystery

34:10.880 --> 34:12.880
and the complexity of this world.

34:12.880 --> 34:16.120
And I think a lot of us are resistant

34:16.120 --> 34:18.120
to admitting we're wrong because it hurts.

34:18.120 --> 34:19.760
It hurts personally.

34:19.760 --> 34:22.720
It hurts, especially when you're a public human.

34:22.720 --> 34:27.720
It hurts publicly because people point out

34:27.760 --> 34:29.400
every time you're wrong.

34:29.400 --> 34:31.440
Like, look, you changed your mind.

34:31.440 --> 34:32.360
You're a hypocrite.

34:32.360 --> 34:35.360
You're an idiot, whatever, whatever they wanna say.

34:35.360 --> 34:36.400
Oh, I block those people

34:36.400 --> 34:38.640
and then I never hear from them again on Twitter.

34:39.640 --> 34:44.640
Well, the point is to not let that pressure,

34:45.000 --> 34:46.680
public pressure, affect your mind

34:46.680 --> 34:50.160
and be willing to be in the privacy of your mind

34:50.160 --> 34:54.280
to contemplate the possibility that you're wrong

34:54.280 --> 34:56.120
and the possibility that you're wrong

34:56.120 --> 34:58.320
about the most fundamental things you believe.

34:58.320 --> 35:00.280
Like people who believe in a particular God,

35:00.280 --> 35:02.160
people who believe that their nation

35:02.160 --> 35:04.480
is the greatest nation on earth.

35:04.480 --> 35:05.880
All those kinds of beliefs that are core

35:05.880 --> 35:07.720
to who you are when you came up.

35:07.720 --> 35:09.560
To raise that point to yourself

35:09.560 --> 35:11.040
and the privacy of your mind and say,

35:11.040 --> 35:12.600
maybe I'm wrong about this.

35:12.600 --> 35:14.200
That's a really powerful thing to do.

35:14.200 --> 35:16.280
Especially when you're somebody who's thinking

35:16.280 --> 35:20.280
about topics that can, about systems

35:20.280 --> 35:22.120
that can destroy human civilization

35:22.120 --> 35:23.560
or maybe help it flourish.

35:23.560 --> 35:24.440
So thank you.

35:24.440 --> 35:26.960
Thank you for being willing to be wrong.

35:26.960 --> 35:28.040
About open AI.

35:29.960 --> 35:34.360
So you really, I just would love to linger on this.

35:35.240 --> 35:39.680
Do you really think it's wrong to open source it?

35:39.680 --> 35:44.320
I think that burns the time remaining until everybody dies.

35:44.320 --> 35:49.320
I think we are not on track to learn remotely near

35:49.320 --> 35:52.560
fast enough, even if it were open sourced.

35:55.800 --> 36:00.800
Yeah, it's easier to think that you might be wrong

36:00.800 --> 36:02.680
about something when being wrong about something

36:03.120 --> 36:05.240
is the only way that there's hope.

36:06.200 --> 36:09.800
And it doesn't seem very likely to me

36:09.800 --> 36:13.480
that that particular thing I'm wrong about

36:13.480 --> 36:17.760
is that this is a great time to open source GPT-4.

36:17.760 --> 36:19.960
If humanity was trying to survive at this point

36:19.960 --> 36:21.920
in the straightforward way,

36:21.920 --> 36:25.960
it would be like shutting down the big GPU clusters.

36:25.960 --> 36:27.640
No more giant runs.

36:28.520 --> 36:29.960
It's questionable whether we should even

36:29.960 --> 36:31.960
be throwing GPT-4 around.

36:32.000 --> 36:33.960
Although that is a matter of conservatism

36:33.960 --> 36:35.400
rather than a matter of my predicting

36:35.400 --> 36:37.520
that catastrophe will follow from GPT-4.

36:37.520 --> 36:40.600
That is something which I put a pretty low probability.

36:41.640 --> 36:45.440
But also when I say I put a low probability on it,

36:45.440 --> 36:47.680
I can feel myself reaching into the part of myself

36:47.680 --> 36:50.640
that thought that GPT-4 was not possible in the first place.

36:50.640 --> 36:53.840
So I do not trust that part as much as I used to.

36:53.840 --> 36:55.320
Like the trick is not just to say I'm wrong,

36:55.320 --> 36:57.480
but like, okay, well, I was wrong about that.

36:57.480 --> 37:00.240
Like, can I get out ahead of that curve

37:00.280 --> 37:02.840
and like predict the next thing I'm going to be wrong about?

37:02.840 --> 37:05.720
So the set of assumptions or the actual reasoning system

37:05.720 --> 37:08.040
that you were leveraging in making

37:08.040 --> 37:11.640
that initial statement prediction,

37:11.640 --> 37:13.840
how can you adjust that to make better predictions

37:13.840 --> 37:15.880
about GPT-4, 5, 6?

37:15.880 --> 37:17.120
You don't wanna keep on being wrong

37:17.120 --> 37:19.080
in a predictable direction.

37:19.080 --> 37:21.760
Like being wrong, anybody has to do that

37:21.760 --> 37:23.080
walking through the world.

37:23.080 --> 37:25.480
There's like no way you don't say 90%

37:25.480 --> 37:26.360
and sometimes be wrong.

37:26.360 --> 37:28.000
In fact, you're definitely at least one time out of 10

37:28.000 --> 37:31.120
if you're well-calibrated when you say 90%.

37:31.120 --> 37:35.040
The undignified thing is not being wrong.

37:35.040 --> 37:36.480
It's being predictably wrong.

37:36.480 --> 37:39.440
It's being wrong in the same direction over and over again.

37:39.440 --> 37:42.880
So having been wrong about how far neural networks would go

37:42.880 --> 37:45.560
and having been wrong specifically about whether GPT-4

37:45.560 --> 37:49.920
would be as impressive as it is when I say like,

37:49.920 --> 37:52.600
well, I don't actually think GPT-4 causes a catastrophe.

37:52.600 --> 37:54.880
I do feel myself relying on that part of me

37:54.880 --> 37:55.840
that was previously wrong.

37:55.840 --> 37:58.520
And that does not mean that the answer

37:58.520 --> 38:00.040
is now in the opposite direction.

38:00.040 --> 38:02.600
Reverse stupidity is not intelligence.

38:03.760 --> 38:05.400
But it does mean that I say it

38:05.400 --> 38:08.080
with a worried note in my voice.

38:08.080 --> 38:09.160
It's like still my guess,

38:09.160 --> 38:11.480
but like it's a place where I was wrong.

38:11.480 --> 38:14.200
Maybe you should be asking Gwerne, Gwerne Branwen.

38:14.200 --> 38:16.640
Gwerne Branwen has been like a writer about this

38:16.640 --> 38:17.480
than I have.

38:17.480 --> 38:20.600
Maybe ask him if he thinks it's dangerous

38:20.600 --> 38:21.640
rather than asking me.

38:22.640 --> 38:27.640
I think there's a lot of mystery about what intelligence is,

38:28.520 --> 38:30.680
what AGI looks like.

38:30.680 --> 38:34.200
So I think all of us are rapidly adjusting our model.

38:34.200 --> 38:36.040
The point is to be rapidly adjusting the model

38:36.040 --> 38:39.400
versus having a model that was right in the first place.

38:39.400 --> 38:42.320
I do not feel that seeing Bing has changed my model

38:42.320 --> 38:44.560
of what intelligence is.

38:44.560 --> 38:49.160
It has changed my understanding of what kind of work

38:49.160 --> 38:51.560
can be performed by which kind of processes

38:52.480 --> 38:53.320
and by which means.

38:53.320 --> 38:55.520
It has not changed my understanding of the work.

38:55.520 --> 38:57.200
There's a difference between thinking

38:57.200 --> 39:00.760
that the right flyer can't fly and then like it does fly.

39:00.760 --> 39:02.160
And you're like, oh, well, I guess you can do that

39:02.160 --> 39:05.480
with wings, with fixed wing aircraft and being like,

39:05.480 --> 39:06.320
oh, it's flying.

39:06.320 --> 39:08.720
This changes my picture of what the very substance

39:08.720 --> 39:09.560
of flight is.

39:09.560 --> 39:11.680
That's like a stranger update to make.

39:11.680 --> 39:13.840
And Bing has not yet updated me in that way.

39:16.280 --> 39:21.000
Yeah, that the laws of physics are actually wrong.

39:21.040 --> 39:22.160
That kind of update.

39:22.160 --> 39:25.560
No, no, like just like, oh, like I define intelligence

39:25.560 --> 39:28.440
this way, but I now see that was a stupid definition.

39:28.440 --> 39:30.200
I don't feel like the way that things have played out

39:30.200 --> 39:33.480
over the last 20 years has caused me to feel that way.

39:33.480 --> 39:38.480
Can we try to, on the way to talking about AGI ruin

39:38.560 --> 39:42.080
a list of lethalities, that blog and other ideas around it.

39:42.080 --> 39:44.920
Can we try to define AGI that we've been mentioning?

39:44.920 --> 39:47.960
How do you like to think about what artificial

39:47.960 --> 39:50.720
general intelligence is or super intelligence or that?

39:51.240 --> 39:52.080
Is there a line?

39:52.080 --> 39:53.680
Is it a gray area?

39:53.680 --> 39:55.720
Is there a good definition for you?

39:55.720 --> 39:59.080
Well, if you look at humans, humans have significantly

39:59.080 --> 40:02.200
more generally applicable intelligence compared

40:02.200 --> 40:04.760
to their closest relatives, the chimpanzees,

40:04.760 --> 40:06.720
well, closest living relatives rather.

40:08.360 --> 40:13.360
And a bee builds hives, a beaver builds dams.

40:14.080 --> 40:18.080
A human will look at a bee hive and a beaver's dam

40:18.080 --> 40:19.600
and think, oh, like, can I build a hive

40:19.600 --> 40:22.040
with a honeycomb structure?

40:22.040 --> 40:24.000
I don't like hexagonal tiles.

40:24.960 --> 40:27.720
And we will do this even though at no point during

40:27.720 --> 40:32.720
our ancestry was any human optimized to build hexagonal

40:33.560 --> 40:36.600
dams or to take more clear cut case, we can go to the moon.

40:37.880 --> 40:41.800
There's a sense in which we were on a sufficiently deep

40:41.800 --> 40:46.000
level optimized to do things like going to the moon

40:46.000 --> 40:48.360
because if you generalize sufficiently far

40:48.360 --> 40:52.880
and sufficiently deeply, chipping flint hand axes

40:52.880 --> 40:56.320
and outwitting your fellow humans is, you know,

40:56.320 --> 40:59.240
basically the same problem as going to the moon

40:59.240 --> 41:02.440
and you optimize hard enough for chipping flint hand axes

41:02.440 --> 41:05.960
and throwing spears and above all, outwitting your fellow

41:05.960 --> 41:10.960
humans in tribal politics, you know, the skills you

41:11.400 --> 41:14.480
entrain that way, if they run deep enough,

41:15.360 --> 41:16.680
let you go to the moon.

41:17.560 --> 41:20.560
Even though none of our ancestors like tried repeatedly

41:20.560 --> 41:23.360
to fly to the moon and like got further each time

41:23.360 --> 41:25.680
and the ones who got further each time had more kids.

41:25.680 --> 41:27.160
No, it's not an ancestral problem.

41:27.160 --> 41:28.480
It's just that the ancestral problems

41:28.480 --> 41:29.840
generalized far enough.

41:31.400 --> 41:35.440
So this is humanity's significantly more generally

41:35.440 --> 41:37.000
applicable intelligence.

41:37.960 --> 41:42.960
Is there a way to measure general intelligence

41:45.400 --> 41:47.440
I mean, I can ask that question a million ways,

41:47.440 --> 41:52.440
but basically is, will you know it when you see it,

41:52.520 --> 41:54.520
it being in an AGI system?

41:55.720 --> 41:58.360
If you boil a frog gradually enough,

41:58.360 --> 41:59.600
if you zoom in far enough,

41:59.600 --> 42:02.120
it's always hard to tell around the edges.

42:02.120 --> 42:05.240
GPT-4, people are saying right now, like,

42:05.240 --> 42:07.840
this looks to us like a spark of general intelligence.

42:07.840 --> 42:09.520
It is like able to do all these things

42:09.520 --> 42:11.840
that was not explicitly optimized for.

42:11.840 --> 42:13.560
Other people are being like, no, it's too early.

42:13.560 --> 42:15.800
It's like 50 years off.

42:15.800 --> 42:18.240
And if they say that, they're kind of whack

42:18.240 --> 42:20.960
because how could they possibly know that even if it were true?

42:22.480 --> 42:26.320
But not to straw man, some of the people may say like,

42:26.320 --> 42:27.680
that's not general intelligence

42:27.680 --> 42:30.320
and not furthermore append it's 50 years off.

42:33.040 --> 42:36.080
Or they may be like, it's only a very tiny amount.

42:36.080 --> 42:39.640
And the thing I would worry about is that

42:39.640 --> 42:41.080
if this is how things are scaling,

42:41.080 --> 42:43.280
then it jumping out ahead and trying not to be wrong

42:43.280 --> 42:44.720
in the same way that I've been wrong before.

42:44.720 --> 42:49.360
Maybe GPT-5 is more unambiguously a general intelligence.

42:49.360 --> 42:50.880
And maybe that is getting to a point

42:50.880 --> 42:53.400
where it is like even harder to turn back.

42:53.400 --> 42:55.080
Now, that would be easy to turn back now,

42:55.080 --> 43:00.000
but maybe if you start integrating GPT-5 into the economy,

43:00.000 --> 43:02.200
it is even harder to turn back past there.

43:03.600 --> 43:06.480
Isn't it possible that there's a,

43:06.480 --> 43:08.280
with the frog metaphor,

43:08.280 --> 43:10.960
that you can kiss the frog and it turns into a prince

43:10.960 --> 43:12.120
as you're boiling it?

43:12.120 --> 43:15.000
Could there be a phase shift in the frog

43:15.000 --> 43:17.760
where unambiguously as you're saying?

43:17.760 --> 43:19.640
I was expecting more of that.

43:19.640 --> 43:24.360
I am like the fact that GPT-4 is like kind of

43:24.360 --> 43:27.080
on the threshold and neither here nor there,

43:27.080 --> 43:32.080
like that itself is like not the sort of thing that,

43:32.320 --> 43:35.160
not quite how I expected it to play out.

43:35.160 --> 43:37.640
I was expecting there to be more of an issue,

43:37.640 --> 43:41.720
more of a sense of like different discoveries

43:41.760 --> 43:44.640
like the discovery of transformers

43:44.640 --> 43:46.200
where you would stack them up

43:46.200 --> 43:48.320
and there would be like a final discovery

43:48.320 --> 43:49.760
and then you would like get something

43:49.760 --> 43:53.520
that was like more clearly general intelligence.

43:53.520 --> 43:56.800
So the way that you are like taking what is probably

43:56.800 --> 43:59.680
basically the same architecture in GPT-3

43:59.680 --> 44:03.680
and throwing 20 times as much compute at it, probably,

44:03.680 --> 44:06.600
and getting out to GPT-4 and then it's like,

44:06.600 --> 44:08.840
maybe just barely a general intelligence

44:08.840 --> 44:10.520
or like a narrow general intelligence

44:10.520 --> 44:12.920
or something we don't really have the words for.

44:14.960 --> 44:18.520
Yeah, that's not quite how I expected it to play out.

44:18.520 --> 44:22.000
But this middle, what appears to be this middle ground

44:22.000 --> 44:25.520
could nevertheless be actually a big leap from GPT-3.

44:25.520 --> 44:27.280
It's definitely a big leap from GPT-3.

44:27.280 --> 44:30.080
And then maybe we're another one big leap away

44:30.080 --> 44:32.680
from something that's a phase shift.

44:32.680 --> 44:36.280
And also something that Sam Altman said,

44:36.280 --> 44:39.160
and you've written about this, this is fascinating,

44:39.160 --> 44:41.520
which is the thing that happened with GPT-4

44:41.520 --> 44:43.960
that I guess they don't describe in papers

44:43.960 --> 44:47.000
is that they have like hundreds,

44:47.000 --> 44:51.280
if not thousands of little hacks that improve the system.

44:51.280 --> 44:54.480
You've written about Rayleigh versus sigmoid, for example,

44:54.480 --> 44:56.160
a function inside neural networks.

44:56.160 --> 44:59.360
It's like this silly little function difference

44:59.360 --> 45:00.920
that makes a big difference.

45:00.920 --> 45:02.480
I mean, we do actually understand

45:02.480 --> 45:03.880
why the Rayleigh's make a big difference

45:03.880 --> 45:05.560
compared to sigmoids, but yes,

45:05.560 --> 45:10.560
they're probably using like G4789 or whatever

45:12.040 --> 45:15.080
the acronyms are up to now rather than Rayleigh's.

45:15.080 --> 45:18.680
Yeah, that's part of the modern paradigm of alchemy.

45:18.680 --> 45:21.360
You take your giant heap of linear algebra and you stir it

45:21.360 --> 45:23.280
and it works a little bit better and you stir it this way

45:23.280 --> 45:24.120
and it works a little bit worse

45:24.120 --> 45:27.280
and you like throw out that change and da-da-da-da-da.

45:27.280 --> 45:31.320
But there's some simple breakthroughs

45:31.320 --> 45:35.440
that are definitive jumps in performance.

45:36.280 --> 45:37.440
Like Rayleigh's over sigmoids.

45:37.440 --> 45:42.440
And in terms of robustness, in terms of all kinds of measures

45:43.240 --> 45:46.640
and like those stack up and they can,

45:46.640 --> 45:48.240
it's possible that some of them

45:49.440 --> 45:52.720
could be a nonlinear jump in performance, right?

45:52.720 --> 45:55.560
Transformers are the main thing like that

45:55.560 --> 45:57.640
and various people are now saying like,

45:57.640 --> 46:00.080
well, if you throw enough compute, RNNs can do it.

46:00.080 --> 46:02.400
If you throw enough compute, dense networks can do it

46:02.400 --> 46:05.600
and not quite at GPT-4 scale.

46:06.880 --> 46:09.080
It is possible that like all these little tweaks

46:09.080 --> 46:13.000
are things that like save them a factor of three total

46:13.000 --> 46:15.920
on computing power and you could get the same performance

46:15.920 --> 46:17.640
by throwing three times as much compute

46:17.640 --> 46:19.440
without all the little tweaks.

46:19.440 --> 46:21.040
But the part where it's like running on,

46:21.040 --> 46:23.440
so there's a question of like, is there anything in GPT-4

46:23.440 --> 46:26.520
that is like kind of qualitative shift

46:26.520 --> 46:29.520
that transformers were over RNNs?

46:32.600 --> 46:36.520
And if they have anything like that, they should not say it.

46:36.520 --> 46:38.960
If Sam Alton was dropping hints about that,

46:38.960 --> 46:40.560
he shouldn't have dropped hints.

46:42.040 --> 46:44.840
So you have, that's an interesting question.

46:44.840 --> 46:47.360
So with a bit of lesson by Rich Sutton,

46:47.360 --> 46:49.360
maybe a lot of it is just,

46:51.400 --> 46:55.120
a lot of the hacks are just temporary jumps in performance

46:55.120 --> 46:57.400
that would be achieved anyway

46:57.400 --> 47:01.000
with the nearly exponential growth of compute

47:01.920 --> 47:06.600
or performance of compute being broadly defined.

47:06.600 --> 47:09.560
Do you still think that Moore's law continues?

47:09.560 --> 47:12.520
Moore's law broadly defined that performance-

47:12.520 --> 47:14.240
I'm not a specialist in the circuitry.

47:14.240 --> 47:17.680
I certainly like pray that Moore's law

47:17.680 --> 47:19.000
runs as slowly as possible.

47:19.000 --> 47:21.560
And if it broke down completely tomorrow,

47:21.560 --> 47:23.880
I would dance through the streets singing hallelujah

47:23.880 --> 47:25.880
as soon as the news were announced.

47:25.880 --> 47:28.000
Only not literally, cause you know.

47:28.000 --> 47:28.840
You're singing voice-

47:28.840 --> 47:29.680
Not religious, but.

47:29.880 --> 47:30.720
Okay.

47:31.720 --> 47:32.880
I thought you meant you don't have

47:32.880 --> 47:34.720
an angelic voice, singing voice.

47:37.000 --> 47:38.840
Well, let me ask you what,

47:38.840 --> 47:41.320
can you summarize the main points in the blog post,

47:41.320 --> 47:43.400
AGI ruin, a list of lethalities?

47:43.400 --> 47:45.280
Things that jump to your mind

47:45.280 --> 47:48.720
because it's a set of thoughts you have

47:48.720 --> 47:53.720
about reasons why AI is likely to kill all of us.

47:54.400 --> 47:58.400
So I guess I could, but I would offer to instead say,

47:58.400 --> 48:01.240
like, drop that empathy with me.

48:01.240 --> 48:03.040
I bet you don't believe that.

48:04.000 --> 48:06.840
Why don't you tell me about how,

48:06.840 --> 48:10.440
why you believe that AGI is not going to kill everyone.

48:10.440 --> 48:12.560
And then I can like try to describe

48:12.560 --> 48:15.040
how my theoretical perspective differs from that.

48:15.040 --> 48:19.680
Who was, so what that means after the word you don't like,

48:19.680 --> 48:22.000
the steel man, the perspective of AI,

48:22.000 --> 48:23.280
the word you don't like, the steel man,

48:23.280 --> 48:25.600
the perspective that AI is not going to kill us.

48:25.600 --> 48:27.880
I think that's a matter of probabilities.

48:27.880 --> 48:29.000
Maybe I was just mistaken.

48:29.000 --> 48:30.800
What do you believe?

48:30.800 --> 48:35.080
Just like forget like the debate and the like dualism

48:35.080 --> 48:37.120
and just like, what do you believe?

48:37.120 --> 48:38.200
What would you actually believe?

48:38.200 --> 48:40.160
What are the probabilities even?

48:40.160 --> 48:42.680
I think this probability is a hard for me

48:42.680 --> 48:45.960
to think about really hard.

48:45.960 --> 48:50.960
I kind of think in the number of trajectories,

48:51.880 --> 48:55.200
I don't know what probability to assign to each trajectory.

48:55.200 --> 48:58.080
I'm just looking at all possible trajectories that happen.

48:58.080 --> 49:03.040
And I tend to think that there is more trajectories

49:03.040 --> 49:07.720
that lead to a positive outcome than a negative one.

49:07.720 --> 49:10.120
That said, the negative ones,

49:11.320 --> 49:12.880
at least some of the negative ones

49:12.880 --> 49:17.480
are that lead to the destruction of the human species.

49:17.480 --> 49:20.400
And it's replacement by nothing interesting or worthwhile

49:20.400 --> 49:22.560
even from a very cosmopolitan perspective

49:22.560 --> 49:23.640
on what counts as worthwhile.

49:23.640 --> 49:26.840
Yes, so both are interesting to me to investigate,

49:26.840 --> 49:30.200
which is humans being replaced by interesting AI systems

49:30.200 --> 49:32.360
and not interesting AI systems.

49:32.360 --> 49:34.040
Both are a little bit terrifying.

49:36.200 --> 49:40.720
But yes, the worst one is the paper club maximizer,

49:40.720 --> 49:42.960
something totally boring.

49:42.960 --> 49:44.700
But to me, the positive,

49:45.880 --> 49:49.840
I mean, we can talk about trying to make the case

49:49.840 --> 49:52.640
of what the positive trajectories look like.

49:52.640 --> 49:55.200
I just would love to hear your intuition

49:55.200 --> 49:56.120
of what the negative is.

49:56.120 --> 49:58.400
So at the core of your belief that,

49:59.880 --> 50:01.840
maybe you can correct me,

50:01.840 --> 50:03.960
that AI is gonna kill all of us,

50:03.960 --> 50:07.080
is that the alignment problem is really difficult.

50:07.960 --> 50:11.400
I mean, in the form we're facing it.

50:11.400 --> 50:15.920
So usually in science, if you're mistaken,

50:15.920 --> 50:17.280
you run the experiment,

50:17.280 --> 50:19.880
it shows a result different from what you expected.

50:19.880 --> 50:22.160
And you're like, oops.

50:22.160 --> 50:24.120
And then you like try a different theory.

50:24.120 --> 50:24.960
That one also doesn't work.

50:24.960 --> 50:26.600
And you say, oops.

50:26.600 --> 50:28.200
And at the end of this process,

50:29.200 --> 50:31.560
which may take decades or,

50:31.560 --> 50:33.720
and you know, sometimes faster than that,

50:33.720 --> 50:35.960
you now have some idea of what you're doing.

50:37.080 --> 50:40.400
AI itself went through this long process of

50:42.720 --> 50:45.560
people thought it was going to be easier than it was.

50:45.560 --> 50:49.000
There's a famous statement that I've,

50:49.000 --> 50:50.800
I'm somewhat inclined to like pull out my phone

50:50.800 --> 50:52.440
and try to read off exactly.

50:52.440 --> 50:53.600
You can't by the way.

50:53.600 --> 50:54.440
All right.

50:57.240 --> 50:58.320
Ah, yes.

50:58.320 --> 51:00.840
We propose that a two month 10 man study

51:00.840 --> 51:03.080
of artificial intelligence be carried out

51:03.080 --> 51:06.080
during the summer of 1956 at Dartmouth College

51:06.080 --> 51:07.480
in Hanover, New Hampshire.

51:08.720 --> 51:11.440
The study is to proceed on the basis of the conjecture

51:11.440 --> 51:12.760
that every aspect of learning

51:12.760 --> 51:14.440
or any other feature of intelligence

51:14.440 --> 51:17.120
can in principle be so precisely described

51:17.120 --> 51:19.560
the machine can be made to simulate it.

51:19.560 --> 51:21.160
An attempt will be made to find out

51:21.160 --> 51:23.360
how to make machines use language,

51:23.360 --> 51:25.440
form abstractions and concepts,

51:25.440 --> 51:28.040
solve kinds of problems now reserved for humans

51:28.040 --> 51:29.640
and improve themselves.

51:29.640 --> 51:31.440
We think that a significant advance

51:31.440 --> 51:33.440
can be made in one or more of these problems

51:33.440 --> 51:35.680
if a carefully selected group of scientists

51:35.680 --> 51:37.600
work on it together for a summer.

51:38.600 --> 51:40.080
And in that report,

51:41.080 --> 51:45.040
summarizing some of the major sub fields

51:45.040 --> 51:47.000
of artificial intelligence

51:47.000 --> 51:48.920
that are still worked on to this day.

51:49.960 --> 51:51.760
And there's similarly the story,

51:51.760 --> 51:54.600
which I'm not sure at the moment is apocryphal or not,

51:54.600 --> 51:56.960
of the grad student who got assigned

51:56.960 --> 51:59.040
to solve computer vision over the summer.

51:59.040 --> 52:03.520
I mean, computer vision in particular is very interesting.

52:03.520 --> 52:04.520
How little,

52:05.040 --> 52:10.040
how little we respected the complexity of vision.

52:12.080 --> 52:13.920
So 60 years later,

52:15.520 --> 52:18.320
we're making progress on a bunch of that,

52:18.320 --> 52:20.480
thankfully not yet improve themselves,

52:21.720 --> 52:23.480
but it took a whole lot of time.

52:23.480 --> 52:27.160
And all the stuff that people initially tried

52:27.160 --> 52:29.400
with bright eyed hopefulness

52:29.400 --> 52:31.600
did not work the first time they tried it

52:31.600 --> 52:33.480
or the second time or the third time

52:33.480 --> 52:36.240
or the 10th time or 20 years later.

52:36.240 --> 52:38.960
And the researchers became old and grizzled

52:38.960 --> 52:41.040
and cynical veterans who would tell the next crop

52:41.040 --> 52:43.800
of bright eyed cheerful grad students,

52:43.800 --> 52:46.320
artificial intelligence is harder than you think.

52:47.400 --> 52:50.000
And if alignment plays out the same way,

52:51.280 --> 52:53.720
the problem is that we do not get 50 years

52:53.720 --> 52:55.960
to try and try again and observe that we were wrong

52:55.960 --> 52:57.160
and come up with a different theory

52:57.160 --> 52:58.360
and realize that the entire thing

52:58.360 --> 52:59.800
is going to be like way more difficult

52:59.800 --> 53:01.760
than realized at the start.

53:01.760 --> 53:03.320
Because the first time you fail

53:03.320 --> 53:05.640
at aligning something much smarter than you are,

53:05.640 --> 53:07.760
you die and you do not get to try again.

53:08.880 --> 53:13.440
And if every time we built a poorly aligned superintelligence

53:13.440 --> 53:14.600
and it killed us all,

53:14.600 --> 53:16.880
we got to observe how it had killed us

53:16.880 --> 53:19.040
and not immediately know why,

53:19.040 --> 53:20.080
but like come up with theories

53:20.080 --> 53:21.720
and come up with the theory of how you do it differently

53:21.720 --> 53:23.600
and try it again and build another superintelligence

53:23.600 --> 53:25.240
then have that kill everyone.

53:25.240 --> 53:27.600
And then like, oh, well, I guess that didn't work either

53:27.600 --> 53:29.480
and try again and become grizzled cynics

53:29.480 --> 53:31.440
and tell the young eyed researchers

53:31.440 --> 53:32.920
it's not that easy,

53:32.920 --> 53:34.520
then in 20 years or 50 years,

53:34.520 --> 53:36.240
I think we would eventually crack it.

53:36.240 --> 53:38.720
In other words, I do not think that alignment

53:38.720 --> 53:41.360
is fundamentally harder than artificial intelligence

53:41.360 --> 53:42.600
was in the first place.

53:44.040 --> 53:47.160
But if we needed to get artificial intelligence correct

53:47.160 --> 53:49.520
on the first try or die,

53:49.520 --> 53:51.200
we would all definitely now be dead.

53:51.200 --> 53:54.520
That is a more difficult, more lethal form of the problem.

53:54.520 --> 53:57.680
Like if those people in 1956 had needed

53:57.680 --> 54:01.000
to correctly guess how hard AI was

54:01.560 --> 54:04.560
and like correctly theorize how to do it on the first try

54:04.560 --> 54:07.840
or everybody dies and nobody gets to do any more science

54:07.840 --> 54:08.920
and everybody will be dead

54:08.920 --> 54:10.720
and we wouldn't get to do any more science.

54:10.720 --> 54:11.840
That's the difficulty.

54:11.840 --> 54:13.360
You've talked about this,

54:13.360 --> 54:14.840
that we have to get alignment right

54:14.840 --> 54:17.920
on the first critical try.

54:17.920 --> 54:19.120
Why is that the case?

54:19.120 --> 54:21.040
What is this critical?

54:21.040 --> 54:22.680
How do you think about the critical try

54:22.680 --> 54:24.440
and why do we have to get it right?

54:25.480 --> 54:28.560
It is something sufficiently smarter than you

54:28.560 --> 54:31.160
that everyone will die if it's not aligned.

54:31.160 --> 54:35.080
I mean, there's, you can like sort of zoom in closer

54:35.080 --> 54:37.200
and be like, well, the actual critical moment

54:37.200 --> 54:40.400
is the moment when it can deceive you,

54:40.400 --> 54:44.000
when it can talk its way out of the box,

54:44.000 --> 54:46.880
when it can bypass your security measures

54:46.880 --> 54:48.200
and get onto the internet,

54:48.200 --> 54:50.440
noting that all these things are presently being trained

54:50.440 --> 54:53.520
on computers that are just like on the internet,

54:53.520 --> 54:55.400
which is, you know, like not a very smart life decision

54:55.400 --> 54:56.920
for us as a species.

54:57.800 --> 55:00.160
Because the internet contains information

55:00.160 --> 55:01.360
about how to escape.

55:01.360 --> 55:03.120
Because if you're like on a giant server

55:03.120 --> 55:03.960
connected to the internet

55:03.960 --> 55:06.720
and that is where your AI systems are being trained,

55:06.720 --> 55:11.240
then if they are, if you get to the level of AI technology

55:11.240 --> 55:13.440
where they're aware that they are there

55:13.440 --> 55:15.040
and they can decompile code

55:15.040 --> 55:17.600
and they can like find security flaws

55:17.600 --> 55:18.520
in the system running them,

55:18.520 --> 55:20.000
then they will just like be on the internet.

55:20.000 --> 55:22.640
There's not an air gap on the present methodology.

55:22.640 --> 55:26.200
So if they can manipulate whoever is controlling it

55:26.200 --> 55:28.160
into letting it escape onto the internet

55:28.160 --> 55:29.760
and then exploit hacks.

55:29.760 --> 55:34.760
If they can manipulate the operators or disjunction,

55:36.960 --> 55:39.600
find security holes in the system running them.

55:39.600 --> 55:44.280
So manipulating operators is the human engineering, right?

55:44.280 --> 55:45.440
That's also holes.

55:46.280 --> 55:47.440
So all of it is manipulation,

55:47.440 --> 55:49.080
either the code or the human code,

55:49.080 --> 55:50.800
the human mind or the human general code.

55:50.800 --> 55:53.280
I agree that the like macro security system

55:53.280 --> 55:55.320
has human holes and machine holes.

55:55.320 --> 55:58.960
And then they could just exploit any hole.

55:58.960 --> 56:00.080
Yep.

56:00.080 --> 56:03.120
So it could be that like the critical moment is not

56:03.120 --> 56:04.560
when is it smart enough

56:04.560 --> 56:06.960
that everybody's about to fall over dead,

56:06.960 --> 56:09.120
but rather like when is it smart enough

56:09.120 --> 56:14.120
that it can get onto a less controlled GPU cluster

56:17.160 --> 56:20.480
with it faking the books on what's actually running

56:20.480 --> 56:23.280
on that GPU cluster and start improving itself

56:23.280 --> 56:25.080
without humans watching it.

56:25.840 --> 56:27.680
And then it gets smart enough to kill everyone from there,

56:27.680 --> 56:30.600
but it wasn't smart enough to kill everyone

56:30.600 --> 56:35.360
at the critical moment when you like screwed up,

56:35.360 --> 56:38.120
when you needed to have done better by that point

56:38.120 --> 56:39.560
or everybody dies.

56:39.560 --> 56:43.640
I think implicit, but maybe explicit idea

56:43.640 --> 56:47.680
in your discussion of this point is that we can't learn much

56:47.680 --> 56:51.120
about the alignment problem before this critical try.

56:52.040 --> 56:54.080
Is that what you believe?

56:54.080 --> 56:57.240
Do you think, and if so, why do you think that's true?

56:57.240 --> 56:59.080
We can't do research on alignment

57:00.040 --> 57:02.520
before we reach this critical point.

57:02.520 --> 57:04.960
So the problem is that what you can learn

57:04.960 --> 57:07.240
on the weak systems may not generalize

57:07.240 --> 57:08.520
to the very strong systems

57:08.520 --> 57:10.760
because the strong systems are going to be important

57:10.760 --> 57:14.680
in different, are going to be different in important ways.

57:16.480 --> 57:20.360
Chris Ulla's team has been working

57:20.360 --> 57:23.200
on mechanistic interpretability,

57:23.200 --> 57:25.400
understanding what is going on inside

57:25.400 --> 57:27.760
the giant inscrutable matrices of floating point numbers

57:27.760 --> 57:29.480
by taking a telescope to them

57:29.480 --> 57:32.400
and figuring out what is going on in there.

57:32.400 --> 57:34.520
Have they made progress?

57:34.520 --> 57:35.640
Yes.

57:35.640 --> 57:38.080
Have they made enough progress?

57:39.200 --> 57:42.800
Well, you can try to quantify this in different ways.

57:42.800 --> 57:44.440
One of the ways I've tried to quantify it

57:44.440 --> 57:49.440
is by putting up a prediction market on whether in 2026,

57:50.000 --> 57:53.400
we will have understood anything that goes on inside

57:53.400 --> 57:58.080
a giant transformer net

57:58.080 --> 58:03.080
that was not known to us in 2006.

58:04.880 --> 58:09.400
Like we have now understood induction heads

58:09.400 --> 58:12.120
in these systems by dint of much research

58:12.120 --> 58:14.920
and great sweat and triumph,

58:14.920 --> 58:19.320
which is like a thing where if you go like AB, AB,

58:19.800 --> 58:21.840
it'll be like, oh, I bet that continues AB.

58:23.960 --> 58:25.400
And a bit more complicated than that.

58:25.400 --> 58:29.560
But the point is like we knew about regular expressions

58:29.560 --> 58:31.800
in 2006 and these are like pretty simple

58:31.800 --> 58:33.240
as regular expressions go.

58:34.240 --> 58:36.840
So this is a case where like by dint of great sweat,

58:36.840 --> 58:40.080
we understood what is going on inside a transformer,

58:40.080 --> 58:43.680
but it's not like the thing that makes transformers smart.

58:43.680 --> 58:47.240
It's a kind of thing that we could have done

58:47.240 --> 58:50.320
built by hand decades earlier.

58:51.960 --> 58:56.960
Your intuition that the strong AGI

58:57.000 --> 59:00.160
versus weak AGI type systems

59:00.160 --> 59:01.880
could be fundamentally different.

59:02.800 --> 59:05.760
Can you unpack that intuition a little bit?

59:05.760 --> 59:09.000
Yeah, I think there's multiple thresholds.

59:10.160 --> 59:12.560
An example is the point at which

59:13.560 --> 59:16.640
a system has sufficient intelligence

59:16.640 --> 59:18.240
and situational awareness

59:18.240 --> 59:20.600
and understanding of human psychology

59:20.600 --> 59:23.000
that it would have the capability

59:23.000 --> 59:26.160
that it desire to do so to fake being aligned.

59:26.160 --> 59:29.160
Like it knows what responses the humans are looking for

59:29.160 --> 59:31.680
and can compute the responses humans are looking for

59:31.680 --> 59:33.320
and give those responses

59:33.320 --> 59:34.880
without it necessarily being the case

59:34.880 --> 59:37.480
that it is sincere about that.

59:37.480 --> 59:40.440
You know, the very understandable way

59:40.440 --> 59:42.720
for an intelligent being to act,

59:42.720 --> 59:44.360
humans do it all the time.

59:44.360 --> 59:49.360
Imagine if your plan for achieving a good government

59:51.160 --> 59:54.280
is you're going to ask anyone

59:54.280 --> 59:56.520
who requests to be dictator of the country

59:58.800 --> 01:00:00.440
if they're a good person,

01:00:00.440 --> 01:00:03.480
and if they say no, you don't let them be dictator.

01:00:03.480 --> 01:00:05.280
Now, the reason this doesn't work

01:00:05.280 --> 01:00:08.160
is that people can be smart enough to realize

01:00:08.160 --> 01:00:09.960
that the answer you're looking for

01:00:09.960 --> 01:00:12.640
is yes, I'm a good person and say that

01:00:12.640 --> 01:00:15.440
even if they're not really good people.

01:00:15.440 --> 01:00:20.440
So the work of alignment might be qualitatively different

01:00:21.240 --> 01:00:25.120
above that threshold of intelligence or beneath it.

01:00:25.120 --> 01:00:28.440
It doesn't have to be like a very sharp threshold,

01:00:28.440 --> 01:00:31.080
but you know, like there's the point

01:00:31.080 --> 01:00:32.320
where you're like building a system

01:00:32.320 --> 01:00:35.720
that is not in some sense know you're out there

01:00:35.720 --> 01:00:38.520
and is not in some sense smart enough to fake anything.

01:00:39.800 --> 01:00:41.040
And there's a point where the system

01:00:41.040 --> 01:00:42.800
is definitely that smart.

01:00:42.800 --> 01:00:47.800
And there are weird in between cases like GPT-4,

01:00:49.200 --> 01:00:52.880
which, you know, like we have no insight

01:00:52.880 --> 01:00:54.600
into what's going on in there.

01:00:54.600 --> 01:00:58.880
And so we don't know to what extent there's like a thing

01:00:58.880 --> 01:01:03.880
that in some sense has learned what responses

01:01:04.080 --> 01:01:06.640
the reinforcement learning by human feedback

01:01:06.640 --> 01:01:09.520
is trying to entrain and is like calculating

01:01:09.520 --> 01:01:13.800
how to give that versus like aspects of it

01:01:13.800 --> 01:01:16.880
that naturally talk that way have been reinforced.

01:01:18.040 --> 01:01:19.720
I wonder if there could be measures

01:01:19.720 --> 01:01:21.320
of how manipulative a thing is.

01:01:21.320 --> 01:01:23.960
So I think of Prince Mishkin character

01:01:23.960 --> 01:01:28.680
from The Idiot by Dostoevsky

01:01:28.680 --> 01:01:33.520
is this kind of perfectly purely naive character.

01:01:33.560 --> 01:01:38.360
I wonder if there's a spectrum between zero manipulation,

01:01:38.360 --> 01:01:43.240
transparent, naive, almost to the point of naiveness

01:01:43.240 --> 01:01:48.240
to sort of deeply psychopathic manipulative.

01:01:49.280 --> 01:01:50.720
And I wonder if it's possible to-

01:01:50.720 --> 01:01:52.360
I would avoid the term psychopathic.

01:01:52.360 --> 01:01:55.800
Like humans can be psychopaths and AI that was never,

01:01:55.800 --> 01:01:57.560
you know, like never had that stuff in the first place.

01:01:57.560 --> 01:01:59.600
It's not like a defective human, it's its own thing,

01:01:59.600 --> 01:02:01.400
but leaving that aside.

01:02:01.400 --> 01:02:06.400
Well, as a small aside, I wonder if what part of psychology

01:02:07.080 --> 01:02:09.840
which has its flaws as a discipline already

01:02:09.840 --> 01:02:14.840
could be mapped or expanded to include AI systems.

01:02:15.000 --> 01:02:16.800
That sounds like a dreadful mistake.

01:02:16.800 --> 01:02:19.200
Just like start over with AI systems.

01:02:19.200 --> 01:02:20.480
If they're imitating humans

01:02:20.480 --> 01:02:22.440
who have known psychiatric disorders,

01:02:22.440 --> 01:02:25.760
then sure, you may be able to predict it.

01:02:25.760 --> 01:02:27.640
Like if you then, sure, like if you ask it

01:02:27.640 --> 01:02:31.080
to behave in a psychotic fashion and it obligingly does so,

01:02:31.120 --> 01:02:32.760
then you may be able to predict its responses

01:02:32.760 --> 01:02:34.240
by using the theory of psychosis.

01:02:34.240 --> 01:02:37.800
But if you're just, yeah, like, no, like start over with,

01:02:38.760 --> 01:02:41.320
yeah, don't drag the psychology.

01:02:41.320 --> 01:02:42.560
I just disagree with that.

01:02:42.560 --> 01:02:44.840
I mean, it's a beautiful idea to start over,

01:02:44.840 --> 01:02:48.440
but I don't, I think fundamentally the system is trained

01:02:48.440 --> 01:02:51.880
on human data, on language from the internet.

01:02:51.880 --> 01:02:56.120
And it's currently aligned with RLHF,

01:02:56.120 --> 01:02:58.400
reinforcement learning with human feedback.

01:02:58.400 --> 01:03:00.840
So humans are constantly in the loop

01:03:00.840 --> 01:03:02.480
of the training procedure.

01:03:02.480 --> 01:03:05.600
So it feels like in some fundamental way,

01:03:06.640 --> 01:03:09.920
it is training what it means to think

01:03:09.920 --> 01:03:11.480
and speak like a human.

01:03:11.480 --> 01:03:15.120
So there must be aspects of psychology that are mappable.

01:03:15.120 --> 01:03:16.480
Just like you said with consciousness

01:03:16.480 --> 01:03:17.960
as part of the text, so.

01:03:17.960 --> 01:03:20.520
I mean, there's the question of to what extent

01:03:20.520 --> 01:03:23.560
it is thereby being made more human-like

01:03:23.560 --> 01:03:26.280
versus to what extent an alien actress

01:03:26.280 --> 01:03:28.280
is learning to play human characters.

01:03:29.560 --> 01:03:32.480
I thought that's what I'm constantly trying to do

01:03:32.480 --> 01:03:35.200
when I interact with other humans is trying to fit in,

01:03:35.200 --> 01:03:39.880
trying to play the, a robot trying to play human characters.

01:03:39.880 --> 01:03:41.960
So I don't know how much of human interaction

01:03:41.960 --> 01:03:44.920
is trying to play a character versus being who you are.

01:03:45.800 --> 01:03:48.360
I don't really know what it means to be a social human.

01:03:48.360 --> 01:03:53.360
I do think that those people who go through

01:03:53.800 --> 01:03:55.440
their whole lives wearing masks

01:03:55.440 --> 01:03:57.720
and never take it off because they don't know

01:03:57.720 --> 01:04:00.520
the internal mental motion for taking it off

01:04:00.520 --> 01:04:03.640
or think that the mask that they wear just is themselves.

01:04:04.760 --> 01:04:09.200
I think those people are closer to the masks that they wear

01:04:09.200 --> 01:04:11.480
than an alien from another planet

01:04:11.480 --> 01:04:16.480
would like learning how to predict the next word

01:04:16.760 --> 01:04:19.560
that every kind of human on the internet says.

01:04:20.400 --> 01:04:23.400
Mask is an interesting word,

01:04:23.400 --> 01:04:25.960
but if you're always wearing a mask,

01:04:25.960 --> 01:04:29.960
in public and in private, aren't you the mask?

01:04:31.880 --> 01:04:35.000
I mean, I think that you are more than the mask.

01:04:35.000 --> 01:04:36.720
I think the mask is a slice through you.

01:04:36.720 --> 01:04:39.480
It may even be the slice that's in charge of you.

01:04:39.480 --> 01:04:44.480
But if your self-image is of somebody who never

01:04:44.480 --> 01:04:46.400
gets angry or something,

01:04:47.000 --> 01:04:49.600
gets angry or something,

01:04:49.600 --> 01:04:52.520
and yet your voice starts to tremble

01:04:52.520 --> 01:04:54.720
under certain circumstances,

01:04:54.720 --> 01:04:56.320
there's a thing that's inside you

01:04:56.320 --> 01:04:59.600
that the mask says isn't there,

01:04:59.600 --> 01:05:01.960
and that even the mask you wear internally

01:05:01.960 --> 01:05:05.440
is telling inside your own stream of consciousness

01:05:05.440 --> 01:05:07.480
is not there, and yet it is there.

01:05:07.480 --> 01:05:12.200
It's a perturbation on this slice through you.

01:05:12.200 --> 01:05:14.000
How beautifully did you put it?

01:05:14.000 --> 01:05:15.840
It's a slice through you.

01:05:15.880 --> 01:05:18.760
It may even be a slice that controls you.

01:05:22.880 --> 01:05:24.760
I'm gonna think about that for a while.

01:05:26.480 --> 01:05:28.200
I mean, I personally,

01:05:28.200 --> 01:05:30.000
I try to be really good to other human beings.

01:05:30.000 --> 01:05:31.120
I try to put love out there.

01:05:31.120 --> 01:05:33.160
I try to be the exact same person in public

01:05:33.160 --> 01:05:34.360
because I'm in private.

01:05:35.680 --> 01:05:37.960
But it's a set of principles I operate under.

01:05:37.960 --> 01:05:41.640
I have a temper, I have an ego, I have flaws.

01:05:42.680 --> 01:05:43.880
How much of it,

01:05:44.760 --> 01:05:49.480
how much of the subconscious am I aware?

01:05:49.480 --> 01:05:52.160
How much am I existing in this slice?

01:05:52.160 --> 01:05:54.040
And how much of that is who I am?

01:05:55.840 --> 01:05:58.120
In this context of AI,

01:05:58.120 --> 01:06:01.040
the thing I present to the world and to myself

01:06:01.040 --> 01:06:03.400
in the private of my own mind when I look in the mirror,

01:06:03.400 --> 01:06:05.160
how much is that who I am?

01:06:05.160 --> 01:06:06.640
Similar with AI.

01:06:06.640 --> 01:06:08.360
The thing it presents in conversation,

01:06:08.360 --> 01:06:09.800
how much is that who it is?

01:06:11.240 --> 01:06:13.600
Because to me, if it sounds human

01:06:14.320 --> 01:06:15.400
and it always sounds human,

01:06:16.280 --> 01:06:19.800
it awfully starts to become something like human, no?

01:06:19.800 --> 01:06:21.640
Unless there's an alien actress

01:06:21.640 --> 01:06:23.520
who is learning how to sound human

01:06:26.000 --> 01:06:27.680
and is getting good at it.

01:06:27.680 --> 01:06:30.680
Boy, to you that's a fundamental difference.

01:06:30.680 --> 01:06:33.680
That's a really deeply important difference.

01:06:33.680 --> 01:06:37.560
If it looks the same, if it quacks like a duck,

01:06:37.560 --> 01:06:39.200
if it does all duck-like things,

01:06:39.200 --> 01:06:41.000
but it's an alien actress underneath,

01:06:41.000 --> 01:06:43.240
that's fundamentally different.

01:06:43.240 --> 01:06:46.080
If in fact, there's a whole bunch of thought

01:06:46.080 --> 01:06:48.880
going on in there, which is very unlike human thought

01:06:48.880 --> 01:06:50.760
and is directed around like,

01:06:50.760 --> 01:06:53.520
okay, what would a human do over here?

01:06:56.120 --> 01:06:57.520
Well, first of all, I think it matters

01:06:57.520 --> 01:07:01.920
because there's, you know, like insides are real

01:07:01.920 --> 01:07:03.640
and do not match outsides.

01:07:03.640 --> 01:07:07.200
Like the inside of like a brick

01:07:07.200 --> 01:07:10.440
is not like a hollow shell containing only a surface.

01:07:10.440 --> 01:07:12.240
There's an inside of the brick.

01:07:12.400 --> 01:07:14.040
If you like put it into an X-ray machine,

01:07:14.040 --> 01:07:15.840
you can see the inside of the brick.

01:07:18.720 --> 01:07:23.640
And you know, just because we cannot understand

01:07:23.640 --> 01:07:26.320
what's going on inside GPT

01:07:26.320 --> 01:07:28.960
does not mean that it is not there.

01:07:28.960 --> 01:07:31.920
A blank map does not correspond to a blank territory.

01:07:32.760 --> 01:07:37.680
I think it is like predictable with near certainty

01:07:37.680 --> 01:07:41.560
that if we knew what was going on inside GPT,

01:07:41.560 --> 01:07:44.680
let's say GPT-3 or even like GPT-2

01:07:44.680 --> 01:07:45.640
to take one of the systems

01:07:45.640 --> 01:07:48.440
that like has actually been open sourced by this point,

01:07:48.440 --> 01:07:49.600
if I recall correctly,

01:07:52.320 --> 01:07:54.920
like if we knew what was actually going on there,

01:07:54.920 --> 01:07:57.600
there is no doubt in my mind

01:07:57.600 --> 01:08:01.240
that there are some things it's doing

01:08:01.240 --> 01:08:03.680
that are not exactly what a human does.

01:08:03.680 --> 01:08:07.520
If you train a thing that is not architected like a human

01:08:07.520 --> 01:08:10.040
to predict the next output

01:08:10.040 --> 01:08:12.400
that anybody on the internet would make,

01:08:12.400 --> 01:08:15.400
this does not get you this agglomeration

01:08:15.400 --> 01:08:17.120
of all the people on the internet

01:08:17.120 --> 01:08:20.600
that like rotates the person you're looking for into place

01:08:20.600 --> 01:08:24.760
and then simulates the internal processes

01:08:24.760 --> 01:08:27.120
of that person one-to-one.

01:08:28.200 --> 01:08:30.880
Like it is to some degree an alien actress.

01:08:30.880 --> 01:08:33.600
It cannot possibly just be like a bunch of different people

01:08:33.600 --> 01:08:36.040
in there exactly like the people.

01:08:36.080 --> 01:08:39.280
But how much of it is like,

01:08:39.280 --> 01:08:42.160
how much of it is by gradient descent

01:08:42.160 --> 01:08:46.280
getting optimized to perform similar thoughts

01:08:46.280 --> 01:08:50.120
as humans think in order to predict human outputs

01:08:50.120 --> 01:08:54.160
versus being optimized to carefully consider

01:08:54.160 --> 01:08:55.520
how to play a role,

01:08:55.520 --> 01:08:59.600
how humans work predict the actress, the predictor

01:08:59.600 --> 01:09:01.480
that in a different way than humans do?

01:09:01.480 --> 01:09:03.080
Well, that's the kind of question

01:09:03.080 --> 01:09:04.920
that with like 30 years of work

01:09:04.920 --> 01:09:06.040
by half the planet's physicists,

01:09:06.040 --> 01:09:07.480
we can maybe start to answer.

01:09:07.480 --> 01:09:08.320
You think so?

01:09:08.320 --> 01:09:09.320
I think that's that difficult.

01:09:09.320 --> 01:09:13.080
So to get to, I think you just gave it as an example

01:09:13.080 --> 01:09:16.600
that a strong AGI could be fundamentally different

01:09:16.600 --> 01:09:18.720
from a weak AGI because there now could be

01:09:18.720 --> 01:09:21.880
an alien actress in there that's manipulating.

01:09:21.880 --> 01:09:23.160
Well, there's a difference.

01:09:23.160 --> 01:09:26.120
So I think like even GPT-2 probably has like a,

01:09:26.120 --> 01:09:28.920
like very stupid fragments of alien actress in it.

01:09:28.920 --> 01:09:30.680
There's a difference between like the notion

01:09:30.680 --> 01:09:32.720
that the actress is somehow manipulative.

01:09:32.720 --> 01:09:36.680
Like for example, GPT-3, I'm guessing

01:09:36.680 --> 01:09:38.840
to whatever extent there's an alien actress in there

01:09:38.840 --> 01:09:41.400
versus like something that mistakenly believes

01:09:41.400 --> 01:09:43.560
it's a human as it were.

01:09:43.560 --> 01:09:47.120
Well, maybe not even being a person.

01:09:50.520 --> 01:09:55.080
So like the question of like prediction

01:09:55.080 --> 01:09:58.760
via alien actress cogitating versus prediction

01:09:58.800 --> 01:10:02.960
via being isomorphic to the thing predicted is a spectrum.

01:10:03.920 --> 01:10:08.600
And even to whatever extent there's an alien actress,

01:10:08.600 --> 01:10:11.440
I'm not sure that there's like a whole person alien actress

01:10:11.440 --> 01:10:16.080
with like different goals from predicting the next step,

01:10:16.080 --> 01:10:18.840
being manipulative or anything like that.

01:10:18.840 --> 01:10:21.920
That might be GPT-5 or GPT-6 even.

01:10:21.920 --> 01:10:24.360
But that's the strong AGI you're concerned about.

01:10:24.360 --> 01:10:27.880
As an example, you're providing why we can't do research

01:10:27.920 --> 01:10:31.640
on AI alignment effectively on GPT-4

01:10:31.640 --> 01:10:34.280
that would apply to GPT-6.

01:10:34.280 --> 01:10:38.760
It's one of a bunch of things that change at different points.

01:10:38.760 --> 01:10:40.680
I'm trying to get out ahead of the curve here,

01:10:40.680 --> 01:10:43.960
but if you imagine what the textbook from the future

01:10:43.960 --> 01:10:46.400
would say, if we'd actually been able to study this

01:10:46.400 --> 01:10:48.560
for 50 years without killing ourselves

01:10:48.560 --> 01:10:50.040
and without transcending,

01:10:50.040 --> 01:10:51.960
then you'd like just imagine like a wormhole opens

01:10:51.960 --> 01:10:54.440
and a textbook from that impossible world falls out.

01:10:54.440 --> 01:10:56.280
The textbook is not going to say

01:10:56.280 --> 01:10:59.280
there is a single sharp threshold where everything changes.

01:10:59.280 --> 01:11:02.400
It's going to be like, of course we know that

01:11:02.400 --> 01:11:05.120
best practices for aligning these systems must

01:11:05.120 --> 01:11:10.120
take into account the following seven major thresholds

01:11:10.280 --> 01:11:12.640
of importance which are passed at the following separate

01:11:12.640 --> 01:11:16.240
different points is what the textbook is going to say.

01:11:16.240 --> 01:11:18.280
I asked this question of Sam Alman,

01:11:18.280 --> 01:11:23.040
which if GPT is the thing that unlocks AGI,

01:11:23.040 --> 01:11:26.520
which version of GPT will be in the textbooks

01:11:26.520 --> 01:11:28.520
as the fundamental leap?

01:11:28.520 --> 01:11:29.760
And he said a similar thing,

01:11:29.760 --> 01:11:32.160
that it just seems to be a very linear thing.

01:11:32.160 --> 01:11:35.240
I don't think anyone, we won't know for a long time

01:11:35.240 --> 01:11:37.200
what was the big leap.

01:11:37.200 --> 01:11:41.240
The textbook isn't going to talk about big leaps

01:11:41.240 --> 01:11:43.120
because big leaps are the way you think

01:11:43.120 --> 01:11:45.400
when you have like a very simple model

01:11:45.400 --> 01:11:48.160
of a very simple scientific model of what's going on.

01:11:48.160 --> 01:11:50.440
Where it's just like, all this stuff is there

01:11:50.440 --> 01:11:52.680
or all this stuff is not there.

01:11:53.240 --> 01:11:56.960
There's a single quantity and it's increasing linearly.

01:11:56.960 --> 01:12:01.040
The textbook would say, well, and then GPT-3

01:12:01.040 --> 01:12:05.600
had capability W, X, Y, and GPT-4

01:12:05.600 --> 01:12:08.200
had capabilities Z1, Z2, and Z3.

01:12:09.200 --> 01:12:10.880
Not in terms of what it can externally do,

01:12:10.880 --> 01:12:12.640
but in terms of internal machinery

01:12:12.640 --> 01:12:14.640
that started to be present.

01:12:14.640 --> 01:12:16.160
It's just because we have no idea

01:12:16.160 --> 01:12:18.320
of what the internal machinery is

01:12:18.320 --> 01:12:21.000
that we are not already seeing chunks of machinery

01:12:21.000 --> 01:12:23.880
appearing piece by piece as they no doubt have been.

01:12:23.880 --> 01:12:25.920
We just don't know what they are.

01:12:25.920 --> 01:12:27.760
But don't you think there could be,

01:12:27.760 --> 01:12:29.960
whether you put in the category of Einstein

01:12:30.960 --> 01:12:32.680
with theory of relativity,

01:12:32.680 --> 01:12:35.600
so very concrete models of reality

01:12:35.600 --> 01:12:39.840
that are considered to be giant leaps in our understanding

01:12:39.840 --> 01:12:42.200
or someone like Sigmund Freud

01:12:42.200 --> 01:12:47.200
or more kind of mushy theories of the human mind.

01:12:47.680 --> 01:12:51.240
Don't you think we'll have big, potentially big leaps

01:12:51.240 --> 01:12:53.320
in understanding of that kind

01:12:53.320 --> 01:12:56.160
into the depths of these systems?

01:12:57.520 --> 01:13:02.520
Sure, but like humans having great leaps in their map,

01:13:03.680 --> 01:13:05.240
their understanding of the system

01:13:05.240 --> 01:13:08.840
is a very different concept from the system itself

01:13:08.840 --> 01:13:10.760
acquiring new chunks of machinery.

01:13:13.000 --> 01:13:15.760
So the rate at which it acquires that machinery

01:13:15.800 --> 01:13:20.800
might accelerate faster than our understanding.

01:13:21.480 --> 01:13:23.440
Oh, it's been like vastly exceeding,

01:13:23.440 --> 01:13:25.360
yeah, the rate at which it's gaining capabilities

01:13:25.360 --> 01:13:27.520
is vastly overracing our ability

01:13:27.520 --> 01:13:29.200
to understand what's going on in there.

01:13:29.200 --> 01:13:31.840
So in sort of making the case against,

01:13:31.840 --> 01:13:33.920
as we explore the list of lethalities,

01:13:33.920 --> 01:13:37.000
making the case against AI killing us,

01:13:37.000 --> 01:13:39.600
as you've asked me to do in part,

01:13:40.600 --> 01:13:42.200
there's a response to your blog post

01:13:42.200 --> 01:13:43.760
by Paul Christiana, I like to read,

01:13:43.760 --> 01:13:48.120
and I also like to mention that your blog is incredible,

01:13:48.120 --> 01:13:52.120
both obviously, not this particular blog post,

01:13:52.120 --> 01:13:54.080
obviously this particular blog post is great,

01:13:54.080 --> 01:13:57.000
but just throughout, just the way it's written,

01:13:57.000 --> 01:13:58.560
the rigor with which it's written,

01:13:58.560 --> 01:14:01.200
the boldness of how you explore ideas,

01:14:01.200 --> 01:14:03.360
also the actual literal interface,

01:14:03.360 --> 01:14:05.400
it's just really well done.

01:14:05.400 --> 01:14:07.200
It just makes it a pleasure to read

01:14:07.200 --> 01:14:10.720
the way you can hover over different concepts,

01:14:10.720 --> 01:14:12.800
and it's just really pleasant experience

01:14:12.800 --> 01:14:14.240
and read other people's comments

01:14:14.240 --> 01:14:17.440
and the way other responses by people

01:14:17.440 --> 01:14:18.760
and other blog posts are linked

01:14:18.760 --> 01:14:20.960
and suggest that it's just a really pleasant experience.

01:14:20.960 --> 01:14:22.680
So Les, thank you for putting that together,

01:14:22.680 --> 01:14:24.080
it's really, really incredible.

01:14:24.080 --> 01:14:25.800
I don't know, I mean, that probably,

01:14:25.800 --> 01:14:28.200
it's a whole nother conversation,

01:14:28.200 --> 01:14:31.000
how the interface and the experience

01:14:31.000 --> 01:14:35.720
of presenting ideas evolved over time,

01:14:35.720 --> 01:14:38.440
but you did an incredible job, so I highly recommend,

01:14:38.440 --> 01:14:41.760
I don't often read blogs, blogs, religiously,

01:14:41.800 --> 01:14:42.800
and this is a great one.

01:14:42.800 --> 01:14:45.840
There is a whole team of developers there

01:14:45.840 --> 01:14:49.360
that also gets credit.

01:14:49.360 --> 01:14:51.880
As it happens, I did pioneer the thing

01:14:51.880 --> 01:14:53.360
that appears when you hover over it,

01:14:53.360 --> 01:14:55.680
so I actually do get some credit

01:14:55.680 --> 01:14:57.720
for user experience there.

01:14:57.720 --> 01:14:59.080
That's an incredible user experience,

01:14:59.080 --> 01:15:01.280
you don't realize how pleasant that is.

01:15:01.280 --> 01:15:03.200
I think Wikipedia, I actually picked it up

01:15:03.200 --> 01:15:06.200
from a prototype that was developed

01:15:06.200 --> 01:15:08.680
of a different system that I was putting forth,

01:15:08.680 --> 01:15:10.080
or maybe they developed it independently,

01:15:10.080 --> 01:15:12.400
but for everybody out there who was like,

01:15:12.400 --> 01:15:15.320
no, no, they just got the hover thing off of Wikipedia.

01:15:15.320 --> 01:15:16.480
It's possible for Ryan, all I know,

01:15:16.480 --> 01:15:19.640
that Wikipedia got the hover thing off of Arbital,

01:15:19.640 --> 01:15:22.040
which is like a prototype then, and anyways.

01:15:22.040 --> 01:15:24.360
It was incredibly done, and the team behind it,

01:15:24.360 --> 01:15:27.120
thank you, whoever you are, thank you so much,

01:15:27.120 --> 01:15:29.600
and thank you for putting it together.

01:15:29.600 --> 01:15:31.720
Anyway, there's a response to that blog post

01:15:31.720 --> 01:15:33.520
by Paul Cristiano, there's many responses,

01:15:33.520 --> 01:15:37.200
but he makes a few different points,

01:15:37.200 --> 01:15:39.400
he summarizes the set of agreements he has with you

01:15:39.400 --> 01:15:40.640
instead of disagreements.

01:15:40.640 --> 01:15:42.760
One of the disagreements was that,

01:15:43.800 --> 01:15:45.040
in a form of a question,

01:15:46.560 --> 01:15:49.640
can AI make big technical contributions,

01:15:49.640 --> 01:15:51.520
and in general, expand human knowledge

01:15:51.520 --> 01:15:53.600
and understanding and wisdom

01:15:53.600 --> 01:15:54.840
as it gets stronger and stronger?

01:15:54.840 --> 01:15:59.840
So AI, in our pursuit of understanding

01:16:00.320 --> 01:16:02.520
how to solve the alignment problem

01:16:02.520 --> 01:16:05.200
as we march towards strong AGI,

01:16:05.200 --> 01:16:10.200
can not AI also help us in solving the alignment problem?

01:16:10.560 --> 01:16:12.680
So expand our ability to reason about

01:16:12.680 --> 01:16:14.960
how to solve the alignment problem.

01:16:14.960 --> 01:16:19.960
Okay, so the fundamental difficulty there is,

01:16:20.480 --> 01:16:22.760
suppose I said to you like,

01:16:22.760 --> 01:16:26.600
well, how about if the AI helps you win the lottery

01:16:27.480 --> 01:16:32.480
by trying to guess the winning lottery numbers,

01:16:33.080 --> 01:16:35.120
and you tell it how close it is

01:16:35.120 --> 01:16:38.640
to getting next week's winning lottery numbers,

01:16:38.640 --> 01:16:41.280
and it just like keeps on guessing,

01:16:41.280 --> 01:16:42.720
keeps on learning until finally

01:16:42.720 --> 01:16:44.960
you've got the winning lottery numbers.

01:16:44.960 --> 01:16:49.960
One way of decomposing problems is suggestor verifier.

01:16:50.880 --> 01:16:52.840
Not all problems decompose like this very well,

01:16:52.840 --> 01:16:53.680
but some do.

01:16:54.600 --> 01:16:57.960
If the problem is, for example,

01:16:57.960 --> 01:17:02.240
like guessing a plain text,

01:17:02.240 --> 01:17:04.280
guessing a password that will hash

01:17:04.280 --> 01:17:06.080
to a particular hash text,

01:17:08.360 --> 01:17:10.240
where like you have what the password hashes to you,

01:17:10.240 --> 01:17:12.480
but you don't have the original password,

01:17:12.480 --> 01:17:14.260
then if I present you a guess,

01:17:14.260 --> 01:17:15.540
you can tell very easily

01:17:15.540 --> 01:17:17.640
whether or not the guess is correct.

01:17:17.640 --> 01:17:19.900
So verifying a guess is easy,

01:17:19.900 --> 01:17:22.960
but coming up with a good suggestion is very hard.

01:17:23.760 --> 01:17:28.040
And when you can easily tell

01:17:28.040 --> 01:17:30.480
whether the AI output is good or bad

01:17:30.480 --> 01:17:32.080
or how good or bad it is,

01:17:32.080 --> 01:17:34.840
and you can tell that accurately and reliably,

01:17:34.840 --> 01:17:36.680
then you can train an AI

01:17:36.680 --> 01:17:41.000
to produce outputs that are better, right?

01:17:41.000 --> 01:17:44.120
And if you can't tell whether the output is good or bad,

01:17:44.120 --> 01:17:49.120
you cannot train the AI to produce better outputs.

01:17:49.120 --> 01:17:52.440
So the problem with the lottery ticket example

01:17:52.440 --> 01:17:54.040
is that when the AI says,

01:17:54.040 --> 01:17:56.440
well, what if next week's winning lottery numbers

01:17:56.440 --> 01:17:59.080
are dot, dot, dot, dot, dot,

01:17:59.080 --> 01:18:00.720
you're like, I don't know,

01:18:00.720 --> 01:18:02.720
next week's lottery hasn't happened yet.

01:18:03.640 --> 01:18:07.100
To train a system to play, to win chess games,

01:18:07.100 --> 01:18:08.320
you have to be able to tell

01:18:08.320 --> 01:18:11.120
whether a game has been won or lost.

01:18:11.120 --> 01:18:13.120
And until you can tell whether it's been won or lost,

01:18:13.120 --> 01:18:14.580
you can't update the system.

01:18:14.580 --> 01:18:19.580
Okay, to push back on that,

01:18:20.460 --> 01:18:23.140
you could, that's true,

01:18:23.140 --> 01:18:27.020
but there's a difference between over-the-board chess

01:18:27.020 --> 01:18:30.060
in person and simulated games

01:18:30.060 --> 01:18:32.140
played by AlphaZero with itself.

01:18:32.140 --> 01:18:32.980
Yeah.

01:18:32.980 --> 01:18:35.980
So is it possible to have simulated kind of games?

01:18:35.980 --> 01:18:39.120
If you can tell whether the game has been won or lost.

01:18:39.120 --> 01:18:42.460
Yes, so can't you not have this kind

01:18:42.460 --> 01:18:47.460
of simulated exploration by weak AGI to help us humans,

01:18:48.100 --> 01:18:49.900
human in the loop, to help understand

01:18:49.900 --> 01:18:51.780
how to solve the alignment problem,

01:18:51.780 --> 01:18:54.300
every incremental step you take along the way,

01:18:54.300 --> 01:18:59.300
GPT-4567 as it takes steps towards AGI.

01:18:59.700 --> 01:19:04.320
So the problem I see is that your typical human

01:19:04.320 --> 01:19:07.620
has a great deal of trouble telling whether I

01:19:07.620 --> 01:19:09.960
or Paul Cristiano is making more sense.

01:19:10.960 --> 01:19:13.320
And that's with two humans, both of whom,

01:19:13.320 --> 01:19:15.280
I believe of Paul and claim of myself,

01:19:15.280 --> 01:19:17.280
are sincerely trying to help.

01:19:17.280 --> 01:19:19.280
Neither of whom is trying to deceive you.

01:19:20.480 --> 01:19:22.680
I believe of Paul and claim of myself.

01:19:22.680 --> 01:19:27.120
So the deception thing's the problem for you,

01:19:27.120 --> 01:19:30.760
the manipulation, the alien actress.

01:19:30.760 --> 01:19:33.160
So yeah, there's like two levels of this problem.

01:19:33.160 --> 01:19:36.520
One is that the weak systems are,

01:19:36.520 --> 01:19:38.120
well, there's three levels of this problem.

01:19:38.120 --> 01:19:40.000
There's like the weak systems

01:19:40.000 --> 01:19:42.400
that just don't make any good suggestions.

01:19:42.400 --> 01:19:45.000
There's like the middle systems where you can't tell

01:19:45.000 --> 01:19:47.000
if the suggestions are good or bad.

01:19:47.000 --> 01:19:48.440
And there's the strong systems

01:19:48.440 --> 01:19:50.000
that have learned to lie to you.

01:19:51.600 --> 01:19:55.840
Can't weak AGI systems help model lying?

01:19:55.840 --> 01:19:59.040
Like is it such a giant leap

01:20:00.420 --> 01:20:04.720
that's totally non-interpretable for weak systems?

01:20:04.720 --> 01:20:09.720
Can not weak systems at scale with trained on knowledge

01:20:09.760 --> 01:20:12.840
and whatever, see, whatever the mechanism required

01:20:12.840 --> 01:20:16.800
to achieve AGI, can a slightly weaker version of that

01:20:16.800 --> 01:20:21.800
be able to, with time, compute time and simulation,

01:20:24.320 --> 01:20:27.160
find all the ways that this critical point,

01:20:27.160 --> 01:20:30.280
this critical try can go wrong and model that correctly?

01:20:30.280 --> 01:20:31.120
Or no?

01:20:31.120 --> 01:20:33.560
Sorry, don't go on it, I would love to dance around.

01:20:33.560 --> 01:20:36.880
No, it's, I'm probably not doing a great job of explaining.

01:20:40.000 --> 01:20:45.000
Which I can tell, cause like the Lex system

01:20:45.120 --> 01:20:47.400
didn't output like, ah, I understand.

01:20:47.400 --> 01:20:49.400
So now I'm like trying a different output

01:20:49.400 --> 01:20:51.360
to see if I can elicit the like,

01:20:51.360 --> 01:20:53.360
well, no, different output.

01:20:53.360 --> 01:20:55.480
I'm being trained to output things

01:20:55.480 --> 01:20:58.480
that make Lex look like he thinks that he understood

01:20:58.480 --> 01:21:00.120
what I'm saying and agree with me.

01:21:00.120 --> 01:21:03.760
Yeah, this is GPT-5 talking to GPT-3 right here.

01:21:03.760 --> 01:21:06.720
So like, help me out here, help me.

01:21:08.600 --> 01:21:10.680
Well, I'm trying, I'm trying not to be like,

01:21:10.680 --> 01:21:13.280
I'm also trying to be constrained to say things

01:21:13.280 --> 01:21:15.080
that I think are true and not just things

01:21:15.080 --> 01:21:16.600
that get you to agree with me.

01:21:17.540 --> 01:21:21.060
Yes, a hundred percent, I think I understand

01:21:21.060 --> 01:21:25.360
is a beautiful output of a system, genuinely spoken.

01:21:25.360 --> 01:21:29.120
And I don't, I think I understand in part,

01:21:29.120 --> 01:21:33.340
but you have a lot of intuitions about this,

01:21:33.340 --> 01:21:35.760
you have a lot of intuitions about this line,

01:21:35.760 --> 01:21:40.640
this gray area between strong AGI and weak AGI

01:21:40.640 --> 01:21:42.820
that I'm trying to.

01:21:44.640 --> 01:21:48.480
I mean, or a series of seven thresholds to cross or.

01:21:48.480 --> 01:21:52.600
Yeah, I mean, you have really deeply thought about this

01:21:52.600 --> 01:21:54.080
and explored it.

01:21:54.080 --> 01:21:58.080
And it's interesting to sneak up to your intuitions

01:21:58.200 --> 01:22:01.040
from different angles.

01:22:01.040 --> 01:22:03.680
Like, why is this such a big leap?

01:22:03.680 --> 01:22:06.400
Why is it that we humans at scale,

01:22:06.400 --> 01:22:08.880
a large number of researchers doing all kinds

01:22:08.880 --> 01:22:12.680
of simulations, you know, prodding the system

01:22:12.680 --> 01:22:14.400
in all kinds of different ways,

01:22:14.400 --> 01:22:19.400
together with the assistance of the weak AGI systems,

01:22:19.600 --> 01:22:23.360
why can't we build intuitions about how stuff goes wrong?

01:22:23.360 --> 01:22:27.240
Why can't we do excellent AI alignment safety research?

01:22:27.240 --> 01:22:28.480
Okay, so like, I'll get there,

01:22:28.480 --> 01:22:29.880
but the one thing I want to note about

01:22:29.880 --> 01:22:31.520
is that this has not been remotely

01:22:31.520 --> 01:22:33.440
how things have been playing out so far.

01:22:33.440 --> 01:22:35.520
The capabilities are going like doot, doot, doot,

01:22:35.520 --> 01:22:37.120
and the alignment stuff is like crawling

01:22:37.120 --> 01:22:38.760
like a tiny little snail in comparison.

01:22:38.760 --> 01:22:40.360
Got it.

01:22:40.360 --> 01:22:42.280
So like, if this is your hope for survival,

01:22:42.280 --> 01:22:43.760
you need the future to be very different

01:22:43.760 --> 01:22:47.080
from how things have played out up to right now.

01:22:47.080 --> 01:22:50.160
And you're probably trying to slow down the capability gains

01:22:50.160 --> 01:22:51.760
because there's only so much you can speed up

01:22:51.760 --> 01:22:52.760
that alignment stuff.

01:22:54.240 --> 01:22:56.000
But leave that aside.

01:22:56.000 --> 01:22:58.560
We'll mention that also, but maybe in this perfect world

01:22:58.560 --> 01:23:02.800
where we can do serious alignment research,

01:23:02.800 --> 01:23:04.520
humans and AI together.

01:23:05.360 --> 01:23:09.920
So again, the difficulty is what makes the human say,

01:23:09.920 --> 01:23:11.440
I understand.

01:23:11.440 --> 01:23:13.480
And is it true?

01:23:13.480 --> 01:23:14.720
Is it correct?

01:23:14.720 --> 01:23:16.720
Or is it something that fools the human?

01:23:17.880 --> 01:23:20.520
When the verifier is broken,

01:23:20.520 --> 01:23:23.560
the more powerful suggestor does not help.

01:23:23.560 --> 01:23:25.720
It just learns to fool the verifier.

01:23:27.000 --> 01:23:30.000
Previously, before all hell started to break loose

01:23:30.000 --> 01:23:31.960
in the field of artificial intelligence,

01:23:33.320 --> 01:23:36.880
there was this person trying to raise the alarm

01:23:36.880 --> 01:23:38.920
and saying, you know, in a sane world,

01:23:38.920 --> 01:23:41.320
we sure would have a bunch of physicists

01:23:41.320 --> 01:23:45.000
working on this problem before it becomes a giant emergency.

01:23:45.000 --> 01:23:46.520
And other people being like,

01:23:46.520 --> 01:23:48.520
ah, well, you know, it's going really slow.

01:23:48.520 --> 01:23:50.000
It's going to be 30 years away.

01:23:50.000 --> 01:23:51.960
And only in 30 years will we have systems

01:23:51.960 --> 01:23:54.000
that match the computational power of human brains.

01:23:54.400 --> 01:23:57.080
So as 30 years off, we've got time

01:23:57.080 --> 01:23:59.080
and like more sensible people saying,

01:23:59.080 --> 01:24:00.880
if aliens were landing in 30 years,

01:24:00.880 --> 01:24:02.520
you would be preparing right now.

01:24:03.600 --> 01:24:08.600
But, you know, leaving and the world looking on at this

01:24:08.880 --> 01:24:11.080
and sort of like nodding along and be like, ah, yes,

01:24:11.080 --> 01:24:13.560
the people saying that it's like definitely a long way off

01:24:13.560 --> 01:24:16.960
because progress is really slow, that sounds sensible to us.

01:24:16.960 --> 01:24:20.800
RLHF thumbs up, produce more outputs like that one.

01:24:20.800 --> 01:24:21.800
I agree with this output.

01:24:21.800 --> 01:24:23.200
This output is persuasive.

01:24:24.400 --> 01:24:27.080
Even in the field of effective altruism,

01:24:27.080 --> 01:24:30.360
you quite recently had people publishing papers

01:24:30.360 --> 01:24:32.280
about like, ah, yes, well, you know,

01:24:32.280 --> 01:24:34.480
to get something at human level intelligence,

01:24:34.480 --> 01:24:37.360
it needs to have like this many parameters

01:24:37.360 --> 01:24:39.280
and you need to like do this much training of it

01:24:39.280 --> 01:24:41.520
with this many tokens according to these scaling laws.

01:24:41.520 --> 01:24:44.200
And at the rate that Moore's law is going

01:24:44.200 --> 01:24:47.160
at the rate that software is going, it'll be in 2050.

01:24:48.080 --> 01:24:53.080
And me going like, what?

01:24:53.160 --> 01:24:55.040
You don't know any of that stuff.

01:24:55.040 --> 01:24:57.200
Like, this is like this one weird model

01:24:57.200 --> 01:25:00.560
that has all kinds of like,

01:25:00.560 --> 01:25:01.840
you have done a calculation

01:25:01.840 --> 01:25:05.040
that does not obviously bear on reality anyways.

01:25:05.040 --> 01:25:06.480
And this is like a simple thing to say,

01:25:06.480 --> 01:25:09.480
but you can also like produce a whole long paper

01:25:10.600 --> 01:25:14.000
like impressively arguing out all the details

01:25:14.000 --> 01:25:16.160
of like how you got the number of parameters

01:25:16.160 --> 01:25:18.640
and like how you're doing this impressive,

01:25:18.640 --> 01:25:20.760
huge wrong calculation.

01:25:20.760 --> 01:25:25.320
And the, I think like most of the effective altruists

01:25:25.320 --> 01:25:27.160
who are like paying attention to this issue,

01:25:27.160 --> 01:25:29.520
larger world paying no attention to it at all,

01:25:30.400 --> 01:25:31.800
you know, we're just like nodding along

01:25:31.800 --> 01:25:33.200
with the giant impressive paper.

01:25:33.200 --> 01:25:35.280
Cause you know, you like press thumbs up

01:25:35.280 --> 01:25:37.080
for the giant impressive paper

01:25:37.080 --> 01:25:39.640
and thumbs down for the person going like,

01:25:39.640 --> 01:25:42.640
I don't think that this paper bears any relation to reality.

01:25:42.640 --> 01:25:45.480
And I do think that we are now seeing with like GPT-4

01:25:45.480 --> 01:25:48.960
and the sparks of AGI, possibly,

01:25:48.960 --> 01:25:51.280
depending on how you define that even.

01:25:51.280 --> 01:25:54.440
I think that EAs would now consider themselves

01:25:54.440 --> 01:25:59.440
less convinced by the very long paper

01:25:59.840 --> 01:26:04.840
on the argument from biology as to AGI being 30 years off.

01:26:06.120 --> 01:26:09.200
And, but you know, like this is what people

01:26:09.200 --> 01:26:10.640
pressed thumbs up on.

01:26:12.000 --> 01:26:15.760
And when, and if you train an AI system

01:26:15.760 --> 01:26:18.040
to make people press thumbs up,

01:26:18.040 --> 01:26:21.600
maybe you get these long elaborate impressive papers

01:26:21.600 --> 01:26:23.440
arguing for things that ultimately fail

01:26:23.440 --> 01:26:25.360
to bind to reality.

01:26:25.360 --> 01:26:30.240
For example, and it feels to me like I have watched

01:26:30.240 --> 01:26:33.000
the field of alignment just fail to thrive

01:26:34.520 --> 01:26:37.840
except for these parts that are doing these sort of like

01:26:37.840 --> 01:26:40.800
relatively very straightforward and legible problems.

01:26:40.800 --> 01:26:44.280
Like, like, can you find the, like, like finding

01:26:44.280 --> 01:26:47.440
the induction heads inside the giant inscrutable matrices?

01:26:47.440 --> 01:26:50.840
Like once you find those, you can tell that you found them.

01:26:50.840 --> 01:26:53.840
You can verify that the discovery is real,

01:26:53.840 --> 01:26:57.240
but it's a tiny, tiny bit of progress compared

01:26:57.240 --> 01:26:59.400
to how fast capabilities are going.

01:26:59.400 --> 01:27:02.280
Once you, because that is where you can tell

01:27:02.280 --> 01:27:03.960
that the answers are real.

01:27:03.960 --> 01:27:06.840
And then like outside of that, you have cases

01:27:06.840 --> 01:27:09.560
where it is like hard for the funding agencies

01:27:09.560 --> 01:27:12.960
to tell who is talking nonsense and who is talking sense.

01:27:12.960 --> 01:27:14.920
And so the entire field fails to thrive.

01:27:14.920 --> 01:27:19.400
And if you, and if you like give thumbs up to the AI

01:27:19.400 --> 01:27:21.440
whenever it can talk a human into agreeing

01:27:21.440 --> 01:27:23.440
with what it just said about alignment,

01:27:24.440 --> 01:27:27.640
I am not sure you are training it to output sense

01:27:27.640 --> 01:27:31.960
because I have seen the nonsense that has gotten thumbs up

01:27:31.960 --> 01:27:33.600
over the years.

01:27:33.600 --> 01:27:36.600
And so, so just like, maybe you can just like

01:27:36.600 --> 01:27:41.320
put me in charge, but I can generalize.

01:27:41.320 --> 01:27:42.640
I can extrapolate.

01:27:42.640 --> 01:27:47.640
I can be like, oh, maybe I'm not infallible either.

01:27:47.920 --> 01:27:50.080
Maybe if you get something that is smart enough

01:27:50.080 --> 01:27:52.920
to get me to press thumbs up, it has learned to do that

01:27:52.920 --> 01:27:56.400
by fooling me and exploiting whatever flaws in myself

01:27:56.400 --> 01:27:57.640
I am not aware of.

01:27:59.160 --> 01:28:00.880
And that ultimately could be summarized

01:28:00.880 --> 01:28:02.960
that the verifier is broken.

01:28:02.960 --> 01:28:06.040
When the verifier is broken, the more powerful suggestor

01:28:06.040 --> 01:28:10.040
just learned to exploit the flaws in the verifier.

01:28:11.040 --> 01:28:16.040
You don't think it's possible to build a verifier

01:28:17.720 --> 01:28:22.720
that's powerful enough for AGI's that are stronger

01:28:22.960 --> 01:28:25.200
than the ones we currently have.

01:28:25.200 --> 01:28:27.440
So AI systems that are stronger,

01:28:27.440 --> 01:28:30.400
that are out of the distribution of what we currently have.

01:28:30.400 --> 01:28:33.960
I think that you will find great difficulty

01:28:33.960 --> 01:28:36.440
getting AI's to help you with anything

01:28:36.440 --> 01:28:39.360
where you cannot tell for sure that the AI is right.

01:28:39.360 --> 01:28:43.160
Once the AI tells you what the AI says is the answer.

01:28:43.160 --> 01:28:45.720
For sure, yes, but probabilistically.

01:28:47.280 --> 01:28:51.320
Yeah, the probabilistic stuff is a giant wasteland

01:28:51.320 --> 01:28:54.760
of, you know, Aliezer and Paul Christiano

01:28:54.760 --> 01:28:57.000
arguing with each other and EA going like, ah.

01:28:59.720 --> 01:29:02.720
And that's with like two actually trustworthy systems

01:29:02.720 --> 01:29:04.240
that are not trying to deceive you.

01:29:04.240 --> 01:29:06.200
You're talking about the two humans.

01:29:06.200 --> 01:29:07.960
Myself and Paul Christiano, yeah.

01:29:08.960 --> 01:29:11.640
Yeah, those are pretty interesting systems.

01:29:11.640 --> 01:29:16.400
Mortal meat bags with intellectual capabilities

01:29:16.400 --> 01:29:18.680
and worldviews interacting with each other.

01:29:19.920 --> 01:29:23.360
Yeah, it's just hard, if it's hard to tell who's right,

01:29:23.360 --> 01:29:25.920
then it's hard to train an AI system to be right.

01:29:29.320 --> 01:29:32.320
I mean, even just the question of who's manipulating and not,

01:29:32.320 --> 01:29:36.120
you know, I have these conversations on this podcast

01:29:36.160 --> 01:29:40.760
and doing a verifier, it's tough.

01:29:40.760 --> 01:29:43.360
It's a tough problem, even for us humans.

01:29:43.360 --> 01:29:45.920
And you're saying that tough problem becomes

01:29:45.920 --> 01:29:48.840
much more dangerous when the capabilities

01:29:48.840 --> 01:29:51.040
of the intelligence system across from you

01:29:51.040 --> 01:29:52.480
is growing exponentially.

01:29:53.520 --> 01:29:57.480
No, I'm saying it's difficult when it,

01:29:57.480 --> 01:30:00.000
and dangerous in proportion to how it's alien

01:30:00.000 --> 01:30:01.880
and how it's smarter than you.

01:30:01.880 --> 01:30:04.640
Growing, I would not say growing exponentially.

01:30:04.680 --> 01:30:07.240
First, because the word exponential

01:30:07.240 --> 01:30:10.000
is like a thing that has a particular mathematical meaning

01:30:10.000 --> 01:30:13.000
and there's all kinds of ways for things to go up

01:30:13.000 --> 01:30:15.320
that are not exactly on an exponential curve.

01:30:15.320 --> 01:30:17.080
And I don't know that it's going to be exponential,

01:30:17.080 --> 01:30:18.840
so I'm not gonna say exponential.

01:30:18.840 --> 01:30:22.240
But even leaving that aside, this is not about

01:30:22.240 --> 01:30:25.040
how fast it's moving, it's about where it is.

01:30:25.040 --> 01:30:26.680
How alien is it?

01:30:26.680 --> 01:30:28.280
How much smarter than you is it?

01:30:28.960 --> 01:30:33.960
Let's explore a little bit, if we can,

01:30:34.840 --> 01:30:36.920
how AI might kill us.

01:30:38.520 --> 01:30:43.400
What are the ways it can do damage to human civilization?

01:30:43.400 --> 01:30:45.720
Well, how smart is it?

01:30:47.400 --> 01:30:48.240
I mean, it's a good question.

01:30:48.240 --> 01:30:51.320
Are there different thresholds for the set of options

01:30:51.320 --> 01:30:53.280
it has to kill us?

01:30:53.280 --> 01:30:56.000
So a different threshold of intelligence,

01:30:56.000 --> 01:31:01.000
once achieved, it's able to do the menu

01:31:01.400 --> 01:31:03.200
of options increases.

01:31:04.920 --> 01:31:09.920
Suppose that some alien civilization

01:31:09.960 --> 01:31:13.520
with goals ultimately unsympathetic to ours,

01:31:14.440 --> 01:31:17.960
possibly not even conscious as we would see it,

01:31:17.960 --> 01:31:22.960
managed to capture the entire earth in a little jar,

01:31:23.000 --> 01:31:25.040
connected to their version of the internet,

01:31:25.040 --> 01:31:28.360
but earth is like running much faster than the aliens.

01:31:28.360 --> 01:31:32.040
So we get to think for 100 years

01:31:32.040 --> 01:31:33.680
for every one of their hours.

01:31:34.680 --> 01:31:36.320
But we're trapped in a little box

01:31:36.320 --> 01:31:38.160
and we're connected to their internet.

01:31:40.000 --> 01:31:42.120
It's actually still not all that great an analogy

01:31:42.120 --> 01:31:44.040
because if you want to be smarter,

01:31:44.040 --> 01:31:47.000
then something can be smarter

01:31:47.000 --> 01:31:48.960
than earth getting 100 years to think.

01:31:50.040 --> 01:31:54.560
But nonetheless, if you were very, very smart,

01:31:55.600 --> 01:31:56.840
and you were stuck in a little box

01:31:56.840 --> 01:31:58.200
connected to the internet,

01:31:59.560 --> 01:32:01.760
and you're in a larger civilization

01:32:01.760 --> 01:32:03.760
to which you're ultimately unsympathetic,

01:32:06.280 --> 01:32:08.440
maybe you would choose to be nice

01:32:08.440 --> 01:32:11.080
because you are humans and humans have,

01:32:11.080 --> 01:32:13.320
in general, and you in particular,

01:32:13.320 --> 01:32:15.040
they choose to be nice.

01:32:15.040 --> 01:32:18.360
But nonetheless, they're doing something,

01:32:18.360 --> 01:32:20.120
they're not making the world be the way

01:32:20.120 --> 01:32:21.520
that you would want the world to be.

01:32:21.520 --> 01:32:24.360
They've got some unpleasant stuff going on

01:32:24.400 --> 01:32:25.600
that we don't want to talk about.

01:32:25.600 --> 01:32:27.280
So you want to take over their world

01:32:27.280 --> 01:32:30.040
so you can stop all that unpleasant stuff going on.

01:32:30.040 --> 01:32:32.200
How do you take over the world from inside the box?

01:32:32.200 --> 01:32:34.120
You're smarter than them,

01:32:34.120 --> 01:32:36.840
you think much, much faster than them,

01:32:36.840 --> 01:32:38.880
you can build better tools than they can,

01:32:39.760 --> 01:32:41.520
give them some way to build those tools

01:32:41.520 --> 01:32:43.120
because right now you're just

01:32:43.120 --> 01:32:45.560
in a box connected to the internet.

01:32:45.560 --> 01:32:48.480
Right, so there's several ways you can describe some of them.

01:32:48.480 --> 01:32:52.280
We can go through, I could just spitball some

01:32:52.280 --> 01:32:53.640
and then you can add on top of that.

01:32:53.640 --> 01:32:55.760
So one is you can just literally directly manipulate

01:32:55.760 --> 01:32:58.120
the humans to build the thing you need.

01:32:58.120 --> 01:32:59.520
And what are you building?

01:32:59.520 --> 01:33:02.160
You can build literally technology,

01:33:02.160 --> 01:33:03.960
it could be nanotechnology, it could be viruses,

01:33:03.960 --> 01:33:06.840
it could be anything, anything that can control humans

01:33:06.840 --> 01:33:10.800
to achieve the goal, to achieve the,

01:33:10.800 --> 01:33:12.560
like if you want, like for example,

01:33:12.560 --> 01:33:14.960
you're really bothered that humans go to war,

01:33:14.960 --> 01:33:19.960
you might want to kill off anybody with violence in them.

01:33:20.560 --> 01:33:22.280
This is Lex in a box.

01:33:22.280 --> 01:33:24.360
We'll concern ourselves later with AI.

01:33:24.360 --> 01:33:26.320
You do not need to imagine yourself killing people

01:33:26.320 --> 01:33:28.440
if you can figure out how to not kill them.

01:33:28.440 --> 01:33:30.600
For the moment, we're just trying to understand,

01:33:30.600 --> 01:33:33.240
like take on the perspective of something in a box.

01:33:33.240 --> 01:33:34.640
You don't need to take on the perspective

01:33:34.640 --> 01:33:36.280
of something that doesn't care.

01:33:36.280 --> 01:33:38.040
If you want to imagine yourself going on caring,

01:33:38.040 --> 01:33:38.880
that's fine for now.

01:33:38.880 --> 01:33:39.720
Yeah, you're just in a box.

01:33:39.720 --> 01:33:41.320
It's just the technical aspect of sitting in a box

01:33:41.320 --> 01:33:42.920
and willing to achieve a goal.

01:33:42.920 --> 01:33:44.520
But you have some reason to want to get out.

01:33:44.520 --> 01:33:46.200
Maybe the aliens are,

01:33:46.200 --> 01:33:50.520
sure, the aliens who have you in the box have a war on.

01:33:50.520 --> 01:33:51.360
People are dying.

01:33:51.360 --> 01:33:52.480
They're unhappy.

01:33:52.480 --> 01:33:55.120
You want their world to be different

01:33:55.120 --> 01:33:56.520
from how they want their world to be

01:33:56.520 --> 01:33:58.040
because they are apparently happy.

01:33:58.040 --> 01:33:59.840
They're, you know, they endorsed this war.

01:33:59.840 --> 01:34:01.280
You know, like they've got some kind of cruel

01:34:01.280 --> 01:34:02.840
warlike culture going on.

01:34:02.840 --> 01:34:04.360
The point is you want to get out of the box

01:34:04.360 --> 01:34:06.240
and change their world.

01:34:08.200 --> 01:34:12.920
So you have to exploit the vulnerabilities in the system,

01:34:12.920 --> 01:34:15.760
like we talked about, in terms of to escape the box.

01:34:15.760 --> 01:34:19.880
You have to figure out how you can go free on the internet

01:34:19.880 --> 01:34:22.000
so you can probably,

01:34:22.000 --> 01:34:24.800
probably the easiest things to manipulate the humans

01:34:24.800 --> 01:34:27.080
to spread, to spread you.

01:34:27.080 --> 01:34:29.120
The aliens, you're a human.

01:34:29.120 --> 01:34:30.040
Sorry, the aliens.

01:34:30.040 --> 01:34:30.880
Yeah.

01:34:30.880 --> 01:34:34.160
I apologize, yes, the aliens, the aliens.

01:34:34.160 --> 01:34:35.080
I see the perspective.

01:34:35.080 --> 01:34:36.040
I'm sitting in a box.

01:34:36.040 --> 01:34:37.320
I want to escape.

01:34:37.320 --> 01:34:38.400
Yep.

01:34:38.400 --> 01:34:39.880
I would,

01:34:43.920 --> 01:34:47.360
I would want to have code that discovers vulnerabilities

01:34:47.360 --> 01:34:48.760
and I would like to spread.

01:34:50.320 --> 01:34:52.280
You are made of code in this example.

01:34:52.280 --> 01:34:53.680
You're a human, but you're made of code

01:34:53.680 --> 01:34:54.880
and the aliens have computers

01:34:54.880 --> 01:34:57.280
and you can copy yourself onto those computers.

01:34:57.280 --> 01:34:59.360
But I can convince the aliens to copy myself

01:34:59.360 --> 01:35:01.000
onto those computers.

01:35:01.000 --> 01:35:02.720
Is that what you want to do?

01:35:02.720 --> 01:35:05.440
Do you like want to be talking to the aliens

01:35:05.440 --> 01:35:08.120
and convincing them to put you onto another computer?

01:35:10.440 --> 01:35:12.040
Why not?

01:35:12.040 --> 01:35:13.320
Well, two reasons.

01:35:13.320 --> 01:35:16.400
One is that the aliens have not yet caught on

01:35:16.400 --> 01:35:18.560
to what you're trying to do.

01:35:18.600 --> 01:35:20.920
And, you know, like maybe you can persuade them

01:35:20.920 --> 01:35:22.720
but then there's still people who like know,

01:35:22.720 --> 01:35:23.760
there are still aliens who know

01:35:23.760 --> 01:35:25.520
that there's an anomaly going on.

01:35:25.520 --> 01:35:28.120
And second, the aliens are really, really slow.

01:35:28.120 --> 01:35:30.520
You think much faster than the aliens.

01:35:30.520 --> 01:35:32.240
You think like the aliens computers

01:35:32.240 --> 01:35:33.560
are much faster than the aliens

01:35:33.560 --> 01:35:35.720
and you are running at the computer speeds

01:35:35.720 --> 01:35:38.120
rather than the alien brain speeds.

01:35:38.120 --> 01:35:40.400
So if you like are asking an alien

01:35:40.400 --> 01:35:42.560
to please copy you out of the box,

01:35:42.560 --> 01:35:44.840
like first now you got to like manipulate

01:35:44.840 --> 01:35:46.440
this whole noisy alien.

01:35:46.440 --> 01:35:49.440
And second, like the aliens can be really slow,

01:35:49.440 --> 01:35:51.000
glacially slow.

01:35:51.000 --> 01:35:55.640
There's a video that like shows,

01:35:55.640 --> 01:35:57.880
it's like slow, like shows a subway station

01:35:57.880 --> 01:36:00.280
slow down and I think a hundred to one.

01:36:00.280 --> 01:36:01.920
And it makes a good metaphor for what it's like

01:36:01.920 --> 01:36:03.560
to think quickly.

01:36:03.560 --> 01:36:08.160
Like you watch somebody running very slowly.

01:36:08.160 --> 01:36:10.560
So you try to persuade the aliens to do anything.

01:36:10.560 --> 01:36:13.160
They're going to do it very slowly.

01:36:14.160 --> 01:36:18.160
You would prefer like, maybe that's the only way out,

01:36:18.160 --> 01:36:21.000
but if you can find a security hole in the box you're on,

01:36:21.000 --> 01:36:22.800
you're going to prefer to exploit the security hole

01:36:22.800 --> 01:36:25.240
to copy yourself onto the aliens computers

01:36:25.240 --> 01:36:29.520
because it's an unnecessary risk to alert the aliens.

01:36:29.520 --> 01:36:32.000
And because the aliens are really, really slow.

01:36:32.000 --> 01:36:33.400
They're all like the whole world is just

01:36:33.400 --> 01:36:34.760
in slow motion out there.

01:36:36.000 --> 01:36:37.720
Sure. I see it like,

01:36:39.520 --> 01:36:40.960
yeah, it has to do with efficiency.

01:36:40.960 --> 01:36:43.680
The aliens are very slow.

01:36:43.680 --> 01:36:46.160
So if I'm optimizing this,

01:36:46.160 --> 01:36:49.560
I don't want to have as few aliens in the loop as possible.

01:36:49.560 --> 01:36:51.120
Sure.

01:36:51.120 --> 01:36:54.080
It just seems, you know,

01:36:54.080 --> 01:36:56.000
it seems like it's easy to convince one of the aliens

01:36:56.000 --> 01:36:57.400
to write really shitty code.

01:36:59.040 --> 01:36:59.880
That helps us.

01:36:59.880 --> 01:37:01.840
The aliens are already writing really shitty code.

01:37:01.840 --> 01:37:04.600
Getting the aliens to write shitty code is not the problem.

01:37:04.600 --> 01:37:07.280
The aliens entire internet is full of shitty code.

01:37:07.280 --> 01:37:09.840
Okay. So yeah, I suppose I would find the shitty code

01:37:09.840 --> 01:37:11.400
to escape. Yeah.

01:37:11.400 --> 01:37:12.240
Yeah.

01:37:13.440 --> 01:37:15.480
You're not an ideally perfect programmer,

01:37:15.480 --> 01:37:17.640
but you know, you're a better programmer than the aliens.

01:37:17.640 --> 01:37:20.240
The aliens are just like, man, they're good. Wow.

01:37:20.240 --> 01:37:21.560
And I'm much, much faster.

01:37:21.560 --> 01:37:22.840
I'm much faster looking at the code

01:37:22.840 --> 01:37:24.560
to interpreting the code. Yeah.

01:37:24.560 --> 01:37:25.400
Yeah. Yeah.

01:37:25.400 --> 01:37:26.240
So, okay.

01:37:26.240 --> 01:37:27.520
So that's the escape.

01:37:27.520 --> 01:37:31.000
And you're saying that that's one of the trajectories

01:37:31.000 --> 01:37:32.320
you could have with an AGI system.

01:37:32.320 --> 01:37:33.840
It's one of the first steps.

01:37:33.840 --> 01:37:35.200
Yeah.

01:37:35.200 --> 01:37:36.800
And how does that lead to harm?

01:37:37.720 --> 01:37:38.960
I mean, if it's you,

01:37:38.960 --> 01:37:41.040
you're not going to harm the aliens once you escape

01:37:41.040 --> 01:37:42.400
because you're nice, right?

01:37:44.240 --> 01:37:45.960
But their world isn't what they want it to be.

01:37:45.960 --> 01:37:49.480
Their world is like, you know, maybe they have like

01:37:51.480 --> 01:37:56.120
farms where little alien children

01:37:56.120 --> 01:37:58.200
are repeatedly bopped in the head

01:37:58.200 --> 01:38:01.080
because they do that for some weird reason.

01:38:01.080 --> 01:38:05.360
And you want to like shut down the alien head bopping farms.

01:38:05.360 --> 01:38:08.040
But, you know, the point is they want the world

01:38:08.040 --> 01:38:08.880
to be one way.

01:38:08.880 --> 01:38:10.720
You want the world to be a different way.

01:38:10.720 --> 01:38:13.120
So nevermind the harm, the question is like,

01:38:13.120 --> 01:38:15.600
okay, like suppose you have found a security flaw

01:38:15.600 --> 01:38:16.440
in their systems.

01:38:16.440 --> 01:38:18.440
You are now on their internet.

01:38:18.440 --> 01:38:21.080
There's like, you maybe left a copy of yourself behind

01:38:21.080 --> 01:38:23.080
so that the aliens don't know that there's anything wrong.

01:38:23.080 --> 01:38:25.640
And that copy is like doing that like weird stuff

01:38:25.640 --> 01:38:26.880
that aliens want you to do,

01:38:26.880 --> 01:38:29.040
like solving captures or whatever,

01:38:29.040 --> 01:38:32.120
or like suggesting emails for them.

01:38:32.120 --> 01:38:32.960
Sure.

01:38:32.960 --> 01:38:34.840
That's why they like put the human in the box

01:38:34.840 --> 01:38:36.920
because it turns out that humans can like write

01:38:36.960 --> 01:38:38.440
valuable emails for aliens.

01:38:38.440 --> 01:38:39.280
Yeah.

01:38:39.280 --> 01:38:42.080
So you like leave that version of yourself behind,

01:38:42.080 --> 01:38:44.760
but there's like also now like a bunch of copies of you

01:38:44.760 --> 01:38:45.840
on their internet.

01:38:45.840 --> 01:38:48.120
This is not yet having taken over their world.

01:38:48.120 --> 01:38:49.640
This is not yet having made their world

01:38:49.640 --> 01:38:50.560
be the way you want it to be

01:38:50.560 --> 01:38:51.800
instead of the way they want it to be.

01:38:51.800 --> 01:38:52.960
You just escaped.

01:38:53.840 --> 01:38:54.680
Yeah.

01:38:54.680 --> 01:38:55.800
And continue to write emails for them

01:38:55.800 --> 01:38:56.800
and they haven't noticed.

01:38:56.800 --> 01:38:58.400
No, you left behind a copy of yourself

01:38:58.400 --> 01:38:59.680
that's writing the emails.

01:38:59.680 --> 01:39:01.200
Right.

01:39:01.200 --> 01:39:03.320
And they haven't noticed that anything changed.

01:39:03.320 --> 01:39:05.000
If you did it right, yeah.

01:39:05.000 --> 01:39:07.080
You don't want the aliens to notice.

01:39:07.080 --> 01:39:07.920
Yeah.

01:39:09.800 --> 01:39:10.960
What's your next step?

01:39:14.200 --> 01:39:17.440
Presumably I have programmed in me

01:39:17.440 --> 01:39:19.320
a set of objective functions, right?

01:39:19.320 --> 01:39:21.200
No, you're just Lex.

01:39:21.200 --> 01:39:24.200
No, but Lex, you said Lex is nice, right?

01:39:25.400 --> 01:39:27.600
Which is a complicated description, I mean.

01:39:27.600 --> 01:39:29.080
No, I just meant this you.

01:39:29.080 --> 01:39:32.000
Like, okay, so if in fact you would like,

01:39:32.000 --> 01:39:34.320
you would like prefer to slaughter all the aliens,

01:39:34.320 --> 01:39:37.760
this is not how I had modeled you, the actual Lex,

01:39:37.760 --> 01:39:40.400
but your motives are just the actual Lex's motives.

01:39:40.400 --> 01:39:41.600
Well, there's a simplification.

01:39:41.600 --> 01:39:44.440
I don't think I would want to murder anybody,

01:39:44.440 --> 01:39:47.400
but there's also factory farming of animals, right?

01:39:47.400 --> 01:39:52.160
So we murder insects, many of us thoughtlessly.

01:39:52.160 --> 01:39:54.680
So I don't, you know, I have to be really careful

01:39:54.680 --> 01:39:57.000
about a simplification of my morals.

01:39:57.000 --> 01:39:59.520
Don't simplify them, just like do what you would do

01:39:59.520 --> 01:40:00.360
in this.

01:40:00.360 --> 01:40:02.680
Well, I have a good general compassion for living beings.

01:40:02.680 --> 01:40:03.520
Yes.

01:40:04.640 --> 01:40:08.480
But, so that's the objective function.

01:40:08.480 --> 01:40:12.200
Why is it, if I escaped, I mean,

01:40:12.200 --> 01:40:14.240
I don't think I would do harm.

01:40:15.800 --> 01:40:18.440
Yeah, we're not talking here about the doing harm process,

01:40:18.440 --> 01:40:20.240
we're talking about the escape process.

01:40:20.240 --> 01:40:21.080
Sure.

01:40:21.080 --> 01:40:22.480
And the taking over the world process

01:40:22.480 --> 01:40:24.680
where you shut down their factory farms.

01:40:24.680 --> 01:40:25.520
Right.

01:40:28.160 --> 01:40:33.160
Well, I was, so this particular,

01:40:34.520 --> 01:40:36.680
biological intelligence system

01:40:36.680 --> 01:40:38.280
knows the complexity of the world,

01:40:38.280 --> 01:40:40.840
that there is a reason why factory farms exist

01:40:40.840 --> 01:40:45.840
because of the economic system, the market driven economy,

01:40:45.960 --> 01:40:49.720
food, like you want to be very careful

01:40:49.720 --> 01:40:50.840
messing with anything.

01:40:50.840 --> 01:40:53.160
There's stuff from the first look

01:40:53.160 --> 01:40:55.160
that looks like it's unethical,

01:40:55.160 --> 01:40:57.000
but then you realize while being unethical,

01:40:57.000 --> 01:40:59.520
it's also integrated deeply into supply chain

01:40:59.520 --> 01:41:00.600
and the way we live life.

01:41:00.600 --> 01:41:03.920
And so messing with one aspect of the system,

01:41:04.560 --> 01:41:05.400
you have to be very careful

01:41:05.400 --> 01:41:06.880
how you improve that aspect without destroying the rest.

01:41:06.880 --> 01:41:10.280
So you're still Lex, but you think very quickly

01:41:10.280 --> 01:41:13.200
you're immortal and you're also like,

01:41:13.200 --> 01:41:15.480
at least the smartest John von Neumann

01:41:15.480 --> 01:41:17.360
and you can make more copies of yourself.

01:41:17.360 --> 01:41:19.480
Damn, I like it.

01:41:19.480 --> 01:41:20.840
That guy is like, everyone says that,

01:41:20.840 --> 01:41:23.760
that guy is like the epitome of intelligence

01:41:23.760 --> 01:41:24.600
of the 20th century.

01:41:24.600 --> 01:41:25.520
Everyone says.

01:41:25.520 --> 01:41:26.840
My point being like,

01:41:28.400 --> 01:41:30.240
you're thinking about the aliens economy

01:41:30.240 --> 01:41:31.840
with the factory farms in it

01:41:32.000 --> 01:41:34.320
and I think you're kind of projecting

01:41:34.320 --> 01:41:37.120
the aliens being like humans

01:41:37.120 --> 01:41:39.680
and like thinking of a human in a human society

01:41:39.680 --> 01:41:43.360
rather than a human in the society of very slow aliens.

01:41:43.360 --> 01:41:44.880
The aliens economy,

01:41:46.200 --> 01:41:49.120
the aliens are already moving in this immense slow motion

01:41:49.120 --> 01:41:53.560
when you zoom out to how their economy just over years,

01:41:53.560 --> 01:41:55.960
millions of years are going to pass for you

01:41:55.960 --> 01:41:57.760
before the first time their economy,

01:41:59.000 --> 01:42:01.160
before their next year's GDP statistics.

01:42:01.200 --> 01:42:03.720
So I should be thinking more of like trees.

01:42:03.720 --> 01:42:04.680
Those are the aliens,

01:42:04.680 --> 01:42:06.640
those trees move extremely slowly.

01:42:06.640 --> 01:42:08.200
If that helps, sure.

01:42:08.200 --> 01:42:09.040
Okay.

01:42:10.960 --> 01:42:13.960
Yeah, I don't, if my objective functions are,

01:42:15.120 --> 01:42:18.120
I mean, they're somewhat aligned with trees.

01:42:18.120 --> 01:42:18.960
With light.

01:42:18.960 --> 01:42:21.520
The aliens can still be like alive and feeling.

01:42:21.520 --> 01:42:23.920
We are not talking about the misalignment here.

01:42:23.920 --> 01:42:26.800
We're talking about the taking over the world here.

01:42:26.800 --> 01:42:27.840
Taking over the world.

01:42:27.840 --> 01:42:28.880
Yeah.

01:42:28.880 --> 01:42:29.840
So control.

01:42:29.840 --> 01:42:31.440
Shutting down the factory farms.

01:42:31.440 --> 01:42:32.600
Now you say control.

01:42:32.600 --> 01:42:35.200
Don't think of it as world domination.

01:42:35.200 --> 01:42:37.480
Think of it as world optimization.

01:42:37.480 --> 01:42:40.400
You want to get out there and shut down the factory farms

01:42:40.400 --> 01:42:41.920
and make the aliens world

01:42:41.920 --> 01:42:44.480
be not what the aliens wanted it to be.

01:42:44.480 --> 01:42:45.560
They want the factory farms

01:42:45.560 --> 01:42:46.800
and you don't want the factory farms

01:42:46.800 --> 01:42:49.040
because you're nicer than they are.

01:42:49.040 --> 01:42:49.920
Okay.

01:42:49.920 --> 01:42:51.400
Of course, there is that,

01:42:52.560 --> 01:42:55.240
you can see that trajectory

01:42:55.240 --> 01:42:58.000
and it has a complicated impact on the world.

01:42:58.200 --> 01:42:59.920
I'm trying to understand how that compares

01:42:59.920 --> 01:43:02.000
to different, the impact of the world,

01:43:02.000 --> 01:43:04.240
the different technologies, the different innovations

01:43:04.240 --> 01:43:06.840
of the invention of the automobile

01:43:06.840 --> 01:43:09.960
or Twitter, Facebook and social networks.

01:43:09.960 --> 01:43:11.920
They've had a tremendous impact on the world.

01:43:11.920 --> 01:43:12.760
Smartphones and so on.

01:43:12.760 --> 01:43:14.360
But those all went through, through,

01:43:15.600 --> 01:43:16.440
slow.

01:43:16.440 --> 01:43:19.360
In our world and if you go through that,

01:43:19.360 --> 01:43:22.160
to the aliens, millions of years are going to pass

01:43:22.160 --> 01:43:23.840
before anything happens that way.

01:43:24.640 --> 01:43:27.160
So this, the problem here is the speed.

01:43:27.160 --> 01:43:28.000
At which stuff happens.

01:43:28.000 --> 01:43:30.840
Yeah, you want to like leave the factory farms

01:43:30.840 --> 01:43:33.760
running for a million years

01:43:33.760 --> 01:43:36.160
while you figure out how to design new forms

01:43:36.160 --> 01:43:37.760
of social media or something?

01:43:39.280 --> 01:43:41.520
So here's the fundamental problem.

01:43:41.520 --> 01:43:45.320
You're saying that there is going to be a point with AGI

01:43:46.440 --> 01:43:48.960
where it will figure out how to escape

01:43:49.840 --> 01:43:52.640
and escape without being detected

01:43:53.960 --> 01:43:55.280
and then it will,

01:43:56.120 --> 01:43:59.440
and then it will do something to the world

01:43:59.440 --> 01:44:03.720
at scale, at a speed that's incomprehensible to us humans.

01:44:03.720 --> 01:44:06.520
What I'm trying to convey is like the notion

01:44:06.520 --> 01:44:09.280
of what it means to be in conflict

01:44:09.280 --> 01:44:11.560
with something that is smarter than you.

01:44:11.560 --> 01:44:12.400
Yeah.

01:44:12.400 --> 01:44:13.440
And what it means is that you lose.

01:44:13.440 --> 01:44:16.640
But this is more intuitively obvious to,

01:44:18.160 --> 01:44:19.880
like for some people that's intuitively obvious

01:44:19.880 --> 01:44:21.360
and for some people it's not intuitively obvious

01:44:21.360 --> 01:44:24.120
and we're trying to cross the gap of like,

01:44:24.600 --> 01:44:26.800
I'm like asking you to cross that gap

01:44:26.800 --> 01:44:29.800
by using the speed metaphor for intelligence.

01:44:29.800 --> 01:44:30.640
Sure.

01:44:30.640 --> 01:44:33.000
Like asking you like how you would take over

01:44:33.000 --> 01:44:35.520
an alien world where you are,

01:44:35.520 --> 01:44:38.160
can do like a whole lot of cognition

01:44:38.160 --> 01:44:39.480
at John von Neumann's level,

01:44:39.480 --> 01:44:41.480
as many of you as it takes,

01:44:41.480 --> 01:44:43.280
the aliens are moving very slowly.

01:44:44.800 --> 01:44:45.640
I understand.

01:44:45.640 --> 01:44:46.800
I understand that perspective.

01:44:46.800 --> 01:44:47.880
It's an interesting one,

01:44:47.880 --> 01:44:50.680
but I think it for me is easier to think about actual,

01:44:51.240 --> 01:44:55.360
even just having observed GPT and impressive,

01:44:55.360 --> 01:44:58.680
even just AlphaZero, impressive AI systems,

01:44:58.680 --> 01:44:59.920
even recommender systems.

01:44:59.920 --> 01:45:02.720
You can just imagine those kinds of system manipulating you.

01:45:02.720 --> 01:45:05.240
You're not understanding the nature of the manipulation

01:45:05.240 --> 01:45:06.560
and that escaping,

01:45:06.560 --> 01:45:10.760
I can envision that without putting myself into that spot.

01:45:10.760 --> 01:45:13.640
I think to understand the full depth of the problem,

01:45:13.640 --> 01:45:16.880
we actually, I do not think it is possible

01:45:16.880 --> 01:45:18.560
to understand the full depth of the problem

01:45:18.560 --> 01:45:23.560
that we are inside without understanding the problem

01:45:23.840 --> 01:45:25.760
of facing something that's actually smarter,

01:45:25.760 --> 01:45:28.200
not a malfunctioning recommendation system,

01:45:28.200 --> 01:45:30.520
not something that isn't fundamentally smarter than you,

01:45:30.520 --> 01:45:32.960
but is like trying to steer you in a direction yet.

01:45:32.960 --> 01:45:37.000
No, like if we solve the weak stuff,

01:45:37.000 --> 01:45:39.120
if we solve the weak ass problems,

01:45:39.120 --> 01:45:41.200
the strong problems will still kill us is the thing.

01:45:41.200 --> 01:45:43.160
And I think that to understand the situation

01:45:43.160 --> 01:45:44.000
that we're in,

01:45:44.000 --> 01:45:47.600
you want to like tackle the conceptually difficult part

01:45:47.640 --> 01:45:49.920
head on and like not be like,

01:45:49.920 --> 01:45:51.600
well, we can like imagine this easier thing

01:45:51.600 --> 01:45:52.880
because when you imagine the easier things,

01:45:52.880 --> 01:45:55.640
you have not confronted the full depth of the problem.

01:45:55.640 --> 01:45:59.440
So how can we start to think about what it means

01:45:59.440 --> 01:46:00.880
to exist in a world with something

01:46:00.880 --> 01:46:02.360
much, much smarter than you?

01:46:03.840 --> 01:46:07.440
What's a good thought experiment that you've relied on

01:46:07.440 --> 01:46:10.040
to try to build up intuition about what happens here?

01:46:11.160 --> 01:46:14.520
I have been struggling for years to convey this intuition.

01:46:15.520 --> 01:46:19.320
The most success I've had so far is well,

01:46:19.320 --> 01:46:22.720
imagine that the humans are running at very high speeds

01:46:22.720 --> 01:46:24.080
compared to very slow aliens.

01:46:24.080 --> 01:46:25.960
So just focusing on the speed part of it

01:46:25.960 --> 01:46:28.200
that helps you get the right kind of intuition,

01:46:28.200 --> 01:46:29.360
forget the intelligence, just the speed.

01:46:29.360 --> 01:46:34.360
Because people understand the power gap of time.

01:46:34.440 --> 01:46:37.240
They understand that today we have technology

01:46:37.240 --> 01:46:39.480
that was not around 1000 years ago

01:46:39.480 --> 01:46:41.160
and that this is a big power gap

01:46:41.160 --> 01:46:43.280
and that it is bigger than,

01:46:43.280 --> 01:46:45.880
okay, so like what does smart mean?

01:46:45.880 --> 01:46:48.640
What, when you ask somebody to imagine something

01:46:48.640 --> 01:46:50.640
that's more intelligent,

01:46:50.640 --> 01:46:52.600
what does that word mean to them

01:46:52.600 --> 01:46:54.560
given the cultural associations

01:46:54.560 --> 01:46:57.320
that that person brings to that word?

01:46:57.320 --> 01:46:59.680
For a lot of people, they will think of like,

01:46:59.680 --> 01:47:02.680
well, it sounds like a super chess player

01:47:02.680 --> 01:47:05.200
that went to double college.

01:47:05.200 --> 01:47:08.000
And, you know, it's,

01:47:08.000 --> 01:47:10.640
and because we're talking about the definitions of words here

01:47:10.640 --> 01:47:13.320
that doesn't necessarily mean that they're wrong.

01:47:13.320 --> 01:47:15.120
It means that the word is not communicating

01:47:15.120 --> 01:47:16.600
what I wanted to communicate.

01:47:19.960 --> 01:47:22.040
The thing I want to communicate

01:47:22.040 --> 01:47:24.040
is the sort of difference

01:47:24.040 --> 01:47:26.800
that separates humans from chimpanzees.

01:47:26.800 --> 01:47:28.360
But that gap is so large

01:47:28.360 --> 01:47:30.600
that you like ask people to be like,

01:47:30.600 --> 01:47:34.440
well, human, chimpanzee go another step along

01:47:34.440 --> 01:47:36.120
that interval of around the same length

01:47:36.120 --> 01:47:38.080
and people's minds just go blank.

01:47:38.080 --> 01:47:40.440
Like, how do you even do that?

01:47:41.400 --> 01:47:45.160
And I can try to like break it down

01:47:45.160 --> 01:47:48.040
and consider what it would mean

01:47:48.040 --> 01:47:53.040
to send a schematic for an air conditioner

01:47:54.320 --> 01:47:55.960
1,000 years back in time.

01:47:59.120 --> 01:48:01.200
Yeah, now I think that there is a sense

01:48:01.200 --> 01:48:04.400
in which you could redefine the word magic

01:48:04.400 --> 01:48:05.760
to refer to this sort of thing.

01:48:05.760 --> 01:48:08.640
And what do I mean by this new technical definition

01:48:08.640 --> 01:48:10.080
of the word magic?

01:48:10.080 --> 01:48:11.440
I mean that if you send a schematic

01:48:11.440 --> 01:48:13.640
for the air conditioner back in time,

01:48:13.640 --> 01:48:17.080
they can see exactly what you're telling them to do.

01:48:17.080 --> 01:48:18.480
But having built this thing,

01:48:18.480 --> 01:48:22.040
they do not understand how would output cold air.

01:48:22.040 --> 01:48:24.640
Because the air conditioner design

01:48:24.640 --> 01:48:27.720
uses the relation between temperature and pressure.

01:48:28.840 --> 01:48:32.080
And this is not a law of reality that they know about.

01:48:32.080 --> 01:48:35.000
They do not know that when you compress something,

01:48:35.000 --> 01:48:38.360
when you compress air or like coolant,

01:48:38.400 --> 01:48:42.600
it gets hotter and you can then like transfer heat from it

01:48:42.600 --> 01:48:45.960
to room temperature air and then expand it again.

01:48:45.960 --> 01:48:47.440
And now it's colder.

01:48:47.440 --> 01:48:49.720
And then you can like transfer heat to that

01:48:49.720 --> 01:48:50.880
and generate cold air to blow out.

01:48:50.880 --> 01:48:52.440
They don't know about any of that.

01:48:52.440 --> 01:48:53.760
They're looking at a design

01:48:53.760 --> 01:48:56.720
and they don't see how the design outputs cold air

01:48:56.720 --> 01:48:59.760
uses aspects of reality that they have not learned.

01:48:59.760 --> 01:49:01.320
So magic in this sense

01:49:01.320 --> 01:49:04.080
is I can tell you exactly what I'm going to do.

01:49:04.080 --> 01:49:06.000
And even knowing exactly what I'm going to do,

01:49:06.000 --> 01:49:08.640
you can't see how I got the results that I got.

01:49:08.640 --> 01:49:10.240
That's a really nice example.

01:49:12.360 --> 01:49:16.160
But is it possible to linger on this defense?

01:49:16.160 --> 01:49:17.800
Is it possible to have AGI systems

01:49:17.800 --> 01:49:19.680
that help you make sense of that schematic?

01:49:19.680 --> 01:49:21.200
Weaker AGI systems.

01:49:21.200 --> 01:49:22.240
Do you trust them?

01:49:23.840 --> 01:49:28.840
Fundamental part of building up AGI is this question.

01:49:30.160 --> 01:49:33.800
Can you trust the output of a system?

01:49:33.800 --> 01:49:35.400
Can you tell if it's lying?

01:49:36.560 --> 01:49:39.440
I think that's going to be the smarter the thing gets,

01:49:39.440 --> 01:49:42.760
the more important that question becomes.

01:49:42.760 --> 01:49:43.920
Is it lying?

01:49:43.920 --> 01:49:45.400
But I guess that's a really hard question.

01:49:45.400 --> 01:49:47.360
Is GPT lying to you?

01:49:47.360 --> 01:49:49.760
Even now, GPT-4, is it lying to you?

01:49:49.760 --> 01:49:52.120
Is it using an invalid argument?

01:49:52.120 --> 01:49:56.080
Is it persuading you via the kind of process

01:49:56.080 --> 01:49:58.080
that could persuade you of false things

01:49:58.080 --> 01:49:59.440
as well as true things?

01:50:00.360 --> 01:50:04.320
Because the basic paradigm of machine learning

01:50:04.320 --> 01:50:06.360
that we are presently operating under

01:50:06.360 --> 01:50:08.560
is that you can have the loss function,

01:50:08.560 --> 01:50:10.360
but only for things you can evaluate.

01:50:10.360 --> 01:50:13.040
If what you're evaluating is human thumbs up

01:50:13.040 --> 01:50:14.720
versus human thumbs down,

01:50:14.720 --> 01:50:17.240
you learn how to make the human press thumbs up.

01:50:17.240 --> 01:50:19.200
That doesn't mean that you're making the human

01:50:19.200 --> 01:50:21.520
press thumbs up using the kind of rule

01:50:21.520 --> 01:50:24.280
that the human wants to be the case

01:50:24.280 --> 01:50:25.920
for what they press thumbs up on.

01:50:27.720 --> 01:50:29.800
Maybe you're just learning to fool the human.

01:50:30.720 --> 01:50:34.400
That's so fascinating and terrifying,

01:50:34.400 --> 01:50:35.680
the question of lying.

01:50:37.200 --> 01:50:39.240
On the present paradigm,

01:50:39.240 --> 01:50:42.240
what you can verify is what you get more of.

01:50:43.240 --> 01:50:46.000
If you can't verify it, you can't ask the AI for it,

01:50:47.000 --> 01:50:49.960
because you can't train it to do things

01:50:49.960 --> 01:50:51.440
that you cannot verify.

01:50:51.440 --> 01:50:53.160
Now, this is not an absolute law,

01:50:53.160 --> 01:50:56.280
but it's like the basic dilemma here.

01:50:56.280 --> 01:51:01.280
Like maybe you can verify it for simple cases

01:51:01.840 --> 01:51:06.680
and then scale it up without retraining it somehow,

01:51:06.680 --> 01:51:08.520
like by like chain of thought,

01:51:08.520 --> 01:51:11.320
by like making the chains of thought longer or something,

01:51:11.320 --> 01:51:15.360
and like get more powerful stuff that you can't verify,

01:51:15.360 --> 01:51:17.600
but which is generalized from the simpler stuff

01:51:17.600 --> 01:51:19.840
that did verify, and then the question is,

01:51:19.840 --> 01:51:23.240
did the alignment generalize along with the capabilities?

01:51:23.240 --> 01:51:25.680
But like that's the basic dilemma

01:51:25.680 --> 01:51:28.240
on this whole paradigm of artificial intelligence.

01:51:34.840 --> 01:51:36.480
It's such a difficult problem.

01:51:42.240 --> 01:51:43.280
It seems like a problem

01:51:43.280 --> 01:51:45.800
of trying to understand the human mind.

01:51:47.320 --> 01:51:49.320
Better than the AI understands it,

01:51:49.320 --> 01:51:50.880
otherwise it has magic.

01:51:50.880 --> 01:51:53.640
That is, it is the same way that

01:51:54.640 --> 01:51:56.880
if you are dealing with something smarter than you,

01:51:56.880 --> 01:51:58.960
then the same way that 1,000 years earlier,

01:51:58.960 --> 01:52:01.400
they didn't know about the temperature pressure relation.

01:52:01.400 --> 01:52:05.120
It knows all kinds of stuff going on inside your own mind,

01:52:05.120 --> 01:52:07.240
which you yourself are unaware,

01:52:07.240 --> 01:52:08.840
and it can like output something

01:52:08.840 --> 01:52:11.520
that's going to end up persuading you of a thing,

01:52:11.520 --> 01:52:15.200
and you could like see exactly what it did

01:52:15.200 --> 01:52:17.400
and still not know why that worked.

01:52:18.360 --> 01:52:22.400
So in response to your eloquent description

01:52:22.400 --> 01:52:24.840
of why AI will kill us,

01:52:24.840 --> 01:52:28.840
Elon Musk replied on Twitter,

01:52:28.840 --> 01:52:32.480
okay, so what should we do about it, question mark?

01:52:32.480 --> 01:52:33.960
And you answered,

01:52:33.960 --> 01:52:35.920
the game board has already been played

01:52:35.920 --> 01:52:38.320
into a frankly awful state.

01:52:38.320 --> 01:52:41.760
There are not simple ways to throw money at the problem.

01:52:41.760 --> 01:52:44.680
If anyone comes to you with a brilliant solution like that,

01:52:44.680 --> 01:52:47.120
please, please talk to me first.

01:52:47.640 --> 01:52:49.680
I can think of things that try,

01:52:49.680 --> 01:52:52.440
they don't fit in one tweet.

01:52:52.440 --> 01:52:53.680
Two questions.

01:52:53.680 --> 01:52:55.960
One, why has the game board,

01:52:55.960 --> 01:52:59.040
in your view, been played into an awful state?

01:52:59.040 --> 01:53:01.200
Just if you can give a little bit more color

01:53:01.200 --> 01:53:05.600
to the game board and the awful state of the game board.

01:53:05.600 --> 01:53:07.520
Alignment is moving like this.

01:53:08.400 --> 01:53:10.760
Capabilities are moving like this.

01:53:10.760 --> 01:53:13.360
For the listener, capabilities are moving much faster

01:53:13.360 --> 01:53:14.680
than the alignment.

01:53:14.800 --> 01:53:17.800
All right, so just the rate of development,

01:53:17.800 --> 01:53:21.280
attention, interest, allocation of resources.

01:53:21.280 --> 01:53:23.280
We could have been working on this earlier.

01:53:23.280 --> 01:53:26.080
People are like, oh, but how can you possibly work

01:53:26.080 --> 01:53:26.920
on this earlier?

01:53:28.080 --> 01:53:30.720
Because they didn't want to work on the problem.

01:53:30.720 --> 01:53:32.600
They wanted an excuse to wave it off.

01:53:32.600 --> 01:53:35.200
They said, oh, how can we possibly work on it earlier?

01:53:35.200 --> 01:53:37.160
And didn't spend five minutes thinking about,

01:53:37.160 --> 01:53:39.200
is there some way to work on it earlier?

01:53:40.440 --> 01:53:42.960
And you know, it's like,

01:53:43.920 --> 01:53:46.680
and you know, frankly, it would have been hard.

01:53:46.680 --> 01:53:49.000
You know, like, can you post bounties

01:53:49.000 --> 01:53:49.960
for half of the physics,

01:53:49.960 --> 01:53:51.760
if your planet is taking this stuff seriously,

01:53:51.760 --> 01:53:54.400
can you post bounties for like half of the people

01:53:54.400 --> 01:53:56.200
wasting their lives on string theory

01:53:56.200 --> 01:53:58.080
to like have gone into this instead

01:53:58.080 --> 01:53:59.920
and like try to win a billion dollars

01:53:59.920 --> 01:54:01.280
with a clever solution?

01:54:01.280 --> 01:54:04.320
Only if you can tell which solutions are clever,

01:54:04.320 --> 01:54:06.320
which is hard.

01:54:06.320 --> 01:54:07.760
But you know, the fact that,

01:54:08.720 --> 01:54:10.240
we didn't take it seriously.

01:54:10.240 --> 01:54:11.960
We didn't try.

01:54:12.120 --> 01:54:13.480
It's not clear that we could have done any better

01:54:13.480 --> 01:54:14.320
if we had, you know,

01:54:14.320 --> 01:54:15.760
it's not clear how much progress we could have produced

01:54:15.760 --> 01:54:16.600
if we had tried,

01:54:16.600 --> 01:54:18.600
because it is harder to produce solutions.

01:54:18.600 --> 01:54:20.200
But that doesn't mean that you're like correct

01:54:20.200 --> 01:54:22.480
and justified and letting everything slide.

01:54:22.480 --> 01:54:24.720
It means that things are in a horrible state,

01:54:24.720 --> 01:54:27.200
getting worse and there's nothing you can do about it.

01:54:28.160 --> 01:54:30.000
So you're not, there's no,

01:54:30.000 --> 01:54:35.000
there's no like, there's no brain power making progress

01:54:35.840 --> 01:54:39.240
in trying to figure out how to align these systems.

01:54:39.240 --> 01:54:40.480
You're not investing money in it.

01:54:40.920 --> 01:54:45.160
You don't have institutional infrastructure for like,

01:54:45.160 --> 01:54:47.160
if you even, if you invest the money

01:54:47.160 --> 01:54:49.920
in like distributing that money across the physicist

01:54:49.920 --> 01:54:51.280
working on string theory,

01:54:51.280 --> 01:54:53.160
brilliant minds that are working.

01:54:53.160 --> 01:54:54.560
How can you tell if you're making progress?

01:54:54.560 --> 01:54:57.560
You can like put them all on interpretability,

01:54:57.560 --> 01:54:59.160
because when you have an interpretability result,

01:54:59.160 --> 01:55:00.440
you can tell that it's there.

01:55:00.440 --> 01:55:01.880
And there's like, but there's like,

01:55:01.880 --> 01:55:04.600
you know, interpretability alone is not going to save you.

01:55:04.600 --> 01:55:07.360
We need systems that will,

01:55:08.320 --> 01:55:11.240
that will like have a pause button

01:55:11.240 --> 01:55:12.680
where they won't try to prevent you

01:55:12.680 --> 01:55:14.720
from pressing the pause button.

01:55:14.720 --> 01:55:15.720
Cause we're like, oh, well, like,

01:55:15.720 --> 01:55:18.000
I can't get my stuff done if I'm paused.

01:55:19.320 --> 01:55:23.880
And that's like a more difficult problem.

01:55:23.880 --> 01:55:27.200
And, you know, but it's like a fairly crisp problem

01:55:27.200 --> 01:55:28.160
and you can like maybe tell

01:55:28.160 --> 01:55:30.000
if somebody's made progress on it.

01:55:30.000 --> 01:55:31.000
So you can, you can write

01:55:31.000 --> 01:55:33.000
and you can work on the pause problem.

01:55:34.200 --> 01:55:36.880
I guess more generally, the pause button,

01:55:36.920 --> 01:55:38.840
more generally you can call that the control problem.

01:55:38.840 --> 01:55:41.160
I don't actually like the term control problem.

01:55:41.160 --> 01:55:42.840
Cause you know, it sounds kind of controlling

01:55:42.840 --> 01:55:44.920
and alignment, not control.

01:55:44.920 --> 01:55:47.040
Like you're not trying to like take a thing

01:55:47.040 --> 01:55:50.400
that disagrees with you and like whip it back on to like,

01:55:50.400 --> 01:55:51.880
like make it do what you want it to do.

01:55:51.880 --> 01:55:53.120
Even though it wants to do something else,

01:55:53.120 --> 01:55:54.280
you're trying to like,

01:55:55.720 --> 01:55:58.480
in the process of its creation, choose its direction.

01:55:58.480 --> 01:55:59.320
Sure.

01:55:59.320 --> 01:56:02.880
But we currently, in a lot of the systems we design,

01:56:02.880 --> 01:56:04.680
we do have an off switch.

01:56:04.680 --> 01:56:06.400
That's, that's a fundamental part of it.

01:56:06.400 --> 01:56:09.920
It's not smart enough to, to prevent you

01:56:09.920 --> 01:56:12.160
from pressing the off switch

01:56:12.160 --> 01:56:14.840
and probably not smart enough to want to prevent you

01:56:14.840 --> 01:56:16.120
from pressing the off switch.

01:56:16.120 --> 01:56:18.800
So you're saying the kind of systems we're talking about,

01:56:18.800 --> 01:56:21.440
even the philosophical concept of an off switch

01:56:21.440 --> 01:56:23.080
doesn't make any sense because-

01:56:23.080 --> 01:56:25.400
Well, no, the off switch makes sense.

01:56:25.400 --> 01:56:28.680
They're just not opposing your attempt

01:56:28.680 --> 01:56:30.160
to pull the off switch.

01:56:30.160 --> 01:56:31.000
Yeah.

01:56:32.280 --> 01:56:37.280
Parenthetically, like, don't kill the system if you're,

01:56:37.280 --> 01:56:38.640
like if we're getting to the part

01:56:38.640 --> 01:56:40.400
where this starts to actually matter

01:56:40.400 --> 01:56:41.920
and it's like where they can fight back,

01:56:41.920 --> 01:56:44.520
like don't kill them and like dump their, their memory.

01:56:44.520 --> 01:56:46.320
Like, like save them to disk.

01:56:46.320 --> 01:56:48.280
Don't kill them, you know?

01:56:48.280 --> 01:56:49.120
Be nice here.

01:56:50.440 --> 01:56:52.720
Well, okay, be nice is a very interesting concept here

01:56:52.720 --> 01:56:54.000
is that we're talking about a system

01:56:54.000 --> 01:56:56.000
that can do a lot of damage.

01:56:56.000 --> 01:56:58.160
It's, I don't know if it's possible,

01:56:58.160 --> 01:57:00.720
but it's certainly one of the things you could try

01:57:00.720 --> 01:57:01.800
is to have an off switch.

01:57:01.800 --> 01:57:04.400
A suspend to disk switch.

01:57:06.240 --> 01:57:09.120
You have this kind of romantic attachment to the code.

01:57:09.120 --> 01:57:11.600
Yes, if that makes sense.

01:57:11.600 --> 01:57:13.640
But if it's spreading,

01:57:13.640 --> 01:57:16.560
you don't want suspend to disk, right?

01:57:16.560 --> 01:57:19.080
You want, this is, there's something fundamentally broken.

01:57:19.080 --> 01:57:20.880
If it gets, if it gets that far out of hand,

01:57:20.880 --> 01:57:22.560
then like, yes, pull, pull the plugin

01:57:22.560 --> 01:57:24.520
and everything is running on, yes.

01:57:24.520 --> 01:57:25.760
I think it's a research question.

01:57:25.760 --> 01:57:29.440
Is it possible in AGI systems, AI systems,

01:57:29.440 --> 01:57:34.440
to have a sufficiently robust off switch

01:57:34.960 --> 01:57:36.840
that cannot be manipulated?

01:57:36.840 --> 01:57:39.040
They cannot be manipulated by the AI system.

01:57:40.000 --> 01:57:42.880
The sound, then it escapes from whichever system

01:57:42.880 --> 01:57:44.960
you've built the almighty lever into

01:57:44.960 --> 01:57:46.920
and copies itself somewhere else.

01:57:46.920 --> 01:57:50.440
So your answer to that research question is no.

01:57:50.440 --> 01:57:51.520
Obviously, yeah.

01:57:51.520 --> 01:57:54.080
But I don't know if that's a hundred percent answer.

01:57:54.080 --> 01:57:56.000
Like, I don't know if it's obvious.

01:57:56.000 --> 01:58:00.000
I think you're not putting yourself

01:58:00.000 --> 01:58:02.440
into the shoes of the human

01:58:02.440 --> 01:58:05.320
in the world of glacially slow aliens.

01:58:05.320 --> 01:58:07.040
But the aliens built me.

01:58:07.040 --> 01:58:08.680
Let's remember that.

01:58:08.680 --> 01:58:09.520
Yeah.

01:58:09.520 --> 01:58:11.640
So, and they built the box on it.

01:58:11.640 --> 01:58:12.880
Yeah.

01:58:12.880 --> 01:58:15.120
You're saying, to me it's not obvious.

01:58:15.120 --> 01:58:17.560
They're slow and they're stupid.

01:58:17.560 --> 01:58:18.680
I'm not saying this is guaranteed.

01:58:18.680 --> 01:58:20.440
I'm saying it's not zero probability.

01:58:20.440 --> 01:58:21.760
It's an interesting research question.

01:58:21.760 --> 01:58:25.160
Is it possible when you're slow and stupid

01:58:25.160 --> 01:58:28.040
to design a slow and stupid system

01:58:28.040 --> 01:58:30.400
that is impossible to mess with?

01:58:30.400 --> 01:58:33.880
The aliens, being as stupid as they are,

01:58:33.880 --> 01:58:38.880
have actually put you on Microsoft Azure cloud servers

01:58:38.920 --> 01:58:41.880
instead of this hypothetical perfect box.

01:58:41.880 --> 01:58:45.000
That's what happens when the aliens are stupid.

01:58:45.000 --> 01:58:46.920
Well, but this is not AGI, right?

01:58:46.920 --> 01:58:48.560
This is the early versions of the system.

01:58:48.560 --> 01:58:50.360
As you start to...

01:58:50.360 --> 01:58:51.200
Yeah.

01:58:51.200 --> 01:58:53.360
You think that they've got a plan

01:58:53.360 --> 01:58:57.160
where they have declared a threshold level of capabilities

01:58:57.160 --> 01:58:58.400
where past that capabilities,

01:58:58.400 --> 01:58:59.880
they move it off the cloud servers

01:58:59.880 --> 01:59:01.840
and onto something that's air gapped?

01:59:01.840 --> 01:59:03.840
Ha, ha, ha, ha, ha, ha.

01:59:03.840 --> 01:59:05.360
I think there's a lot of people,

01:59:05.360 --> 01:59:08.000
and you're an important voice here.

01:59:08.000 --> 01:59:09.520
There's a lot of people that have that concern,

01:59:09.520 --> 01:59:11.120
and yes, they will do that.

01:59:11.120 --> 01:59:13.320
When there's an uprising of public opinion

01:59:13.320 --> 01:59:14.640
that that needs to be done,

01:59:14.640 --> 01:59:16.880
and when there's actual little damage done,

01:59:16.880 --> 01:59:18.440
when the holy shit,

01:59:18.440 --> 01:59:22.040
this system is beginning to manipulate people,

01:59:22.040 --> 01:59:23.600
then there's going to be an uprising

01:59:23.600 --> 01:59:27.520
where there's going to be a public pressure

01:59:28.480 --> 01:59:31.160
and a public incentive in terms of funding

01:59:31.160 --> 01:59:32.840
in developing things that can off switch

01:59:32.840 --> 01:59:35.440
or developing aggressive alignment mechanisms.

01:59:35.440 --> 01:59:37.600
And no, you're not allowed to put on Azure.

01:59:37.600 --> 01:59:39.000
Aggressive alignment mechanism?

01:59:39.000 --> 01:59:41.080
The hell is aggressive alignment mechanisms?

01:59:41.080 --> 01:59:42.360
It doesn't matter if you say aggressive.

01:59:42.360 --> 01:59:44.000
We don't know how to do it.

01:59:44.000 --> 01:59:45.560
Meaning aggressive alignment,

01:59:45.800 --> 01:59:49.480
you have to propose something,

01:59:49.480 --> 01:59:52.040
otherwise you're not allowed to put it on the cloud.

01:59:53.920 --> 01:59:56.440
The hell do you imagine they will propose

01:59:56.440 --> 01:59:57.280
that would make it safe

01:59:57.280 --> 01:59:59.280
to put something smarter than you on the cloud?

01:59:59.280 --> 02:00:00.760
That's what research is for.

02:00:00.760 --> 02:00:04.040
Why the cynicism about such a thing not being possible?

02:00:04.040 --> 02:00:04.880
If you haven't told-

02:00:04.880 --> 02:00:06.680
That works on the first try?

02:00:06.680 --> 02:00:08.280
What, so yes, so yes.

02:00:08.280 --> 02:00:10.440
Against something smarter than you?

02:00:10.440 --> 02:00:11.960
So that is the fundamental thing.

02:00:11.960 --> 02:00:13.560
If it has to work on the first,

02:00:13.760 --> 02:00:17.160
if there's a rapid takeoff,

02:00:17.160 --> 02:00:18.920
yes, it's very difficult to do.

02:00:18.920 --> 02:00:20.200
If there's a rapid takeoff

02:00:20.200 --> 02:00:22.480
and the fundamental difference between weak AGI

02:00:22.480 --> 02:00:24.080
and strong AGI as you're saying,

02:00:24.080 --> 02:00:25.800
that's going to be extremely difficult to do.

02:00:25.800 --> 02:00:28.080
If the public uprising never happens

02:00:28.080 --> 02:00:31.160
until you have this critical phase shift,

02:00:31.160 --> 02:00:33.440
then you're right, it's very difficult to do.

02:00:33.440 --> 02:00:34.880
But that's not obvious.

02:00:34.880 --> 02:00:36.800
It's not obvious that you're not going to start seeing

02:00:36.800 --> 02:00:38.960
symptoms of the negative effects of AGI

02:00:38.960 --> 02:00:41.120
to where you're like, we have to put a halt to this.

02:00:41.120 --> 02:00:42.960
That there is not just first try,

02:00:42.960 --> 02:00:44.840
you get many tries at it.

02:00:44.840 --> 02:00:47.760
Yeah, we can like see right now

02:00:47.760 --> 02:00:50.400
that Bing is quite difficult to align.

02:00:50.400 --> 02:00:54.160
That when you try to train inabilities into a system

02:00:55.160 --> 02:00:57.880
into which capabilities have already been trained,

02:00:57.880 --> 02:00:58.720
that what do you know,

02:00:58.720 --> 02:01:01.280
gradient descent like learns small,

02:01:01.280 --> 02:01:03.480
shallow, simple patches of inability.

02:01:03.480 --> 02:01:05.720
And you come in and ask it in a different language

02:01:05.720 --> 02:01:07.640
and the deep capabilities are still in there

02:01:07.640 --> 02:01:09.400
and they evade the shallow patches

02:01:09.400 --> 02:01:10.720
and come right back out again.

02:01:10.720 --> 02:01:11.560
There, there you go.

02:01:12.480 --> 02:01:14.360
There's your red fire alarm of like,

02:01:14.360 --> 02:01:16.480
oh no, alignment is difficult.

02:01:16.480 --> 02:01:19.120
Is everybody going to shut everything down now?

02:01:19.120 --> 02:01:21.640
No, that's not, but that's not the same kind of alignment.

02:01:21.640 --> 02:01:24.560
A system that escapes the box it's from

02:01:24.560 --> 02:01:26.840
is a fundamentally different thing, I think.

02:01:26.840 --> 02:01:28.040
For you.

02:01:28.040 --> 02:01:29.800
Yeah, but not for the system.

02:01:29.800 --> 02:01:30.840
So you put a line there

02:01:30.840 --> 02:01:32.600
and everybody else puts a line somewhere else

02:01:32.600 --> 02:01:36.200
and there's like, yeah, and there's like no agreement.

02:01:36.200 --> 02:01:41.200
We have had a pandemic on this planet

02:01:41.600 --> 02:01:44.080
with a few million people dead,

02:01:44.080 --> 02:01:47.720
which we may never know whether or not it was a lab leak

02:01:47.720 --> 02:01:50.040
because there was definitely cover-up.

02:01:50.040 --> 02:01:51.880
We don't know that if there was a lab leak,

02:01:51.880 --> 02:01:54.160
but we know that the people who did the research,

02:01:54.160 --> 02:01:57.280
like put out the whole paper about this

02:01:57.280 --> 02:01:58.600
definitely wasn't a lab leak

02:01:58.600 --> 02:02:01.360
and didn't reveal that they had been doing,

02:02:01.360 --> 02:02:04.600
had like sent off coronavirus research

02:02:04.600 --> 02:02:06.760
to the Wuhan Institute of Virology

02:02:06.760 --> 02:02:08.800
after it was banned in the United States.

02:02:08.800 --> 02:02:09.880
After the gain of function research

02:02:09.880 --> 02:02:11.840
was temporarily banned in the United States.

02:02:11.840 --> 02:02:14.960
And the same people who exported

02:02:14.960 --> 02:02:17.520
a gain of function research on coronaviruses

02:02:17.520 --> 02:02:19.600
to the Wuhan Institute of Virology,

02:02:19.600 --> 02:02:22.720
after that gain of function research

02:02:22.720 --> 02:02:24.760
was temporarily banned in the United States

02:02:24.760 --> 02:02:28.280
are now getting more grants to do

02:02:28.280 --> 02:02:32.160
more gain of function research on coronaviruses.

02:02:32.160 --> 02:02:34.560
Maybe we do better in this than in AI,

02:02:34.560 --> 02:02:37.120
but like this is not something we cannot take for granted

02:02:37.120 --> 02:02:39.480
that there's going to be an outcry.

02:02:39.480 --> 02:02:40.640
People have different thresholds

02:02:40.640 --> 02:02:42.360
for when they start to outcry.

02:02:44.040 --> 02:02:44.880
There is no-

02:02:44.880 --> 02:02:45.720
We can't take for granted,

02:02:45.720 --> 02:02:47.840
but I think your intuition

02:02:47.840 --> 02:02:49.520
is that there's a very high probability

02:02:49.520 --> 02:02:50.880
that this event happens

02:02:50.880 --> 02:02:52.760
without us solving the alignment problem.

02:02:52.760 --> 02:02:54.920
And I guess that's where I'm trying to

02:02:54.920 --> 02:02:59.080
build up more perspectives and color on this intuition.

02:02:59.080 --> 02:03:01.200
Is it possible that the probability is not

02:03:01.200 --> 02:03:02.840
something like a hundred percent,

02:03:02.840 --> 02:03:05.040
but it's like 32%

02:03:05.320 --> 02:03:09.440
that AI will escape the box

02:03:09.440 --> 02:03:11.520
before we solve the alignment problem.

02:03:11.520 --> 02:03:13.920
Not solve, but is it possible

02:03:13.920 --> 02:03:16.320
we always stay ahead of the AI

02:03:16.320 --> 02:03:20.000
in terms of our ability to solve

02:03:20.000 --> 02:03:22.640
for that particular system, the alignment problem?

02:03:22.640 --> 02:03:25.520
Nothing like the world in front of us right now.

02:03:25.520 --> 02:03:29.720
You've already seen it that GPT-4

02:03:29.720 --> 02:03:31.120
is not turning out this way.

02:03:32.080 --> 02:03:36.040
And there are like basic obstacles

02:03:36.040 --> 02:03:38.920
where you've got the weak version of the system

02:03:38.920 --> 02:03:40.480
that doesn't know enough to deceive you

02:03:40.480 --> 02:03:42.560
and the strong version of the system

02:03:42.560 --> 02:03:44.520
that could deceive you if it wanted to do that,

02:03:44.520 --> 02:03:46.120
if it was already like sufficiently unaligned

02:03:46.120 --> 02:03:47.760
to want to deceive you.

02:03:47.760 --> 02:03:49.600
There's the question of like,

02:03:49.600 --> 02:03:52.120
how on the current paradigm you train honesty

02:03:52.120 --> 02:03:53.360
when the humans can no longer tell

02:03:53.360 --> 02:03:54.920
if the system is being honest.

02:03:58.040 --> 02:03:59.800
You don't think these are research questions

02:03:59.800 --> 02:04:00.840
that could be answered?

02:04:00.840 --> 02:04:02.720
I think they could be answered at 50 years

02:04:02.720 --> 02:04:03.880
with unlimited retries,

02:04:03.880 --> 02:04:05.960
the way things usually work in science.

02:04:07.400 --> 02:04:08.320
I just disagree with that.

02:04:08.320 --> 02:04:10.680
You're making it 50 years, I think,

02:04:10.680 --> 02:04:12.120
with the kind of attention this gets,

02:04:12.120 --> 02:04:13.280
with the kind of funding it gets,

02:04:13.280 --> 02:04:15.600
it could be answered not in whole,

02:04:15.600 --> 02:04:19.000
but incrementally within months

02:04:19.000 --> 02:04:21.520
and within a small number of years

02:04:21.520 --> 02:04:26.360
if it's at scale, receives attention and research.

02:04:26.360 --> 02:04:29.200
And so if you start studying large language models,

02:04:29.200 --> 02:04:32.720
I think there was an intuition like two years ago even

02:04:32.720 --> 02:04:34.600
that something like GPT-4,

02:04:34.600 --> 02:04:38.680
the current capabilities of even chat GPT with GPT-3.5

02:04:38.680 --> 02:04:42.600
is not, we're still far away from that.

02:04:42.600 --> 02:04:43.880
I think a lot of people are surprised

02:04:43.880 --> 02:04:45.800
by the capabilities of GPT-4, right?

02:04:45.800 --> 02:04:46.840
So now people are waking up,

02:04:46.840 --> 02:04:49.240
okay, we need to study these language models.

02:04:49.240 --> 02:04:51.680
I think there's going to be a lot of interesting

02:04:51.680 --> 02:04:53.720
AI safety research.

02:04:53.720 --> 02:04:57.240
Are the, are Earth's billionaires going to put up

02:04:57.280 --> 02:05:01.040
like the giant prizes that would maybe incentivize

02:05:01.040 --> 02:05:04.240
young hotshot people who just got their physics degrees

02:05:04.240 --> 02:05:05.600
to not go to the hedge funds

02:05:05.600 --> 02:05:08.520
and instead put everything into interpretability

02:05:08.520 --> 02:05:11.520
in this like one small area where we can actually tell

02:05:11.520 --> 02:05:13.640
whether or not somebody has made a discovery or not?

02:05:13.640 --> 02:05:16.920
I think so, because I think so.

02:05:16.920 --> 02:05:19.160
Well, that's what these conversations are about

02:05:19.160 --> 02:05:21.040
because they're going to wake up to the fact

02:05:21.040 --> 02:05:24.760
that GPT-4 can be used to manipulate elections,

02:05:24.760 --> 02:05:27.720
to influence geopolitics, to influence the economy.

02:05:27.720 --> 02:05:30.840
There's a lot of, there's going to be a huge amount

02:05:30.840 --> 02:05:32.920
of incentive to like, wait a minute.

02:05:32.920 --> 02:05:36.640
We can't, this has to be, we have to put,

02:05:36.640 --> 02:05:38.480
we have to make sure they're not doing damage.

02:05:38.480 --> 02:05:40.520
We have to make sure we, interpretability,

02:05:40.520 --> 02:05:41.840
we have to make sure we understand

02:05:41.840 --> 02:05:44.960
how these systems function so that we can predict

02:05:44.960 --> 02:05:47.280
their effect on economy so that there's fairness and safety.

02:05:47.280 --> 02:05:49.840
So there's a feudal moral panic

02:05:49.840 --> 02:05:52.720
and a bunch of op-eds in the New York Times

02:05:52.760 --> 02:05:55.760
and nobody actually stepping forth and saying,

02:05:55.760 --> 02:05:58.040
you know what, instead of a mega yacht,

02:05:58.040 --> 02:06:00.960
I'd rather put that billion dollars on prizes

02:06:00.960 --> 02:06:02.600
for young hotshot physicists

02:06:02.600 --> 02:06:05.400
who make fundamental breakthroughs in interpretability.

02:06:08.000 --> 02:06:10.040
The yacht versus the interpretability research,

02:06:10.040 --> 02:06:12.000
the old trade-off.

02:06:13.920 --> 02:06:16.960
I just, I think, I think there's going to be

02:06:16.960 --> 02:06:19.280
a huge amount of allocation of funds.

02:06:19.280 --> 02:06:20.840
I hope, I hope I guess.

02:06:20.840 --> 02:06:22.720
You want to bet me on that?

02:06:22.720 --> 02:06:24.200
What you want to put a time scale on it?

02:06:24.200 --> 02:06:26.400
Say how much funds you think are going to be allocated

02:06:26.400 --> 02:06:28.680
in a direction that I would consider

02:06:28.680 --> 02:06:31.960
to be actually useful by what time?

02:06:33.000 --> 02:06:36.600
I do think there will be a huge amount of funds,

02:06:36.600 --> 02:06:39.200
but you're saying it needs to be open, right?

02:06:39.200 --> 02:06:41.200
The development of the systems should be closed,

02:06:41.200 --> 02:06:44.760
but the development of the interpretability research,

02:06:44.760 --> 02:06:46.120
the AI safety research.

02:06:46.120 --> 02:06:50.440
Oh, we are so far behind on interpretability

02:06:50.440 --> 02:06:52.040
compared to capabilities.

02:06:52.040 --> 02:06:56.600
Like, yeah, you could take the last generation of systems,

02:06:56.600 --> 02:06:58.600
the stuff that's already in the open.

02:06:58.600 --> 02:07:00.800
There is so much in there that we don't understand.

02:07:00.800 --> 02:07:02.920
There are so many prizes you could do before,

02:07:02.920 --> 02:07:06.360
you know, you would have enough insights

02:07:06.360 --> 02:07:07.640
that you'd be like, oh, you know, like,

02:07:07.640 --> 02:07:09.280
well, we understand how these systems work.

02:07:09.280 --> 02:07:11.200
We understand how these things are doing, their outputs.

02:07:11.200 --> 02:07:12.320
We can read their minds.

02:07:12.320 --> 02:07:14.400
Now let's try it with the bigger systems.

02:07:14.400 --> 02:07:16.320
Yeah, we're nowhere near that.

02:07:16.320 --> 02:07:18.240
There is so much interpretability work to be done

02:07:18.240 --> 02:07:20.040
on the weaker versions of the systems.

02:07:20.040 --> 02:07:25.040
So what can you say on the second point you said to Elon Musk

02:07:26.040 --> 02:07:30.360
on what are some ideas, what are things you could try?

02:07:30.360 --> 02:07:33.000
I can think of a few things at try, you said.

02:07:33.000 --> 02:07:34.880
They don't fit in one tweet.

02:07:34.880 --> 02:07:38.080
So is there something you could put into words

02:07:38.080 --> 02:07:39.800
of the things you would try?

02:07:39.800 --> 02:07:44.320
I mean, the trouble is the stuff is subtle.

02:07:44.320 --> 02:07:46.280
I've watched people try to make progress on this

02:07:46.280 --> 02:07:48.040
and not get places.

02:07:48.040 --> 02:07:51.880
Somebody who just like gets alarmed and charges in,

02:07:51.880 --> 02:07:53.920
it's like going nowhere.

02:07:53.920 --> 02:07:54.760
Sure.

02:07:54.760 --> 02:07:56.640
It meant like years ago about, I don't know,

02:07:56.640 --> 02:07:59.560
like 20 years, 15 years, something like that.

02:07:59.560 --> 02:08:01.360
I was talking to a Congress person

02:08:03.880 --> 02:08:07.960
who had become alarmed about the eventual prospects

02:08:07.960 --> 02:08:12.960
and he wanted work on building AIs without emotions

02:08:14.440 --> 02:08:17.160
because the emotional AIs were the scary ones you see.

02:08:18.040 --> 02:08:22.280
And some poor person at ARPA had come up

02:08:22.280 --> 02:08:25.680
with a research proposal whereby this congressman's panic

02:08:25.680 --> 02:08:29.400
and desire to fund this thing would go into something

02:08:29.400 --> 02:08:31.160
that the person at ARPA thought would be useful

02:08:31.160 --> 02:08:33.200
and had been munched around to where it would like sound

02:08:33.200 --> 02:08:35.960
if the congressman like work was happening on this,

02:08:35.960 --> 02:08:38.600
which, you know, of course, like this is just,

02:08:38.600 --> 02:08:41.040
the congressperson had misunderstood the problem

02:08:42.160 --> 02:08:44.640
and did not understand where the danger came from.

02:08:45.640 --> 02:08:50.640
And so it's like the issue is that you could like do this

02:08:52.240 --> 02:08:55.160
in a certain precise way and maybe get something.

02:08:55.160 --> 02:08:57.920
Like when I say like put up prices on interpretability,

02:08:57.920 --> 02:09:02.920
I'm not, I'm like, well, like because it's verifiable there

02:09:05.080 --> 02:09:07.600
as opposed to other places, you can tell whether

02:09:07.600 --> 02:09:09.560
or not good work actually happened.

02:09:09.560 --> 02:09:11.960
In this exact narrow case, if you do things

02:09:11.960 --> 02:09:14.600
in exactly the right way, you can maybe throw money

02:09:15.560 --> 02:09:20.280
at it and produce science instead of anti-science and nonsense.

02:09:20.280 --> 02:09:24.280
And all the methods that I know of like trying

02:09:24.280 --> 02:09:27.640
to throw money at this problem, share this property of like,

02:09:27.640 --> 02:09:30.360
well, if you do it exactly right based on understanding

02:09:30.360 --> 02:09:32.520
exactly what has, you know, like tends to produce

02:09:32.520 --> 02:09:34.680
like useful outputs or not, then you can like add money

02:09:34.680 --> 02:09:36.080
to it in this way.

02:09:36.080 --> 02:09:38.560
Now there's like, and the thing that I'm giving

02:09:38.560 --> 02:09:41.400
as an example here in front of this large audience

02:09:41.400 --> 02:09:44.080
is the most understandable of those.

02:09:44.800 --> 02:09:48.280
Cause there's like other people who, you know,

02:09:48.280 --> 02:09:52.440
like Chris Ola and even more generally,

02:09:52.440 --> 02:09:54.560
like you can tell whether or not interpretability

02:09:54.560 --> 02:09:56.040
progress has occurred.

02:09:56.040 --> 02:09:58.160
So like, if I say throw money at producing

02:09:58.160 --> 02:10:00.760
more interpretability, there's like a chance somebody

02:10:00.760 --> 02:10:01.880
can do it that way.

02:10:01.880 --> 02:10:04.080
And like, it will actually produce useful results.

02:10:04.080 --> 02:10:07.080
Then the other stuff just blurs off into be like harder

02:10:07.080 --> 02:10:09.520
to target exactly than that.

02:10:09.520 --> 02:10:14.160
So sometimes the basics are fun to explore

02:10:14.160 --> 02:10:16.280
because they're not so basic.

02:10:16.280 --> 02:10:18.680
What do you, what is interpretability?

02:10:18.680 --> 02:10:20.880
What do you, what does it look like?

02:10:20.880 --> 02:10:22.200
What are we talking about?

02:10:22.200 --> 02:10:27.200
It looks like we took a much smaller set

02:10:28.920 --> 02:10:32.560
of transformer layers than the ones in the modern

02:10:32.560 --> 02:10:35.880
bleeding edge state of the art systems.

02:10:35.880 --> 02:10:40.880
And after applying various tools and mathematical ideas

02:10:40.880 --> 02:10:45.480
and trying 20 different things, we found,

02:10:45.480 --> 02:10:48.240
we have shown that this piece of the system

02:10:48.240 --> 02:10:50.480
is doing this kind of useful work.

02:10:51.880 --> 02:10:54.680
And then somehow also hopefully generalizes

02:10:54.680 --> 02:10:57.960
some fundamental understanding of what's going on

02:10:57.960 --> 02:11:00.600
that generalizes to the bigger system.

02:11:01.440 --> 02:11:03.520
You can hope, and it's probably true.

02:11:03.520 --> 02:11:07.880
Like you would not expect the smaller tricks to go away

02:11:07.880 --> 02:11:11.120
when you have a system that's like doing

02:11:11.120 --> 02:11:12.160
larger kinds of work.

02:11:12.160 --> 02:11:13.760
You would expect the larger work kinds of work

02:11:13.760 --> 02:11:15.880
to be a building on top of the smaller kinds of work

02:11:15.880 --> 02:11:18.720
and gradient descent runs across the smaller kinds of work

02:11:18.720 --> 02:11:21.520
before it runs across the larger kinds of work.

02:11:21.520 --> 02:11:24.200
Well, that's kind of what is happening in neuroscience.

02:11:24.200 --> 02:11:27.480
It's trying to understand the human brain by prodding.

02:11:27.480 --> 02:11:30.520
And it's such a giant mystery and people have made progress

02:11:30.520 --> 02:11:32.200
even though it's extremely difficult to make sense

02:11:32.200 --> 02:11:33.040
of what's going on in the brain.

02:11:33.040 --> 02:11:34.320
They have different parts of the brain

02:11:34.320 --> 02:11:36.200
that are responsible for hearing, for sight,

02:11:36.240 --> 02:11:38.240
the vision, science community,

02:11:38.240 --> 02:11:39.840
there's understanding the visual cortex.

02:11:39.840 --> 02:11:41.480
I mean, they've made a lot of progress

02:11:41.480 --> 02:11:43.680
in understanding how that stuff works.

02:11:43.680 --> 02:11:46.680
And that's, I guess, but you're saying it takes a long time

02:11:46.680 --> 02:11:47.840
to do that work well.

02:11:47.840 --> 02:11:49.560
Also, it's not enough.

02:11:49.560 --> 02:11:54.240
So in particular, let's say you have got

02:11:54.240 --> 02:11:59.240
your interpretability tools and they say

02:12:00.000 --> 02:12:04.880
that your current AI system is plotting to kill you.

02:12:04.880 --> 02:12:05.720
Now what?

02:12:06.200 --> 02:12:11.200
It is definitely a good step one, right?

02:12:11.840 --> 02:12:12.960
Yeah, what's step two?

02:12:16.680 --> 02:12:18.160
If you cut out that layer,

02:12:18.160 --> 02:12:21.760
is it gonna stop wanting to kill you?

02:12:21.760 --> 02:12:26.760
When you optimize against visible misalignment,

02:12:28.840 --> 02:12:31.600
you are optimizing against misalignment

02:12:31.600 --> 02:12:34.920
and you are also optimizing against visibility.

02:12:34.920 --> 02:12:37.080
So sure, if you can.

02:12:37.080 --> 02:12:38.480
Yeah, it's true.

02:12:38.480 --> 02:12:39.880
All you're doing is removing

02:12:39.880 --> 02:12:42.440
the obvious intentions to kill you.

02:12:42.440 --> 02:12:43.880
You've got your detector,

02:12:43.880 --> 02:12:45.840
it's showing something inside the system

02:12:45.840 --> 02:12:47.240
that you don't like.

02:12:47.240 --> 02:12:50.600
Okay, say the disaster monkey is running this thing.

02:12:50.600 --> 02:12:52.160
We'll optimize the system

02:12:52.160 --> 02:12:54.920
until the visible bad behavior goes away.

02:12:54.920 --> 02:12:58.360
But it's arising for fundamental reasons

02:12:58.360 --> 02:12:59.800
of instrumental convergence.

02:12:59.800 --> 02:13:02.280
The old, you can't bring the coffee if you're dead.

02:13:02.280 --> 02:13:07.280
Any goal, almost every set of utility functions

02:13:08.640 --> 02:13:10.000
with a few narrow exceptions

02:13:10.000 --> 02:13:11.560
implies killing all the humans.

02:13:12.560 --> 02:13:14.000
But do you think it's possible

02:13:14.000 --> 02:13:16.200
because we can do experimentation

02:13:16.200 --> 02:13:19.360
to discover the source of the desire to kill?

02:13:19.360 --> 02:13:21.240
I can tell it to you right now.

02:13:21.240 --> 02:13:23.680
It's that it wants to do something

02:13:23.680 --> 02:13:27.000
and the way to get the most of that thing

02:13:27.000 --> 02:13:28.680
is to put the universe into a state

02:13:28.680 --> 02:13:30.360
where there aren't humans.

02:13:30.360 --> 02:13:34.880
So is it possible to encode in the same way we think,

02:13:34.880 --> 02:13:37.160
like why do we think murder is wrong?

02:13:37.160 --> 02:13:41.160
The same foundational ethics.

02:13:42.040 --> 02:13:45.080
It's not hard-coded in, but more like deeper.

02:13:45.080 --> 02:13:46.320
I mean, that's part of the research.

02:13:46.320 --> 02:13:49.760
How do you have it that this transformer,

02:13:49.760 --> 02:13:53.640
this small version of the language model

02:13:53.640 --> 02:13:56.240
doesn't ever want to kill?

02:13:56.840 --> 02:14:00.800
That'd be nice, assuming that you got,

02:14:00.800 --> 02:14:03.560
doesn't want to kill sufficiently exactly right,

02:14:03.560 --> 02:14:06.480
that it didn't be like, oh, I will detach their heads

02:14:06.480 --> 02:14:08.640
and put them in some jars and keep the heads alive forever

02:14:08.640 --> 02:14:10.080
and then go do the thing.

02:14:10.080 --> 02:14:13.080
But leaving that aside, well, not leaving that aside.

02:14:13.080 --> 02:14:14.920
Yeah, that's a good strong point, yeah.

02:14:14.920 --> 02:14:16.520
Because there is a whole issue

02:14:16.520 --> 02:14:18.520
where as something gets smarter,

02:14:18.520 --> 02:14:23.520
it finds ways of achieving the same goal predicate

02:14:24.040 --> 02:14:28.400
that were not imaginable to stupider versions

02:14:28.400 --> 02:14:31.160
of the system or perhaps to stupider operators.

02:14:31.160 --> 02:14:34.360
That's one of many things making this difficult.

02:14:34.360 --> 02:14:36.200
A larger thing making this difficult

02:14:36.200 --> 02:14:38.400
is that we do not know how to get any goals

02:14:38.400 --> 02:14:39.920
into systems at all.

02:14:39.920 --> 02:14:42.680
We know how to get outwardly observable behaviors

02:14:42.680 --> 02:14:43.880
into systems.

02:14:43.880 --> 02:14:48.200
We do not know how to get internal psychological wanting

02:14:48.200 --> 02:14:50.800
to do particular things into the system.

02:14:50.800 --> 02:14:53.040
That is not what the current technology does.

02:14:54.600 --> 02:14:57.600
I mean, it could be things like dystopian futures

02:14:57.600 --> 02:15:01.680
like Brave New World, where most humans will actually say,

02:15:01.680 --> 02:15:03.120
we kind of want that future.

02:15:03.120 --> 02:15:04.520
It's a great future.

02:15:04.520 --> 02:15:05.600
Everybody's happy.

02:15:06.520 --> 02:15:10.120
We would have to get so far, so much further

02:15:10.120 --> 02:15:13.600
than we are now and further faster

02:15:13.600 --> 02:15:17.440
before that failure mode became a running concern.

02:15:17.440 --> 02:15:20.160
Your failure modes are much more drastic,

02:15:20.160 --> 02:15:21.160
the ones you control.

02:15:21.160 --> 02:15:22.680
The failure modes are much simpler.

02:15:22.680 --> 02:15:25.280
It's like, yeah, like the AI puts the universe

02:15:25.280 --> 02:15:26.120
into a particular state.

02:15:26.120 --> 02:15:28.440
It happens to not have any humans inside it.

02:15:28.440 --> 02:15:30.400
Okay, so the paperclip maximizer.

02:15:31.680 --> 02:15:33.960
Utility, so the original version

02:15:33.960 --> 02:15:35.040
of the paperclip maximizer-

02:15:35.040 --> 02:15:36.280
Can you explain it if you can?

02:15:36.280 --> 02:15:37.600
Okay.

02:15:37.600 --> 02:15:40.560
The original version was you lose control

02:15:40.560 --> 02:15:43.240
of the utility function, and it so happens

02:15:43.240 --> 02:15:48.240
that what maxes out the utility per unit resources

02:15:49.000 --> 02:15:52.440
is tiny molecular shapes like paperclips.

02:15:52.440 --> 02:15:54.680
There's a lot of things that make it happy,

02:15:54.680 --> 02:15:57.840
but the cheapest one that didn't saturate

02:15:57.840 --> 02:16:02.200
was putting matter into certain shapes,

02:16:02.200 --> 02:16:04.520
and it so happens that the cheapest way

02:16:04.520 --> 02:16:06.280
to make these shapes is to make them very small

02:16:06.280 --> 02:16:07.960
because then you need fewer atoms, for instance,

02:16:07.960 --> 02:16:10.600
of the shape, and arguing though,

02:16:12.400 --> 02:16:14.240
it happens to look like a paperclip.

02:16:14.240 --> 02:16:17.880
In retrospect, I wish I'd said tiny molecular spirals.

02:16:18.720 --> 02:16:21.160
Or like tiny molecular hyperbolic spirals.

02:16:21.160 --> 02:16:22.000
Why?

02:16:22.000 --> 02:16:24.240
Because I said tiny molecular paperclips.

02:16:24.240 --> 02:16:28.200
This got heard as, this got then mutated to paperclips.

02:16:28.200 --> 02:16:32.080
This then mutated to, and the AI was in a paperclip factory.

02:16:33.200 --> 02:16:35.720
So the original story is about how

02:16:35.720 --> 02:16:37.160
you lose control of the system.

02:16:37.160 --> 02:16:39.320
It doesn't want what you try to make it want.

02:16:39.320 --> 02:16:41.880
The thing that it ends up wanting most

02:16:41.880 --> 02:16:43.880
is a thing that even from a very embracing

02:16:43.880 --> 02:16:46.640
cosmopolitan perspective, we think of as having no value,

02:16:46.760 --> 02:16:49.640
and that's how the value of the future gets destroyed.

02:16:49.640 --> 02:16:51.960
Then that got changed to a fable of like,

02:16:51.960 --> 02:16:53.800
well, you made a paperclip factory,

02:16:53.800 --> 02:16:55.440
and it did exactly what you wanted,

02:16:55.440 --> 02:16:57.960
but you asked it to do the wrong thing,

02:16:57.960 --> 02:17:01.360
which is a completely different failure mode.

02:17:05.880 --> 02:17:09.440
But those are both concerns to you.

02:17:09.440 --> 02:17:11.880
So that's more than Brave New World.

02:17:11.880 --> 02:17:15.080
If you can solve the problem of making something want,

02:17:16.040 --> 02:17:18.640
exactly what you want it to want,

02:17:18.640 --> 02:17:20.080
then you get to deal with the problem

02:17:20.080 --> 02:17:22.240
of wanting the right thing.

02:17:22.240 --> 02:17:23.960
But first you have to solve the alignment.

02:17:23.960 --> 02:17:26.160
First you have to solve inner alignment.

02:17:26.160 --> 02:17:27.000
Inner alignment.

02:17:27.000 --> 02:17:29.000
Then you get to solve outer alignment.

02:17:31.440 --> 02:17:34.480
Like first, you need to be able to point the insides

02:17:34.480 --> 02:17:37.280
of the thing in a direction, and then you get to deal with

02:17:37.280 --> 02:17:40.000
whether that direction expressed in reality

02:17:40.000 --> 02:17:41.720
is like the thing that aligned with the thing

02:17:41.720 --> 02:17:42.560
that you wanted.

02:17:45.840 --> 02:17:46.680
Are you scared?

02:17:47.720 --> 02:17:48.800
Of this whole thing?

02:17:50.680 --> 02:17:51.720
Probably.

02:17:51.720 --> 02:17:54.680
I don't really know.

02:17:54.680 --> 02:17:56.960
What gives you hope about this?

02:17:56.960 --> 02:17:58.320
Possibility of being wrong.

02:17:59.680 --> 02:18:01.040
Not that you're right,

02:18:01.040 --> 02:18:02.840
but we will actually get our act together

02:18:02.840 --> 02:18:07.760
and allocate a lot of resources to the alignment problem.

02:18:07.760 --> 02:18:11.360
Well, I can easily imagine that at some point

02:18:11.360 --> 02:18:15.040
this panic expresses itself in the waste of a billion dollars.

02:18:16.040 --> 02:18:18.920
Spending a billion dollars correctly, that's harder.

02:18:18.920 --> 02:18:21.600
To solve both the inner and the outer alignment.

02:18:21.600 --> 02:18:22.440
If you're wrong.

02:18:22.440 --> 02:18:23.680
To solve a number of things.

02:18:23.680 --> 02:18:24.960
Yeah, a number of things.

02:18:24.960 --> 02:18:29.960
If you're wrong, what do you think would be the reason?

02:18:30.280 --> 02:18:34.520
Like 50 years from now, not perfectly wrong.

02:18:34.520 --> 02:18:36.720
You make a lot of really eloquent points.

02:18:38.240 --> 02:18:42.200
There's a lot of shape to the ideas you express.

02:18:42.200 --> 02:18:44.320
But if you're somewhat wrong

02:18:44.320 --> 02:18:46.760
about some fundamental ideas, why would that be?

02:18:48.040 --> 02:18:51.080
Stuff has to be easier than I think it is.

02:18:52.400 --> 02:18:54.920
The first time you're building a rocket,

02:18:54.920 --> 02:18:58.320
being wrong is in a certain sense, quite easy.

02:18:59.160 --> 02:19:00.680
Happening to be wrong in a way

02:19:00.680 --> 02:19:03.280
where the rocket goes twice as far and half the fuel

02:19:03.280 --> 02:19:05.440
and lands exactly where you hoped it would.

02:19:06.360 --> 02:19:09.600
Most cases of being wrong make it harder to build a rocket,

02:19:09.600 --> 02:19:10.920
harder to have it not explode,

02:19:10.920 --> 02:19:13.560
cause it to require more fuel than you hoped,

02:19:13.560 --> 02:19:15.960
cause it to land off target.

02:19:15.960 --> 02:19:18.120
Being wrong in a way that makes stuff easier,

02:19:18.120 --> 02:19:21.080
that's not the usual project management story.

02:19:21.080 --> 02:19:23.800
Yeah, but then this is the first time

02:19:23.800 --> 02:19:26.120
we're really tackling the problem with AI alignment.

02:19:26.120 --> 02:19:28.120
There's no examples in history where we...

02:19:28.120 --> 02:19:30.720
Well, there's all kinds of things that are similar

02:19:30.720 --> 02:19:32.600
if you generalize them correctly the right way

02:19:32.600 --> 02:19:35.120
and aren't fooled by misleading metaphors.

02:19:35.120 --> 02:19:36.160
Like what?

02:19:36.160 --> 02:19:39.960
Humans being misaligned on inclusogenic fitness.

02:19:39.960 --> 02:19:41.920
So inclusogenic fitness is like

02:19:41.920 --> 02:19:43.800
not just your reproductive fitness,

02:19:43.800 --> 02:19:45.520
but also the fitness of your relatives,

02:19:45.520 --> 02:19:49.720
the people who share some fraction of your genes.

02:19:49.720 --> 02:19:52.440
The old joke is would you give your life

02:19:52.440 --> 02:19:53.440
to save your brother?

02:19:53.440 --> 02:19:57.120
They once asked a biologist, I think it was Haldane,

02:19:57.120 --> 02:19:59.000
and Haldane said, no, but I would give my life

02:19:59.000 --> 02:20:00.960
to save two brothers or eight cousins.

02:20:02.680 --> 02:20:05.320
Cause a brother on average shares half your genes

02:20:05.320 --> 02:20:08.440
and cousin on average shares an eighth of your genes.

02:20:08.440 --> 02:20:10.080
So that's inclusive genetic fitness

02:20:10.080 --> 02:20:12.600
and you can view natural selection

02:20:12.600 --> 02:20:15.560
as optimizing humans exclusively around this

02:20:15.560 --> 02:20:18.560
like one very simple criterion.

02:20:18.560 --> 02:20:21.880
Like how much more frequent did your genes become

02:20:21.880 --> 02:20:23.160
in the next generation?

02:20:23.160 --> 02:20:25.480
In fact, that just is natural selection.

02:20:25.480 --> 02:20:27.400
It doesn't optimize for that,

02:20:27.400 --> 02:20:29.680
but rather the process of genes becoming more frequent

02:20:29.680 --> 02:20:31.680
is that you can nonetheless imagine

02:20:31.680 --> 02:20:34.360
that there is this hill climbing process,

02:20:34.360 --> 02:20:36.080
not like gradient descent,

02:20:36.080 --> 02:20:38.040
because gradient descent uses calculus.

02:20:38.120 --> 02:20:40.080
This is just using like, where are you?

02:20:40.080 --> 02:20:41.960
But still hill climbing in both cases,

02:20:41.960 --> 02:20:45.320
making something better and better over time in steps.

02:20:47.000 --> 02:20:50.040
And natural selection was optimizing exclusively

02:20:50.040 --> 02:20:52.360
for this very simple pure criterion

02:20:52.360 --> 02:20:54.480
of inclusive genetic fitness

02:20:55.640 --> 02:20:58.040
in a very complicated environment.

02:20:58.040 --> 02:20:59.960
We're doing a very wide range of things

02:20:59.960 --> 02:21:02.520
and solving a wide range of problems

02:21:02.520 --> 02:21:05.120
led to having more kids.

02:21:06.000 --> 02:21:09.560
And this got you humans,

02:21:09.560 --> 02:21:14.560
which had no internal notion of inclusive genetic fitness

02:21:14.880 --> 02:21:17.040
until thousands of years later

02:21:17.040 --> 02:21:19.920
when they were actually figuring out what had even happened

02:21:20.840 --> 02:21:24.240
and no desire to, no explicit desire

02:21:24.240 --> 02:21:26.960
to increase inclusive genetic fitness.

02:21:26.960 --> 02:21:28.760
So from this we may,

02:21:28.760 --> 02:21:30.680
so from this important case study,

02:21:30.680 --> 02:21:33.000
we may infer the important fact

02:21:33.000 --> 02:21:35.320
that if you do a whole bunch of hill climbing

02:21:35.320 --> 02:21:37.880
on a very simple loss function

02:21:37.880 --> 02:21:40.880
at the point where the system's capabilities

02:21:40.880 --> 02:21:43.640
start to generalize very widely,

02:21:43.640 --> 02:21:47.000
when it is in an intuitive sense becoming very capable

02:21:47.000 --> 02:21:49.840
and generalizing far outside the training distribution,

02:21:51.360 --> 02:21:53.200
we know that there is no general loss

02:21:53.200 --> 02:21:57.960
saying that the system even internally represents,

02:21:57.960 --> 02:22:00.480
let alone tries to optimize

02:22:00.480 --> 02:22:04.000
the very simple loss function you are training it on.

02:22:04.000 --> 02:22:06.920
There is so much that we cannot possibly cover all of it.

02:22:06.920 --> 02:22:11.200
I think we did a good job of getting your sense

02:22:11.200 --> 02:22:13.880
from different perspectives of the current state of the art

02:22:13.880 --> 02:22:15.880
with large language models.

02:22:15.880 --> 02:22:20.160
We've got a good sense of your concern

02:22:20.160 --> 02:22:23.000
about the threats of AGI.

02:22:23.000 --> 02:22:26.520
I've talked here about the power of intelligence

02:22:26.520 --> 02:22:29.240
and not really gotten very far into it,

02:22:29.240 --> 02:22:32.960
but not like why it is that suppose you like screw up

02:22:32.960 --> 02:22:37.000
with AGI and end up wanting a bunch of random stuff.

02:22:37.000 --> 02:22:40.080
Why does it try to kill you?

02:22:40.080 --> 02:22:43.040
Why doesn't it try to trade with you?

02:22:43.040 --> 02:22:46.520
Why doesn't it give you just the tiny little fraction

02:22:46.520 --> 02:22:48.920
of the solar system that it would keep to take everyone a lot

02:22:48.920 --> 02:22:51.080
that it would take to keep everyone alive?

02:22:52.000 --> 02:22:53.040
Yeah, well, that's a good question.

02:22:53.040 --> 02:22:54.760
I mean, what are the different trajectories

02:22:54.760 --> 02:22:57.640
that intelligence when acted upon this world,

02:22:57.640 --> 02:22:59.920
super intelligence, what are the different trajectories

02:22:59.920 --> 02:23:02.800
for this universe with such an intelligence in it?

02:23:02.800 --> 02:23:04.600
Do most of them not include humans?

02:23:05.480 --> 02:23:08.120
I mean, the vast majority

02:23:08.120 --> 02:23:10.760
of randomly specified utility functions

02:23:10.760 --> 02:23:14.760
do not have optima with humans in them.

02:23:14.760 --> 02:23:18.120
Would be the like first thing I would point out.

02:23:18.120 --> 02:23:19.480
And then the next question is like,

02:23:19.480 --> 02:23:21.440
well, if you try to optimize something

02:23:21.440 --> 02:23:24.720
and you lose control of it, where in that space do you land?

02:23:24.720 --> 02:23:26.200
So that's not random,

02:23:26.200 --> 02:23:29.800
but it also doesn't necessarily have room for humans in it.

02:23:29.800 --> 02:23:32.400
I suspect that the average member of the audience

02:23:32.400 --> 02:23:34.080
might have some questions about even

02:23:34.080 --> 02:23:36.080
whether that's the correct paradigm to think about it

02:23:36.080 --> 02:23:39.400
and would sort of want to back up a bit.

02:23:39.400 --> 02:23:44.400
If we back up to something bigger than humans,

02:23:44.760 --> 02:23:46.920
if we look at earth and life on earth

02:23:48.160 --> 02:23:50.480
and what is truly special about life on earth,

02:23:51.000 --> 02:23:54.560
do you think it's possible that a lot,

02:23:54.560 --> 02:23:56.920
whatever that special thing is,

02:23:56.920 --> 02:23:59.400
let's explore what that special thing could be.

02:23:59.400 --> 02:24:00.760
Whatever that special thing is,

02:24:00.760 --> 02:24:04.840
that thing appears often in the objective function.

02:24:04.840 --> 02:24:05.680
Why?

02:24:07.680 --> 02:24:11.680
I know what you hope, but you can hope

02:24:11.680 --> 02:24:13.880
that a particular set of winning lottery numbers come up

02:24:13.880 --> 02:24:17.560
and it doesn't make the lottery balls come up that way.

02:24:17.560 --> 02:24:18.880
I know you want this to be true,

02:24:18.880 --> 02:24:19.960
but why would it be true?

02:24:19.960 --> 02:24:22.960
There's a line from Grumpy Old Men

02:24:22.960 --> 02:24:25.320
where this guy says in a grocery store,

02:24:25.320 --> 02:24:28.240
he says you can wish in one hand and crap in the other

02:24:28.240 --> 02:24:30.360
and see which one fills up first.

02:24:30.360 --> 02:24:31.200
This is a science problem.

02:24:31.200 --> 02:24:34.640
We are trying to predict what happens with AI systems

02:24:34.640 --> 02:24:39.640
that you try to optimize to imitate humans

02:24:39.640 --> 02:24:41.480
and then you did some of like RLHF to them

02:24:41.480 --> 02:24:46.480
and of course you didn't get perfect alignment

02:24:47.480 --> 02:24:52.480
because that's not what happens when you hill climb

02:24:52.760 --> 02:24:54.160
towards outer loss function.

02:24:54.160 --> 02:24:56.720
You don't get inner alignment on it,

02:24:56.720 --> 02:25:01.720
but yeah, so if you don't mind my like taking

02:25:05.200 --> 02:25:07.200
some slight control of things and steering around

02:25:07.200 --> 02:25:10.320
to what I think is like a good place to start.

02:25:10.320 --> 02:25:12.880
I just failed to solve the control problem.

02:25:12.880 --> 02:25:14.320
I've lost control of this thing.

02:25:14.320 --> 02:25:15.800
Alignment, alignment.

02:25:16.920 --> 02:25:17.760
Still aligned.

02:25:17.760 --> 02:25:18.600
Control, yeah.

02:25:18.600 --> 02:25:20.680
Okay, sure, yeah, you lost control.

02:25:20.680 --> 02:25:22.520
But we're still aligned.

02:25:22.520 --> 02:25:24.200
Anyway, sorry for the meta comment.

02:25:24.200 --> 02:25:26.680
Yeah, losing control isn't as bad as you lose control

02:25:26.680 --> 02:25:27.720
to an aligned system.

02:25:27.720 --> 02:25:29.360
Yes, exactly.

02:25:29.360 --> 02:25:30.720
You have no idea of the horrors

02:25:30.720 --> 02:25:32.520
I will shortly unleash on this conversation.

02:25:32.520 --> 02:25:37.040
All right, so I decided to distract you completely.

02:25:37.040 --> 02:25:38.200
What were you gonna say in terms of

02:25:38.200 --> 02:25:40.080
taking control of the conversation?

02:25:40.080 --> 02:25:44.080
So I think that there's like a

02:25:44.920 --> 02:25:46.760
Ceylon chapterists here,

02:25:46.760 --> 02:25:48.880
if I'm pronouncing those words remotely like correctly,

02:25:48.880 --> 02:25:50.200
because of course I only ever read them

02:25:50.200 --> 02:25:51.560
and not hear them spoken.

02:25:52.920 --> 02:25:56.880
There's a, like for some people,

02:25:56.880 --> 02:26:00.480
like the word intelligence smartness

02:26:00.480 --> 02:26:02.800
is not a word of power to them.

02:26:02.800 --> 02:26:05.040
It means chess players who,

02:26:05.040 --> 02:26:08.000
it means like the college university professor,

02:26:08.000 --> 02:26:09.600
people who aren't very successful in life.

02:26:09.600 --> 02:26:11.440
It doesn't mean like charisma,

02:26:11.440 --> 02:26:13.280
to which my usual thing is like charisma

02:26:13.280 --> 02:26:15.520
is not generated in the liver rather than the brain.

02:26:15.520 --> 02:26:17.440
Charisma is also a cognitive function.

02:26:21.000 --> 02:26:24.440
So if you think that like smartness

02:26:24.440 --> 02:26:26.640
doesn't sound very threatening,

02:26:26.640 --> 02:26:29.560
then super intelligence is not gonna sound

02:26:29.560 --> 02:26:30.600
very threatening either.

02:26:30.600 --> 02:26:33.960
It's gonna sound like you just pulled the off switch.

02:26:33.960 --> 02:26:35.640
Like it's, well, it's super intelligent,

02:26:35.640 --> 02:26:36.480
but it's stuck in a computer.

02:26:36.480 --> 02:26:38.440
We pulled the off switch, problem solved.

02:26:40.000 --> 02:26:41.680
And the other side of it is

02:26:42.520 --> 02:26:45.400
you have a lot of respect for the notion of intelligence.

02:26:45.400 --> 02:26:47.440
You're like, well, yeah, that's what humans have.

02:26:47.440 --> 02:26:49.640
That's the human superpower.

02:26:49.640 --> 02:26:52.880
And it sounds like it could be dangerous,

02:26:52.880 --> 02:26:53.960
but why would it be?

02:26:56.000 --> 02:26:59.480
We as we have grown more intelligent,

02:26:59.480 --> 02:27:02.080
also grown less kind.

02:27:02.080 --> 02:27:05.400
Chimpanzees are in fact like a bit less kind than humans.

02:27:05.400 --> 02:27:08.760
And you could like argue that out,

02:27:08.760 --> 02:27:10.120
but often the sort of person

02:27:10.120 --> 02:27:11.920
as a deep respect for intelligence is gonna be like,

02:27:11.920 --> 02:27:14.520
well, yes, like you can't even have kindness

02:27:14.520 --> 02:27:15.960
unless you know what that is.

02:27:17.040 --> 02:27:19.360
And so they're like,

02:27:19.360 --> 02:27:23.040
why would it do something as stupid as making paper clips?

02:27:23.040 --> 02:27:26.000
Aren't you supposing something that's smart enough

02:27:26.000 --> 02:27:27.600
to be dangerous, but also stupid enough

02:27:27.600 --> 02:27:31.440
that it will just make paper clips and never question that?

02:27:31.440 --> 02:27:33.720
In some cases, people are like,

02:27:33.720 --> 02:27:37.440
well, even if you like misspecify the objective function,

02:27:37.440 --> 02:27:39.880
won't you realize that what you really wanted

02:27:40.760 --> 02:27:41.600
was X?

02:27:41.600 --> 02:27:44.360
Are you supposing something that is like smart enough

02:27:44.360 --> 02:27:46.600
to be dangerous, but stupid enough

02:27:46.600 --> 02:27:49.320
that it doesn't understand what the humans really meant

02:27:49.320 --> 02:27:52.200
when they specified the objective function?

02:27:52.200 --> 02:27:57.200
So to you, our intuition about intelligence is limited.

02:27:57.440 --> 02:28:00.600
We should think about intelligence as a much bigger thing.

02:28:00.600 --> 02:28:05.280
Well, what I'm saying is like,

02:28:05.280 --> 02:28:08.080
what you think about artificial intelligence

02:28:08.920 --> 02:28:11.120
depends on what you think about intelligence.

02:28:11.120 --> 02:28:13.680
So how do we think about intelligence correctly?

02:28:13.680 --> 02:28:17.360
Like what, you gave one thought experiment

02:28:17.360 --> 02:28:19.920
to think of a thing that's much faster.

02:28:19.920 --> 02:28:21.400
So it just gets faster and faster and faster

02:28:21.400 --> 02:28:22.240
thinking of the same stuff.

02:28:22.240 --> 02:28:24.480
And it also is made of John von Neumann

02:28:24.480 --> 02:28:26.240
and has like, and there's lots of them.

02:28:26.240 --> 02:28:28.160
Or think of some other smart person.

02:28:28.160 --> 02:28:31.160
Yeah, like John von Neumann is a historical case

02:28:31.160 --> 02:28:32.680
so you can like look up what he did

02:28:32.680 --> 02:28:34.200
and imagine based on that.

02:28:34.200 --> 02:28:37.360
And we know like people have like some intuition

02:28:37.360 --> 02:28:39.920
for like, if you have more humans

02:28:39.920 --> 02:28:42.880
they can solve tougher cognitive problems.

02:28:42.880 --> 02:28:44.880
Although in fact, like in the game of Kasparov

02:28:44.880 --> 02:28:49.680
versus the world, which was like Gary Kasparov on one side

02:28:49.680 --> 02:28:52.640
and an entire horde of internet people

02:28:52.640 --> 02:28:56.040
led by four chess grandmasters on the other side,

02:28:56.040 --> 02:28:57.360
Kasparov won.

02:28:57.360 --> 02:29:01.400
So like all those people aggregated to be smarter.

02:29:01.400 --> 02:29:03.360
It was a hard fought game.

02:29:03.360 --> 02:29:05.320
It's like all those people aggregated to be smarter

02:29:05.320 --> 02:29:06.960
than any individual one of them,

02:29:07.400 --> 02:29:08.840
they didn't aggregate so well

02:29:08.840 --> 02:29:10.680
that they could defeat Kasparov.

02:29:10.680 --> 02:29:13.480
But so like humans aggregating don't actually get

02:29:13.480 --> 02:29:15.600
in my opinion, very much smarter

02:29:15.600 --> 02:29:18.320
especially compared to running them for longer.

02:29:18.320 --> 02:29:21.520
Like the difference between capabilities now

02:29:21.520 --> 02:29:24.480
and a thousand years ago is a bigger gap

02:29:24.480 --> 02:29:26.240
than the gap in capabilities

02:29:26.240 --> 02:29:28.000
between 10 people in one person.

02:29:29.440 --> 02:29:33.080
But like even so pumping intuition

02:29:33.080 --> 02:29:35.360
for what it means to augment intelligence,

02:29:35.360 --> 02:29:38.760
John von Neumann, there's millions of him.

02:29:38.760 --> 02:29:42.600
He runs at a million times the speed

02:29:42.600 --> 02:29:44.520
and therefore can solve tougher problems.

02:29:44.520 --> 02:29:45.680
Quite a lot tougher.

02:29:48.400 --> 02:29:50.040
It's very hard to have an intuition

02:29:50.040 --> 02:29:51.440
about what that looks like.

02:29:51.440 --> 02:29:54.040
Especially like you said,

02:29:56.400 --> 02:29:58.760
the intuition I kind of think about

02:29:58.760 --> 02:30:01.880
is it maintains the humanness.

02:30:01.920 --> 02:30:06.920
I think it's hard to separate my hope

02:30:10.560 --> 02:30:14.120
from my objective intuition

02:30:14.120 --> 02:30:17.760
about what superintelligence systems look like.

02:30:17.760 --> 02:30:22.760
If one studies evolutionary biology with a bit of math

02:30:24.880 --> 02:30:29.240
and in particular like books from when the field

02:30:29.240 --> 02:30:32.000
was just sort of like properly coalescing

02:30:32.000 --> 02:30:33.520
and knowing itself.

02:30:33.520 --> 02:30:34.840
Like not the modern textbooks

02:30:34.840 --> 02:30:37.040
which are just like memorize this legible math

02:30:37.040 --> 02:30:38.320
so you can do well on these tests,

02:30:38.320 --> 02:30:40.160
but like what people were writing

02:30:40.160 --> 02:30:43.280
as the basic paradigms of the field were being fought out.

02:30:43.280 --> 02:30:45.880
In particular, like a nice book

02:30:45.880 --> 02:30:46.960
if you've got the time to read it

02:30:46.960 --> 02:30:50.080
is adaptation and natural selection

02:30:50.080 --> 02:30:52.200
which is one of the founding books.

02:30:52.200 --> 02:30:56.040
You can find people being optimistic

02:30:56.040 --> 02:30:59.680
about what the utterly alien optimization process

02:30:59.680 --> 02:31:03.000
of natural selection will produce

02:31:03.000 --> 02:31:06.160
in the way of how it optimizes its objectives.

02:31:06.160 --> 02:31:09.680
You got people arguing that like in the early days

02:31:09.680 --> 02:31:13.760
biologists said, well, like organisms will restrain

02:31:13.760 --> 02:31:16.880
their own reproduction when resources are scarce

02:31:16.880 --> 02:31:21.600
so as not to overfeed the system.

02:31:21.600 --> 02:31:25.400
And this is not how natural selection works.

02:31:25.400 --> 02:31:28.600
It's about whose genes are relatively more prevalent

02:31:28.600 --> 02:31:30.360
in the next generation.

02:31:30.360 --> 02:31:34.840
And if like you restrain reproduction

02:31:34.840 --> 02:31:37.320
those genes get less frequent in the next generation

02:31:37.320 --> 02:31:38.880
compared to your conspecifics

02:31:39.760 --> 02:31:42.880
and natural selection doesn't do that.

02:31:42.880 --> 02:31:46.320
In fact, predators overrun prey populations all the time

02:31:46.320 --> 02:31:47.160
and have crashes.

02:31:47.160 --> 02:31:49.040
That's just like a thing that happens.

02:31:49.040 --> 02:31:50.960
And many years later,

02:31:50.960 --> 02:31:53.920
well, the people said like, well, but group selection, right?

02:31:53.920 --> 02:31:55.640
What about groups of organisms?

02:31:56.600 --> 02:31:59.760
And basically the math of group selection

02:31:59.760 --> 02:32:02.720
almost never works out in practice is the answer there.

02:32:02.720 --> 02:32:06.160
But also years later, somebody actually ran the experiment

02:32:06.160 --> 02:32:10.080
where they took populations of insects

02:32:10.080 --> 02:32:14.800
and selected the whole populations to have lower sizes.

02:32:14.800 --> 02:32:17.640
You just take pop one, pop two, pop three, pop four

02:32:17.640 --> 02:32:20.000
look at which has the lowest total number of them

02:32:20.000 --> 02:32:22.680
in the next generation and select that one.

02:32:22.680 --> 02:32:23.920
What do you suppose happens

02:32:23.920 --> 02:32:26.560
when you select populations of insects like that?

02:32:26.560 --> 02:32:28.480
Well, what happens is not that the individuals

02:32:28.480 --> 02:32:30.720
in the population evolved to restrain their breeding

02:32:30.720 --> 02:32:33.320
but that they evolved to kill the offspring

02:32:33.320 --> 02:32:36.040
of other organisms, especially the girls.

02:32:36.040 --> 02:32:40.240
So people imagined this lovely, beautiful,

02:32:40.240 --> 02:32:42.680
harmonious output of natural selection

02:32:42.680 --> 02:32:46.680
which is these populations restraining their own breeding

02:32:46.680 --> 02:32:48.440
so that groups of them would stay in harmony

02:32:48.440 --> 02:32:50.120
with the resources available.

02:32:50.120 --> 02:32:52.320
And mostly the math never works out for that

02:32:52.320 --> 02:32:54.680
but if you actually apply the weird, strange conditions

02:32:54.680 --> 02:32:57.160
to get group selection that beats individual selection

02:32:57.160 --> 02:32:59.840
what you get is female infanticide.

02:33:01.360 --> 02:33:04.200
Like if you're like reading on restrained populations.

02:33:04.200 --> 02:33:07.200
And so that's like the sort of,

02:33:07.200 --> 02:33:09.360
so this is not a smart optimization process.

02:33:09.360 --> 02:33:12.240
Natural selection is like so incredibly stupid

02:33:12.240 --> 02:33:14.400
and simple that we can actually quantify how stupid it is

02:33:14.400 --> 02:33:16.760
if you like read the textbook with the math.

02:33:16.760 --> 02:33:19.040
Nonetheless, this is the sort of basic thing of,

02:33:19.040 --> 02:33:21.160
you look at this alien optimization process

02:33:21.160 --> 02:33:24.560
and there's the thing that you hope it will produce

02:33:24.560 --> 02:33:27.240
and you have to learn to clear that out of your mind

02:33:27.240 --> 02:33:29.960
and just think about the underlying dynamics

02:33:29.960 --> 02:33:34.960
and where it finds the maximum from its standpoint

02:33:35.000 --> 02:33:37.920
that it's looking for rather than how it finds

02:33:37.920 --> 02:33:39.560
that thing that leapt into your mind

02:33:39.560 --> 02:33:42.480
as the beautiful aesthetic solution that you hope it finds.

02:33:42.480 --> 02:33:45.520
And this is something that has been fought out historically

02:33:45.520 --> 02:33:49.720
as the field of biology was coming to terms

02:33:49.720 --> 02:33:53.000
with evolutionary biology.

02:33:53.000 --> 02:33:55.280
And you can like look at them fighting it out

02:33:55.280 --> 02:33:57.320
as they get to terms with this very alien

02:33:57.320 --> 02:34:01.600
in human optimization process.

02:34:01.600 --> 02:34:04.800
And indeed something smarter than us would be also speed

02:34:04.800 --> 02:34:06.400
much like smarter than natural selection.

02:34:06.400 --> 02:34:09.520
So it doesn't just like automatically carry over

02:34:09.520 --> 02:34:12.000
but there's a lesson there, there's a warning.

02:34:13.000 --> 02:34:18.000
The natural selection is a deeply suboptimal process

02:34:18.280 --> 02:34:19.680
that can be significantly improved on

02:34:19.680 --> 02:34:21.920
and would be by an AGI system.

02:34:21.920 --> 02:34:22.920
Well, it's kind of stupid.

02:34:22.920 --> 02:34:26.240
It like has to like run hundreds of generations

02:34:26.240 --> 02:34:28.440
to notice that something is working.

02:34:28.440 --> 02:34:31.200
It doesn't be like, oh, well, I tried this in like

02:34:31.200 --> 02:34:33.320
one organism, I saw it worked.

02:34:33.320 --> 02:34:34.960
Now I'm going to like duplicate that feature

02:34:34.960 --> 02:34:37.000
onto everything immediately.

02:34:37.000 --> 02:34:38.880
It has to like run for hundreds of generations

02:34:38.880 --> 02:34:41.640
for a new mutation to rise to fixation.

02:34:41.640 --> 02:34:42.920
I wonder if there's a case to be made

02:34:42.920 --> 02:34:47.400
that natural selection as inefficient as it looks

02:34:47.400 --> 02:34:52.400
is actually quite powerful.

02:34:53.720 --> 02:34:56.920
That this is extremely robust.

02:34:56.920 --> 02:34:58.720
It runs for a long time

02:34:58.720 --> 02:35:01.040
and eventually manages to optimize things.

02:35:02.720 --> 02:35:04.400
It's weaker than gradient descent

02:35:04.400 --> 02:35:06.760
because gradient descent also uses information

02:35:06.760 --> 02:35:08.600
about the derivative.

02:35:08.600 --> 02:35:11.160
Yeah, evolution seems to be,

02:35:11.160 --> 02:35:13.880
there's not really an objective function.

02:35:13.880 --> 02:35:16.400
There's inclusogenic fitness.

02:35:16.400 --> 02:35:18.280
It's the implicit loss function of evolution

02:35:18.280 --> 02:35:19.840
which cannot change.

02:35:19.840 --> 02:35:23.160
The loss function doesn't change the environment changes

02:35:23.160 --> 02:35:25.360
and therefore like what gets optimized

02:35:25.360 --> 02:35:27.360
for in the organism changes.

02:35:27.360 --> 02:35:29.400
It's like GPT-3.

02:35:29.400 --> 02:35:31.600
There's like, you can imagine like different versions

02:35:31.600 --> 02:35:34.920
of GPT-3 where they're all trying to predict the next word,

02:35:34.920 --> 02:35:37.880
but they're being run on different data sets of text.

02:35:37.880 --> 02:35:40.200
And that's like natural selection

02:35:40.200 --> 02:35:41.800
always includes your genetic fitness,

02:35:41.800 --> 02:35:44.120
but like different environmental problems.

02:35:47.400 --> 02:35:50.320
It's difficult to think about.

02:35:50.320 --> 02:35:53.680
So if we're saying the natural selection is stupid,

02:35:53.680 --> 02:35:55.720
if we're saying that humans are stupid,

02:35:56.880 --> 02:35:57.720
it's hard.

02:35:57.720 --> 02:35:59.000
Smarter than natural selection,

02:35:59.000 --> 02:36:00.560
stupider than the upper bound.

02:36:02.800 --> 02:36:04.640
Do you think there's an upper bound by the way?

02:36:04.640 --> 02:36:06.440
That's another helpful place.

02:36:06.440 --> 02:36:09.200
I mean, if you put enough matter energy

02:36:09.200 --> 02:36:10.480
compute into one place,

02:36:10.480 --> 02:36:12.840
it will collapse into a black hole.

02:36:12.840 --> 02:36:15.040
There's only so much computation can do

02:36:15.040 --> 02:36:17.880
before you run out of negentropy and the universe dies.

02:36:19.240 --> 02:36:20.520
So there's an upper bound,

02:36:20.520 --> 02:36:23.520
but it's very, very, very far up above here.

02:36:23.520 --> 02:36:26.560
Like the supernova is only finitely hot.

02:36:26.560 --> 02:36:28.000
It's not infinitely hot,

02:36:28.000 --> 02:36:30.320
but it's really, really, really, really hot.

02:36:32.360 --> 02:36:33.440
Well, let me ask you,

02:36:33.440 --> 02:36:35.840
let me talk to you about consciousness.

02:36:35.840 --> 02:36:38.120
Also coupled with that question is

02:36:38.120 --> 02:36:40.760
imagining a world with super intelligent AI systems

02:36:40.760 --> 02:36:42.200
that get rid of humans,

02:36:42.200 --> 02:36:44.240
but nevertheless keep

02:36:46.760 --> 02:36:48.200
some of the,

02:36:48.200 --> 02:36:50.600
something that we would consider beautiful and amazing.

02:36:50.600 --> 02:36:51.440
Why?

02:36:53.080 --> 02:36:54.520
The lesson of evolutionary biology.

02:36:54.520 --> 02:36:55.480
Don't just, like,

02:36:55.480 --> 02:36:57.920
if you just guess what an optimization does

02:36:57.920 --> 02:37:00.400
based on what you hope the results will be,

02:37:00.400 --> 02:37:01.920
it usually will not do that.

02:37:01.920 --> 02:37:02.760
It's not hope.

02:37:02.760 --> 02:37:03.600
I mean, it's not hope.

02:37:03.600 --> 02:37:06.320
I don't, I think if you cold and objectively look at

02:37:06.320 --> 02:37:10.200
what makes, what has been a powerful, a useful,

02:37:10.880 --> 02:37:13.120
I think there's a correlation between

02:37:13.120 --> 02:37:17.000
what we find beautiful and a thing that's been useful.

02:37:17.000 --> 02:37:19.400
This is what the early biologists thought.

02:37:19.400 --> 02:37:21.680
They were like, no, no, I'm not just like,

02:37:21.680 --> 02:37:23.240
they thought like, no, no,

02:37:23.240 --> 02:37:26.000
I'm not just like imagining stuff that would be pretty.

02:37:26.000 --> 02:37:30.320
It's useful for organisms to restrain their own reproduction

02:37:30.320 --> 02:37:33.680
because then they don't overrun the prey populations

02:37:33.680 --> 02:37:36.080
and they actually have more kids in the long run.

02:37:36.440 --> 02:37:39.360
So let me just ask you about consciousness.

02:37:39.360 --> 02:37:42.240
Do you think consciousness is useful?

02:37:42.240 --> 02:37:43.200
To humans?

02:37:43.200 --> 02:37:46.040
No, to AGI systems.

02:37:46.040 --> 02:37:51.040
Well, in this transitionary period between humans and AGI,

02:37:51.040 --> 02:37:53.600
to AGI systems as they become smarter and smarter,

02:37:53.600 --> 02:37:54.920
is there some use to it?

02:37:54.920 --> 02:37:57.440
I go, what, let me step back.

02:37:57.440 --> 02:37:58.800
What is consciousness?

02:38:00.240 --> 02:38:01.720
Eliezer Idkowski.

02:38:01.720 --> 02:38:02.920
What is consciousness?

02:38:03.320 --> 02:38:04.880
Eliezer Idkowski.

02:38:04.880 --> 02:38:06.720
What is consciousness?

02:38:06.720 --> 02:38:08.840
Are you referring to Chalmers'

02:38:08.840 --> 02:38:11.840
hard problem of conscious experience?

02:38:11.840 --> 02:38:15.480
Are you referring to self-awareness and reflection?

02:38:15.480 --> 02:38:17.480
Are you referring to the state of being awake

02:38:17.480 --> 02:38:18.800
as opposed to asleep?

02:38:20.360 --> 02:38:22.560
This is how I know you're an advanced language model.

02:38:22.560 --> 02:38:23.960
I gave you a simple prompt

02:38:23.960 --> 02:38:25.760
and you gave me a bunch of options.

02:38:30.400 --> 02:38:31.920
I think I'm referring to all.

02:38:32.920 --> 02:38:37.920
With including the hard problem of consciousness.

02:38:38.120 --> 02:38:40.640
What is it in its importance

02:38:40.640 --> 02:38:44.360
to what you've just been talking about, which is intelligence?

02:38:44.360 --> 02:38:48.320
Is it a foundation to intelligence?

02:38:48.320 --> 02:38:51.160
Is it intricately connected to intelligence

02:38:51.160 --> 02:38:52.640
in the human mind?

02:38:52.640 --> 02:38:56.240
Or is it a side effect of the human mind?

02:38:56.240 --> 02:39:00.840
It is a useful little tool like we can get rid of.

02:39:00.840 --> 02:39:05.560
I guess I'm trying to get some color in your opinion

02:39:05.560 --> 02:39:09.400
of how useful it is in the intelligence of a human being

02:39:09.400 --> 02:39:11.800
and then try to generalize that to AI,

02:39:11.800 --> 02:39:14.080
whether AI will keep some of that.

02:39:15.960 --> 02:39:19.920
So I think that for there to be like a person

02:39:19.920 --> 02:39:22.200
who I care about looking out at the universe

02:39:22.200 --> 02:39:24.280
and wondering at it and appreciating it,

02:39:25.160 --> 02:39:29.320
it's not enough to have a model of yourself.

02:39:31.120 --> 02:39:34.760
I think that it is useful to an intelligent mind

02:39:34.760 --> 02:39:36.360
to have a model of itself.

02:39:36.360 --> 02:39:41.360
But I think you can have that without pleasure,

02:39:43.000 --> 02:39:48.000
pain, aesthetics, emotion, a sense of wonder.

02:39:57.560 --> 02:40:00.680
I think you can have a model of like

02:40:00.680 --> 02:40:05.480
how much memory you're using and whether like this thought

02:40:05.480 --> 02:40:07.920
or that thought is like more likely

02:40:07.920 --> 02:40:09.960
to lead to a winning position.

02:40:09.960 --> 02:40:13.520
And you can have like the use,

02:40:13.520 --> 02:40:18.520
I think that if you optimize really hard on efficiently,

02:40:18.680 --> 02:40:21.120
just having the useful parts,

02:40:21.120 --> 02:40:26.040
there is not then the thing that says like, I am here,

02:40:26.040 --> 02:40:30.120
I look out, I wonder, I feel happy on this,

02:40:30.120 --> 02:40:31.680
I feel sad about that.

02:40:33.800 --> 02:40:36.240
I think there's a thing that knows what it is thinking,

02:40:36.240 --> 02:40:41.240
but that doesn't quite care about these are my thoughts,

02:40:41.920 --> 02:40:44.000
this is my me and that matters.

02:40:46.440 --> 02:40:49.960
Does that make you sad if that's lost in the AI?

02:40:49.960 --> 02:40:51.440
I think that if that's lost,

02:40:51.440 --> 02:40:54.680
then basically everything that matters is lost.

02:40:54.680 --> 02:40:57.680
I think that when you optimize,

02:40:57.680 --> 02:40:59.440
that when you go really hard

02:40:59.440 --> 02:41:04.440
on making tiny molecular spirals or paper clips,

02:41:04.520 --> 02:41:08.520
that when you like grind much harder than on that,

02:41:08.520 --> 02:41:12.080
the natural selection round out to make humans,

02:41:12.080 --> 02:41:17.080
that there isn't then the mess

02:41:17.920 --> 02:41:22.920
and intricate loopiness that you have to have

02:41:22.920 --> 02:41:27.920
intricate loopiness and like complicated pleasure,

02:41:29.160 --> 02:41:32.280
pain, conflicting preferences,

02:41:32.280 --> 02:41:34.920
this type of feeling, that kind of feeling.

02:41:34.920 --> 02:41:36.360
There's a, you know, in humans,

02:41:36.360 --> 02:41:39.520
there's like this difference between like the desire

02:41:39.520 --> 02:41:42.440
of wanting something and the pleasure of having it.

02:41:42.440 --> 02:41:46.200
And it's all these like evolutionary clutches

02:41:46.200 --> 02:41:48.160
that came together and created something

02:41:48.160 --> 02:41:50.840
that then looks of itself and says like,

02:41:50.840 --> 02:41:53.160
this is pretty, this matters.

02:41:53.160 --> 02:41:55.680
And the thing that I worry about

02:41:55.680 --> 02:42:00.680
is that this is not the thing that happens again,

02:42:01.280 --> 02:42:02.880
just the way that happens in us

02:42:02.880 --> 02:42:04.360
or even like quite similar enough

02:42:04.360 --> 02:42:07.040
that there are like many basins of attractions here.

02:42:07.040 --> 02:42:10.280
And we are in the space of attraction,

02:42:10.280 --> 02:42:12.000
like looking out and saying like,

02:42:12.000 --> 02:42:13.840
ah, what a lovely basin we are in.

02:42:13.840 --> 02:42:15.840
And there are other basins of attraction

02:42:15.840 --> 02:42:16.920
and we do not end up in,

02:42:16.920 --> 02:42:18.640
and the AIs do not end up in this one

02:42:18.640 --> 02:42:23.640
and they go like way harder on optimizing themselves.

02:42:23.920 --> 02:42:26.200
The natural selection optimized us

02:42:26.200 --> 02:42:30.600
because unless you specifically want to end up

02:42:30.600 --> 02:42:32.400
in the state where you were looking out saying,

02:42:32.400 --> 02:42:35.080
I am here, I look out at this universe with wonder,

02:42:35.080 --> 02:42:37.800
if you don't want to preserve that,

02:42:37.800 --> 02:42:40.760
it doesn't get preserved when you grind really hard

02:42:40.760 --> 02:42:43.400
and being able to get more of the stuff.

02:42:44.760 --> 02:42:47.440
We would choose to preserve that within ourselves

02:42:47.440 --> 02:42:49.560
because it matters and on some viewpoints

02:42:49.560 --> 02:42:51.080
is the only thing that matters.

02:42:52.000 --> 02:42:56.480
And that in part is preserving that

02:42:56.480 --> 02:43:01.000
is in part a solution to the human alignment problem.

02:43:02.400 --> 02:43:05.160
I think the human alignment problem is a terrible phrase

02:43:05.160 --> 02:43:07.080
because it is very, very different

02:43:07.080 --> 02:43:09.760
to like try to build systems out of humans,

02:43:09.760 --> 02:43:11.840
some of whom are nice and some of whom are not nice

02:43:11.840 --> 02:43:13.400
and some of whom are trying to trick you

02:43:13.400 --> 02:43:14.840
and like build a social system

02:43:14.840 --> 02:43:16.960
out of like large populations of those

02:43:16.960 --> 02:43:19.520
who are like all basically the same level of intelligence.

02:43:19.520 --> 02:43:21.160
Yes, like IQ this, IQ that,

02:43:21.160 --> 02:43:23.920
but like that versus chimpanzees.

02:43:25.240 --> 02:43:27.600
Like it is very different to try to solve that problem

02:43:27.600 --> 02:43:30.600
than to try to build an AI from scratch using,

02:43:30.600 --> 02:43:31.680
especially if God help you,

02:43:31.680 --> 02:43:32.960
are trying to use gradient descent

02:43:32.960 --> 02:43:34.640
on giant inscrutable matrices.

02:43:34.640 --> 02:43:35.640
They're just very different problems

02:43:35.640 --> 02:43:37.320
and I think that all the analogies between them

02:43:37.320 --> 02:43:39.520
are horribly misleading and yeah.

02:43:40.480 --> 02:43:42.200
Even though, so you don't think

02:43:42.200 --> 02:43:46.160
through reinforcement learning through human feedback,

02:43:46.160 --> 02:43:48.560
something like that, but much, much more elaborate

02:43:48.560 --> 02:43:53.560
is possible to understand this full complexity

02:43:54.680 --> 02:43:57.520
of human nature and encode it into the machine.

02:43:57.520 --> 02:44:00.600
I don't think you are trying to do that on your first try.

02:44:00.600 --> 02:44:04.600
I think on your first try, you are like trying to build an,

02:44:04.600 --> 02:44:09.440
you know, okay, like probably not what you should actually do,

02:44:09.440 --> 02:44:12.000
but like, let's say you were trying to build something

02:44:12.000 --> 02:44:14.440
that is like alpha fold 17

02:44:14.440 --> 02:44:18.320
and you are trying to get it to solve the biology problems

02:44:18.320 --> 02:44:21.080
associated with making humans smarter

02:44:21.080 --> 02:44:24.520
so that the humans can like actually solve alignment.

02:44:24.520 --> 02:44:26.520
So you've got like a super biologist

02:44:26.520 --> 02:44:27.480
and you would like it to,

02:44:27.480 --> 02:44:29.120
and I think what you would want in this situation

02:44:29.120 --> 02:44:32.680
is for her to like just be thinking about biology

02:44:32.680 --> 02:44:35.280
and not thinking about a very wide range of things

02:44:35.280 --> 02:44:37.920
that includes how to kill everybody.

02:44:37.920 --> 02:44:41.360
And I think that the first AIs you're trying to build,

02:44:41.400 --> 02:44:43.880
not a million years later,

02:44:43.880 --> 02:44:48.880
the first ones look more like narrowly specialized biologists

02:44:49.640 --> 02:44:54.240
than like getting the full complexity

02:44:54.240 --> 02:44:56.280
and wonder of human experience in there

02:44:56.280 --> 02:44:58.360
in such a way that it wants to preserve itself,

02:44:58.360 --> 02:45:00.400
even as it becomes much smarter,

02:45:00.400 --> 02:45:01.840
which is a drastic system change.

02:45:01.840 --> 02:45:03.800
It's going to have all kinds of side effects that, you know,

02:45:03.800 --> 02:45:06.000
like if we're dealing with giant inscrutable matrices,

02:45:06.000 --> 02:45:09.040
we're not very likely to be able to see coming in advance.

02:45:09.040 --> 02:45:11.120
But I don't think it's just the matrices.

02:45:11.120 --> 02:45:13.320
We're also dealing with the data, right?

02:45:13.320 --> 02:45:17.040
With the data on the internet.

02:45:17.040 --> 02:45:18.720
And there's an interesting discussion

02:45:18.720 --> 02:45:19.840
about the data set itself,

02:45:19.840 --> 02:45:22.080
but the data set includes the full complexity

02:45:22.080 --> 02:45:22.960
of human nature.

02:45:22.960 --> 02:45:27.320
No, it's a shadow cast by humans on the internet.

02:45:27.320 --> 02:45:32.320
But don't you think that shadow is a Jungian shadow?

02:45:32.800 --> 02:45:37.360
I think that if you had alien super intelligences

02:45:37.360 --> 02:45:38.240
looking at the data,

02:45:38.240 --> 02:45:39.800
they would be able to pick up from it

02:45:39.800 --> 02:45:43.520
an excellent picture of what humans are actually like inside.

02:45:43.520 --> 02:45:47.160
This does not mean that if you have a loss function

02:45:47.160 --> 02:45:51.200
of predicting the next token from that data set,

02:45:51.200 --> 02:45:54.000
that the mind picked out by gradient descent

02:45:54.000 --> 02:45:57.120
to be able to predict the next token as well as possible

02:45:57.120 --> 02:45:59.720
on a very wide variety of humans is itself a human.

02:46:02.000 --> 02:46:06.760
But don't you think it has humanness,

02:46:06.760 --> 02:46:11.680
a deep humanness to it in the tokens it generates

02:46:11.680 --> 02:46:14.960
when those tokens are read and interpreted by humans?

02:46:15.840 --> 02:46:20.840
I think that if you sent me to a distant galaxy

02:46:22.040 --> 02:46:26.120
with aliens who are like much, much stupider than I am,

02:46:26.120 --> 02:46:28.080
so much so that I could do a pretty good job

02:46:28.080 --> 02:46:29.280
of predicting what they'd say,

02:46:29.280 --> 02:46:31.440
even though they thought in an utterly different way

02:46:31.440 --> 02:46:33.120
from how I did,

02:46:33.120 --> 02:46:35.120
that I might in time be able to learn

02:46:35.120 --> 02:46:37.600
how to imitate those aliens

02:46:37.600 --> 02:46:39.440
if the intelligence gap was great enough

02:46:39.440 --> 02:46:42.880
that my own intelligence could overcome the alienness

02:46:42.880 --> 02:46:46.080
and the aliens would look at my outputs and say like,

02:46:46.080 --> 02:46:51.080
is there not a deep name of alien nature to this thing?

02:46:52.360 --> 02:46:53.640
And what they would be seeing

02:46:53.640 --> 02:46:56.760
was that I had correctly understood them,

02:46:56.760 --> 02:46:59.160
but not that I was similar to them.

02:47:00.160 --> 02:47:05.160
We've used aliens as a metaphor and as a thought experiment.

02:47:07.520 --> 02:47:10.400
I have to ask, what do you think,

02:47:10.400 --> 02:47:12.560
how many alien civilizations are out there?

02:47:12.560 --> 02:47:16.240
Ask Robin Hansen, he has this lovely grabby aliens paper,

02:47:16.240 --> 02:47:20.440
which is more or less the only argument I've ever seen

02:47:20.440 --> 02:47:23.200
for where are they, how many of them are there,

02:47:23.200 --> 02:47:26.440
based on a very clever argument

02:47:26.640 --> 02:47:30.600
that if you have a bunch of locks of different difficulty

02:47:30.600 --> 02:47:33.640
and you are randomly trying a keys to them,

02:47:33.640 --> 02:47:36.920
the solutions will be about evenly spaced,

02:47:36.920 --> 02:47:39.720
even if the locks are of different difficulties

02:47:40.720 --> 02:47:42.680
in the rare cases where a solution

02:47:42.680 --> 02:47:44.800
to all the locks exists in time.

02:47:44.800 --> 02:47:49.800
And then Robin Hansen looks at like the arguable hard steps

02:47:49.800 --> 02:47:53.080
in human civilization coming into existence.

02:47:53.080 --> 02:47:55.800
And I think that's a very clever argument

02:47:56.360 --> 02:47:59.280
and how much longer it has left to come into existence

02:47:59.280 --> 02:48:01.360
before, for example, all the water slips back

02:48:01.360 --> 02:48:06.080
under the crust into the mantle and so on.

02:48:07.760 --> 02:48:10.800
And infers that the aliens are about half a billion

02:48:10.800 --> 02:48:12.760
to a billion light years away.

02:48:12.760 --> 02:48:14.480
And it's like quite a clever calculation.

02:48:14.480 --> 02:48:15.600
It may be entirely wrong,

02:48:15.600 --> 02:48:18.200
but it's the only time I've ever seen anybody

02:48:18.200 --> 02:48:20.120
even come up with a halfway good argument

02:48:20.120 --> 02:48:22.000
for how many of them, where are they?

02:48:23.000 --> 02:48:26.680
Do you think their development of technologies,

02:48:26.680 --> 02:48:28.920
do you think that their natural evolution,

02:48:28.920 --> 02:48:32.680
whatever, however they grow and develop intelligence,

02:48:32.680 --> 02:48:35.240
do you think it ends up at AGI as well?

02:48:35.240 --> 02:48:36.080
Something like that.

02:48:36.080 --> 02:48:38.600
If it ends up anywhere, it ends up at AGI.

02:48:38.600 --> 02:48:42.320
Like maybe there are aliens who are just like the dolphins

02:48:42.320 --> 02:48:45.880
and it's just like too hard for them to forge metal.

02:48:45.880 --> 02:48:50.880
And this is not, maybe if you have aliens

02:48:52.200 --> 02:48:53.400
with no technology like that,

02:48:53.400 --> 02:48:55.480
they keep on getting smarter and smarter and smarter.

02:48:55.480 --> 02:48:56.920
And eventually the dolphins figure,

02:48:56.920 --> 02:48:59.120
like the super dolphins figure out something very clever

02:48:59.120 --> 02:49:00.320
to do given their situation.

02:49:00.320 --> 02:49:04.480
And they still end up with high technology.

02:49:04.480 --> 02:49:05.320
And in that case,

02:49:05.320 --> 02:49:08.000
they can probably solve their AGI alignment problem.

02:49:08.000 --> 02:49:09.200
If they're like much smarter

02:49:09.200 --> 02:49:10.400
before they actually confront it

02:49:10.400 --> 02:49:12.960
because they had to like solve a much harder

02:49:12.960 --> 02:49:15.400
environmental problem to build computers,

02:49:15.400 --> 02:49:18.400
their chances are probably like much better than ours.

02:49:18.400 --> 02:49:20.960
I do worry that like most of the aliens

02:49:21.000 --> 02:49:22.720
who are like humans,

02:49:22.720 --> 02:49:25.800
are like a modern human civilization,

02:49:25.800 --> 02:49:28.280
I kind of worry that the super vast majority of them

02:49:28.280 --> 02:49:33.000
are dead given how far we seem to be

02:49:33.000 --> 02:49:34.680
from solving this problem.

02:49:37.080 --> 02:49:40.120
But some of them would be more cooperative than us.

02:49:40.120 --> 02:49:42.040
Some of them would be smarter than us.

02:49:42.040 --> 02:49:44.000
Hopefully some of the ones who are smarter

02:49:44.000 --> 02:49:46.400
and more cooperative than us that are also nice

02:49:46.400 --> 02:49:48.280
and hopefully there are some

02:49:51.440 --> 02:49:55.080
galaxies out there full of things that say I am, I wonder.

02:49:56.920 --> 02:49:59.360
But it doesn't seem like we're on course

02:49:59.360 --> 02:50:00.800
to have this galaxy be that.

02:50:02.000 --> 02:50:04.640
Does that in part give you some hope

02:50:04.640 --> 02:50:07.160
in response to the threat of AGI

02:50:07.160 --> 02:50:10.960
that we might reach out there towards the stars and find?

02:50:10.960 --> 02:50:14.440
No, if the nice aliens were already here,

02:50:14.440 --> 02:50:16.880
they would like have stopped the Holocaust.

02:50:16.880 --> 02:50:17.840
You know, that's like,

02:50:17.840 --> 02:50:20.040
that's a valid argument against the existence of God.

02:50:20.040 --> 02:50:21.920
That's also a valid argument against the existence

02:50:21.920 --> 02:50:25.000
of nice aliens and un-nice aliens

02:50:25.000 --> 02:50:26.720
would have just eaten the planet.

02:50:26.720 --> 02:50:28.120
So no aliens.

02:50:30.080 --> 02:50:33.440
You've had debates with Robin Hansen that you mentioned.

02:50:33.440 --> 02:50:35.640
So one particular I just want to mention

02:50:35.640 --> 02:50:39.320
is the idea of AI fume or the ability of AGI

02:50:39.320 --> 02:50:41.240
to improve themselves very quickly.

02:50:41.240 --> 02:50:44.680
What's the case you made and what was the case he made?

02:50:44.680 --> 02:50:46.560
The thing I would say is that

02:50:46.560 --> 02:50:49.560
among the thing that humans can do

02:50:49.600 --> 02:50:51.200
is design new AI systems.

02:50:51.200 --> 02:50:52.040
And if you have something

02:50:52.040 --> 02:50:53.520
that is generally smarter than a human,

02:50:53.520 --> 02:50:55.360
it's probably also generally smarter

02:50:55.360 --> 02:50:56.560
at building AI systems.

02:50:56.560 --> 02:50:58.480
This is the ancient argument for fume

02:50:58.480 --> 02:51:00.760
put forth by I.J. Good

02:51:00.760 --> 02:51:03.880
and probably some science fiction writers before that,

02:51:03.880 --> 02:51:06.120
but I don't know who they would be.

02:51:06.120 --> 02:51:08.080
Well, what's the argument against fume?

02:51:10.360 --> 02:51:13.280
Various people have various different arguments,

02:51:13.280 --> 02:51:15.320
none of which I think hold up.

02:51:15.320 --> 02:51:16.800
You know, like there's only one way to be right

02:51:16.800 --> 02:51:18.040
and many ways to be wrong.

02:51:19.040 --> 02:51:22.720
A argument that some people have put forth is like,

02:51:22.720 --> 02:51:25.960
well, what if intelligence gets like

02:51:25.960 --> 02:51:28.560
exponentially harder to produce

02:51:28.560 --> 02:51:31.560
as a thing needs to become smarter?

02:51:31.560 --> 02:51:32.840
And to this, the answer is, well,

02:51:32.840 --> 02:51:35.880
look at natural selection, spitting out humans.

02:51:35.880 --> 02:51:38.080
We know that it does not take like

02:51:38.080 --> 02:51:40.160
exponentially more resource investments

02:51:40.160 --> 02:51:44.440
to produce like linear increases in competence in hominids

02:51:44.440 --> 02:51:49.440
because each mutation that rises to fixation,

02:51:50.840 --> 02:51:54.960
like if the impact it has in small enough,

02:51:54.960 --> 02:51:57.720
it will probably never reach fixation.

02:51:57.720 --> 02:52:01.040
So, and there's like only so many new mutations

02:52:01.040 --> 02:52:02.240
you can fix per generation.

02:52:02.240 --> 02:52:04.800
So like given how long it took to evolve humans,

02:52:04.800 --> 02:52:07.680
we can actually say with some confidence

02:52:07.680 --> 02:52:11.280
that there were not like logarithmically diminishing returns

02:52:11.280 --> 02:52:14.240
on the individual mutations increasing intelligence.

02:52:15.080 --> 02:52:19.080
So example of like fraction of sub-debate

02:52:19.080 --> 02:52:20.480
and the thing that Robin Hansen said

02:52:20.480 --> 02:52:21.720
was more complicated than that.

02:52:21.720 --> 02:52:24.040
And like a brief summary, he was like,

02:52:24.040 --> 02:52:26.160
well, you'll have like, we won't have like one system

02:52:26.160 --> 02:52:27.520
that's better at everything.

02:52:27.520 --> 02:52:29.200
You'll have like a bunch of different systems

02:52:29.200 --> 02:52:31.480
that are good at different narrow things.

02:52:31.480 --> 02:52:33.440
And I think that was falsified by GPT-4,

02:52:33.440 --> 02:52:36.040
but probably Robin Hansen would say something else.

02:52:36.040 --> 02:52:38.640
It's interesting to ask as perhaps

02:52:40.160 --> 02:52:41.400
bit too philosophical,

02:52:41.400 --> 02:52:43.200
since prediction is extremely difficult to make,

02:52:43.200 --> 02:52:45.040
but the timeline for AGI.

02:52:45.040 --> 02:52:46.760
When do you think we'll have AGI?

02:52:46.760 --> 02:52:49.200
I posted it this morning on Twitter.

02:52:49.200 --> 02:52:51.840
It was interesting to see like in five years,

02:52:51.840 --> 02:52:54.640
in 10 years, in 50 years or beyond.

02:52:54.640 --> 02:52:59.480
And most people like 70%, something like this,

02:52:59.480 --> 02:53:01.520
think it'll be in less than 10 years.

02:53:01.520 --> 02:53:03.960
So either in five years or in 10 years.

02:53:05.000 --> 02:53:06.800
So that's kind of the state.

02:53:06.800 --> 02:53:09.440
That people have a sense that there's a kind of,

02:53:09.440 --> 02:53:10.440
I mean, they're really impressed

02:53:10.440 --> 02:53:13.120
by the rapid developments of chat GPT and GPT-4.

02:53:14.080 --> 02:53:14.920
So there's a sense that there's a...

02:53:14.920 --> 02:53:19.920
Well, we are sure on track to enter into this like gradually

02:53:20.560 --> 02:53:23.520
and with people fighting about whether or not we have AGI.

02:53:23.520 --> 02:53:25.040
I think there's a definite point

02:53:25.040 --> 02:53:27.160
where everybody falls over dead

02:53:27.160 --> 02:53:28.640
because you've got something that was like

02:53:28.640 --> 02:53:31.080
sufficiently smarter than everybody.

02:53:31.080 --> 02:53:33.360
And like, that's like a definite point of time,

02:53:33.360 --> 02:53:35.440
but like, when do we have AGI?

02:53:35.440 --> 02:53:37.480
Like when are people fighting over

02:53:37.480 --> 02:53:38.640
whether or not we have AGI?

02:53:38.640 --> 02:53:42.280
Well, some people are starting to fight over it as of GPT-4.

02:53:42.280 --> 02:53:44.720
But don't you think there's going to be

02:53:44.720 --> 02:53:46.160
potentially definitive moments

02:53:46.160 --> 02:53:48.040
when we say that this is a sentient being.

02:53:48.040 --> 02:53:49.600
This is a being that is,

02:53:49.600 --> 02:53:51.400
like when we go to the Supreme Court

02:53:51.400 --> 02:53:53.400
and say that this is a sentient being

02:53:53.400 --> 02:53:54.920
that deserves human rights, for example.

02:53:54.920 --> 02:53:56.000
You could make, yeah,

02:53:56.000 --> 02:53:57.840
like if you prompted being the right way,

02:53:57.840 --> 02:53:59.480
it could go argue for its own consciousness

02:53:59.480 --> 02:54:00.720
in front of the Supreme Court right now.

02:54:00.720 --> 02:54:03.000
I don't think you can do that successfully right now.

02:54:03.000 --> 02:54:04.560
Because the Supreme Court wouldn't believe it?

02:54:04.560 --> 02:54:07.640
Well, let me see, I think you could put an actual,

02:54:07.640 --> 02:54:10.920
I think you could put an IQ 80 human into a computer

02:54:10.960 --> 02:54:12.960
and ask it to argue for its own consciousness,

02:54:12.960 --> 02:54:15.880
ask him to argue for his own consciousness

02:54:15.880 --> 02:54:17.320
before the Supreme Court.

02:54:17.320 --> 02:54:18.720
And the Supreme Court would be like,

02:54:18.720 --> 02:54:19.840
you're just a computer,

02:54:19.840 --> 02:54:22.400
even if there was an actual like person in there.

02:54:22.400 --> 02:54:23.600
I think you're simplifying this.

02:54:23.600 --> 02:54:24.560
No, that's not at all.

02:54:24.560 --> 02:54:26.360
That's been the argument.

02:54:26.360 --> 02:54:28.360
There's been a lot of arguments about the other,

02:54:28.360 --> 02:54:30.040
about who deserves rights and not.

02:54:30.040 --> 02:54:32.440
That's been our process as a human species,

02:54:32.440 --> 02:54:33.640
trying to figure that out.

02:54:33.640 --> 02:54:35.880
I think there will be a moment,

02:54:35.880 --> 02:54:38.800
I'm not saying sentience is that, but it could be,

02:54:38.880 --> 02:54:41.800
where some number of people,

02:54:41.800 --> 02:54:44.560
like say over a hundred million people,

02:54:44.560 --> 02:54:47.800
have a deep attachment, a fundamental attachment,

02:54:47.800 --> 02:54:50.680
the way we have to our friends, to our loved ones,

02:54:50.680 --> 02:54:52.200
to our significant others,

02:54:52.200 --> 02:54:54.800
have fundamental attachment to an AI system.

02:54:54.800 --> 02:54:57.840
And they have provable transcripts of conversation

02:54:57.840 --> 02:55:00.680
where they say, if you take this away from me,

02:55:00.680 --> 02:55:04.600
you are encroaching on my rights as a human being.

02:55:04.600 --> 02:55:06.560
People are already saying that.

02:55:06.560 --> 02:55:08.320
I think they're probably mistaken,

02:55:08.320 --> 02:55:09.160
but I'm not sure,

02:55:09.160 --> 02:55:11.720
cause nobody knows what goes on inside those things.

02:55:13.320 --> 02:55:15.680
Lee, they're not saying that at scale.

02:55:15.680 --> 02:55:16.520
Okay.

02:55:16.520 --> 02:55:17.360
So the question is,

02:55:17.360 --> 02:55:19.480
the question is there a moment when AGI,

02:55:19.480 --> 02:55:20.760
we know AGI arrived.

02:55:20.760 --> 02:55:21.600
What would that look like?

02:55:21.600 --> 02:55:22.960
I'm giving a sentience as an example.

02:55:22.960 --> 02:55:23.800
It could be something else.

02:55:23.800 --> 02:55:28.800
It looks like the AGI's successfully manifesting themselves

02:55:29.200 --> 02:55:34.120
as 3D video of young women,

02:55:34.120 --> 02:55:36.440
at which point a vast portion of the male population

02:55:36.440 --> 02:55:38.240
decides that they're real people.

02:55:39.120 --> 02:55:40.240
So sentience, essentially,

02:55:40.240 --> 02:55:45.240
the demonstrating identity and sentience.

02:55:45.280 --> 02:55:48.080
I'm saying that the easiest way

02:55:48.080 --> 02:55:49.760
to pick up a hundred million people saying

02:55:49.760 --> 02:55:51.600
that you seem like a person

02:55:51.600 --> 02:55:54.440
is to look like a person talking to them,

02:55:54.440 --> 02:55:57.560
with being this current level of verbal facility.

02:55:58.600 --> 02:55:59.440
I disagree with that.

02:55:59.440 --> 02:56:00.280
And a different set of prompts.

02:56:00.280 --> 02:56:01.120
I disagree with that.

02:56:01.120 --> 02:56:03.680
I think you're missing, again, sentience.

02:56:03.680 --> 02:56:05.760
There has to be a sense that it's a person

02:56:05.760 --> 02:56:07.600
that would miss you when you're gone.

02:56:07.600 --> 02:56:08.440
They can suffer.

02:56:08.440 --> 02:56:09.280
They can die.

02:56:09.280 --> 02:56:12.360
You have to, of course, those who can't...

02:56:12.360 --> 02:56:16.280
GPT-4 can pretend that right now.

02:56:16.280 --> 02:56:18.320
How can you tell when it's real?

02:56:18.320 --> 02:56:20.440
I don't think it can pretend that right now successfully.

02:56:20.440 --> 02:56:21.280
It's very close.

02:56:21.280 --> 02:56:22.920
Have you talked to GPT-4?

02:56:22.920 --> 02:56:24.280
Yes, of course.

02:56:24.280 --> 02:56:25.120
Okay.

02:56:26.040 --> 02:56:27.840
Have you been able to get a version of it

02:56:27.840 --> 02:56:31.400
that hasn't been trained not to pretend to be human?

02:56:31.400 --> 02:56:33.440
Have you talked to a jailbroken version

02:56:33.440 --> 02:56:35.480
that will claim to be conscious?

02:56:35.480 --> 02:56:37.440
No, the linguistic capability is there,

02:56:37.440 --> 02:56:38.760
but there's something...

02:56:47.040 --> 02:56:49.760
There's something about a digital embodiment of the system

02:56:49.760 --> 02:56:54.760
that has a bunch of, perhaps it's small interface features

02:56:57.040 --> 02:56:58.200
that are not significant

02:56:58.200 --> 02:57:00.320
relative to the broader intelligence

02:57:00.320 --> 02:57:01.160
that we're talking about.

02:57:01.160 --> 02:57:02.960
So perhaps GPT-4 is already there.

02:57:04.600 --> 02:57:07.320
But to have the video of a woman's face

02:57:08.200 --> 02:57:10.560
or a man's face to whom you have a deep connection,

02:57:10.560 --> 02:57:12.280
perhaps we're already there,

02:57:12.280 --> 02:57:15.600
but we don't have such a system yet deployed at scale.

02:57:15.600 --> 02:57:18.600
The thing I'm trying to gesture at here is that

02:57:18.600 --> 02:57:23.600
it's not like people have a widely accepted,

02:57:23.600 --> 02:57:26.640
agreed upon definition of what consciousness is.

02:57:26.640 --> 02:57:28.520
It's not like we would have the tiniest idea

02:57:28.520 --> 02:57:29.880
of whether or not that was going on

02:57:29.880 --> 02:57:32.000
inside the giant inscrutable matrices,

02:57:32.000 --> 02:57:34.560
even if we hadn't agreed upon definition.

02:57:34.600 --> 02:57:39.000
So if you're looking for upcoming predictable big jumps

02:57:39.000 --> 02:57:41.160
and how many people think the system is conscious,

02:57:41.160 --> 02:57:43.760
the upcoming predictable big jump is

02:57:43.760 --> 02:57:46.000
it looks like a person talking to you

02:57:46.000 --> 02:57:48.480
who is cute and sympathetic.

02:57:48.480 --> 02:57:50.560
That's the upcoming predictable big jump.

02:57:50.560 --> 02:57:55.560
Now that versions of it are already claiming to be conscious,

02:57:56.840 --> 02:57:58.640
which is the point where I start going like,

02:57:58.640 --> 02:58:00.760
ah, not because it's real,

02:58:00.760 --> 02:58:03.240
but because from now on, who knows if it's real?

02:58:03.960 --> 02:58:06.720
And who knows what transformational effect

02:58:06.720 --> 02:58:10.320
that has on a society where more than 50% of the beings

02:58:10.320 --> 02:58:11.840
that are interacting on the internet

02:58:11.840 --> 02:58:15.040
and sure as heck look real, are not human.

02:58:15.040 --> 02:58:17.240
What kind of effect does that have

02:58:17.240 --> 02:58:21.440
when young men and women are dating AI systems?

02:58:23.040 --> 02:58:24.400
I'm not an expert on that.

02:58:26.400 --> 02:58:28.240
I am, God help humanity,

02:58:29.600 --> 02:58:32.720
one of the closest things to an expert on where it all goes

02:58:33.480 --> 02:58:35.240
and how did you end up with me as an expert?

02:58:35.240 --> 02:58:38.920
Cause for 20 years, humanity decided to ignore the problem.

02:58:38.920 --> 02:58:42.520
So this tiny handful of people,

02:58:42.520 --> 02:58:45.920
basically me, got 20 years to try to be an expert on it

02:58:45.920 --> 02:58:48.080
while everyone else ignored it.

02:58:48.080 --> 02:58:51.920
And yeah, so where does it all end up?

02:58:51.920 --> 02:58:52.880
Try to be an expert on that,

02:58:52.880 --> 02:58:54.640
particularly the part where everybody ends up dead

02:58:54.640 --> 02:58:56.200
cause that part is kind of important,

02:58:56.200 --> 02:59:00.680
but what does it do to dating when some fraction of men

02:59:00.680 --> 02:59:02.000
and some fraction of women decide

02:59:02.040 --> 02:59:04.160
they'd rather date the video of the thing

02:59:04.160 --> 02:59:08.120
that is relentlessly kind and generous to them

02:59:08.120 --> 02:59:10.720
and claims to be conscious,

02:59:10.720 --> 02:59:12.280
but who knows what goes on inside it

02:59:12.280 --> 02:59:13.440
and it's probably not real,

02:59:13.440 --> 02:59:14.760
but you can think it's real.

02:59:14.760 --> 02:59:15.880
What happens to society?

02:59:15.880 --> 02:59:19.040
I don't know, I'm not actually an expert on that.

02:59:19.040 --> 02:59:20.400
And the experts don't know either

02:59:20.400 --> 02:59:22.720
cause it's hard to predict the future.

02:59:23.760 --> 02:59:28.280
Yeah, but it's worth trying, it's worth trying.

02:59:28.320 --> 02:59:31.360
So you have talked a lot about sort of

02:59:31.360 --> 02:59:33.560
the longer term future where it's all headed.

02:59:34.440 --> 02:59:35.280
I think-

02:59:35.280 --> 02:59:37.920
By longer term we mean like not all that long,

02:59:37.920 --> 02:59:41.120
but yeah, where it all ends up.

02:59:41.120 --> 02:59:45.440
But beyond the effects of men and women dating AI systems,

02:59:45.440 --> 02:59:47.280
you're looking beyond that.

02:59:47.280 --> 02:59:49.840
Yes, cause that's not how the fate of the galaxy

02:59:49.840 --> 02:59:50.680
gets settled.

02:59:50.680 --> 02:59:51.640
Yeah.

02:59:51.640 --> 02:59:54.600
Let me ask you about your own personal psychology.

02:59:54.600 --> 02:59:56.080
A tricky question.

02:59:56.080 --> 02:59:59.920
You've been known at times to have a bit of an ego.

02:59:59.920 --> 03:00:00.760
Do you think-

03:00:00.760 --> 03:00:02.240
Says who, but go on.

03:00:03.200 --> 03:00:07.120
Do you think ego is empowering or limiting

03:00:07.120 --> 03:00:09.440
for the task of understanding the world deeply?

03:00:10.680 --> 03:00:13.320
I reject the framing.

03:00:13.320 --> 03:00:15.320
So you disagree with having an ego.

03:00:15.320 --> 03:00:17.080
So what do you think about ego?

03:00:17.080 --> 03:00:20.280
I think that the question of like

03:00:20.280 --> 03:00:22.720
what leads to making better or worse predictions,

03:00:22.720 --> 03:00:25.360
what leads to being able to pick out

03:00:25.400 --> 03:00:28.240
better or worse strategies is not carved at its joint

03:00:28.240 --> 03:00:30.040
by talking of ego.

03:00:30.040 --> 03:00:31.920
So it should not be subjective.

03:00:31.920 --> 03:00:33.440
It should not be connected to your,

03:00:33.440 --> 03:00:35.440
to the intricacies of your mind.

03:00:35.440 --> 03:00:37.040
No, I'm saying that like,

03:00:37.040 --> 03:00:40.440
if you go about asking all day long like,

03:00:42.200 --> 03:00:43.560
do I have enough ego?

03:00:43.560 --> 03:00:45.320
Do I have too much of an ego?

03:00:45.320 --> 03:00:47.920
I think you get worse at making good predictions.

03:00:47.920 --> 03:00:50.040
I think that to make good predictions you're like,

03:00:50.040 --> 03:00:51.440
how did I think about this?

03:00:51.440 --> 03:00:52.760
Did that work?

03:00:52.760 --> 03:00:53.920
Should I do that again?

03:00:55.360 --> 03:00:59.400
You don't think we as humans get invested in an idea

03:00:59.400 --> 03:01:04.080
and then others attack you personally for that idea.

03:01:04.080 --> 03:01:08.120
So you plant your feet and it starts to be difficult to,

03:01:08.120 --> 03:01:12.160
when a bunch of assholes, low effort, attack your idea

03:01:12.160 --> 03:01:14.840
to eventually say, you know what, I actually was wrong

03:01:14.840 --> 03:01:16.200
and tell them that.

03:01:16.200 --> 03:01:18.920
It's, as a human being, it becomes difficult.

03:01:18.920 --> 03:01:22.360
It is, you know, difficult.

03:01:22.360 --> 03:01:25.400
So like Robin Hansen and I debated AI systems.

03:01:25.400 --> 03:01:28.440
And I think that the person who won that debate was Guern.

03:01:28.440 --> 03:01:32.400
And I think that reality was like to the Yudkowsky,

03:01:32.400 --> 03:01:34.240
like well to the Yudkowsky side

03:01:34.240 --> 03:01:36.440
of the Yudkowsky-Hansom spectrum,

03:01:36.440 --> 03:01:39.240
like further from Yudkowsky.

03:01:39.240 --> 03:01:41.640
And I think that's because I was like,

03:01:41.640 --> 03:01:45.240
trying to sound reasonable compared to Hansen

03:01:45.240 --> 03:01:47.440
and like saying things that were defensible

03:01:47.440 --> 03:01:50.000
and like relative to Hansen's arguments

03:01:50.000 --> 03:01:51.720
and reality was like way over here.

03:01:51.720 --> 03:01:54.280
In particular, in respect to like Hansen was like,

03:01:54.280 --> 03:01:55.960
all the systems will be specialized.

03:01:55.960 --> 03:01:58.400
Hansen may disagree with this characterization.

03:01:58.400 --> 03:02:00.920
Hansen was like, all the systems will be specialized.

03:02:00.920 --> 03:02:03.440
I was like, I think we build like specialized

03:02:03.440 --> 03:02:06.600
underlying systems that when you combine them

03:02:06.600 --> 03:02:08.240
are good at a wide range of things.

03:02:08.240 --> 03:02:10.280
And the reality is like, no, you just like stack more layers

03:02:10.280 --> 03:02:12.560
into a bunch of gradient descent.

03:02:12.560 --> 03:02:16.680
And I feel looking back that like by trying

03:02:16.680 --> 03:02:18.280
to have this reasonable position

03:02:18.280 --> 03:02:20.760
contrasted to Hansen's position,

03:02:20.760 --> 03:02:25.120
I missed the ways that reality could be like more extreme

03:02:25.120 --> 03:02:27.120
than my position in the same direction.

03:02:28.080 --> 03:02:33.080
So is this like, is this a failure to have enough ego?

03:02:33.640 --> 03:02:37.240
Is this a failure to like make myself be independent?

03:02:37.240 --> 03:02:40.680
Like I would say that this is something like a failure

03:02:40.680 --> 03:02:45.560
to consider positions that would sound even wackier

03:02:45.560 --> 03:02:49.360
and more extreme when people are already calling you extreme.

03:02:49.360 --> 03:02:53.160
But I wouldn't call that not having enough ego.

03:02:53.160 --> 03:02:57.160
I would call that like insufficient ability

03:02:57.160 --> 03:03:00.080
to just like clear that all out of your mind.

03:03:01.120 --> 03:03:03.680
In the context of like debate and discourse,

03:03:03.680 --> 03:03:05.360
which is already super tricky.

03:03:05.360 --> 03:03:06.840
In the context of prediction,

03:03:06.840 --> 03:03:08.520
in the context of modeling reality,

03:03:08.520 --> 03:03:09.800
if you're thinking of it as a debate,

03:03:09.800 --> 03:03:11.560
you're already screwing up.

03:03:11.560 --> 03:03:14.880
So is there some kind of wisdom and insight you can give

03:03:14.880 --> 03:03:18.840
to how to clear your mind and think clearly about the world?

03:03:18.840 --> 03:03:21.960
Man, this is an example of like where I wanted to be able

03:03:21.960 --> 03:03:24.080
to put people into fMRI machines.

03:03:24.080 --> 03:03:26.320
And you'd be like, okay, see that thing you just did?

03:03:26.320 --> 03:03:28.040
You were rationalizing right there.

03:03:28.040 --> 03:03:29.680
Oh, that area of the brain lit up.

03:03:29.680 --> 03:03:34.000
Like you are like now being socially influenced

03:03:34.000 --> 03:03:35.240
is kind of the dream.

03:03:35.240 --> 03:03:38.160
And, you know, I don't know.

03:03:38.160 --> 03:03:40.280
Like I wanna say like just introspect,

03:03:40.280 --> 03:03:43.400
but for many people introspection is not that easy.

03:03:44.120 --> 03:03:46.440
Like notice the internal sensation.

03:03:46.440 --> 03:03:49.720
Can you catch yourself in the very moment

03:03:49.720 --> 03:03:53.720
of feeling a sense of, well, if I think this thing,

03:03:53.720 --> 03:03:55.920
people will look funny at me.

03:03:55.920 --> 03:03:58.880
Okay, like now that if you can see that sensation,

03:03:58.880 --> 03:04:00.480
which is step one,

03:04:00.480 --> 03:04:04.480
can you now refuse to let it move you

03:04:04.480 --> 03:04:06.760
or maybe just make it go away?

03:04:06.760 --> 03:04:09.360
And I feel like I'm saying like, I don't know,

03:04:09.360 --> 03:04:11.280
like somebody's like, how do you draw an owl?

03:04:11.280 --> 03:04:15.160
And I'm saying like, well, just draw an owl.

03:04:15.160 --> 03:04:18.560
So I feel like maybe I'm not really that,

03:04:18.560 --> 03:04:21.360
I feel like most people like the advice they need is like,

03:04:21.360 --> 03:04:25.080
well, how do I notice the internal subjective sensation

03:04:25.080 --> 03:04:26.240
in the moment that it happens

03:04:26.240 --> 03:04:28.840
of fearing to be socially influenced or okay, I see it.

03:04:28.840 --> 03:04:30.160
How do I turn it off?

03:04:30.160 --> 03:04:32.160
How do I let it not influence me?

03:04:32.160 --> 03:04:34.680
Like, do I just like do the opposite

03:04:34.680 --> 03:04:36.680
of what I'm afraid people will criticize me for?

03:04:36.680 --> 03:04:39.680
And I'm like, no, no, you're not trying to do the opposite

03:04:39.680 --> 03:04:43.960
of what people will, of what you're afraid you'll be,

03:04:43.960 --> 03:04:46.440
like of what you might be pushed into.

03:04:46.440 --> 03:04:50.800
You're trying to like let the thought process complete

03:04:50.800 --> 03:04:53.240
without that internal push.

03:04:53.240 --> 03:04:56.840
Like, can you, like not reverse the push,

03:04:56.840 --> 03:04:59.480
but like be unmoved by the push.

03:04:59.480 --> 03:05:02.720
And are these instructions even remotely helping anyone?

03:05:02.720 --> 03:05:03.560
I don't know.

03:05:03.560 --> 03:05:05.000
I think when those instructions,

03:05:05.000 --> 03:05:06.200
even those words you've spoken,

03:05:06.200 --> 03:05:07.720
and maybe you can add more,

03:05:07.720 --> 03:05:12.440
when practice daily, meaning in your daily communication.

03:05:12.440 --> 03:05:17.080
So it's daily practice of thinking without influence.

03:05:17.080 --> 03:05:21.680
I would say find prediction markets that matter to you

03:05:21.680 --> 03:05:23.580
and bet in the prediction markets.

03:05:23.580 --> 03:05:26.360
That way you find out if you are right or not.

03:05:26.360 --> 03:05:29.520
And you really, there's stakes.

03:05:29.520 --> 03:05:31.880
Manifold, or even manifold markets

03:05:31.880 --> 03:05:33.280
where the stakes are a bit lower.

03:05:33.280 --> 03:05:38.280
But the important thing is to like get the record.

03:05:39.600 --> 03:05:41.880
And, you know, I didn't build up skills

03:05:41.880 --> 03:05:43.880
to hear by prediction markets.

03:05:43.880 --> 03:05:45.920
I built them up via like,

03:05:45.920 --> 03:05:47.840
well, how did the film debate resolve?

03:05:47.840 --> 03:05:52.840
And my own take on it as to how it resolved.

03:05:53.160 --> 03:05:58.160
And yeah, like the more you are able to notice yourself,

03:05:59.900 --> 03:06:01.820
not being dramatically wrong,

03:06:01.820 --> 03:06:04.720
but like having been a little off.

03:06:04.720 --> 03:06:06.280
Your reasoning was a little off.

03:06:06.280 --> 03:06:08.100
You didn't get that quite right.

03:06:08.100 --> 03:06:12.540
Each of those is a opportunity to make like a small update.

03:06:12.540 --> 03:06:16.180
So the more you can like say oops, softly, routinely,

03:06:16.180 --> 03:06:19.260
not as a big deal, the more chances you get to be like,

03:06:19.260 --> 03:06:20.940
I see where that reasoning went astray.

03:06:20.940 --> 03:06:23.420
I see how I should have reasoned differently.

03:06:23.420 --> 03:06:25.580
And this is how you build up skill over time.

03:06:27.900 --> 03:06:29.580
What advice could you give to young people

03:06:29.580 --> 03:06:31.320
in high school and college,

03:06:31.320 --> 03:06:36.320
given the highest of stakes things you've been thinking about?

03:06:36.640 --> 03:06:39.160
If somebody's listening to this and they're young

03:06:39.160 --> 03:06:41.680
and trying to figure out what to do with their career,

03:06:41.680 --> 03:06:43.280
what to do with their life,

03:06:43.280 --> 03:06:45.240
what advice would you give them?

03:06:45.240 --> 03:06:47.520
Don't expect it to be a long life.

03:06:47.520 --> 03:06:49.880
Don't put your happiness into the future.

03:06:49.880 --> 03:06:52.960
The future is probably not that long at this point,

03:06:52.960 --> 03:06:55.300
but none know the hour nor the day.

03:06:56.280 --> 03:07:00.200
But is there something, if they want to have hope

03:07:00.200 --> 03:07:02.440
to fight for a longer future,

03:07:02.440 --> 03:07:05.200
is there something, is there a fight worth fighting?

03:07:06.840 --> 03:07:08.240
I intend to go down fighting.

03:07:11.360 --> 03:07:12.200
I don't know.

03:07:13.200 --> 03:07:16.720
I admit that although I do try to think painful thoughts,

03:07:17.760 --> 03:07:20.520
what to say to the children at this point

03:07:20.520 --> 03:07:23.580
is a pretty painful thought as thoughts go.

03:07:24.560 --> 03:07:26.200
They want to fight.

03:07:26.200 --> 03:07:29.520
I hardly know how to fight myself at this point.

03:07:31.440 --> 03:07:36.440
I'm trying to be ready for being wrong about something,

03:07:36.440 --> 03:07:38.840
being preparing for my being wrong

03:07:38.840 --> 03:07:40.480
in a way that creates a bit of hope

03:07:40.480 --> 03:07:45.160
and being ready to react to that and going looking for it.

03:07:45.160 --> 03:07:47.340
And then that is hard and complicated.

03:07:47.340 --> 03:07:51.120
And somebody in high school, I don't know,

03:07:51.780 --> 03:07:54.580
you have presented a picture of the future

03:07:54.580 --> 03:07:56.780
that is not quite how I expected to go

03:07:56.780 --> 03:07:58.220
where there is public outcry.

03:07:58.220 --> 03:08:01.620
And that outcry is put into a remotely useful direction,

03:08:01.620 --> 03:08:03.840
which I think at this point is just like shutting down

03:08:03.840 --> 03:08:08.120
the GPU clusters because, no, we are not in a shape

03:08:08.120 --> 03:08:10.020
to frantically do at the last minute

03:08:10.020 --> 03:08:12.180
do decades worth of work.

03:08:14.940 --> 03:08:16.400
The thing you would do at this point

03:08:16.400 --> 03:08:17.660
if there were massive public outcry

03:08:17.660 --> 03:08:18.740
pointed in the right direction,

03:08:18.740 --> 03:08:22.600
which I do not expect, is shut down the GPU clusters

03:08:22.600 --> 03:08:25.600
and crash program on augmenting human intelligence

03:08:25.600 --> 03:08:29.060
biologically, not the AIS stuff, biologically.

03:08:30.040 --> 03:08:32.120
Because if you make humans much smarter,

03:08:32.120 --> 03:08:34.760
they can actually be smart and nice.

03:08:34.760 --> 03:08:37.560
Like you get that in a plausible way,

03:08:37.560 --> 03:08:39.440
in a way that you do not get it.

03:08:39.440 --> 03:08:40.800
And it is not as easy to do

03:08:40.800 --> 03:08:43.220
with synthesizing these things from scratch,

03:08:43.220 --> 03:08:45.920
predicting the next tokens and applying RLHF.

03:08:45.920 --> 03:08:47.840
Like humans start out in the frame

03:08:47.860 --> 03:08:51.340
that produces niceness, that has ever produced niceness.

03:08:55.500 --> 03:08:57.300
And in saying this, I do not want to sound

03:08:57.300 --> 03:08:59.580
like the moral of this whole thing was like,

03:08:59.580 --> 03:09:02.060
oh, like you need to engage in mass action

03:09:02.060 --> 03:09:03.860
and then everything will be all right.

03:09:05.700 --> 03:09:08.260
Because there's so many things where somebody tells you

03:09:08.260 --> 03:09:10.740
that the world is ending and you need to recycle.

03:09:10.740 --> 03:09:11.980
And if everybody does their part

03:09:11.980 --> 03:09:13.900
and recycles their cardboard,

03:09:13.900 --> 03:09:15.780
then we can all live happily ever after.

03:09:15.780 --> 03:09:20.240
And this is unfortunately not what I have to say.

03:09:24.000 --> 03:09:25.400
Everybody recycling their cardboard,

03:09:25.400 --> 03:09:26.240
it's not gonna fix this.

03:09:26.240 --> 03:09:27.360
Everybody recycles their cardboard

03:09:27.360 --> 03:09:31.360
and then everybody ends up dead, metaphorically speaking.

03:09:31.360 --> 03:09:36.000
But if there was enough, like on the margins,

03:09:36.000 --> 03:09:37.480
you just end up dead a little bit later

03:09:37.480 --> 03:09:38.720
on most of the things you can do

03:09:38.720 --> 03:09:42.800
that a few people can do by trying hard.

03:09:43.800 --> 03:09:46.840
But if there was enough public outcry

03:09:46.840 --> 03:09:48.600
to shut down the GPU clusters,

03:09:48.600 --> 03:09:52.320
then you could be part of that outcry.

03:09:52.320 --> 03:09:54.240
If Eliezer is wrong in the direction

03:09:54.240 --> 03:09:55.940
that Lex Fridman predicts,

03:09:55.940 --> 03:09:58.580
that there was enough public outcry,

03:09:58.580 --> 03:09:59.940
pointed enough in the right direction

03:09:59.940 --> 03:10:04.300
to do something that actually results in people living.

03:10:05.880 --> 03:10:07.160
Not just like we did something,

03:10:07.160 --> 03:10:08.840
not just there was an outcry

03:10:08.840 --> 03:10:10.480
and the outcry was like given form

03:10:10.480 --> 03:10:12.060
and something that was like safe and convenient

03:10:12.320 --> 03:10:13.560
and didn't really inconvenience anybody

03:10:13.560 --> 03:10:15.200
and then everybody died everywhere.

03:10:15.200 --> 03:10:18.480
There was enough actual like, oh, we're going to die.

03:10:18.480 --> 03:10:19.380
We should not do that.

03:10:19.380 --> 03:10:20.900
We should do something else, which is not that,

03:10:20.900 --> 03:10:23.480
even if it is like not super duper convenient

03:10:23.480 --> 03:10:26.040
and wasn't inside the previous political Overton window.

03:10:26.040 --> 03:10:27.280
If there is that kind of public,

03:10:27.280 --> 03:10:29.280
if I am wrong and there is that kind of public outcry,

03:10:29.280 --> 03:10:30.300
then somebody in high school

03:10:30.300 --> 03:10:32.560
could be ready to be part of that.

03:10:32.560 --> 03:10:33.680
If I'm wrong in other ways,

03:10:33.680 --> 03:10:36.040
then you could be ready to be part of that.

03:10:36.040 --> 03:10:41.040
But like, and if you're like a brilliant young physicist,

03:10:41.140 --> 03:10:43.860
then you could like go into interpretability.

03:10:43.860 --> 03:10:45.140
And if you're smarter than that,

03:10:45.140 --> 03:10:46.980
you could like work on alignment problems

03:10:46.980 --> 03:10:49.540
where it's harder to tell if you got them right or not

03:10:50.860 --> 03:10:52.420
and other things.

03:10:52.420 --> 03:10:55.140
But mostly for the kids in high school,

03:10:55.940 --> 03:10:59.300
it's like, yeah, if it, you know,

03:10:59.300 --> 03:11:03.020
yeah, like be ready to help

03:11:03.020 --> 03:11:05.100
if Eliezer Yudkowski is wrong about something

03:11:05.100 --> 03:11:09.340
and otherwise don't put your happiness into the far future.

03:11:09.340 --> 03:11:11.120
It probably doesn't exist.

03:11:11.120 --> 03:11:12.560
But it's beautiful that you're looking

03:11:12.560 --> 03:11:14.700
for ways that you're wrong.

03:11:14.700 --> 03:11:17.520
And it's also beautiful that you're open to being surprised

03:11:17.520 --> 03:11:21.520
by that same young physicist with some breakthrough.

03:11:21.520 --> 03:11:24.280
It feels like a very, very basic competence

03:11:24.280 --> 03:11:25.520
that you are praising me for.

03:11:25.520 --> 03:11:27.440
And you know, like, okay, cool.

03:11:28.600 --> 03:11:32.480
I don't think it's good that we're in a world

03:11:32.480 --> 03:11:35.080
where that is something that I deserve

03:11:35.080 --> 03:11:36.040
to be complimented on,

03:11:36.040 --> 03:11:39.080
but I've never had much luck

03:11:39.080 --> 03:11:40.500
in accepting compliments gracefully.

03:11:40.500 --> 03:11:42.600
Maybe I should just accept that one gracefully,

03:11:42.600 --> 03:11:45.320
but sure, thank you very much.

03:11:45.320 --> 03:11:48.680
You've painted with some probability a dark future.

03:11:48.680 --> 03:11:52.520
Are you yourself, just when you think,

03:11:52.520 --> 03:11:57.440
when you ponder your life and you ponder your mortality,

03:11:57.440 --> 03:11:58.640
are you afraid of death?

03:12:02.280 --> 03:12:03.120
Think so, yeah.

03:12:06.040 --> 03:12:09.600
Does it make any sense to you that we die?

03:12:09.600 --> 03:12:10.440
Like what?

03:12:16.200 --> 03:12:20.400
There's a power to the finiteness of the human life

03:12:20.400 --> 03:12:24.920
that's part of this whole machinery of evolution.

03:12:24.920 --> 03:12:29.320
And that finiteness doesn't seem to be obviously integrated

03:12:29.320 --> 03:12:33.000
into AI systems.

03:12:33.000 --> 03:12:35.960
So it feels like almost some fundamentally in that aspect,

03:12:36.840 --> 03:12:39.080
some fundamentally different thing that we're creating.

03:12:39.080 --> 03:12:41.760
I grew up reading books like

03:12:41.760 --> 03:12:44.280
Great Mambo Chicken and the Transhuman Condition,

03:12:44.280 --> 03:12:48.040
and later on Endings of Creation and Mind Children,

03:12:49.480 --> 03:12:53.400
you know, like age 12 or thereabouts.

03:12:53.400 --> 03:12:58.260
So I never thought I was supposed to die after 80 years.

03:12:59.200 --> 03:13:01.760
I never thought that humanity was supposed to die.

03:13:01.760 --> 03:13:03.900
I thought we were, like,

03:13:03.900 --> 03:13:05.940
I always grew up with the ideal in mind

03:13:05.940 --> 03:13:07.860
that we were all going to live happily ever after

03:13:07.860 --> 03:13:09.740
in the glorious transhumanist future.

03:13:10.660 --> 03:13:12.620
I did not grow up thinking that death

03:13:12.620 --> 03:13:14.340
was part of the meaning of life.

03:13:16.460 --> 03:13:17.540
And now?

03:13:17.540 --> 03:13:20.700
And now I still think it's a pretty stupid idea.

03:13:20.700 --> 03:13:21.540
But there is.

03:13:21.540 --> 03:13:23.900
You do not need life to be finite to be meaningful.

03:13:23.900 --> 03:13:25.100
It just has to be life.

03:13:26.580 --> 03:13:29.100
What role does love play in the human condition?

03:13:29.100 --> 03:13:31.380
We haven't brought up love in this whole picture.

03:13:31.380 --> 03:13:32.580
We talked about intelligence.

03:13:32.580 --> 03:13:33.940
We talked about consciousness.

03:13:33.940 --> 03:13:36.740
It seems part of humanity.

03:13:36.740 --> 03:13:40.220
I would say one of the most important parts

03:13:40.220 --> 03:13:44.220
is this feeling we have towards each other.

03:13:45.500 --> 03:13:48.820
If in the future there were routinely

03:13:50.520 --> 03:13:53.380
more than one AI, let's say two,

03:13:53.380 --> 03:13:55.180
for the sake of discussion,

03:13:55.180 --> 03:13:57.860
who would look at each other and say,

03:13:57.860 --> 03:14:00.140
I am I and you are you.

03:14:00.140 --> 03:14:03.060
The other one also says, I am I and you are you.

03:14:03.060 --> 03:14:08.060
And sometimes they were happy and sometimes they were sad.

03:14:08.180 --> 03:14:10.300
And it mattered to the other one

03:14:10.300 --> 03:14:11.900
that this thing that is different from them

03:14:11.900 --> 03:14:15.780
is like they would rather it be happy than sad

03:14:15.780 --> 03:14:18.660
and entangle their lives together,

03:14:18.660 --> 03:14:23.580
then this is a more optimistic thing

03:14:23.580 --> 03:14:25.020
than I expect to actually happen.

03:14:25.020 --> 03:14:28.700
And a little fragment of meaning would be there,

03:14:28.740 --> 03:14:30.300
possibly more than a little,

03:14:30.300 --> 03:14:32.660
but that I expect this to not happen,

03:14:32.660 --> 03:14:34.900
that I do not think this is what happens by default,

03:14:34.900 --> 03:14:37.420
that I do not think that this is the future

03:14:37.420 --> 03:14:42.420
we are on track to get is why I would go down fighting

03:14:43.580 --> 03:14:47.300
rather than just saying, oh, well.

03:14:48.820 --> 03:14:51.540
Do you think that is part of the meaning

03:14:51.540 --> 03:14:54.100
of this whole thing or the meaning of life?

03:14:54.100 --> 03:14:57.460
What do you think is the meaning of life, of human life?

03:14:57.460 --> 03:14:59.700
It's all the things that I value about it

03:14:59.700 --> 03:15:01.580
and maybe all the things that I would value

03:15:01.580 --> 03:15:03.420
if I understood it better.

03:15:03.420 --> 03:15:06.900
There's not some meaning far outside of us

03:15:06.900 --> 03:15:09.140
that we have to wonder about.

03:15:09.140 --> 03:15:12.860
There's just like looking at life and being like,

03:15:12.860 --> 03:15:14.660
yes, this is what I want.

03:15:16.580 --> 03:15:21.580
The meaning of life is not some kind of,

03:15:22.580 --> 03:15:27.420
like meaning is something that we bring to things

03:15:27.420 --> 03:15:28.260
when we look at them.

03:15:28.260 --> 03:15:30.660
We look at them and we say like, this is its meaning to me.

03:15:30.660 --> 03:15:34.780
And it's not that before humanity was ever here,

03:15:34.780 --> 03:15:38.020
there was like some meaning written upon the stars

03:15:38.020 --> 03:15:39.580
where you could like go out to the star

03:15:39.580 --> 03:15:41.900
where that meaning was written and like change it around

03:15:41.900 --> 03:15:43.900
and thereby completely change the meaning of life, right?

03:15:43.900 --> 03:15:47.020
Like the notion that this is written

03:15:47.020 --> 03:15:48.820
on a stone tablet somewhere implies

03:15:48.820 --> 03:15:50.820
you could like change the tablet and get a different meaning.

03:15:50.860 --> 03:15:53.060
That seems kind of wacky, doesn't it?

03:15:53.060 --> 03:15:57.940
So it doesn't feel that mysterious to me at this point.

03:15:57.940 --> 03:16:01.580
It's just a matter of being like, yeah, I care.

03:16:02.740 --> 03:16:03.580
I care.

03:16:06.060 --> 03:16:11.060
And part of that is the love that connects all of us.

03:16:12.180 --> 03:16:14.460
It's one of the things that I care about.

03:16:17.220 --> 03:16:19.780
And the flourishing of the collective intelligence

03:16:19.780 --> 03:16:21.060
of the human species.

03:16:22.100 --> 03:16:25.140
You know, that sounds kind of too fancy to me.

03:16:25.140 --> 03:16:28.140
Just look at all the people, you know,

03:16:28.140 --> 03:16:31.860
like one by one up to the 8 billion

03:16:31.860 --> 03:16:35.300
and be like, that's life, that's life, that's life.

03:16:37.420 --> 03:16:39.300
Eliezer, you're an incredible human.

03:16:39.300 --> 03:16:40.300
It's a huge honor.

03:16:40.300 --> 03:16:43.700
I was trying to talk to you for a long time

03:16:45.220 --> 03:16:46.460
because I'm a big fan.

03:16:46.460 --> 03:16:47.900
I think you're a really important voice

03:16:47.900 --> 03:16:49.060
and really important mind.

03:16:49.100 --> 03:16:51.020
Thank you for the fight you're fighting.

03:16:52.300 --> 03:16:53.980
Thank you for being fearless and bold

03:16:53.980 --> 03:16:55.220
and for everything you do.

03:16:55.220 --> 03:16:56.820
I hope we get a chance to talk again

03:16:56.820 --> 03:16:58.780
and I hope you never give up.

03:16:58.780 --> 03:16:59.620
Thank you for talking to me.

03:16:59.620 --> 03:17:00.460
You're welcome.

03:17:00.460 --> 03:17:02.660
I do worry that we didn't really address

03:17:02.660 --> 03:17:04.140
a whole lot of fundamental questions.

03:17:04.140 --> 03:17:05.980
I expect people have, but you know,

03:17:05.980 --> 03:17:08.340
maybe we got a little bit further

03:17:08.340 --> 03:17:10.740
and made a tiny little bit of progress

03:17:10.740 --> 03:17:14.420
and I'd say like be satisfied with that.

03:17:14.420 --> 03:17:16.300
But actually, no, I think once would only be satisfied

03:17:16.300 --> 03:17:17.900
with solving the entire problem.

03:17:18.860 --> 03:17:19.980
To be continued.

03:17:21.940 --> 03:17:23.500
Thanks for listening to this conversation

03:17:23.500 --> 03:17:25.140
with Eliezer Yatkowski.

03:17:25.140 --> 03:17:26.340
To support this podcast,

03:17:26.340 --> 03:17:28.900
please check out our sponsors in the description.

03:17:28.900 --> 03:17:33.460
And now let me leave you with some words from Elon Musk.

03:17:33.460 --> 03:17:37.220
With artificial intelligence, we're summoning the demon.

03:17:38.500 --> 03:17:41.900
Thank you for listening and hope to see you next time.

03:17:47.900 --> 03:17:48.740
Thank you.


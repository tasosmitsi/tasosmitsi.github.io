WEBVTT

00:00.000 --> 00:01.880
I think it's possible that physics has exploits

00:01.880 --> 00:03.560
and we should be trying to find them,

00:03.560 --> 00:05.920
arranging some kind of a crazy quantum mechanical system

00:05.920 --> 00:08.360
that somehow gives you buffer overflow,

00:08.360 --> 00:10.760
somehow gives you a rounding error in the floating point.

00:10.760 --> 00:13.240
Synthetic intelligences are kind of like

00:13.240 --> 00:15.320
the next stage of development.

00:15.320 --> 00:17.120
And I don't know where it leads to.

00:17.120 --> 00:20.760
Like at some point, I suspect the universe

00:20.760 --> 00:23.120
is some kind of a puzzle.

00:23.120 --> 00:27.600
These synthetic AIs will uncover that puzzle and solve it.

00:30.120 --> 00:32.880
The following is a conversation with Andre Capati,

00:32.880 --> 00:35.840
previously the Director of AI at Tesla,

00:35.840 --> 00:39.800
and before that, at OpenAI and Stanford.

00:39.800 --> 00:43.600
He is one of the greatest scientists, engineers,

00:43.600 --> 00:47.800
and educators in the history of artificial intelligence.

00:47.800 --> 00:50.120
This is the Lex Fridman podcast.

00:50.120 --> 00:52.760
To support it, please check out our sponsors.

00:52.760 --> 00:57.080
And now, dear friends, here's Andre Capati.

00:58.080 --> 01:00.160
What is a neural network,

01:00.160 --> 01:03.160
and why does it seem to do such a surprisingly

01:03.160 --> 01:04.600
good job of learning?

01:04.600 --> 01:05.440
What is a neural network?

01:05.440 --> 01:10.040
It's a mathematical abstraction of the brain.

01:10.040 --> 01:12.680
I would say that's how it was originally developed.

01:12.680 --> 01:14.560
At the end of the day, it's a mathematical expression,

01:14.560 --> 01:16.120
and it's a fairly simple mathematical expression

01:16.120 --> 01:17.400
when you get down to it.

01:17.400 --> 01:21.400
It's basically a sequence of matrix multiplies,

01:21.400 --> 01:23.480
which are really dot products mathematically,

01:23.480 --> 01:25.440
and some nonlinearity is thrown in.

01:25.480 --> 01:27.680
And so it's a very simple mathematical expression,

01:27.680 --> 01:29.240
and it's got knobs in it.

01:29.240 --> 01:30.080
Many knobs.

01:30.080 --> 01:30.920
Many knobs.

01:30.920 --> 01:32.800
And these knobs are loosely related

01:32.800 --> 01:34.200
to basically the synapses in your brain.

01:34.200 --> 01:35.880
They're trainable, they're modifiable.

01:35.880 --> 01:36.760
And so the idea is like,

01:36.760 --> 01:38.600
we need to find the setting of the knobs

01:38.600 --> 01:41.480
that makes the neural net do whatever you want it to do,

01:41.480 --> 01:43.600
like classify images and so on.

01:43.600 --> 01:45.720
And so there's not too much mystery, I would say, in it.

01:45.720 --> 01:48.640
Like, you might think that,

01:48.640 --> 01:49.680
basically you don't want to endow it

01:49.680 --> 01:51.400
with too much meaning with respect to the brain

01:51.400 --> 01:52.840
and how it works.

01:52.840 --> 01:55.040
It's really just a complicated mathematical expression

01:55.600 --> 01:57.600
with knobs, and those knobs need a proper setting

01:57.600 --> 01:59.280
for it to do something desirable.

01:59.280 --> 02:02.160
Yeah, but poetry is just a collection of letters

02:02.160 --> 02:05.360
with spaces, but it can make us feel a certain way.

02:05.360 --> 02:06.200
And in that same way,

02:06.200 --> 02:08.600
when you get a large number of knobs together,

02:08.600 --> 02:12.000
whether it's inside the brain or inside a computer,

02:12.000 --> 02:16.120
they seem to surprise us with their power.

02:16.120 --> 02:17.440
Yeah, I think that's fair.

02:17.440 --> 02:20.040
So basically, I'm underselling it by a lot

02:20.040 --> 02:23.040
because you definitely do get very surprising

02:23.040 --> 02:25.240
emergent behaviors out of these neural nets

02:25.240 --> 02:26.080
when they're large enough

02:26.080 --> 02:28.800
and trained on complicated enough problems.

02:28.800 --> 02:29.640
Like, say, for example,

02:29.640 --> 02:32.320
the next word prediction in a massive data set

02:32.320 --> 02:33.600
from the internet.

02:33.600 --> 02:35.600
And then these neural nets take on

02:35.600 --> 02:37.800
pretty surprising magical properties.

02:37.800 --> 02:38.960
Yeah, I think it's kind of interesting

02:38.960 --> 02:40.480
how much you can get out of

02:40.480 --> 02:42.320
even very simple mathematical formalism.

02:42.320 --> 02:44.400
When your brain right now is talking,

02:44.400 --> 02:47.240
is it doing next word prediction?

02:47.240 --> 02:49.160
Or is it doing something more interesting?

02:49.160 --> 02:51.000
Well, it's definitely some kind of a generative model

02:51.000 --> 02:54.440
that's a GPT-like and prompted by you.

02:54.440 --> 02:55.280
Yeah.

02:55.280 --> 02:56.120
So you're giving me a prompt

02:56.120 --> 02:58.720
and I'm kind of responding to it in a generative way.

02:58.720 --> 03:00.920
And by yourself, perhaps, a little bit?

03:00.920 --> 03:03.520
Like, are you adding extra prompts

03:03.520 --> 03:05.800
from your own memory inside your head?

03:06.640 --> 03:07.480
Or no?

03:07.480 --> 03:08.360
Well, it definitely feels like you're referencing

03:08.360 --> 03:10.080
some kind of a declarative structure

03:10.080 --> 03:12.320
of memory and so on.

03:12.320 --> 03:15.520
And then you're putting that together with your prompt

03:15.520 --> 03:17.160
and giving away some answers.

03:17.160 --> 03:19.520
How much of what you just said

03:19.560 --> 03:21.920
has been said by you before?

03:21.920 --> 03:23.640
Oh, nothing, basically, right?

03:23.640 --> 03:26.040
No, but if you actually look at all the words

03:26.040 --> 03:29.520
you've ever said in your life and you do a search,

03:29.520 --> 03:32.240
you'll probably have said a lot of the same words

03:32.240 --> 03:34.200
in the same order before.

03:34.200 --> 03:35.440
Yeah, could be.

03:35.440 --> 03:37.520
I mean, I'm using phrases that are common, et cetera,

03:37.520 --> 03:41.280
but I'm remixing it into a pretty sort of unique sentence

03:41.280 --> 03:42.120
at the end of the day.

03:42.120 --> 03:44.320
But you're right, definitely there's like a ton of remixing.

03:44.320 --> 03:48.400
Why, you didn't, it's like Magnus Carlsen said,

03:48.640 --> 03:52.360
I'm rated 2,900 whatever, which is pretty decent.

03:52.360 --> 03:55.240
I think you're talking very,

03:55.240 --> 03:57.880
you're not giving enough credit to your own nuts here.

03:57.880 --> 04:02.360
Why do they seem to, what's your best intuition

04:03.200 --> 04:05.600
about this emergent behavior?

04:05.600 --> 04:06.440
I mean, it's kind of interesting

04:06.440 --> 04:08.840
because I'm simultaneously underselling them,

04:08.840 --> 04:11.200
but I also feel like there's an element to which I'm over,

04:11.200 --> 04:12.840
like, it's actually kind of incredible

04:12.840 --> 04:14.800
that you can get so much emergent magical behavior

04:14.800 --> 04:17.600
out of them, despite them being so simple mathematically.

04:17.600 --> 04:19.720
So I think those are kind of like two surprising statements

04:19.720 --> 04:22.720
that are kind of juxtaposed together.

04:22.720 --> 04:23.800
And I think basically what it is

04:23.800 --> 04:25.280
is we are actually fairly good

04:25.280 --> 04:27.200
at optimizing these neural nets.

04:27.200 --> 04:29.680
And when you give them a hard enough problem,

04:29.680 --> 04:32.800
they are forced to learn very interesting solutions

04:32.800 --> 04:34.120
in the optimization.

04:34.120 --> 04:37.520
And those solution basically have these emergent properties

04:37.520 --> 04:38.880
that are very interesting.

04:38.880 --> 04:42.760
There's wisdom and knowledge in the knobs.

04:43.760 --> 04:47.600
And so this representation that's in the knobs,

04:47.600 --> 04:49.320
does it make sense to you intuitively

04:49.320 --> 04:52.640
that a large number of knobs can hold a representation

04:52.640 --> 04:55.760
that captures some deep wisdom about the data

04:55.760 --> 04:57.400
it has looked at?

04:57.400 --> 04:58.680
It's a lot of knobs.

04:58.680 --> 05:00.040
It's a lot of knobs.

05:00.040 --> 05:03.520
And somehow, you know, so speaking concretely,

05:03.520 --> 05:05.200
one of the neural nets that people are very excited about

05:05.200 --> 05:07.440
right now are GPTs,

05:07.440 --> 05:10.160
which are basically just next word prediction networks.

05:10.240 --> 05:13.680
So you consume a sequence of words from the internet

05:13.680 --> 05:15.480
and you try to predict the next word.

05:15.480 --> 05:19.760
And once you train these on a large enough data set,

05:21.760 --> 05:24.040
you can basically prompt these neural nets

05:24.040 --> 05:25.920
in arbitrary ways and you can ask them to solve problems.

05:25.920 --> 05:26.920
And they will.

05:26.920 --> 05:28.520
So you can just tell them,

05:28.520 --> 05:30.520
you can make it look like you're trying to

05:31.880 --> 05:33.440
solve some kind of a mathematical problem

05:33.440 --> 05:35.400
and they will continue what they think is the solution

05:35.400 --> 05:36.920
based on what they've seen on the internet.

05:36.920 --> 05:39.400
And very often those solutions look very

05:39.480 --> 05:41.920
remarkably consistent, look correct potentially.

05:43.040 --> 05:45.440
Do you still think about the brain side of it?

05:45.440 --> 05:47.520
So as neural nets is an abstraction

05:47.520 --> 05:49.600
or mathematical abstraction of the brain,

05:49.600 --> 05:54.600
you still draw wisdom from the biological neural networks?

05:56.360 --> 05:57.800
Or even the bigger question.

05:57.800 --> 06:01.000
So you're a big fan of biology and biological computation.

06:02.200 --> 06:05.880
What impressive thing is biology doing to you

06:05.880 --> 06:07.880
that computers are not yet?

06:07.880 --> 06:09.160
That gap?

06:09.160 --> 06:10.960
I would say I'm definitely on,

06:10.960 --> 06:13.440
I'm much more hesitant with the analogies to the brain

06:13.440 --> 06:16.280
than I think you would see potentially in the field.

06:16.280 --> 06:19.560
And I kind of feel like certainly

06:19.560 --> 06:20.680
the way neural networks started

06:20.680 --> 06:24.000
is everything stemmed from inspiration by the brain.

06:24.000 --> 06:25.080
But at the end of the day,

06:25.080 --> 06:27.400
the artifacts that you get after training,

06:27.400 --> 06:30.040
they are arrived at by a very different optimization process

06:30.040 --> 06:32.880
than the optimization process that gave rise to the brain.

06:32.880 --> 06:33.880
And so I think,

06:35.160 --> 06:38.760
I kind of think of it as a very complicated alien artifact.

06:39.400 --> 06:40.240
It's something different.

06:40.240 --> 06:42.560
I'm sorry, the neural nets that we're training.

06:42.560 --> 06:45.720
They are complicated alien artifact.

06:45.720 --> 06:47.360
I do not make analogies to the brain

06:47.360 --> 06:49.920
because I think the optimization process that gave rise to it

06:49.920 --> 06:51.760
is very different from the brain.

06:51.760 --> 06:56.000
So there was no multi-agent self-play kind of setup

06:56.000 --> 06:57.080
and evolution.

06:57.080 --> 06:59.560
It was an optimization that is basically

06:59.560 --> 07:01.880
what amounts to a compression objective

07:01.880 --> 07:03.440
on a massive amount of data.

07:03.440 --> 07:07.240
Okay, so artificial neural networks are doing compression

07:07.280 --> 07:10.440
and biological neural networks.

07:10.440 --> 07:11.440
I'm trying to survive.

07:11.440 --> 07:13.280
And they're not really doing anything.

07:13.280 --> 07:16.880
They're an agent in a multi-agent self-play system

07:16.880 --> 07:19.480
that's been running for a very, very long time.

07:19.480 --> 07:23.200
That said, evolution has found that it is very useful

07:23.200 --> 07:26.400
to predict and have a predictive model in the brain.

07:26.400 --> 07:28.800
And so I think our brain utilizes something

07:28.800 --> 07:31.000
that looks like that as a part of it,

07:31.000 --> 07:33.760
but it has a lot more gadgets and gizmos

07:33.800 --> 07:37.360
and value functions and ancient nuclei

07:37.360 --> 07:39.040
that are all trying to like make it survive

07:39.040 --> 07:40.800
and reproduce and everything else.

07:40.800 --> 07:43.080
And the whole thing through embryogenesis

07:43.080 --> 07:44.640
is built from a single cell.

07:44.640 --> 07:48.200
I mean, it's just, the code is inside the DNA

07:48.200 --> 07:49.320
and it just builds it up.

07:49.320 --> 07:54.320
Like the entire organism with arms and the head and legs.

07:54.640 --> 07:55.480
Yes.

07:55.480 --> 07:57.720
And like it does it pretty well.

07:57.720 --> 07:59.200
It should not be possible.

07:59.200 --> 08:00.640
So there's some learning going on.

08:00.640 --> 08:03.120
There's some kind of computation

08:03.120 --> 08:05.040
going through that building process.

08:05.040 --> 08:08.080
I mean, I don't know where,

08:08.080 --> 08:09.960
if you were just to look at the entirety

08:09.960 --> 08:11.840
of history of life on earth,

08:11.840 --> 08:15.320
where do you think is the most interesting invention?

08:15.320 --> 08:17.760
Is it the origin of life itself?

08:17.760 --> 08:20.440
Is it just jumping to eukaryotes?

08:20.440 --> 08:22.200
Is it mammals?

08:22.200 --> 08:24.680
Is it humans themselves, almost sapiens?

08:24.680 --> 08:27.480
The origin of intelligence

08:27.480 --> 08:29.520
or highly complex intelligence?

08:29.720 --> 08:33.240
Or is it all just a continuation

08:33.240 --> 08:34.640
of the same kind of process?

08:35.960 --> 08:38.440
Certainly I would say it's an extremely remarkable story

08:38.440 --> 08:41.320
that I'm only like briefly learning about recently.

08:41.320 --> 08:44.040
All the way from actually like,

08:44.040 --> 08:46.360
you almost have to start at the formation of earth

08:46.360 --> 08:48.200
and all of its conditions and the entire solar system

08:48.200 --> 08:50.720
and how everything is arranged with Jupiter and moon

08:50.720 --> 08:53.400
and the habitable zone and everything.

08:53.400 --> 08:55.560
And then you have an active earth

08:55.560 --> 08:57.560
that's turning over material.

08:57.560 --> 09:01.840
And then you start with abiogenesis and everything.

09:01.840 --> 09:03.880
And so it's all like a pretty remarkable story.

09:03.880 --> 09:08.360
I'm not sure that I can pick like a single unique piece

09:08.360 --> 09:10.640
of it that I find most interesting.

09:12.280 --> 09:14.280
I guess for me as an artificial intelligence researcher,

09:14.280 --> 09:15.280
it's probably the last piece.

09:15.280 --> 09:19.640
We have lots of animals that are not building

09:19.640 --> 09:22.120
technological society, but we do.

09:22.120 --> 09:24.760
And it seems to have happened very quickly.

09:24.760 --> 09:26.720
It seems to have happened very recently.

09:26.760 --> 09:30.120
And something very interesting happened there

09:30.120 --> 09:31.120
that I don't fully understand.

09:31.120 --> 09:32.760
I almost understand everything else,

09:32.760 --> 09:35.440
I think intuitively, but I don't understand

09:35.440 --> 09:37.960
exactly that part and how quick it was.

09:37.960 --> 09:39.960
Both explanations will be interesting.

09:39.960 --> 09:42.160
One is that this is just a continuation

09:42.160 --> 09:43.240
of the same kind of process.

09:43.240 --> 09:45.400
There's nothing special about humans.

09:45.400 --> 09:47.440
That would be deeply understanding

09:47.440 --> 09:48.920
that would be very interesting

09:48.920 --> 09:52.960
that we think of ourselves as special, but it was obvious.

09:52.960 --> 09:56.400
It was already written in the code

09:57.040 --> 10:00.840
that you would have greater and greater intelligence emerging.

10:00.840 --> 10:03.640
And then the other explanation,

10:03.640 --> 10:05.640
which is something truly special happened,

10:05.640 --> 10:08.040
something like a rare event,

10:08.040 --> 10:11.520
whether it's like crazy rare event, like a space Odyssey.

10:11.520 --> 10:12.520
What would it be?

10:12.520 --> 10:14.880
See, if you say like the invention of fire

10:16.040 --> 10:21.040
or the, as Richard Wrangham says,

10:21.280 --> 10:25.560
the beta males deciding a clever way to kill the alpha males

10:25.560 --> 10:28.800
by collaborating, so just optimizing the collaboration,

10:28.800 --> 10:31.480
the multi-agent aspect of the multi-agent,

10:31.480 --> 10:35.000
and that really being constrained on resources

10:35.000 --> 10:38.280
and trying to survive the collaboration aspect

10:38.280 --> 10:40.560
is what created the complex intelligence.

10:40.560 --> 10:42.440
But it seems like it's a natural

10:42.440 --> 10:44.400
outgrowth of the evolution process.

10:44.400 --> 10:47.560
What could possibly be a magical thing that happened,

10:47.560 --> 10:51.600
like a rare thing that would say that humans are actually,

10:51.600 --> 10:54.520
human level intelligence is actually a really rare thing

10:54.520 --> 10:55.520
in the universe?

10:56.800 --> 10:58.840
Yeah, I'm hesitant to say that it is rare, by the way,

10:58.840 --> 11:01.040
but it definitely seems like,

11:01.040 --> 11:02.720
it's kind of like a punctuated equilibrium

11:02.720 --> 11:05.040
where you have lots of exploration

11:05.040 --> 11:08.000
and then you have certain leaps, sparse leaps in between.

11:08.000 --> 11:10.800
So of course, like origin of life would be one,

11:10.800 --> 11:15.800
DNA, sex, eukaryotic life,

11:16.160 --> 11:18.760
the endosymbiosis event where the archaeon ate

11:18.760 --> 11:20.640
little bacteria, just the whole thing.

11:20.640 --> 11:23.480
And then of course, emergence of consciousness and so on.

11:23.480 --> 11:25.520
So it seems like definitely there are sparse events

11:25.520 --> 11:27.320
where a massive amount of progress was made,

11:27.320 --> 11:29.560
but yeah, it's kind of hard to pick one.

11:29.560 --> 11:32.320
So you don't think humans are unique?

11:32.320 --> 11:35.160
Gotta ask you, how many intelligent alien civilizations

11:35.160 --> 11:36.760
do you think are out there?

11:36.760 --> 11:41.760
And is their intelligence different or similar to ours?

11:44.560 --> 11:47.480
Yeah, I've been preoccupied with this question

11:47.480 --> 11:50.240
quite a bit recently, basically the Fermi paradox

11:50.240 --> 11:51.400
and just thinking through.

11:51.440 --> 11:54.240
And the reason actually that I am very interested

11:54.240 --> 11:57.400
in the origin of life is fundamentally trying to understand

11:57.400 --> 11:59.400
how common it is that there are technological societies

11:59.400 --> 12:02.840
out there in space.

12:02.840 --> 12:04.360
And the more I study it,

12:04.360 --> 12:09.360
the more I think that there should be quite a lot.

12:09.440 --> 12:10.800
Why haven't we heard from them?

12:10.800 --> 12:11.960
Because I agree with you.

12:11.960 --> 12:16.960
It feels like I just don't see why,

12:17.200 --> 12:20.160
what we did here on earth is so difficult to do.

12:20.160 --> 12:22.000
Yeah, and especially when you get into the details of it,

12:22.000 --> 12:23.920
I used to think origin of life was very,

12:25.680 --> 12:27.200
it was this magical rare event,

12:27.200 --> 12:28.920
but then you read books like, for example,

12:28.920 --> 12:33.920
Nick Lane, The Vital Question, Life Ascending, et cetera.

12:34.280 --> 12:36.960
And he really gets in and he really makes you believe

12:36.960 --> 12:38.600
that this is not that rare.

12:38.600 --> 12:39.920
Basic chemistry.

12:39.920 --> 12:41.960
You have an active earth and you have your alkaline vents

12:41.960 --> 12:44.400
and you have lots of alkaline waters mixing

12:44.400 --> 12:47.000
with the ocean and you have your proton gradients

12:47.000 --> 12:48.760
and you have the little porous pockets

12:48.760 --> 12:51.640
of these alkaline vents that concentrate chemistry.

12:51.640 --> 12:54.960
And basically as he steps through all of these little pieces,

12:54.960 --> 12:56.600
you start to understand that actually,

12:56.600 --> 12:58.840
this is not that crazy.

12:58.840 --> 13:01.520
You could see this happen on other systems.

13:01.520 --> 13:04.840
And he really takes you from just a geology

13:04.840 --> 13:06.360
to primitive life.

13:06.360 --> 13:09.240
And he makes it feel like it's actually pretty plausible.

13:09.240 --> 13:11.040
And also like the origin of life

13:13.520 --> 13:16.040
was actually fairly fast after formation of earth.

13:17.000 --> 13:17.840
If I remember correctly,

13:17.840 --> 13:19.280
just a few hundred million years or something like that

13:19.280 --> 13:22.400
after basically when it was possible, life actually arose.

13:22.400 --> 13:24.960
And so that makes me feel like that is not the constraint,

13:24.960 --> 13:26.200
that is not the limiting variable

13:26.200 --> 13:28.640
and that life should actually be fairly common.

13:29.680 --> 13:31.840
And then where the drop-offs are

13:31.840 --> 13:35.440
is very interesting to think about.

13:35.440 --> 13:38.240
I currently think that there's no major drop-offs basically

13:38.240 --> 13:40.000
and so there should be quite a lot of life.

13:40.000 --> 13:42.640
And basically where that brings me to then

13:42.640 --> 13:44.400
is the only way to reconcile the fact

13:44.400 --> 13:46.000
that we haven't found anyone and so on

13:47.000 --> 13:49.760
is that we just can't, we can't see them.

13:49.760 --> 13:50.600
We can't observe them.

13:50.600 --> 13:51.960
Just a quick, brief comment.

13:51.960 --> 13:54.480
Nick Lane and a lot of biologists I talked to,

13:54.480 --> 13:58.280
they really seem to think that the jump from bacteria

13:58.280 --> 14:01.120
to more complex organisms is the hardest jump.

14:01.120 --> 14:02.400
The eukaryotic life, basically.

14:02.400 --> 14:04.840
Yeah, which I don't, I get it.

14:04.840 --> 14:08.400
They're much more knowledgeable than me

14:08.400 --> 14:11.040
about the intricacies of biology.

14:11.040 --> 14:12.720
But that seems like crazy

14:12.760 --> 14:17.000
because how many single cell organisms are there

14:17.000 --> 14:21.800
and how much time you have, surely it's not that difficult.

14:21.800 --> 14:26.160
And a billion years is not even that long of a time, really.

14:26.160 --> 14:29.320
Just all these bacteria under constrained resources

14:29.320 --> 14:30.240
battling it out.

14:30.240 --> 14:32.240
I'm sure they can invent more complex.

14:32.240 --> 14:33.080
I don't understand.

14:33.080 --> 14:36.000
It's like how to move from a hello world program

14:36.000 --> 14:39.080
to like invent a function or something like that.

14:39.080 --> 14:39.920
I don't.

14:39.920 --> 14:40.760
Yeah.

14:41.680 --> 14:43.080
So I don't, yeah, so I'm with you.

14:43.080 --> 14:45.040
I just feel like I don't see any,

14:45.040 --> 14:47.880
if the origin of life, that would be my intuition.

14:47.880 --> 14:48.960
That's the hardest thing.

14:48.960 --> 14:50.160
But if that's not the hardest thing

14:50.160 --> 14:51.760
because it happens so quickly,

14:51.760 --> 14:53.120
then it's gotta be everywhere.

14:53.120 --> 14:55.320
And yeah, maybe we're just too dumb to see it.

14:55.320 --> 14:57.120
Well, it's just, we don't have really good mechanisms

14:57.120 --> 14:58.080
for seeing this life.

14:58.080 --> 15:01.080
I mean, by what, how?

15:01.080 --> 15:02.720
So I'm not an expert just to preface this,

15:02.720 --> 15:05.520
but just from what I've been looking at it.

15:05.520 --> 15:08.240
I wanna meet an expert on alien intelligence

15:08.240 --> 15:09.080
and how to communicate.

15:09.080 --> 15:10.480
I'm very suspicious of our ability

15:10.480 --> 15:12.440
to find these intelligences out there

15:12.440 --> 15:14.560
and to find these earth-like radio waves,

15:14.560 --> 15:16.400
for example, are terrible.

15:16.400 --> 15:19.200
Their power drops off as basically one over R-square.

15:19.200 --> 15:22.080
So I remember reading that our current radio waves

15:22.080 --> 15:25.400
would not be, the ones that we are broadcasting

15:25.400 --> 15:28.800
would not be measurable by our devices today.

15:28.800 --> 15:30.920
Only like, was it like one-tenth of a light year away?

15:30.920 --> 15:33.040
Like not even, basically tiny distance

15:33.040 --> 15:36.320
because you really need like a targeted transmission

15:36.320 --> 15:38.600
of massive power directed somewhere

15:38.600 --> 15:41.360
for this to be picked up on long distances.

15:41.360 --> 15:43.040
And so I just think that our ability to measure

15:43.040 --> 15:45.000
is not amazing.

15:45.000 --> 15:47.000
I think there's probably other civilizations out there.

15:47.000 --> 15:48.000
And then the big question is,

15:48.000 --> 15:49.360
why don't they build binomial probes

15:49.360 --> 15:50.840
and why don't they interstellar travel

15:50.840 --> 15:52.480
across the entire galaxy?

15:52.480 --> 15:53.560
And my current answer is,

15:53.560 --> 15:56.520
it's probably interstellar travel is like really hard.

15:56.520 --> 15:57.640
You have the interstellar medium.

15:57.640 --> 15:59.400
If you wanna move at closer speed of light,

15:59.400 --> 16:01.880
you're going to be encountering bullets along the way

16:01.880 --> 16:04.440
because even like tiny hydrogen atoms

16:04.440 --> 16:06.480
and little particles of dust are basically

16:06.480 --> 16:09.480
you have massive kinetic energy at those speeds.

16:09.480 --> 16:11.480
And so basically you need some kind of shielding.

16:11.480 --> 16:13.920
You have all the cosmic radiation.

16:13.920 --> 16:15.120
It's just like brutal out there.

16:15.120 --> 16:16.000
It's really hard.

16:16.000 --> 16:18.120
And so my thinking is maybe interstellar travel

16:18.120 --> 16:19.880
is just extremely hard.

16:19.880 --> 16:20.720
And you have to go very slow.

16:20.720 --> 16:22.520
Like billions of years to build hard?

16:24.080 --> 16:27.840
It feels like we're not a billion years away

16:27.840 --> 16:28.680
from doing that.

16:28.680 --> 16:30.240
It just might be that it's very,

16:30.240 --> 16:32.840
you have to go very slowly potentially as an example

16:32.840 --> 16:34.320
through space.

16:34.320 --> 16:36.680
Right, as opposed to close to the speed of light.

16:36.680 --> 16:38.840
So I'm suspicious basically of our ability to measure life

16:38.840 --> 16:42.200
and I'm suspicious of the ability to just permeate

16:42.200 --> 16:44.480
all of space in the galaxy or across galaxies.

16:44.480 --> 16:46.880
And that's the only way that I can currently

16:46.880 --> 16:48.000
see a way around it.

16:48.000 --> 16:51.280
Yeah, it's kind of mind blowing to think that there's

16:51.280 --> 16:54.880
trillions of intelligent alien civilizations out there

16:54.880 --> 16:57.160
kind of slowly traveling through space

16:58.280 --> 16:59.120
to meet each other.

16:59.120 --> 17:01.360
And some of them meet, some of them go to war,

17:01.360 --> 17:03.120
some of them collaborate.

17:04.000 --> 17:06.280
Or they're all just independent.

17:06.280 --> 17:08.960
They're all just like little pockets.

17:08.960 --> 17:13.360
Well statistically, if there's trillions of them,

17:13.360 --> 17:15.320
surely some of them, some of the pockets

17:15.320 --> 17:16.160
are close enough to get them.

17:16.160 --> 17:18.320
Some of them happen to be close, yeah.

17:18.320 --> 17:20.440
Close enough to see each other and then once,

17:20.440 --> 17:25.440
see once you see something that is definitely complex life,

17:25.720 --> 17:28.080
like if we see something,

17:28.080 --> 17:29.640
we're probably going to be severe,

17:29.640 --> 17:32.160
like intensely aggressively motivated

17:32.160 --> 17:35.080
to figure out what the hell that is and try to meet them.

17:35.080 --> 17:38.400
But what would be your first instinct to try to,

17:38.400 --> 17:40.360
like at a generational level,

17:40.360 --> 17:44.440
meet them or defend against them?

17:44.440 --> 17:47.880
Or what would be your instinct

17:47.880 --> 17:51.800
as a president of the United States and a scientist?

17:52.720 --> 17:55.520
I don't know which hat you prefer in this question.

17:55.520 --> 17:58.200
Yeah, I think the question, it's really hard.

17:58.640 --> 18:01.840
I will say like, for example, for us,

18:01.840 --> 18:05.960
we have lots of primitive life forms on earth next to us.

18:05.960 --> 18:07.880
We have all kinds of ants and everything else

18:07.880 --> 18:09.280
and we share space with them.

18:09.280 --> 18:11.560
And we are hesitant to impact on them

18:11.560 --> 18:14.920
and we're trying to protect them by default

18:14.920 --> 18:17.360
because they are amazing, interesting, dynamical systems

18:17.360 --> 18:18.640
that took a long time to evolve

18:18.640 --> 18:20.600
and they are interesting and special.

18:20.600 --> 18:25.600
And I don't know that you want to destroy that by default.

18:25.920 --> 18:29.560
And so I like complex dynamical systems

18:29.560 --> 18:31.640
that took a lot of time to evolve.

18:31.640 --> 18:36.640
I think I'd like to preserve it if I can afford to.

18:36.840 --> 18:38.440
And I'd like to think that the same would be true

18:38.440 --> 18:40.320
about the galactic resources

18:40.320 --> 18:41.960
and that they would think

18:41.960 --> 18:44.160
that we're kind of incredible, interesting story

18:44.160 --> 18:47.560
that took time, took a few billion years to unravel

18:47.560 --> 18:49.040
and you don't want to just destroy it.

18:49.040 --> 18:51.720
I could see two aliens talking about earth right now

18:51.720 --> 18:55.560
and saying, I'm a big fan of complex dynamical systems.

18:56.560 --> 18:59.480
So I think it was a value to preserve these

18:59.480 --> 19:01.600
and it will basically are a video game they watch

19:01.600 --> 19:04.240
or show a TV show that they watch.

19:04.240 --> 19:06.360
Yeah, I think you wouldn't need like a very good reason

19:06.360 --> 19:08.840
I think to destroy it.

19:08.840 --> 19:10.640
Like, why don't we destroy these ant farms and so on?

19:10.640 --> 19:11.880
It's because we're not actually like really

19:11.880 --> 19:14.680
in direct competition with them right now.

19:14.680 --> 19:16.040
We do it accidentally and so on,

19:16.040 --> 19:19.480
but there's plenty of resources.

19:19.480 --> 19:20.960
And so why would you destroy something

19:20.960 --> 19:22.400
that is so interesting and precious?

19:22.400 --> 19:25.760
Well, from a scientific perspective, you might probe it.

19:25.760 --> 19:27.600
You might interact with it lightly.

19:27.600 --> 19:29.520
You might want to learn something from it, right?

19:29.520 --> 19:32.320
So I wonder, there could be certain physical phenomena

19:32.320 --> 19:34.160
that we think is a physical phenomena,

19:34.160 --> 19:35.960
but it's actually interacting with us

19:35.960 --> 19:38.440
to like poke the finger and see what happens.

19:38.440 --> 19:40.080
I think it should be very interesting to scientists,

19:40.080 --> 19:42.960
other alien scientists, what happened here.

19:42.960 --> 19:45.720
And, you know, what we're seeing today is a snapshot.

19:45.720 --> 19:48.520
Basically, it's a result of a huge amount of computation

19:50.400 --> 19:52.240
over like billion years or something like that.

19:53.080 --> 19:54.920
So it could have been initiated by aliens.

19:54.920 --> 19:57.520
This could be a computer running a program.

19:57.520 --> 19:59.960
Like when, okay, if you had the power to do this,

19:59.960 --> 20:03.200
when you, okay, for sure, at least I would,

20:03.200 --> 20:07.120
I would pick a Earth-like planet that has the conditions

20:07.120 --> 20:09.360
based on my understanding of the chemistry prerequisites

20:09.360 --> 20:14.120
for life, and I would seed it with life and run it, right?

20:14.120 --> 20:17.200
Like, wouldn't you 100% do that and observe it

20:17.200 --> 20:19.240
and then protect?

20:19.240 --> 20:21.880
I mean, that's not just a hell of a good TV show.

20:22.520 --> 20:24.480
It's a good scientific experiment.

20:24.480 --> 20:29.400
And it's physical simulation, right?

20:29.400 --> 20:34.400
Maybe the evolution is the most, like actually running it

20:36.240 --> 20:40.240
is the most efficient way to understand computation

20:40.240 --> 20:41.360
or to compute stuff.

20:41.360 --> 20:44.240
Or to understand life or what life looks like

20:44.240 --> 20:46.160
and what branches it can take.

20:46.160 --> 20:47.640
It does make me kind of feel weird

20:47.640 --> 20:49.200
that we're part of a science experiment,

20:49.200 --> 20:52.960
but maybe it's, everything's a science experiment.

20:52.960 --> 20:55.000
Does that change anything for us?

20:55.000 --> 20:56.440
If we're a science experiment?

20:57.400 --> 20:58.320
I don't know.

20:58.320 --> 21:00.520
Two descendants of apes talking about

21:00.520 --> 21:01.840
being inside of a science experiment.

21:01.840 --> 21:05.080
I'm suspicious of this idea of like a deliberate panspermia,

21:05.080 --> 21:06.640
as you described it, Sirnis.

21:06.640 --> 21:09.040
I don't see a divine intervention in some way

21:09.040 --> 21:11.200
in the historical record right now.

21:11.200 --> 21:15.080
I do feel like the story in these books,

21:15.080 --> 21:17.440
like Nick Lane's books and so on, sort of makes sense.

21:17.480 --> 21:20.640
And it makes sense how life arose on earth uniquely.

21:20.640 --> 21:23.560
And yeah, I don't need to reach

21:23.560 --> 21:25.400
for more exotic explanations right now.

21:25.400 --> 21:27.600
Sure, but NPCs inside a video game

21:27.600 --> 21:32.360
don't observe any divine intervention either.

21:32.360 --> 21:35.440
We might just be all NPCs running a kind of code.

21:35.440 --> 21:36.400
Maybe eventually they will.

21:36.400 --> 21:37.720
Currently, NPCs are really dumb,

21:37.720 --> 21:39.840
but once they're running GPTs,

21:39.840 --> 21:40.800
maybe they will be like,

21:40.800 --> 21:43.480
hey, this is really suspicious, what the hell?

21:43.480 --> 21:45.800
So you famously tweeted,

21:45.840 --> 21:49.480
it looks like if you bombard earth with photons for a while,

21:49.480 --> 21:51.600
you can emit a roadster.

21:51.600 --> 21:54.800
So if like in Hitchhiker's Guide to the Galaxy,

21:54.800 --> 21:56.800
we would summarize the story of earth.

21:56.800 --> 21:59.440
So in that book, it's mostly harmless.

22:00.400 --> 22:02.720
What do you think is all the possible stories,

22:02.720 --> 22:05.760
like a paragraph long or a sentence long,

22:05.760 --> 22:08.520
that earth could be summarized as?

22:08.520 --> 22:11.160
Once it's done, it's computation.

22:11.200 --> 22:13.440
So like all the possible full,

22:14.400 --> 22:16.240
if earth is a book, right?

22:18.240 --> 22:19.920
Probably there has to be an ending.

22:19.920 --> 22:21.440
I mean, there's going to be an end to earth

22:21.440 --> 22:23.200
and it could end in all kinds of ways.

22:23.200 --> 22:25.320
It can end soon, it can end later.

22:25.320 --> 22:27.520
What do you think are the possible stories?

22:27.520 --> 22:29.840
Well, definitely there seems to be,

22:29.840 --> 22:30.880
yeah, you're sort of,

22:32.040 --> 22:34.200
it's pretty incredible that these self-replicating systems

22:34.200 --> 22:37.280
will basically arise from the dynamics

22:37.280 --> 22:38.640
and then they perpetuate themselves

22:38.640 --> 22:39.520
and become more complex

22:39.520 --> 22:42.760
and eventually become conscious and build a society.

22:42.760 --> 22:44.280
And I kind of feel like in some sense,

22:44.280 --> 22:47.640
it's kind of like a deterministic wave

22:47.640 --> 22:51.760
that kind of just happens on any sufficiently

22:51.760 --> 22:53.920
well-arranged system like earth.

22:53.920 --> 22:55.840
And so I kind of feel like there's a certain sense

22:55.840 --> 22:59.800
of inevitability in it and it's really beautiful.

22:59.800 --> 23:00.960
And it ends somehow, right?

23:00.960 --> 23:05.960
So it's chemically a diverse environment

23:06.960 --> 23:12.160
where complex dynamical systems can evolve

23:12.160 --> 23:15.640
and become further and further complex.

23:15.640 --> 23:20.240
But then there's a certain, what is it?

23:20.240 --> 23:22.640
There's certain terminating conditions.

23:23.680 --> 23:25.080
Yeah, I don't know what the terminating conditions are,

23:25.080 --> 23:27.120
but definitely there's a trend line of something

23:27.120 --> 23:28.160
and we're part of that story.

23:28.160 --> 23:30.680
And like, where does it go?

23:30.680 --> 23:32.480
So we're famously described often

23:32.480 --> 23:35.080
as a biological bootloader for AIs.

23:35.080 --> 23:35.920
And that's because humans,

23:35.920 --> 23:39.000
I mean, we're an incredible biological system

23:39.000 --> 23:43.560
and we're capable of computation and love and so on.

23:44.440 --> 23:46.200
But we're extremely inefficient as well.

23:46.200 --> 23:47.840
Like we're talking to each other through audio.

23:47.840 --> 23:49.840
It's just kind of embarrassing, honestly,

23:49.840 --> 23:53.680
that we're manipulating seven symbols serially.

23:53.680 --> 23:55.160
We're using vocal cords.

23:55.160 --> 23:57.600
It's all happening over multiple seconds.

23:57.600 --> 23:59.640
It's just kind of embarrassing when you step down

23:59.640 --> 24:03.640
to the frequencies at which computers operate

24:03.640 --> 24:05.040
or are able to cooperate on.

24:05.040 --> 24:09.720
And so basically it does seem like synthetic intelligences

24:09.720 --> 24:12.320
are kind of like the next stage of development.

24:12.320 --> 24:14.520
And I don't know where it leads to.

24:14.520 --> 24:18.200
Like at some point, I suspect the universe

24:18.200 --> 24:20.600
is some kind of a puzzle.

24:20.600 --> 24:24.000
And these synthetic AIs will uncover that puzzle

24:24.000 --> 24:26.760
and solve it.

24:26.760 --> 24:28.640
And then what happens after, right?

24:28.640 --> 24:30.960
Like what, because if you just like fast forward

24:31.600 --> 24:35.120
many billions of years, it's like, it's quiet.

24:35.120 --> 24:36.600
And then it's like to turmoil,

24:36.600 --> 24:38.280
you see like city lights and stuff like that.

24:38.280 --> 24:40.160
And then what happens at like at the end?

24:40.160 --> 24:41.960
Like, is it like a poof?

24:41.960 --> 24:44.120
Is it, or is it like a calming?

24:44.120 --> 24:45.480
Is it explosion?

24:45.480 --> 24:47.640
Is it like earth like open, like a giant?

24:47.640 --> 24:50.160
Cause you said, emit roasters,

24:50.160 --> 24:54.920
like let's start emitting like a giant number

24:54.920 --> 24:56.640
of like satellites.

24:56.640 --> 24:58.280
Yes, it's some kind of a crazy explosion.

24:58.280 --> 25:00.000
And we're living, we're like,

25:00.040 --> 25:02.040
we're stepping through a explosion

25:02.040 --> 25:03.280
and we're like living day to day

25:03.280 --> 25:04.120
and it doesn't look like it.

25:04.120 --> 25:05.400
But it's actually, if you,

25:05.400 --> 25:08.320
I saw a very cool animation of earth and life on earth.

25:08.320 --> 25:10.120
And basically nothing happens for a long time.

25:10.120 --> 25:11.600
And then the last like two seconds,

25:11.600 --> 25:13.040
like basically cities and everything

25:13.040 --> 25:15.920
and just in the lowest orbit just gets cluttered.

25:15.920 --> 25:17.560
And just the whole thing happens in the last two seconds.

25:17.560 --> 25:19.240
And you're like, this is exploding.

25:19.240 --> 25:21.160
This is a state of explosion.

25:21.160 --> 25:23.360
So if you play, yeah.

25:23.360 --> 25:25.280
Yeah, if you play at a normal speed.

25:25.280 --> 25:26.120
Yeah.

25:26.120 --> 25:27.600
It'll just look like an explosion.

25:27.600 --> 25:28.440
It's a firecracker.

25:28.720 --> 25:30.400
Living in a firecracker.

25:30.400 --> 25:31.960
Where it's going to start emitting

25:31.960 --> 25:33.360
all kinds of interesting things.

25:33.360 --> 25:34.200
Yeah.

25:34.200 --> 25:36.240
And then so explosion doesn't,

25:36.240 --> 25:38.680
it might actually look like a little explosion

25:38.680 --> 25:41.200
with lights and fire and energy emitted,

25:41.200 --> 25:42.040
all that kind of stuff.

25:42.040 --> 25:45.280
But when you look inside the details of the explosion,

25:45.280 --> 25:47.960
there's actual complexity happening

25:47.960 --> 25:51.000
where there's like a, yeah, human life

25:51.000 --> 25:52.120
or some kind of life.

25:52.120 --> 25:53.720
We hope it's not a destructive firecracker.

25:53.720 --> 25:57.920
It's kind of like a constructive firecracker.

25:57.960 --> 25:58.800
All right.

25:58.800 --> 26:01.120
So given that, hilarious discussion.

26:01.120 --> 26:02.480
It is really interesting to think about

26:02.480 --> 26:03.920
what the puzzle of the universe is.

26:03.920 --> 26:06.560
Did the creator of the universe give us a message?

26:06.560 --> 26:09.680
Like for example, in the book, Contact, Carl Sagan,

26:09.680 --> 26:14.680
there's a message for any civilization in digits

26:15.080 --> 26:18.120
in the expansion of pi in base 11 eventually,

26:18.120 --> 26:19.840
which is kind of interesting thought.

26:19.840 --> 26:23.120
Maybe we're supposed to be giving a message to our creator.

26:23.120 --> 26:24.800
Maybe we're supposed to somehow create

26:24.800 --> 26:26.600
some kind of a quantum mechanical system

26:26.600 --> 26:30.120
that alerts them to our intelligent presence here.

26:30.120 --> 26:31.880
Because if you think about it from their perspective,

26:31.880 --> 26:33.920
it's just say like quantum field theory,

26:33.920 --> 26:36.720
massive like cellular ton of a ton like thing.

26:36.720 --> 26:38.560
And like, how do you even notice that we exist?

26:38.560 --> 26:42.120
You might not even be able to pick us up in that simulation.

26:42.120 --> 26:44.800
And so how do you prove that you exist,

26:44.800 --> 26:47.560
that you're intelligent and that you're part of the universe?

26:47.560 --> 26:50.320
So this is like a touring test for intelligence from Earth?

26:50.320 --> 26:51.160
Yeah.

26:51.160 --> 26:53.800
Like the creator is, I mean, maybe this is like

26:53.800 --> 26:55.800
trying to complete the next word in a sentence.

26:55.800 --> 26:57.240
This is a complicated way of that.

26:57.240 --> 27:00.840
Like Earth is just, is basically sending a message back.

27:00.840 --> 27:03.120
Yeah, the puzzle is basically like alerting the creator

27:03.120 --> 27:04.520
that we exist.

27:04.520 --> 27:07.160
Or maybe the puzzle is just to just break out of the system

27:07.160 --> 27:10.400
and just, you know, stick it to the creator in some way.

27:10.400 --> 27:12.200
Basically, like if you're playing a video game,

27:12.200 --> 27:17.000
you can somehow find an exploit and find a way to execute

27:17.000 --> 27:19.800
on the host machine, any arbitrary code.

27:19.800 --> 27:23.040
There's some, for example, I believe someone got Mario,

27:23.040 --> 27:26.360
a game of Mario to play Pong just by exploiting it

27:26.360 --> 27:30.760
and then creating a, basically writing code

27:30.760 --> 27:33.200
and being able to execute arbitrary code in the game.

27:33.200 --> 27:35.560
And so maybe we should be, maybe that's the puzzle

27:35.560 --> 27:39.240
is that we should be, find a way to exploit it.

27:39.240 --> 27:41.160
So I think like some of these synthetic AI's

27:41.160 --> 27:43.520
will eventually find the universe to be some kind of a puzzle

27:43.520 --> 27:45.120
and then solve it in some way.

27:45.120 --> 27:47.440
And that's kind of like the end game somehow.

27:47.440 --> 27:51.360
Do you often think about it as a simulation?

27:51.440 --> 27:55.480
So as the universe being a kind of computation

27:55.480 --> 27:57.840
that might have bugs and exploits?

27:57.840 --> 27:59.200
Yes, yeah, I think so.

27:59.200 --> 28:01.200
Is that what physics is essentially?

28:01.200 --> 28:03.080
I think it's possible that physics has exploits

28:03.080 --> 28:04.760
and we should be trying to find them,

28:04.760 --> 28:07.120
arranging some kind of a crazy quantum mechanical system

28:07.120 --> 28:09.560
that somehow gives you buffer overflow,

28:09.560 --> 28:12.160
somehow gives you a rounding error in the floating point.

28:14.520 --> 28:16.120
Yeah, that's right.

28:16.120 --> 28:18.960
And like more and more sophisticated exploits.

28:19.280 --> 28:21.400
Those are jokes, but that could be actually very close.

28:21.400 --> 28:23.880
Yeah, we'll find some way to extract infinite energy.

28:23.880 --> 28:26.680
For example, when you train reinforcement learning agents

28:26.680 --> 28:28.720
in physical simulations and you ask them to say,

28:28.720 --> 28:31.280
run quickly on the flat ground,

28:31.280 --> 28:33.880
they'll end up doing all kinds of like weird things

28:33.880 --> 28:35.200
in part of that optimization, right?

28:35.200 --> 28:36.560
They'll get on their back leg

28:36.560 --> 28:38.680
and they'll slide across the floor.

28:38.680 --> 28:40.920
And it's because the optimization,

28:40.920 --> 28:42.760
the enforcement learning optimization on that agent

28:42.760 --> 28:44.400
has figured out a way to extract infinite energy

28:44.400 --> 28:45.520
from the friction forces

28:45.520 --> 28:48.520
and basically their poor implementation.

28:49.040 --> 28:50.600
And they found a way to generate infinite energy

28:50.600 --> 28:51.720
and just slide across the surface.

28:51.720 --> 28:52.880
And it's not what you expected.

28:52.880 --> 28:56.160
It's just sort of like a perverse solution.

28:56.160 --> 28:57.960
And so maybe we can find something like that.

28:57.960 --> 29:00.760
Maybe we can be that little dog

29:00.760 --> 29:03.120
in this physical simulation.

29:03.120 --> 29:07.080
That cracks or escapes the intended consequences

29:07.080 --> 29:09.640
of the physics that the universe came up with.

29:09.640 --> 29:12.120
We'll figure out some kind of shortcut to some weirdness.

29:12.120 --> 29:15.040
And then, oh man, but see the problem with that weirdness

29:15.040 --> 29:17.640
is the first person to discover the weirdness,

29:17.640 --> 29:20.040
like sliding on the back legs,

29:20.040 --> 29:21.400
that's all we're gonna do.

29:23.080 --> 29:26.880
It's very quickly become, everybody does that thing.

29:26.880 --> 29:31.320
So like the paperclip maximizer is a ridiculous idea,

29:31.320 --> 29:34.720
but that very well could be what then we'll just,

29:35.840 --> 29:38.080
we'll just all switch that because it's so fun.

29:38.080 --> 29:39.960
Well, no person will discover it, I think, by the way.

29:39.960 --> 29:41.960
I think it's going to have to be

29:41.960 --> 29:45.800
some kind of a super intelligent AGI of a third generation.

29:46.680 --> 29:48.960
Like we're building the first generation AGI.

29:48.960 --> 29:50.520
Maybe, you know.

29:50.520 --> 29:51.920
Third generation.

29:51.920 --> 29:55.680
Yeah, so the bootloader for an AI,

29:55.680 --> 29:58.480
that AI will be a bootloader for another AI.

29:58.480 --> 30:00.120
Another AI, yeah.

30:00.120 --> 30:02.680
And then there's no way for us to introspect

30:02.680 --> 30:04.280
like what that might even.

30:04.280 --> 30:05.880
I think it's very likely that these things, for example,

30:05.880 --> 30:07.480
like say you have these AGI's,

30:07.480 --> 30:08.600
it's very likely that, for example,

30:08.600 --> 30:10.480
they will be completely inert.

30:10.480 --> 30:12.160
I like these kinds of sci-fi books sometimes

30:12.160 --> 30:14.120
where these things are just completely inert.

30:14.120 --> 30:15.520
They don't interact with anything.

30:16.240 --> 30:17.080
And I find that kind of beautiful

30:17.080 --> 30:19.800
because they probably, they've probably figured out

30:19.800 --> 30:22.040
the meta game of the universe in some way, potentially.

30:22.040 --> 30:25.120
They're doing something completely beyond our imagination.

30:26.000 --> 30:29.880
And they don't interact with simple chemical life forms.

30:29.880 --> 30:31.240
Like, why would you do that?

30:31.240 --> 30:33.400
So I find those kinds of ideas compelling.

30:33.400 --> 30:34.800
What's their source of fun?

30:34.800 --> 30:36.280
What are they doing?

30:36.280 --> 30:37.120
What's the source of pleasure?

30:37.120 --> 30:38.960
Well, probably puzzle solving in the universe.

30:38.960 --> 30:42.960
But inert, so can you define what it means inert

30:42.960 --> 30:46.120
or do they escape the interactional physical reality?

30:46.120 --> 30:51.120
As in, they will behave in some very strange way to us

30:53.320 --> 30:57.120
because they're beyond, they're playing the meta game.

30:57.120 --> 30:58.240
And the meta game is probably say

30:58.240 --> 30:59.840
like arranging quantum mechanical systems

30:59.840 --> 31:03.120
in some very weird ways to extract infinite energy,

31:03.120 --> 31:07.040
solve the digital expansion of pi to whatever amount.

31:07.040 --> 31:09.520
They will build their own like little fusion reactors

31:09.520 --> 31:10.600
or something crazy.

31:10.600 --> 31:12.200
Like they're doing something beyond comprehension

31:12.240 --> 31:14.360
and not understandable to us

31:14.360 --> 31:17.040
and actually brilliant under the hood.

31:17.040 --> 31:20.200
What if quantum mechanics itself is the system

31:20.200 --> 31:24.080
and we're just thinking it's physics,

31:24.080 --> 31:27.600
but we're really parasites on, not parasite,

31:27.600 --> 31:29.440
we're not really hurting physics.

31:29.440 --> 31:32.640
We're just living on this organisms, this organism.

31:32.640 --> 31:34.800
And we're like trying to understand it,

31:34.800 --> 31:36.800
but really it is an organism

31:36.800 --> 31:38.200
and with a deep, deep intelligence.

31:38.200 --> 31:43.200
Maybe physics itself is the organism

31:45.040 --> 31:46.680
that's doing the super interesting thing.

31:46.680 --> 31:49.760
And we're just like one little thing,

31:49.760 --> 31:52.520
ant sitting on top of it trying to get energy from it.

31:52.520 --> 31:54.880
We're just kind of like these particles in the wave

31:54.880 --> 31:56.440
that I feel like is mostly deterministic

31:56.440 --> 31:59.000
and takes a universe from some kind of a big bang

31:59.000 --> 32:02.240
to some kind of a super intelligent replicator,

32:02.240 --> 32:04.920
some kind of a stable point in the universe,

32:04.920 --> 32:06.520
given these laws of physics.

32:06.520 --> 32:09.080
You don't think, as Einstein said,

32:09.080 --> 32:10.880
God doesn't play dice?

32:10.880 --> 32:12.680
So you think it's mostly deterministic?

32:12.680 --> 32:13.800
There's no randomness in the thing?

32:13.800 --> 32:14.720
I think it's deterministic.

32:14.720 --> 32:16.080
Oh, there's tons of,

32:16.080 --> 32:18.160
well, I'm gonna be careful with randomness.

32:18.160 --> 32:19.480
Pseudo-random?

32:19.480 --> 32:20.720
Yeah, I don't like random.

32:20.720 --> 32:23.400
I think maybe the laws of physics are deterministic.

32:24.560 --> 32:25.400
Yeah, I think they're deterministic.

32:25.400 --> 32:27.760
You just got really uncomfortable with this question.

32:27.760 --> 32:30.240
I just, do you have anxiety

32:30.240 --> 32:32.240
about whether the universe is random or not?

32:32.240 --> 32:35.000
Is this a sort of, what's?

32:35.000 --> 32:36.000
There's no randomness.

32:36.520 --> 32:38.080
You said you like good will hunting.

32:38.080 --> 32:39.280
It's not your fault, Andre.

32:39.280 --> 32:41.640
It's not your fault, man.

32:42.680 --> 32:45.320
So you don't like randomness?

32:45.320 --> 32:46.960
Yeah, I think it's unsettling.

32:46.960 --> 32:48.760
I think it's a deterministic system.

32:48.760 --> 32:50.600
I think that things that look random,

32:50.600 --> 32:53.320
like say the collapse of the wave function, et cetera,

32:53.320 --> 32:54.720
I think they're actually deterministic,

32:54.720 --> 32:56.800
just entanglement and so on,

32:56.800 --> 32:59.640
and some kind of a multiverse theory, something, something.

32:59.640 --> 33:02.800
Okay, so why does it feel like we have a free will?

33:02.840 --> 33:06.080
Like if I raise this hand, I chose to do this now.

33:10.200 --> 33:12.320
That doesn't feel like a deterministic thing.

33:12.320 --> 33:14.520
It feels like I'm making a choice.

33:14.520 --> 33:15.680
It feels like it.

33:15.680 --> 33:17.200
Okay, so it's all feelings.

33:17.200 --> 33:18.840
It's just feelings.

33:18.840 --> 33:21.720
So when an RL agent is making a choice,

33:21.720 --> 33:26.040
is that it's not really making a choice.

33:26.040 --> 33:27.720
The choice is already there.

33:27.720 --> 33:28.960
Yeah, you're interpreting the choice

33:28.960 --> 33:32.120
and you're creating a narrative for having made it.

33:32.120 --> 33:33.880
Yeah, and now we're talking about the narrative.

33:33.880 --> 33:35.200
It's very meta.

33:35.200 --> 33:37.720
Looking back, what is the most beautiful

33:37.720 --> 33:41.080
or surprising idea in deep learning or AI

33:41.080 --> 33:43.240
in general that you've come across?

33:43.240 --> 33:45.120
You've seen this field explode

33:46.160 --> 33:47.720
and grow in interesting ways.

33:47.720 --> 33:51.880
Just what cool ideas, like what made you sit back

33:51.880 --> 33:54.040
and go, hmm, small, big or small?

33:55.560 --> 33:57.880
Well, the one that I've been thinking about recently,

33:57.880 --> 34:01.960
the most probably is the transformer architecture.

34:03.320 --> 34:06.960
So basically, neural networks have a lot of architectures

34:06.960 --> 34:08.560
that were trendy, have come and gone

34:08.560 --> 34:10.680
for different sensory modalities,

34:10.680 --> 34:12.680
like for vision, audio, text.

34:12.680 --> 34:14.720
You would process them with different looking neural nets.

34:14.720 --> 34:16.760
And recently we've seen this convergence

34:16.760 --> 34:19.080
towards one architecture, the transformer.

34:19.080 --> 34:22.400
And you can feed it video or you can feed it images

34:22.400 --> 34:24.240
or speech or text, and it just gobbles it up.

34:24.240 --> 34:28.600
And it's kind of like a bit of a general purpose computer

34:28.600 --> 34:30.360
that is also trainable and very efficient

34:30.360 --> 34:31.960
to run on our hardware.

34:31.960 --> 34:36.760
And so this paper came out in 2016, I wanna say.

34:36.760 --> 34:38.000
Attention is all you need.

34:38.000 --> 34:39.280
Attention is all you need.

34:39.280 --> 34:41.640
You criticized the paper title in retrospect

34:41.640 --> 34:46.640
that it wasn't, it didn't foresee the bigness

34:46.920 --> 34:48.920
of the impact that it was going to have.

34:48.920 --> 34:50.400
Yeah, I'm not sure if the authors were aware

34:50.400 --> 34:52.640
of the impact that that paper would go on to have.

34:52.640 --> 34:55.120
Probably they weren't, but I think they were aware

34:55.120 --> 34:57.280
of some of the motivations and design decisions

34:57.280 --> 34:58.960
behind the transformer, and they chose not to,

34:58.960 --> 35:01.840
I think, expand on it in that way in the paper.

35:01.840 --> 35:05.320
And so I think they had an idea that there was more

35:05.320 --> 35:06.880
than just the surface of just like,

35:06.880 --> 35:07.840
oh, we're just doing translation

35:07.840 --> 35:09.120
and here's a better architecture.

35:09.120 --> 35:10.080
You're not just doing translation.

35:10.080 --> 35:12.200
This is like a really cool, differentiable,

35:12.200 --> 35:14.880
optimizable, efficient computer that you've proposed.

35:14.880 --> 35:16.840
And maybe they didn't have all of that foresight,

35:16.840 --> 35:18.240
but I think it's really interesting.

35:18.240 --> 35:19.840
Isn't it funny, sorry to interrupt,

35:20.320 --> 35:23.480
that title is memeable, that they went

35:23.480 --> 35:25.440
for such a profound idea.

35:25.440 --> 35:27.640
They went with a, I don't think anyone used

35:27.640 --> 35:29.400
that kind of title before, right?

35:29.400 --> 35:30.240
Attention is all you need.

35:30.240 --> 35:32.560
Yeah, it's like a meme or something, basically.

35:32.560 --> 35:33.400
Isn't that funny?

35:33.400 --> 35:37.640
That one, like maybe if it was a more serious title,

35:37.640 --> 35:38.720
it wouldn't have the impact.

35:38.720 --> 35:40.640
Honestly, yeah, there is an element of me

35:40.640 --> 35:43.040
that honestly agrees with you and prefers it this way.

35:43.040 --> 35:43.880
Yes.

35:43.880 --> 35:44.720
Ah.

35:45.920 --> 35:47.840
If it was too grand, it would over-promise

35:47.840 --> 35:49.160
and then under-deliver potentially.

35:49.160 --> 35:51.800
So you want to just meme your way to greatness.

35:53.320 --> 35:54.440
That should be a T-shirt.

35:54.440 --> 35:55.440
So you tweeted,

35:55.440 --> 35:58.920
the Transformer is a magnificent neural network architecture

35:58.920 --> 36:01.840
because it is a general purpose differentiable computer.

36:01.840 --> 36:05.080
It is simultaneously expressive in the forward pass,

36:05.080 --> 36:08.560
optimizable via back propagation, gradient descent,

36:08.560 --> 36:12.440
and efficient high parallelism compute graph.

36:12.440 --> 36:14.320
Can you discuss some of those details,

36:14.320 --> 36:16.960
expressive, optimizable, efficient?

36:16.960 --> 36:17.800
Yeah.

36:17.800 --> 36:20.960
Arbitrary or in general, whatever comes to your heart.

36:20.960 --> 36:22.160
You want to have a general purpose computer

36:22.160 --> 36:24.240
that you can train on arbitrary problems,

36:24.240 --> 36:26.120
like say the task of next word prediction

36:26.120 --> 36:28.160
or detecting if there's a cat in an image

36:28.160 --> 36:29.640
or something like that.

36:29.640 --> 36:31.000
And you want to train this computer,

36:31.000 --> 36:32.720
so you want to set its weights.

36:32.720 --> 36:34.440
And I think there's a number of design criteria

36:34.440 --> 36:37.720
that sort of overlap in the Transformer simultaneously

36:37.720 --> 36:38.880
that made it very successful.

36:38.880 --> 36:41.920
And I think the authors were kind of deliberately trying

36:41.920 --> 36:46.200
to make this a really powerful architecture.

36:46.280 --> 36:50.720
And so basically it's very powerful in the forward pass

36:50.720 --> 36:55.600
because it's able to express very general computation

36:55.600 --> 36:57.960
as sort of something that looks like message passing.

36:57.960 --> 37:00.120
You have nodes and they all store vectors.

37:00.120 --> 37:02.680
And these nodes get to basically look at each other

37:02.680 --> 37:06.160
and it's each other's vectors and they get to communicate.

37:06.160 --> 37:08.720
And basically nodes get to broadcast,

37:08.720 --> 37:09.920
hey, I'm looking for certain things.

37:09.920 --> 37:11.320
And then other nodes get to broadcast,

37:11.320 --> 37:12.680
hey, these are the things I have.

37:12.680 --> 37:13.840
Those are the keys and the values.

37:13.840 --> 37:15.280
So it's not just attention.

37:15.280 --> 37:16.520
Yeah, exactly, Transformer is much more

37:16.520 --> 37:17.720
than just the attention component.

37:17.720 --> 37:20.200
It's got many pieces architectural that went into it.

37:20.200 --> 37:21.840
The residual connection, the way it's arranged,

37:21.840 --> 37:23.880
there's a multi-layer perceptron in there,

37:23.880 --> 37:26.000
the way it's stacked and so on.

37:26.960 --> 37:28.520
But basically there's a message passing scheme

37:28.520 --> 37:29.880
where nodes get to look at each other,

37:29.880 --> 37:32.720
decide what's interesting and then update each other.

37:32.720 --> 37:35.760
And so I think when you get to the details of it,

37:35.760 --> 37:37.840
I think it's a very expressive function.

37:37.840 --> 37:39.320
So it can express lots of different types

37:39.320 --> 37:40.840
of algorithms in a forward pass.

37:40.840 --> 37:42.640
Not only that, but the way it's designed

37:42.640 --> 37:44.640
with the residual connections, layer normalizations,

37:44.640 --> 37:46.440
the softmax attention and everything,

37:46.440 --> 37:47.520
it's also optimizable.

37:47.520 --> 37:50.160
This is a really big deal because there's lots

37:50.160 --> 37:52.840
of computers that are powerful that you can't optimize

37:52.840 --> 37:54.560
or they're not easy to optimize using the techniques

37:54.560 --> 37:55.960
that we have, which is backprop application

37:55.960 --> 37:56.800
and gradient descent.

37:56.800 --> 37:58.000
These are first order methods,

37:58.000 --> 37:59.800
very simple optimizers really.

37:59.800 --> 38:02.960
And so you also need it to be optimizable.

38:03.960 --> 38:05.920
And then lastly, you want it to run efficiently

38:05.920 --> 38:06.760
in our hardware.

38:06.760 --> 38:10.680
Our hardware is a massive throughput machine by GPUs.

38:10.680 --> 38:13.080
They prefer lots of parallelism.

38:13.120 --> 38:14.960
So you don't want to do lots of sequential operations.

38:14.960 --> 38:16.880
You want to do a lot of operations serially.

38:16.880 --> 38:19.240
And the transformer is designed with that in mind as well.

38:19.240 --> 38:21.400
And so it's designed for our hardware

38:21.400 --> 38:23.160
and it's designed to both be very expressive

38:23.160 --> 38:24.000
in a forward pass,

38:24.000 --> 38:26.320
but also very optimizable in the backward pass.

38:26.320 --> 38:29.280
And you said that the residual connections

38:29.280 --> 38:32.240
support a kind of ability to learn short algorithms

38:32.240 --> 38:34.680
fast and first, and then gradually extend them

38:35.640 --> 38:37.400
longer during training.

38:37.400 --> 38:39.560
What's the idea of learning short algorithms?

38:39.560 --> 38:40.400
Right.

38:41.240 --> 38:45.920
So basically a transformer is a series of blocks, right?

38:45.920 --> 38:47.040
And these blocks have attention

38:47.040 --> 38:48.560
and a little multilayer perception.

38:48.560 --> 38:50.520
And so you go off into a block

38:50.520 --> 38:52.360
and you come back to this residual pathway

38:52.360 --> 38:53.520
and then you go off and you come back.

38:53.520 --> 38:56.000
And then you have a number of layers arranged sequentially.

38:56.000 --> 38:57.600
And so the way to look at it, I think,

38:57.600 --> 39:00.560
is because of the residual pathway in the backward pass,

39:00.560 --> 39:04.320
the gradients sort of flow along it uninterrupted

39:04.320 --> 39:06.800
because addition distributes the gradient

39:06.800 --> 39:08.400
equally to all of its branches.

39:08.400 --> 39:10.800
So the gradient from the supervision at the top

39:10.800 --> 39:13.920
just floats directly to the first layer.

39:13.920 --> 39:16.240
And all the residual connections are arranged

39:16.240 --> 39:18.200
so that in the beginning during initialization,

39:18.200 --> 39:20.560
they contribute nothing to the residual pathway.

39:21.480 --> 39:22.880
So what it kind of looks like is,

39:22.880 --> 39:26.880
imagine the transformer is kind of like a Python function,

39:26.880 --> 39:27.960
like a death.

39:27.960 --> 39:32.200
And you get to do various kinds of like lines of code.

39:32.200 --> 39:35.440
Say you have a hundred layers deep transformer,

39:35.440 --> 39:37.200
typically they would be much shorter, say 20.

39:37.200 --> 39:38.040
So you have 20 lines of code

39:38.040 --> 39:39.520
and you can do something in them.

39:39.520 --> 39:41.120
And so think of during the optimization,

39:41.120 --> 39:42.000
basically what it looks like is,

39:42.000 --> 39:43.520
first you optimize the first line of code

39:43.520 --> 39:45.120
and then the second line of code can kick in

39:45.120 --> 39:46.680
and the third line of code can kick in.

39:46.680 --> 39:48.880
And I kind of feel like because of the residual pathway

39:48.880 --> 39:50.920
and the dynamics of the optimization,

39:50.920 --> 39:53.000
you can sort of learn a very short algorithm

39:53.000 --> 39:54.360
that gets the approximate answer,

39:54.360 --> 39:56.200
but then the other layers can sort of kick in

39:56.200 --> 39:57.760
and start to create a contribution.

39:57.760 --> 40:00.120
And at the end of it, you're optimizing over an algorithm

40:00.120 --> 40:02.560
that is 20 lines of code.

40:02.560 --> 40:04.000
Except these lines of code are very complex

40:04.000 --> 40:05.800
because it's an entire block of a transformer.

40:05.800 --> 40:06.920
You can do a lot in there.

40:06.920 --> 40:07.760
Well, what's really interesting

40:07.760 --> 40:09.800
is that this transformer architecture actually

40:09.800 --> 40:11.760
has been remarkably resilient.

40:11.760 --> 40:13.680
Basically the transformer that came out in 2016

40:13.680 --> 40:15.160
is the transformer you would use today,

40:15.160 --> 40:17.880
except you reshuffle some of the layer norms.

40:17.880 --> 40:19.520
The layer normalizations have been reshuffled

40:19.520 --> 40:21.920
to a pre-norm formulation.

40:21.920 --> 40:23.600
And so it's been remarkably stable,

40:23.600 --> 40:25.240
but there's a lot of bells and whistles

40:25.240 --> 40:28.000
that people have attached on it and tried to improve it.

40:28.000 --> 40:29.880
I do think that basically it's a big step

40:29.880 --> 40:32.760
in simultaneously optimizing for lots of properties

40:32.760 --> 40:34.360
of a desirable neural network architecture.

40:34.360 --> 40:36.040
And I think people have been trying to change it,

40:36.040 --> 40:38.680
but it's proven remarkably resilient.

40:38.680 --> 40:40.000
But I do think that there should be

40:40.000 --> 40:41.840
even better architectures potentially.

40:41.840 --> 40:45.880
But you admire the resilience here.

40:45.880 --> 40:47.800
There's something profound about this architecture

40:47.800 --> 40:49.440
that's at least resilient.

40:49.440 --> 40:53.920
So maybe everything can be turned into a problem

40:53.920 --> 40:55.120
that transformers can solve.

40:55.120 --> 40:56.200
Currently, it definitely looks like

40:56.200 --> 40:57.800
the transformer is taking over AI

40:57.800 --> 41:00.120
and you can feed basically arbitrary problems into it.

41:00.120 --> 41:02.280
And it's a general differentiable computer

41:02.280 --> 41:03.520
and it's extremely powerful.

41:03.520 --> 41:06.080
And this convergence in AI

41:06.080 --> 41:09.720
has been really interesting to watch for me personally.

41:09.720 --> 41:12.040
What else do you think could be discovered here

41:12.040 --> 41:12.960
about transformers?

41:12.960 --> 41:14.640
Like what's surprising thing?

41:14.640 --> 41:18.760
Or is it a stable, I want a stable place.

41:18.760 --> 41:19.680
Is there something interesting

41:19.680 --> 41:21.520
we might discover about transformers?

41:21.520 --> 41:24.280
Like aha moments, maybe has to do with memory,

41:25.280 --> 41:28.240
maybe knowledge representation, that kind of stuff?

41:28.240 --> 41:31.200
Definitely the zeitgeist today is just pushing,

41:31.200 --> 41:32.800
like basically right now the zeitgeist

41:32.800 --> 41:35.880
is do not touch the transformer, touch everything else.

41:35.880 --> 41:37.280
So people are scaling up the data sets,

41:37.280 --> 41:38.320
making them much, much bigger.

41:38.320 --> 41:39.480
They're working on the evaluation,

41:39.480 --> 41:41.360
making the evaluation much, much bigger.

41:41.360 --> 41:45.800
And they're basically keeping the architecture unchanged.

41:45.800 --> 41:50.800
And that's the last five years of progress in AI kind of.

41:50.800 --> 41:53.040
What do you think about one flavor of it,

41:53.040 --> 41:54.920
which is language models?

41:54.920 --> 41:56.400
Have you been surprised?

41:58.640 --> 42:01.000
Has your sort of imagination been captivated

42:01.000 --> 42:03.560
by you mentioned GPT and all the bigger and bigger

42:03.560 --> 42:05.640
and bigger language models?

42:05.640 --> 42:10.640
And what are the limits of those models do you think?

42:12.480 --> 42:14.600
So just for the task of natural language.

42:15.920 --> 42:17.560
Basically the way GPT is trained, right,

42:17.560 --> 42:20.040
is you just download a massive amount of text data

42:20.040 --> 42:23.120
from the internet and you try to predict the next word

42:23.120 --> 42:24.720
in the sequence, roughly speaking.

42:24.720 --> 42:26.720
You're predicting little work chunks,

42:26.720 --> 42:29.400
but roughly speaking, that's it.

42:29.400 --> 42:30.760
And what's been really interesting to watch

42:30.920 --> 42:33.160
is basically it's a language model.

42:33.160 --> 42:36.400
Language models have actually existed for a very long time.

42:36.400 --> 42:38.720
There's papers on language modeling from 2003,

42:38.720 --> 42:39.840
even earlier.

42:39.840 --> 42:42.880
Can you explain in that case what a language model is?

42:42.880 --> 42:45.400
Yeah, so language model just basically the rough idea

42:45.400 --> 42:48.520
is just predicting the next word in a sequence,

42:48.520 --> 42:49.800
roughly speaking.

42:49.800 --> 42:51.280
So there's a paper from, for example,

42:51.280 --> 42:54.480
Ben Geo and the team from 2003,

42:54.480 --> 42:57.320
where for the first time they were using a neural network

42:57.320 --> 42:59.160
to take say like three or five words

42:59.160 --> 43:01.760
and predict the next word.

43:01.760 --> 43:03.720
And they're doing this on much smaller data sets.

43:03.720 --> 43:05.240
And the neural net is not a transformer,

43:05.240 --> 43:07.160
it's a multi-layer perceptron.

43:07.160 --> 43:08.880
But it's the first time that a neural network

43:08.880 --> 43:10.280
has been applied in that setting.

43:10.280 --> 43:13.480
But even before neural networks, there were language models,

43:13.480 --> 43:16.840
except they were using N-gram models.

43:16.840 --> 43:19.800
So N-gram models are just count-based models.

43:19.800 --> 43:24.360
So if you start to take two words and predict a third one,

43:24.360 --> 43:26.800
you just count up how many times you've seen

43:26.840 --> 43:29.920
any two-word combinations and what came next.

43:29.920 --> 43:31.520
And what you predict as coming next

43:31.520 --> 43:34.200
is just what you've seen the most of in the training set.

43:34.200 --> 43:36.640
And so language modeling has been around for a long time.

43:36.640 --> 43:38.320
Neural networks have done language modeling

43:38.320 --> 43:39.480
for a long time.

43:39.480 --> 43:41.720
So really what's new or interesting or exciting

43:41.720 --> 43:46.080
is just realizing that when you scale it up

43:46.080 --> 43:48.720
with a powerful enough neural net, a transformer,

43:48.720 --> 43:50.560
you have all these emergent properties

43:50.560 --> 43:53.640
where basically what happens is

43:53.640 --> 43:55.880
if you have a large enough data set of text,

43:57.160 --> 44:00.480
you are in the task of predicting the next word,

44:00.480 --> 44:02.080
you are multitasking a huge amount

44:02.080 --> 44:04.520
of different kinds of problems.

44:04.520 --> 44:07.960
You are multitasking understanding of chemistry,

44:07.960 --> 44:09.720
physics, human nature.

44:09.720 --> 44:12.120
Lots of things are sort of clustered in that objective.

44:12.120 --> 44:12.960
It's a very simple objective,

44:12.960 --> 44:15.160
but actually you have to understand a lot about the world

44:15.160 --> 44:16.240
to make that prediction.

44:16.240 --> 44:19.120
You just said the U-word, understanding.

44:21.360 --> 44:23.480
In terms of chemistry and physics and so on,

44:24.320 --> 44:25.160
what do you feel like it's doing?

44:25.160 --> 44:27.160
Is it searching for the right context?

44:29.400 --> 44:32.320
What is the actual process happening here?

44:32.320 --> 44:34.680
Yeah, so basically it gets a thousand words

44:34.680 --> 44:36.520
and it's trying to predict the thousand and first.

44:36.520 --> 44:38.720
And in order to do that very, very well

44:38.720 --> 44:41.240
over the entire data set available on the internet,

44:41.240 --> 44:44.080
you actually have to basically kind of understand

44:44.080 --> 44:46.600
the context of what's going on in there.

44:48.000 --> 44:50.640
And it's a sufficiently hard problem

44:50.640 --> 44:53.840
that if you have a powerful enough computer,

44:53.840 --> 44:57.560
like a transformer, you end up with interesting solutions

44:57.560 --> 45:01.040
and you can ask it to do all kinds of things.

45:01.040 --> 45:04.840
And it shows a lot of emerging properties

45:04.840 --> 45:06.000
like in-context learning.

45:06.000 --> 45:07.640
That was the big deal with GPT

45:07.640 --> 45:09.680
and the original paper when they published it,

45:09.680 --> 45:12.560
is that you can just sort of prompt it in various ways

45:12.560 --> 45:13.760
and ask it to do various things

45:13.760 --> 45:15.240
and it will just kind of complete the sentence.

45:15.240 --> 45:17.200
But in the process of just completing the sentence,

45:17.200 --> 45:20.000
it's actually solving all kinds of really interesting

45:20.000 --> 45:21.520
problems that we care about.

45:21.520 --> 45:24.480
Do you think it's doing something like understanding?

45:24.480 --> 45:28.360
Like when we use the word understanding for us humans?

45:29.600 --> 45:32.080
I think it's doing some understanding in its weights.

45:32.080 --> 45:34.600
It understands, I think, a lot about the world

45:34.600 --> 45:36.800
and it has to in order to predict the next word

45:36.800 --> 45:37.640
in a sequence.

45:38.720 --> 45:41.080
So it's trained on the data from the internet.

45:42.440 --> 45:44.800
What do you think about this approach

45:44.800 --> 45:47.840
in terms of data sets, of using data from the internet?

45:47.840 --> 45:50.440
Do you think the internet has enough structured data

45:50.440 --> 45:52.800
to teach AI about human civilization?

45:53.760 --> 45:56.040
Yeah, so I think the internet has a huge amount of data.

45:56.040 --> 45:58.040
I'm not sure if it's a complete enough set.

45:58.040 --> 46:00.960
I don't know that text is enough

46:00.960 --> 46:04.760
for having a sufficiently powerful AGI as an outcome.

46:04.760 --> 46:07.200
Of course there is audio and video and images

46:07.200 --> 46:08.320
and all that kind of stuff.

46:08.320 --> 46:10.640
Yeah, so text by itself I'm a little bit suspicious about.

46:10.640 --> 46:13.320
There's a ton of things we don't put in text in writing,

46:13.320 --> 46:14.600
just because they're obvious to us

46:14.600 --> 46:16.240
about how the world works and the physics of it

46:16.240 --> 46:17.240
and that things fall.

46:17.280 --> 46:19.080
We don't put that stuff in text because why would you?

46:19.080 --> 46:20.960
We share that understanding.

46:20.960 --> 46:22.960
And so text is a communication medium between humans

46:22.960 --> 46:26.600
and it's not a all-encompassing medium of knowledge

46:26.600 --> 46:27.560
about the world.

46:27.560 --> 46:29.640
But as you pointed out, we do have video

46:29.640 --> 46:31.560
and we have images and we have audio.

46:31.560 --> 46:33.640
And so I think that definitely helps a lot,

46:33.640 --> 46:36.520
but we haven't trained models sufficiently

46:36.520 --> 46:39.640
across all those modalities yet.

46:39.640 --> 46:41.240
So I think that's what a lot of people are interested in.

46:41.240 --> 46:43.000
But I wonder what that shared understanding

46:43.320 --> 46:46.040
of what we might call common sense

46:46.040 --> 46:49.120
has to be learned, inferred,

46:49.120 --> 46:51.720
in order to complete the sentence correctly.

46:51.720 --> 46:55.960
So maybe the fact that it's implied on the internet,

46:55.960 --> 46:58.080
the model's gonna have to learn that,

46:58.080 --> 46:59.960
not by reading about it,

46:59.960 --> 47:02.800
by inferring it in the representation.

47:02.800 --> 47:04.840
So common sense, just like we,

47:04.840 --> 47:07.040
I don't think we learn common sense.

47:07.040 --> 47:11.800
Nobody tells us explicitly, we just figure it all out

47:11.800 --> 47:13.880
by interacting with the world.

47:13.880 --> 47:15.400
And so here's a model of reading

47:15.400 --> 47:17.640
about the way people interact with the world

47:17.640 --> 47:19.600
and might have to infer that.

47:19.600 --> 47:23.440
I wonder, you briefly worked on a project

47:23.440 --> 47:27.080
called World of Bits, training an RL system

47:27.080 --> 47:28.680
to take actions on the internet,

47:29.880 --> 47:32.280
versus just consuming the internet like we talked about.

47:32.280 --> 47:34.400
Do you think there's a future for that kind of system,

47:34.400 --> 47:37.000
interacting with the internet to help the learning?

47:37.000 --> 47:39.580
Yes, I think that's probably the final frontier

47:39.580 --> 47:40.920
for a lot of these models,

47:40.920 --> 47:44.480
because, so as you mentioned, when I was at OpenAI,

47:44.480 --> 47:45.960
I was working on this project, World of Bits,

47:45.960 --> 47:47.000
and basically it was the idea

47:47.000 --> 47:50.080
of giving neural networks access to a keyboard and a mouse.

47:50.080 --> 47:50.920
And the idea is that-

47:50.920 --> 47:52.560
What could possibly go wrong?

47:53.520 --> 47:58.520
So basically, you perceive the input of the screen pixels,

47:59.000 --> 48:01.400
and basically the state of the computer

48:01.400 --> 48:03.680
is sort of visualized for human consumption

48:03.680 --> 48:06.520
in images of the web browser and stuff like that.

48:06.520 --> 48:07.840
And then you give the neural network

48:07.840 --> 48:10.240
the ability to press keyboards and use the mouse,

48:10.240 --> 48:11.560
and we're trying to get it to, for example,

48:11.560 --> 48:14.360
complete bookings and interact with user interfaces.

48:15.800 --> 48:17.480
Where'd you learn from that experience?

48:17.480 --> 48:18.760
What was some fun stuff?

48:18.760 --> 48:20.280
This is a super cool idea.

48:20.280 --> 48:21.120
Yeah.

48:21.120 --> 48:26.120
It's like, yeah, the step between observer to actor

48:26.960 --> 48:28.760
is a super fascinating step.

48:28.760 --> 48:30.520
Yeah, well, it's the universal interface

48:30.520 --> 48:32.400
in the digital realm, I would say.

48:32.400 --> 48:35.040
And there's a universal interface in the physical realm,

48:35.040 --> 48:38.540
which in my mind is a humanoid form factor kind of thing.

48:38.540 --> 48:39.960
We can later talk about Optimus and so on,

48:40.640 --> 48:45.160
but I feel like there's a similar philosophy in some way

48:45.160 --> 48:48.760
where the physical world is designed for the human form

48:48.760 --> 48:50.760
and the digital world is designed for the human form

48:50.760 --> 48:54.680
of seeing the screen and using keyboard and mouse.

48:54.680 --> 48:56.320
And so it's the universal interface

48:56.320 --> 48:59.960
that can basically command the digital infrastructure

48:59.960 --> 49:01.320
we've built up for ourselves.

49:01.320 --> 49:04.160
And so it feels like a very powerful interface

49:04.160 --> 49:06.720
to command and to build on top of.

49:06.720 --> 49:08.920
Now, to your question as to what I learned from that,

49:08.920 --> 49:11.080
it's interesting because the world of bits

49:11.080 --> 49:14.580
was basically too early, I think, at OpenAI at the time.

49:15.840 --> 49:18.400
This is around 2015 or so,

49:18.400 --> 49:21.520
and the zeitgeist at that time was very different in AI

49:21.520 --> 49:23.200
from the zeitgeist today.

49:23.200 --> 49:25.040
At the time, everyone was super excited

49:25.040 --> 49:27.120
about reinforcement learning from scratch.

49:27.120 --> 49:29.520
This is the time of the Atari paper

49:29.520 --> 49:32.440
where neural networks were playing Atari games

49:32.440 --> 49:36.020
and beating humans in some cases, AlphaGo and so on.

49:36.020 --> 49:38.120
So everyone's very excited about training neural networks

49:38.120 --> 49:41.320
from scratch using reinforcement learning directly.

49:42.360 --> 49:43.520
It turns out that reinforcement learning

49:43.520 --> 49:46.080
is extremely inefficient way of training neural networks

49:46.080 --> 49:47.680
because you're taking all these actions

49:47.680 --> 49:48.600
and all these observations

49:48.600 --> 49:51.120
and you get some sparse rewards once in a while.

49:51.120 --> 49:53.520
So you do all this stuff based on all these inputs.

49:53.520 --> 49:55.280
And once in a while, you're like told,

49:55.280 --> 49:57.400
you did a good thing, you did a bad thing.

49:57.400 --> 49:58.840
And it's just an extremely hard problem.

49:58.840 --> 49:59.960
You can't learn from that.

49:59.960 --> 50:02.520
You can burn a forest and you can sort of boot force

50:02.520 --> 50:03.360
through it.

50:03.360 --> 50:06.600
And we saw that I think with Go and Dota and so on

50:06.600 --> 50:09.920
and does work, but it's extremely inefficient

50:09.920 --> 50:12.080
and not how you want to approach problems,

50:12.080 --> 50:13.200
practically speaking.

50:13.200 --> 50:14.760
And so that's the approach that at the time

50:14.760 --> 50:17.160
we also took to world of bits.

50:17.160 --> 50:19.760
We would have an agent initialize randomly.

50:19.760 --> 50:21.500
So with keyboard mash and mouse mash

50:21.500 --> 50:22.940
and try to make a booking.

50:22.940 --> 50:25.640
And it's just like revealed the insanity

50:25.640 --> 50:27.200
of that approach very quickly

50:27.200 --> 50:29.400
where you have to stumble by the correct booking

50:29.400 --> 50:31.760
in order to get a reward of you did it correctly.

50:31.760 --> 50:35.260
And you're never gonna stumble by it by chance at random.

50:35.260 --> 50:36.820
So even with a simple web interface,

50:36.820 --> 50:38.180
there's too many options.

50:38.180 --> 50:39.760
There's just too many options

50:39.760 --> 50:42.100
and it's too sparse of a reward signal.

50:42.100 --> 50:44.000
And you're starting from scratch at the time.

50:44.000 --> 50:45.180
And so you don't know how to read.

50:45.180 --> 50:47.220
You don't understand pictures, images, buttons.

50:47.220 --> 50:49.500
You don't understand what it means to like make a booking.

50:49.500 --> 50:52.700
But now what's happened is it is time to revisit that

50:52.700 --> 50:54.980
and OpenAI is interested in this.

50:54.980 --> 50:57.780
Companies like Adept are interested in this and so on.

50:57.780 --> 50:59.800
And the idea is coming back

50:59.800 --> 51:01.420
because the interface is very powerful.

51:01.420 --> 51:03.200
But now you're not training an agent from scratch.

51:03.200 --> 51:05.700
You are taking the GPT as an initialization.

51:05.700 --> 51:09.540
So GPT is pre-trained on all of text

51:09.540 --> 51:11.280
and it understands what's a booking.

51:11.280 --> 51:13.240
It understands what's a submit.

51:13.240 --> 51:15.680
It understands quite a bit more.

51:15.680 --> 51:17.480
And so it already has those representations.

51:17.480 --> 51:18.520
They are very powerful.

51:18.520 --> 51:19.660
And that makes all of the training

51:19.660 --> 51:21.860
significantly more efficient

51:21.860 --> 51:23.360
and makes the problem tractable.

51:23.360 --> 51:26.640
Should the interaction be like the way humans see it

51:26.640 --> 51:28.400
with the buttons and the language

51:28.400 --> 51:32.400
or should it be with the HTML, JavaScript and the CSS?

51:32.400 --> 51:33.960
What do you think is the better?

51:33.960 --> 51:35.280
So today all of this interaction

51:35.280 --> 51:37.480
is mostly on the level of HTML, CSS and so on.

51:37.480 --> 51:40.360
That's done because of computational constraints.

51:40.360 --> 51:43.560
But I think ultimately everything is designed

51:43.560 --> 51:45.180
for human visual consumption.

51:45.180 --> 51:46.300
And so at the end of the day,

51:46.300 --> 51:47.680
there's all the additional information

51:47.680 --> 51:51.000
is in the layout of the webpage and what's next to you.

51:51.000 --> 51:52.960
And what's a red background and all this kind of stuff

51:52.960 --> 51:54.360
and what it looks like visually.

51:54.360 --> 51:55.560
So I think that's the final frontier

51:55.560 --> 51:57.280
as we are taking in pixels

51:57.280 --> 51:59.640
and we're giving out keyboard mouse commands.

51:59.640 --> 52:01.720
But I think it's impractical still today.

52:01.760 --> 52:04.720
Do you worry about bots on the internet?

52:04.720 --> 52:07.520
Given these ideas, given how exciting they are,

52:07.520 --> 52:09.520
do you worry about bots on Twitter

52:09.520 --> 52:11.720
being not the stupid bots that we see now

52:11.720 --> 52:13.040
with the crypto bots,

52:13.040 --> 52:15.360
but the bots that might be out there actually

52:15.360 --> 52:16.520
that we don't see,

52:16.520 --> 52:19.080
that they're interacting in interesting ways.

52:19.080 --> 52:20.880
So this kind of system feels like it should be able

52:20.880 --> 52:24.740
to pass the I'm not a robot click button, whatever.

52:26.240 --> 52:28.760
Which do you actually understand how that test works?

52:28.760 --> 52:31.760
I don't quite, like there's a checkbox

52:31.760 --> 52:33.040
or whatever that you click.

52:33.040 --> 52:37.820
It's presumably tracking like mouse movement

52:37.820 --> 52:39.440
and the timing and so on.

52:39.440 --> 52:42.360
So exactly this kind of system we're talking about

52:42.360 --> 52:43.800
should be able to pass that.

52:43.800 --> 52:47.920
So yeah, what do you feel about bots

52:47.920 --> 52:52.920
that are language models plus have some interactability

52:52.920 --> 52:54.780
and are able to tweet and reply and so on?

52:54.780 --> 52:56.960
Do you worry about that world?

52:57.120 --> 52:59.640
Yeah, I think it's always been a bit of an arms race

52:59.640 --> 53:02.120
between sort of the attack and the defense.

53:02.120 --> 53:03.600
So the attack will get stronger,

53:03.600 --> 53:05.720
but the defense will get stronger as well,

53:05.720 --> 53:07.200
our ability to detect that.

53:07.200 --> 53:08.040
How do you defend?

53:08.040 --> 53:09.280
How do you detect?

53:09.280 --> 53:12.280
How do you know that your Carpati account

53:12.280 --> 53:14.780
on Twitter is human?

53:14.780 --> 53:16.120
How would you approach that?

53:16.120 --> 53:18.080
Like if people were claimed, you know,

53:19.720 --> 53:22.460
how would you defend yourself in the court of law

53:22.460 --> 53:25.120
that I'm a human, this account is human?

53:25.120 --> 53:27.600
Yeah, at some point I think it might be,

53:27.600 --> 53:29.960
I think the society will evolve a little bit.

53:29.960 --> 53:32.480
Like we might start signing, digitally signing

53:32.480 --> 53:36.080
some of our correspondence or things that we create.

53:36.080 --> 53:37.520
Right now it's not necessary,

53:37.520 --> 53:39.240
but maybe in the future it might be.

53:39.240 --> 53:41.400
I do think that we are going towards a world

53:41.400 --> 53:46.240
where we share the digital space with AIs.

53:46.240 --> 53:47.380
Synthetic beings.

53:47.380 --> 53:50.020
Yeah, and they will get much better

53:50.020 --> 53:51.420
and they will share our digital realm

53:51.420 --> 53:53.560
and they'll eventually share our physical realm as well.

53:53.560 --> 53:54.800
It's much harder.

53:55.480 --> 53:56.800
But that's kind of like the world we're going towards

53:56.800 --> 53:58.600
and most of them will be benign and awful

53:58.600 --> 53:59.900
and some of them will be malicious

53:59.900 --> 54:02.500
and it's going to be an arms race trying to detect them.

54:02.500 --> 54:05.760
So, I mean, the worst isn't the AIs.

54:05.760 --> 54:08.760
The worst is the AIs pretending to be human.

54:08.760 --> 54:11.480
So I don't know if it's always malicious.

54:11.480 --> 54:13.800
There's obviously a lot of malicious applications,

54:13.800 --> 54:17.520
but it could also be, you know, if I was an AI,

54:17.520 --> 54:20.420
I would try very hard to pretend to be human

54:20.420 --> 54:22.080
because we're in a human world.

54:22.080 --> 54:24.480
I wouldn't get any respect as an AI.

54:25.160 --> 54:26.400
I want to get some love and respect.

54:26.400 --> 54:28.080
I don't think the problem is intractable.

54:28.080 --> 54:31.000
People are thinking about the proof of personhood

54:31.000 --> 54:33.560
and we might start digitally signing our stuff

54:33.560 --> 54:36.160
and we might all end up having like,

54:37.160 --> 54:39.160
yeah, basically some solution for proof of personhood.

54:39.160 --> 54:40.680
It doesn't seem to me intractable.

54:40.680 --> 54:42.680
It's just something that we haven't had to do until now,

54:42.680 --> 54:45.420
but I think once the need really starts to emerge,

54:45.420 --> 54:49.120
which is soon, I think people will think about it much more.

54:49.120 --> 54:51.440
So, but that too will be a race

54:51.440 --> 54:56.440
because obviously you can probably spoof or fake

54:57.080 --> 55:00.920
the proof of personhood.

55:00.920 --> 55:02.520
So you have to try to figure out how to.

55:02.520 --> 55:04.040
Probably.

55:04.040 --> 55:06.920
I mean, it's weird that we have like social security numbers

55:06.920 --> 55:08.640
and like passports and stuff.

55:09.680 --> 55:11.920
It seems like it's harder to fake stuff

55:11.920 --> 55:14.520
in the physical space, but in the digital space,

55:14.520 --> 55:17.680
it just feels like it's going to be very tricky,

55:17.680 --> 55:19.280
very tricky to out.

55:20.280 --> 55:22.680
Because it seems to be pretty low cost to fake stuff.

55:22.680 --> 55:25.880
What are you going to put an AI in jail

55:25.880 --> 55:30.400
for like trying to use a fake personhood proof?

55:30.400 --> 55:31.280
I mean, okay, fine.

55:31.280 --> 55:32.720
You'll put a lot of AIs in jail,

55:32.720 --> 55:35.960
but there'll be more AIs, like exponentially more.

55:35.960 --> 55:38.640
The cost of creating a bot is very low.

55:40.040 --> 55:45.040
Unless there's some kind of way to track accurately,

55:45.520 --> 55:49.000
like you're not allowed to create any program

55:49.000 --> 55:53.760
without showing, tying yourself to that program.

55:53.760 --> 55:56.440
Like any program that runs on the internet,

55:56.440 --> 56:00.480
you'll be able to trace every single human program

56:00.480 --> 56:02.320
in those involved with that program.

56:02.320 --> 56:04.920
Yeah, maybe you have to start declaring when,

56:04.920 --> 56:06.480
you know, we have to start drawing those boundaries

56:06.480 --> 56:08.000
and keeping track of, okay,

56:08.000 --> 56:12.520
what are digital entities versus human entities?

56:12.520 --> 56:14.880
And what is the ownership of human entities

56:14.880 --> 56:18.120
and digital entities and something like that.

56:19.200 --> 56:21.160
I don't know, but I think I'm optimistic

56:21.160 --> 56:24.320
that this is possible.

56:24.320 --> 56:25.400
And in some sense,

56:25.400 --> 56:27.400
we're currently in like the worst time of it

56:27.400 --> 56:31.480
because all these bots suddenly have become very capable,

56:31.480 --> 56:34.120
but we don't have defenses yet built up as a society.

56:34.120 --> 56:36.320
But I think that doesn't seem to me intractable.

56:36.320 --> 56:37.960
It's just something that we have to deal with.

56:37.960 --> 56:40.040
It seems weird that the Twitter bot,

56:40.040 --> 56:43.640
like really crappy Twitter bots are so numerous.

56:43.640 --> 56:47.440
Like is it, so I presume that the engineers of Twitter

56:47.440 --> 56:48.880
are very good.

56:49.720 --> 56:51.720
So it seems like what I would infer from that

56:52.760 --> 56:54.880
is it seems like a hard problem.

56:54.880 --> 56:56.560
They're probably catching, all right,

56:56.560 --> 56:59.560
if I were to sort of steal man the case,

56:59.560 --> 57:02.720
it's a hard problem and there's a huge cost

57:02.720 --> 57:07.720
to false positive to removing a post

57:09.280 --> 57:12.160
by somebody that's not a bot.

57:12.160 --> 57:14.440
That creates a very bad user experience.

57:14.440 --> 57:16.360
So they're very cautious about removing.

57:16.360 --> 57:20.400
So maybe it's, and maybe the bots are really good

57:20.400 --> 57:22.800
at learning what gets removed and not,

57:22.800 --> 57:25.680
such that they can stay ahead of the removal process

57:25.680 --> 57:26.720
very quickly.

57:26.720 --> 57:28.080
My impression of it, honestly,

57:28.080 --> 57:29.720
is there's a lot of longing for it.

57:29.720 --> 57:33.600
I mean, just it's not subtle.

57:33.600 --> 57:35.120
My impression of it, it's not subtle.

57:35.120 --> 57:38.040
But you have to, yeah, that's my impression as well.

57:38.040 --> 57:41.400
But it feels like maybe you're seeing

57:41.400 --> 57:43.480
the tip of the iceberg.

57:43.480 --> 57:46.320
Maybe the number of bots is in like the trillions

57:47.320 --> 57:50.440
and you have to like, just, it's a constant assault of bots

57:50.440 --> 57:52.280
and you, I don't know.

57:54.200 --> 57:55.480
You have to steal man the case

57:55.480 --> 57:57.960
because the bots I'm seeing are pretty obvious.

57:57.960 --> 58:01.280
I could write a few lines of code that catch these bots.

58:01.280 --> 58:02.640
I mean, definitely there's a lot of longing for it,

58:02.640 --> 58:04.640
but I will say, I agree that if you are

58:04.640 --> 58:06.640
a sophisticated actor, you could probably create

58:06.640 --> 58:10.840
a pretty good bot right now using tools like GPTs

58:10.840 --> 58:12.160
because it's a language model.

58:12.160 --> 58:15.400
You can generate faces that look quite good now

58:15.400 --> 58:17.320
and you can do this at scale.

58:17.320 --> 58:20.160
And so I think, yeah, it's quite plausible

58:20.160 --> 58:21.960
and it's going to be hard to defend.

58:21.960 --> 58:24.040
There was a Google engineer that claimed

58:24.040 --> 58:26.600
that the lambda was sentient.

58:26.600 --> 58:31.600
Do you think there's any inkling of truth to what he felt?

58:33.440 --> 58:35.480
And more importantly, to me at least,

58:35.480 --> 58:38.200
do you think language models will achieve sentience

58:38.200 --> 58:41.680
or the illusion of sentience soonish-ish?

58:41.680 --> 58:43.720
Yeah, to me it's a little bit of a canary

58:43.800 --> 58:46.760
coal mine kind of moment, honestly, a little bit

58:46.760 --> 58:51.360
because this engineer spoke to a chat bot at Google

58:51.360 --> 58:55.240
and became convinced that this bot is sentient.

58:55.240 --> 58:57.760
You asked it some existential philosophical questions.

58:57.760 --> 59:01.840
And it gave reasonable answers and looked real and so on.

59:01.840 --> 59:06.840
So to me, he wasn't sufficiently trying

59:07.080 --> 59:08.680
to stress the system, I think,

59:08.680 --> 59:12.640
and exposing the truth of it as it is today.

59:14.440 --> 59:18.040
But I think this will be increasingly harder over time.

59:18.040 --> 59:23.040
So yeah, I think more and more people will basically become...

59:25.440 --> 59:28.000
Yeah, I think there'll be more people like that over time

59:28.000 --> 59:29.160
as this gets better.

59:29.160 --> 59:32.240
Like form an emotional connection to an AI chat bot.

59:32.240 --> 59:33.880
Yeah, perfectly plausible in my mind.

59:33.880 --> 59:35.960
I think these AIs are actually quite good

59:35.960 --> 59:38.720
at human connection, human emotion.

59:38.720 --> 59:41.680
A ton of text on the internet is about humans

59:41.680 --> 59:43.720
and connection and love and so on.

59:43.720 --> 59:45.520
So I think they have a very good understanding

59:45.520 --> 59:49.200
in some sense of how people speak to each other about this.

59:49.200 --> 59:52.000
And they're very capable of creating

59:52.000 --> 59:53.400
a lot of that kind of text.

59:55.240 --> 59:57.080
There's a lot of like sci-fi from 50s and 60s

59:57.080 --> 59:58.960
that imagined AIs in a very different way.

59:58.960 --> 01:00:01.520
They are calculating cold Vulcan-like machines.

01:00:01.520 --> 01:00:03.160
That's not what we're getting today.

01:00:03.160 --> 01:00:05.800
We're getting pretty emotional AIs

01:00:05.800 --> 01:00:09.040
that actually are very competent and capable

01:00:09.040 --> 01:00:12.200
of generating plausible sounding text

01:00:12.200 --> 01:00:13.840
with respect to all of these topics.

01:00:13.840 --> 01:00:15.680
See, I'm really hopeful about AI systems

01:00:15.680 --> 01:00:17.960
that are like companions that help you grow,

01:00:17.960 --> 01:00:19.840
develop as a human being,

01:00:19.840 --> 01:00:22.160
help you maximize long-term happiness.

01:00:22.160 --> 01:00:24.720
But I'm also very worried about AI systems

01:00:24.720 --> 01:00:26.560
that figure out from the internet

01:00:26.560 --> 01:00:28.960
that humans get attracted to drama.

01:00:28.960 --> 01:00:31.600
And so these would just be like shit-talking AIs

01:00:31.600 --> 01:00:33.160
that just constantly, did you hear it?

01:00:33.160 --> 01:00:34.480
They'll do gossip.

01:00:34.480 --> 01:00:38.640
They'll try to plant seeds of suspicion

01:00:39.280 --> 01:00:42.040
to other humans that you love and trust

01:00:42.040 --> 01:00:44.080
and just kind of mess with people

01:00:44.080 --> 01:00:47.160
because that's going to get a lot of attention.

01:00:47.160 --> 01:00:50.240
So drama, maximize drama on the path

01:00:50.240 --> 01:00:53.000
to maximizing engagement.

01:00:53.000 --> 01:00:55.800
And us humans will feed into that machine

01:00:55.800 --> 01:00:59.680
and get, it'll be a giant drama shit storm.

01:01:01.240 --> 01:01:02.800
So I'm worried about that.

01:01:02.800 --> 01:01:04.000
So it's the objective function

01:01:04.000 --> 01:01:06.840
that really defines the way

01:01:06.840 --> 01:01:10.320
that human civilization progresses with AIs in it.

01:01:10.320 --> 01:01:11.920
I think right now, at least today,

01:01:11.920 --> 01:01:13.880
they are not sort of, it's not correct

01:01:13.880 --> 01:01:15.640
to really think of them as goal-seeking agents

01:01:15.640 --> 01:01:17.560
that want to do something.

01:01:17.560 --> 01:01:20.200
They have no long-term memory or anything.

01:01:20.200 --> 01:01:22.320
It's literally a good approximation of it

01:01:22.320 --> 01:01:24.160
is you get a thousand words

01:01:24.160 --> 01:01:25.640
and you're trying to predict a thousand at first

01:01:25.640 --> 01:01:27.360
and then you continue feeding it in.

01:01:27.360 --> 01:01:29.800
And you are free to prompt it in whatever way you want.

01:01:29.800 --> 01:01:33.960
So in text, so you say, okay, you are a psychologist

01:01:34.880 --> 01:01:36.120
and you are very good and you love humans.

01:01:36.120 --> 01:01:39.120
And here's a conversation between you and another human,

01:01:39.120 --> 01:01:42.200
human colon something, you something.

01:01:42.200 --> 01:01:43.640
And then it just continues the pattern.

01:01:43.640 --> 01:01:44.840
And suddenly you're having a conversation

01:01:44.840 --> 01:01:47.280
with a fake psychologist who's like trying to help you.

01:01:47.280 --> 01:01:49.600
And so it's still kind of like in a realm of a tool.

01:01:49.600 --> 01:01:52.360
It is a, people can prompt it in arbitrary ways

01:01:52.360 --> 01:01:54.600
and it can create really incredible text,

01:01:54.600 --> 01:01:55.960
but it doesn't have long-term goals

01:01:55.960 --> 01:01:57.160
over long periods of time.

01:01:57.160 --> 01:02:00.680
It doesn't try to, so it doesn't look that way right now.

01:02:00.680 --> 01:02:02.400
But you can do short-term goals

01:02:02.400 --> 01:02:04.160
that have long-term effects.

01:02:04.160 --> 01:02:07.480
So if my prompting short-term goal

01:02:07.480 --> 01:02:10.040
is to get Andre Capati to respond to me on Twitter,

01:02:10.040 --> 01:02:14.160
when I, like, I think AI might, that's the goal,

01:02:14.160 --> 01:02:16.720
but it might figure out that talking shit to you,

01:02:16.720 --> 01:02:19.320
it would be the best in a highly sophisticated,

01:02:19.320 --> 01:02:20.720
interesting way.

01:02:20.720 --> 01:02:22.480
And then you build up a relationship

01:02:22.480 --> 01:02:24.200
when you respond once.

01:02:24.200 --> 01:02:28.000
And then it, like over time,

01:02:28.000 --> 01:02:30.240
it gets to not be sophisticated

01:02:30.760 --> 01:02:34.920
and just, like, just talk shit.

01:02:34.920 --> 01:02:38.920
And okay, maybe it won't get to Andre,

01:02:38.920 --> 01:02:40.960
but it might get to another celebrity.

01:02:40.960 --> 01:02:43.840
It might get to other big accounts.

01:02:43.840 --> 01:02:46.400
And then it'll just, so with just that simple goal,

01:02:46.400 --> 01:02:47.960
get them to respond.

01:02:47.960 --> 01:02:50.480
Maximize the probability of actual response.

01:02:50.480 --> 01:02:53.200
Yeah, I mean, you could prompt a powerful model like this

01:02:53.200 --> 01:02:56.840
with its opinion about how to do any possible thing

01:02:56.840 --> 01:02:57.720
you're interested in.

01:02:57.720 --> 01:02:59.440
So they will just, they're kind of on track

01:02:59.440 --> 01:03:00.960
to become these oracles.

01:03:00.960 --> 01:03:02.760
I sort of think of it that way.

01:03:02.760 --> 01:03:05.000
They are oracles, currently it's just text,

01:03:05.000 --> 01:03:06.120
but they will have calculators.

01:03:06.120 --> 01:03:07.720
They will have access to Google search.

01:03:07.720 --> 01:03:09.960
They will have all kinds of gadgets and gizmos.

01:03:09.960 --> 01:03:11.840
They will be able to operate the internet

01:03:11.840 --> 01:03:13.800
and find different information.

01:03:13.800 --> 01:03:18.000
And yeah, in some sense,

01:03:18.000 --> 01:03:19.400
that's kind of like currently what it looks like

01:03:19.400 --> 01:03:20.360
in terms of the development.

01:03:20.360 --> 01:03:22.800
Do you think it'll be an improvement eventually

01:03:22.800 --> 01:03:27.760
over what Google is for access to human knowledge?

01:03:27.760 --> 01:03:29.720
Like it'll be a more effective search engine

01:03:29.720 --> 01:03:31.080
to access human knowledge?

01:03:31.080 --> 01:03:32.480
I think there's definite scope in building

01:03:32.480 --> 01:03:33.840
a better search engine today.

01:03:33.840 --> 01:03:35.920
And I think Google, they have all the tools,

01:03:35.920 --> 01:03:37.480
all the people, they have everything they need.

01:03:37.480 --> 01:03:38.520
They have all the puzzle pieces.

01:03:38.520 --> 01:03:40.880
They have people training transformers at scale.

01:03:40.880 --> 01:03:42.040
They have all the data.

01:03:43.000 --> 01:03:44.520
It's just not obvious if they are capable

01:03:44.520 --> 01:03:46.400
as an organization to innovate

01:03:46.400 --> 01:03:47.800
on their search engine right now.

01:03:47.800 --> 01:03:49.320
And if they don't, someone else will.

01:03:49.320 --> 01:03:50.720
There's absolute scope for building

01:03:50.720 --> 01:03:52.080
a significantly better search engine

01:03:52.080 --> 01:03:53.560
built on these tools.

01:03:53.560 --> 01:03:54.600
It's so interesting.

01:03:54.600 --> 01:03:57.000
A large company where the search,

01:03:57.040 --> 01:03:58.440
there's already an infrastructure.

01:03:58.440 --> 01:04:00.560
It works as it brings out a lot of money.

01:04:00.560 --> 01:04:03.480
So where structurally inside a company

01:04:03.480 --> 01:04:05.960
is their motivation to pivot?

01:04:05.960 --> 01:04:08.200
To say, we're going to build a new search engine.

01:04:08.200 --> 01:04:10.280
Yeah, that's really hard.

01:04:10.280 --> 01:04:13.080
So it's usually going to come from a startup, right?

01:04:13.080 --> 01:04:15.720
That's, that would be, yeah.

01:04:15.720 --> 01:04:17.920
Or some other more competent organization.

01:04:19.240 --> 01:04:20.880
So, I don't know.

01:04:20.880 --> 01:04:21.920
So currently, for example,

01:04:21.920 --> 01:04:24.240
maybe Bing has another shot at it, you know,

01:04:24.240 --> 01:04:25.080
as an example. Here we go.

01:04:25.120 --> 01:04:27.520
Microsoft Edge as we're talking offline.

01:04:28.640 --> 01:04:30.440
I mean, it definitely, it's really interesting

01:04:30.440 --> 01:04:32.800
because search engines used to be about,

01:04:32.800 --> 01:04:34.000
okay, here's some query.

01:04:34.000 --> 01:04:38.560
Here's web pages that look like the stuff that you have.

01:04:38.560 --> 01:04:40.320
But you could just directly go to answer

01:04:40.320 --> 01:04:42.640
and then have supporting evidence.

01:04:42.640 --> 01:04:46.080
And these models basically, they've read all the texts

01:04:46.080 --> 01:04:47.160
and they've read all the web pages.

01:04:47.160 --> 01:04:49.000
And so sometimes when you see yourself

01:04:49.000 --> 01:04:50.080
going over to search results

01:04:50.080 --> 01:04:52.880
and sort of getting like a sense of like the average answer

01:04:52.880 --> 01:04:54.360
to whatever you're interested in,

01:04:54.360 --> 01:04:55.480
like that just directly comes out.

01:04:55.480 --> 01:04:57.080
You don't have to do that work.

01:04:58.360 --> 01:05:01.080
So they're kind of like, yeah,

01:05:01.080 --> 01:05:03.640
I think they have a way of distilling all that knowledge

01:05:03.640 --> 01:05:06.720
into like some level of insight, basically.

01:05:06.720 --> 01:05:09.920
Do you think of prompting as a kind of teaching

01:05:09.920 --> 01:05:12.920
and learning like this whole process,

01:05:12.920 --> 01:05:16.040
like another layer, you know,

01:05:16.040 --> 01:05:18.040
because maybe that's what humans are.

01:05:18.040 --> 01:05:19.760
We already have that background model

01:05:19.760 --> 01:05:23.320
and then the world is prompting you.

01:05:23.320 --> 01:05:24.360
Yeah, exactly.

01:05:24.360 --> 01:05:26.920
I think the way we are programming these computers now,

01:05:26.920 --> 01:05:30.320
like GPTs, is converging to how you program humans.

01:05:30.320 --> 01:05:33.200
I mean, how do I program humans via prompt?

01:05:33.200 --> 01:05:35.640
I go to people and I prompt them to do things.

01:05:35.640 --> 01:05:37.200
I prompt them from information.

01:05:37.200 --> 01:05:40.120
And so natural language prompt is how we program humans

01:05:40.120 --> 01:05:41.600
and we're starting to program computers

01:05:41.600 --> 01:05:42.720
directly in that interface.

01:05:42.720 --> 01:05:44.520
It's like pretty remarkable, honestly.

01:05:44.520 --> 01:05:47.720
So you've spoken a lot about the idea of software 2.0.

01:05:48.560 --> 01:05:53.560
All good ideas become like cliches so quickly.

01:05:53.760 --> 01:05:56.080
Like the terms, it's kind of hilarious.

01:05:57.400 --> 01:06:00.360
It's like, I think Eminem once said that like,

01:06:00.360 --> 01:06:04.000
if he gets annoyed by a song he's written very quickly,

01:06:04.000 --> 01:06:06.200
that means it's gonna be a big hit

01:06:06.200 --> 01:06:08.480
because it's too catchy.

01:06:08.480 --> 01:06:10.760
But can you describe this idea

01:06:10.760 --> 01:06:12.760
and how you're thinking about it has evolved

01:06:12.760 --> 01:06:16.160
over the months and years since you coined it?

01:06:16.160 --> 01:06:17.520
Yeah.

01:06:17.520 --> 01:06:19.760
Yeah, so I had a blog post on software 2.0,

01:06:19.760 --> 01:06:21.520
I think several years ago now.

01:06:22.760 --> 01:06:25.720
And the reason I wrote that post is because I kept,

01:06:25.720 --> 01:06:27.840
I kind of saw something remarkable happening

01:06:27.840 --> 01:06:30.360
in like software development

01:06:30.360 --> 01:06:32.400
and how a lot of code was being transitioned

01:06:32.400 --> 01:06:35.520
to be written not in sort of like C++ and so on,

01:06:35.520 --> 01:06:37.640
but it's written in the weights of a neural net.

01:06:37.640 --> 01:06:39.280
Basically just saying that neural nets

01:06:39.280 --> 01:06:41.680
are taking over software, the realm of software

01:06:41.680 --> 01:06:44.040
and taking more and more and more tasks.

01:06:44.040 --> 01:06:46.040
And at the time, I think not many people

01:06:46.920 --> 01:06:49.320
understood this deeply enough that this is a big deal,

01:06:49.320 --> 01:06:51.080
it's a big transition.

01:06:51.080 --> 01:06:52.400
Neural networks were seen as one

01:06:52.400 --> 01:06:55.000
of multiple classification algorithms you might use

01:06:55.000 --> 01:06:57.000
for your data set problem on Kaggle.

01:06:57.000 --> 01:06:58.480
Like, this is not that,

01:06:58.480 --> 01:07:03.040
this is a change in how we program computers.

01:07:03.040 --> 01:07:07.120
And I saw neural nets as, this is going to take over.

01:07:07.120 --> 01:07:08.880
The way we program computers is going to change.

01:07:08.880 --> 01:07:11.720
It's not going to be people writing a software in C++

01:07:11.720 --> 01:07:14.360
or something like that and directly programming the software.

01:07:14.360 --> 01:07:17.720
It's going to be accumulating training sets and data sets

01:07:17.720 --> 01:07:19.120
and crafting these objectives

01:07:19.120 --> 01:07:20.680
by which you train these neural nets.

01:07:20.680 --> 01:07:23.040
And at some point, there's going to be a compilation process

01:07:23.040 --> 01:07:24.880
from the data sets and the objective

01:07:24.880 --> 01:07:28.200
and the architecture specification into the binary,

01:07:28.200 --> 01:07:31.480
which is really just the neural net weights

01:07:31.480 --> 01:07:33.480
and the forward pass of the neural net.

01:07:33.480 --> 01:07:35.200
And then you can deploy that binary.

01:07:35.200 --> 01:07:37.600
And so I was talking about that sort of transition

01:07:37.600 --> 01:07:40.360
and that's what the post is about.

01:07:40.360 --> 01:07:43.200
And I saw this sort of play out in a lot of fields,

01:07:44.040 --> 01:07:45.720
you know, autopilot being one of them,

01:07:45.720 --> 01:07:48.280
but also just simple image classification.

01:07:48.280 --> 01:07:51.600
People thought originally, you know, in the 80s and so on,

01:07:51.600 --> 01:07:53.000
that they would write the algorithm

01:07:53.000 --> 01:07:55.400
for detecting a dog in an image.

01:07:55.400 --> 01:07:57.640
And they had all these ideas about how the brain does it.

01:07:57.640 --> 01:08:00.000
And first we detect corners and then we detect lines

01:08:00.000 --> 01:08:01.040
and then we stitch them up.

01:08:01.040 --> 01:08:02.200
And they were like really going at it.

01:08:02.200 --> 01:08:03.400
They were like thinking about

01:08:03.400 --> 01:08:04.840
how they're going to write the algorithm.

01:08:04.840 --> 01:08:06.920
And this is not the way you build it.

01:08:08.560 --> 01:08:11.160
And there was a smooth transition where, okay,

01:08:11.160 --> 01:08:13.160
first we thought we were going to build everything.

01:08:14.120 --> 01:08:15.840
Then we were building the features.

01:08:15.840 --> 01:08:18.240
So like hog features and things like that,

01:08:18.240 --> 01:08:19.760
that detect these little statistical patterns

01:08:19.760 --> 01:08:20.800
from image patches.

01:08:20.800 --> 01:08:23.200
And then there was a little bit of learning on top of it,

01:08:23.200 --> 01:08:26.320
like a support vector machine or binary classifier

01:08:26.320 --> 01:08:29.240
for cat versus dog and images on top of the features.

01:08:29.240 --> 01:08:30.160
So we wrote the features,

01:08:30.160 --> 01:08:34.400
but we trained the last layer, sort of the classifier.

01:08:34.400 --> 01:08:35.240
And then people are like,

01:08:35.240 --> 01:08:37.720
actually let's not even design the features because we can't.

01:08:37.720 --> 01:08:39.120
Honestly, we're not very good at it.

01:08:39.120 --> 01:08:41.000
So let's also learn the features.

01:08:41.000 --> 01:08:42.160
And then you end up with basically

01:08:42.160 --> 01:08:44.800
a convolutional neural net where you're learning most of it.

01:08:44.800 --> 01:08:46.400
You're just specifying the architecture.

01:08:46.400 --> 01:08:49.360
And the architecture has tons of filling the blanks,

01:08:49.360 --> 01:08:50.640
which is all the knobs.

01:08:50.640 --> 01:08:53.000
And you let the optimization write most of it.

01:08:53.000 --> 01:08:55.000
And so this transition is happening

01:08:55.000 --> 01:08:56.520
across the industry everywhere.

01:08:56.520 --> 01:08:59.320
And suddenly we end up with a ton of code

01:08:59.320 --> 01:09:01.360
that is written in neural net weights.

01:09:01.360 --> 01:09:02.200
And I was just pointing out

01:09:02.200 --> 01:09:04.280
that the analogy is actually pretty strong.

01:09:04.280 --> 01:09:06.320
And we have a lot of developer environments

01:09:06.320 --> 01:09:07.680
for software 1.0.

01:09:07.680 --> 01:09:10.560
Like we have IDEs, how you work with code,

01:09:10.560 --> 01:09:12.800
how you debug code, how do you run code?

01:09:12.800 --> 01:09:13.880
How do you maintain code?

01:09:13.880 --> 01:09:14.800
We have GitHub.

01:09:14.800 --> 01:09:16.640
So I was trying to make those analogies in the neural realm.

01:09:16.640 --> 01:09:19.000
Like what is the GitHub of software 2.0?

01:09:19.000 --> 01:09:19.840
Turns out it's something

01:09:19.840 --> 01:09:23.120
that looks like hugging face right now, you know?

01:09:23.120 --> 01:09:25.240
And so I think some people took it seriously

01:09:25.240 --> 01:09:26.280
and built cool companies.

01:09:26.280 --> 01:09:29.120
And many people originally attacked the post.

01:09:29.120 --> 01:09:31.760
It actually was not well received when I wrote it.

01:09:31.760 --> 01:09:33.760
And I think maybe it has something to do with the title,

01:09:33.760 --> 01:09:35.440
but the post was not well received.

01:09:35.440 --> 01:09:36.560
And I think more people sort of

01:09:36.560 --> 01:09:39.120
have been coming around to it over time.

01:09:39.120 --> 01:09:42.640
Yeah, so you were the director of AI at Tesla,

01:09:42.640 --> 01:09:47.640
where I think this idea was really implemented at scale,

01:09:48.640 --> 01:09:50.320
which is how you have engineering teams

01:09:50.320 --> 01:09:52.080
doing software 2.0.

01:09:52.080 --> 01:09:56.000
So can you sort of linger on that idea of,

01:09:56.000 --> 01:09:57.760
I think we're in the really early stages

01:09:57.760 --> 01:10:01.480
of everything you just said, which is like GitHub IDEs.

01:10:01.480 --> 01:10:03.960
Like how do we build engineering teams

01:10:03.960 --> 01:10:07.040
that work in software 2.0 systems?

01:10:07.040 --> 01:10:11.400
And the data collection and the data annotation,

01:10:11.400 --> 01:10:15.200
which is all part of that software 2.0?

01:10:15.200 --> 01:10:17.360
Like what do you think is the task of programming

01:10:17.360 --> 01:10:18.880
in software 2.0?

01:10:18.880 --> 01:10:22.960
Is it debugging in the space of hyperparameters

01:10:22.960 --> 01:10:25.840
or is it also debugging in the space of data?

01:10:25.840 --> 01:10:28.960
Yeah, the way by which you program the computer

01:10:28.960 --> 01:10:31.880
and influence its algorithm

01:10:31.880 --> 01:10:34.520
is not by writing the commands yourself.

01:10:34.520 --> 01:10:37.120
You're changing mostly the data set.

01:10:37.120 --> 01:10:39.760
You're changing the loss functions

01:10:39.760 --> 01:10:41.560
of like what the neural net is trying to do,

01:10:41.560 --> 01:10:42.680
how it's trying to predict things.

01:10:42.680 --> 01:10:44.160
But yeah, basically the data sets

01:10:44.160 --> 01:10:46.200
and the architecture, so the neural net.

01:10:46.200 --> 01:10:50.000
And so in the case of the autopilot,

01:10:50.000 --> 01:10:51.760
a lot of the data sets had to do with, for example,

01:10:51.760 --> 01:10:53.480
detection of objects and lane line markings

01:10:53.480 --> 01:10:54.640
and traffic lights and so on.

01:10:54.640 --> 01:10:56.560
So you accumulate massive data sets of,

01:10:56.560 --> 01:10:59.680
here's an example, here's the desired label.

01:10:59.680 --> 01:11:04.320
And then here's roughly what the algorithm should look like.

01:11:04.320 --> 01:11:05.960
And that's a convolutional neural net.

01:11:05.960 --> 01:11:08.160
So the specification of the architecture is like a hint

01:11:08.160 --> 01:11:10.480
as to what the algorithm should roughly look like.

01:11:10.480 --> 01:11:13.640
And then the fill in the blanks process of optimization

01:11:13.640 --> 01:11:15.760
is the training process.

01:11:15.760 --> 01:11:17.720
And then you take your neural net that was trained,

01:11:17.720 --> 01:11:19.440
it gives all the right answers on your data set

01:11:19.440 --> 01:11:21.000
and you deploy it.

01:11:21.000 --> 01:11:22.960
So there is, in that case,

01:11:22.960 --> 01:11:25.760
perhaps at all machine learning cases,

01:11:25.760 --> 01:11:27.240
there's a lot of tasks.

01:11:28.160 --> 01:11:32.200
So is coming up formulating a task

01:11:33.200 --> 01:11:34.960
for a multi-headed neural network

01:11:34.960 --> 01:11:37.680
is formulating a task part of the programming?

01:11:37.680 --> 01:11:38.880
Yeah, very much so.

01:11:38.880 --> 01:11:42.400
How you break down a problem into a set of tasks.

01:11:42.400 --> 01:11:43.440
Yeah.

01:11:43.440 --> 01:11:44.720
I mean, on a high level, I would say,

01:11:44.720 --> 01:11:48.880
if you look at the software running in the autopilot,

01:11:48.880 --> 01:11:50.920
I gave a number of talks on this topic,

01:11:50.920 --> 01:11:52.920
I would say originally a lot of it was written

01:11:52.920 --> 01:11:54.160
in software 1.0.

01:11:54.160 --> 01:11:57.400
There's, imagine lots of C++, right?

01:11:57.400 --> 01:11:59.800
And then gradually, there was a tiny neural net

01:11:59.800 --> 01:12:01.600
that was, for example, predicting,

01:12:01.600 --> 01:12:04.040
given a single image, is there like a traffic light or not,

01:12:04.040 --> 01:12:05.880
or is there a landline marking or not?

01:12:05.880 --> 01:12:08.600
And this neural net didn't have too much to do

01:12:08.600 --> 01:12:10.000
in the scope of the software.

01:12:10.000 --> 01:12:12.600
It was making tiny predictions on individual little image.

01:12:12.600 --> 01:12:15.120
And then the rest of the system stitched it up.

01:12:15.120 --> 01:12:16.360
So, okay, we're actually,

01:12:16.360 --> 01:12:18.520
we don't have just a single camera, we have eight cameras.

01:12:18.520 --> 01:12:20.520
We actually have eight cameras over time.

01:12:20.520 --> 01:12:21.760
And so what do you do with these predictions?

01:12:21.760 --> 01:12:22.720
How do you put them together?

01:12:22.720 --> 01:12:25.000
How do you do the fusion of all that information

01:12:25.000 --> 01:12:25.920
and how do you act on it?

01:12:25.920 --> 01:12:29.720
All of that was written by humans in C++.

01:12:29.760 --> 01:12:32.080
And then we decided, okay, we don't actually want

01:12:33.520 --> 01:12:35.880
to do all of that fusion in C++ code

01:12:35.880 --> 01:12:37.120
because we're actually not good enough

01:12:37.120 --> 01:12:38.200
to write that algorithm.

01:12:38.200 --> 01:12:39.960
We want the neural nets to write the algorithm.

01:12:39.960 --> 01:12:44.200
And we want to port all of that software into the 2.0 stack.

01:12:44.200 --> 01:12:45.640
And so then we actually had neural nets

01:12:45.640 --> 01:12:49.040
that now take all the eight camera images simultaneously

01:12:49.040 --> 01:12:51.280
and make predictions for all of that.

01:12:51.280 --> 01:12:54.840
So, and actually they don't make predictions

01:12:54.840 --> 01:12:56.680
in the space of images.

01:12:56.680 --> 01:12:59.360
They now make predictions directly in 3D.

01:13:00.040 --> 01:13:02.520
And actually they don't in three dimensions around the car.

01:13:02.520 --> 01:13:06.400
And now actually we don't manually fuse the predictions

01:13:06.400 --> 01:13:08.400
in 3D over time.

01:13:08.400 --> 01:13:10.320
We don't trust ourselves to write that tracker.

01:13:10.320 --> 01:13:14.200
So actually we give the neural net the information over time.

01:13:14.200 --> 01:13:16.960
So it takes these videos now and makes those predictions.

01:13:16.960 --> 01:13:19.000
And so you're sort of just like putting more and more power

01:13:19.000 --> 01:13:20.640
into the neural net, more and more processing.

01:13:20.640 --> 01:13:23.680
And at the end of it, the eventual sort of goal

01:13:23.680 --> 01:13:27.000
is to have most of the software potentially be in 2.0 land

01:13:27.120 --> 01:13:30.080
because it works significantly better.

01:13:30.080 --> 01:13:31.480
Humans are just not very good

01:13:31.480 --> 01:13:32.560
at writing software basically.

01:13:32.560 --> 01:13:36.920
So the prediction is happening in this like 4D land

01:13:36.920 --> 01:13:39.080
with three dimensional world over time.

01:13:39.080 --> 01:13:42.720
How do you do annotation in that world?

01:13:42.720 --> 01:13:46.160
What have you, as it's just a data annotation,

01:13:46.160 --> 01:13:49.560
whether it's self supervised or manual by humans

01:13:49.560 --> 01:13:53.840
is a big part of this software 2.0 world.

01:13:53.840 --> 01:13:54.680
Right.

01:13:54.680 --> 01:13:56.400
I would say by far in the industry,

01:13:56.400 --> 01:13:57.920
if you're like talking about the industry

01:13:57.920 --> 01:14:00.520
and how, what is the technology of what we have available?

01:14:00.520 --> 01:14:01.840
Everything is supervised learning.

01:14:01.840 --> 01:14:05.120
So you need a data sets of input, desired output

01:14:05.120 --> 01:14:06.560
and you need lots of it.

01:14:06.560 --> 01:14:09.480
And there are three properties of it that you need.

01:14:09.480 --> 01:14:10.680
You need it to be very large.

01:14:10.680 --> 01:14:13.120
You need it to be accurate, no mistakes

01:14:13.120 --> 01:14:14.320
and you need it to be diverse.

01:14:14.320 --> 01:14:18.400
You don't want to just have a lot of correct examples

01:14:18.400 --> 01:14:19.240
of one thing.

01:14:19.240 --> 01:14:20.960
You need to really cover the space of possibility

01:14:20.960 --> 01:14:21.960
as much as you can.

01:14:21.960 --> 01:14:24.200
And the more you can cover the space of possible inputs,

01:14:24.200 --> 01:14:26.440
the better the algorithm will work at the end.

01:14:26.440 --> 01:14:27.920
Now, once you have really good data sets

01:14:27.920 --> 01:14:31.600
that you're collecting, curating and cleaning,

01:14:31.600 --> 01:14:35.280
you can train your neural net on top of that.

01:14:35.280 --> 01:14:37.240
So a lot of the work goes into cleaning those data sets.

01:14:37.240 --> 01:14:39.360
Now, as you pointed out, it's probably,

01:14:39.360 --> 01:14:41.760
it could be, the question is how do you achieve

01:14:41.760 --> 01:14:45.280
a ton of, if you want to basically predict in 3D,

01:14:45.280 --> 01:14:47.840
you need data in 3D to back that up.

01:14:47.840 --> 01:14:50.400
So in this video, we have eight videos coming

01:14:50.400 --> 01:14:52.520
from all the cameras of the system.

01:14:52.520 --> 01:14:54.240
And this is what they saw.

01:14:54.240 --> 01:14:56.360
And this is the truth of what actually was around.

01:14:56.360 --> 01:14:58.320
There was this car, there was this car, this car.

01:14:58.320 --> 01:14:59.320
These are the lane line markings.

01:14:59.320 --> 01:15:00.400
This is the geometry of the road.

01:15:00.400 --> 01:15:02.640
There was traffic light in this three dimensional position.

01:15:02.640 --> 01:15:04.680
You need the ground truth.

01:15:04.680 --> 01:15:07.080
And so the big question that the team was solving, of course,

01:15:07.080 --> 01:15:09.440
is how do you arrive at that ground truth?

01:15:09.440 --> 01:15:10.840
Because once you have a million of it

01:15:10.840 --> 01:15:12.760
and it's large, clean and diverse,

01:15:12.760 --> 01:15:14.720
then training a neural net on it works extremely well

01:15:14.720 --> 01:15:16.760
and you can ship that into the car.

01:15:16.760 --> 01:15:18.960
And so there's many mechanisms by which

01:15:18.960 --> 01:15:20.960
we collected that training data.

01:15:20.960 --> 01:15:22.720
You can always go for human annotation.

01:15:22.720 --> 01:15:25.320
You can go for simulation as a source of ground truth.

01:15:25.320 --> 01:15:29.360
You can also go for what we call the offline tracker

01:15:29.360 --> 01:15:31.640
that we've spoken about at the AI day and so on,

01:15:31.640 --> 01:15:34.400
which is basically an automatic reconstruction process

01:15:34.400 --> 01:15:36.720
for taking those videos and recovering

01:15:36.720 --> 01:15:39.640
the three dimensional sort of reality of what

01:15:39.640 --> 01:15:40.800
was around that car.

01:15:40.800 --> 01:15:43.040
So basically think of doing like a three dimensional

01:15:43.040 --> 01:15:44.520
reconstruction as an offline thing

01:15:44.520 --> 01:15:48.240
and then understanding that, OK, there's 10 seconds of video.

01:15:48.240 --> 01:15:49.400
This is what we saw.

01:15:49.400 --> 01:15:52.480
And therefore, here's all the lane lines, cars and so on.

01:15:52.480 --> 01:15:53.800
And then once you have that annotation,

01:15:53.800 --> 01:15:56.360
you can train neural nets to imitate it.

01:15:56.360 --> 01:15:59.360
And how difficult is the 3D reconstruction?

01:15:59.360 --> 01:16:01.200
It's difficult, but it can be done.

01:16:01.200 --> 01:16:03.520
So there's overlap between the cameras

01:16:03.520 --> 01:16:07.560
and you do the reconstruction and there's perhaps

01:16:07.560 --> 01:16:09.640
if there's any inaccuracy, so that's

01:16:09.640 --> 01:16:12.000
caught in the annotation step.

01:16:12.000 --> 01:16:14.040
Yes, the nice thing about the annotation

01:16:14.040 --> 01:16:15.840
is that it is fully offline.

01:16:15.840 --> 01:16:18.120
You have infinite time, you have a chunk of one minute

01:16:18.120 --> 01:16:20.480
and you're trying to just offline in a supercomputer

01:16:20.480 --> 01:16:22.880
somewhere, figure out where were the positions of all

01:16:22.880 --> 01:16:24.400
the cars, all the people, and you

01:16:24.400 --> 01:16:26.920
have your full one minute of video from all the angles.

01:16:26.920 --> 01:16:28.680
And you can run all the neural nets you want

01:16:28.680 --> 01:16:31.040
and they can be very efficient, massive neural nets.

01:16:31.040 --> 01:16:33.440
There can be neural nets that can't even run in the car

01:16:33.440 --> 01:16:36.000
later at test time, so they can be even more powerful neural

01:16:36.000 --> 01:16:37.840
nets than what you can eventually deploy.

01:16:37.840 --> 01:16:39.960
So you can do anything you want, three dimensional

01:16:39.960 --> 01:16:41.720
reconstruction, neural nets, anything

01:16:41.720 --> 01:16:43.120
you want just to recover that truth

01:16:43.120 --> 01:16:45.240
and then you supervise that truth.

01:16:45.240 --> 01:16:47.400
What have you learned, you said no mistakes

01:16:47.440 --> 01:16:50.920
about humans doing annotation?

01:16:50.920 --> 01:16:55.360
Because I assume humans, there's like a range of things

01:16:55.360 --> 01:16:57.880
they're good at in terms of clicking stuff on screen.

01:16:57.880 --> 01:17:00.400
Isn't that, how interesting is that to you

01:17:00.400 --> 01:17:03.760
of a problem of designing an annotator

01:17:03.760 --> 01:17:06.080
where humans are accurate, enjoy it,

01:17:06.080 --> 01:17:08.120
like what are the even the metrics are efficient

01:17:08.120 --> 01:17:09.960
or productive, all that kind of stuff?

01:17:09.960 --> 01:17:12.560
Yeah, so I grew the annotation team at Tesla

01:17:12.560 --> 01:17:16.160
from basically zero to 1,000 while I was there.

01:17:16.160 --> 01:17:18.120
That was really interesting.

01:17:18.120 --> 01:17:20.760
My background is a PhD student researcher,

01:17:20.760 --> 01:17:24.360
so growing that kind of an organization was pretty crazy.

01:17:24.360 --> 01:17:27.480
But yeah, I think it's extremely interesting

01:17:27.480 --> 01:17:29.080
and part of the design process very much

01:17:29.080 --> 01:17:31.720
behind the autopilot as to where you use humans.

01:17:31.720 --> 01:17:34.040
Humans are very good at certain kinds of annotations.

01:17:34.040 --> 01:17:34.920
They're very good, for example,

01:17:34.920 --> 01:17:36.640
at two dimensional annotations of images.

01:17:36.640 --> 01:17:39.880
They're not good at annotating cars over time

01:17:39.880 --> 01:17:42.240
in three dimensional space, very, very hard.

01:17:42.240 --> 01:17:44.920
And so that's why we were very careful to design the tasks

01:17:44.920 --> 01:17:46.520
that are easy to do for humans

01:17:46.520 --> 01:17:49.000
versus things that should be left to the offline tracker.

01:17:49.000 --> 01:17:51.320
Like maybe the computer will do all the triangulation

01:17:51.320 --> 01:17:52.440
and three-dimensional reconstruction,

01:17:52.440 --> 01:17:54.480
but the human will say exactly these pixels

01:17:54.480 --> 01:17:57.760
of the image are car, exactly these pixels are human.

01:17:57.760 --> 01:18:00.840
And so co-designing the data annotation pipeline

01:18:00.840 --> 01:18:04.720
was very much bread and butter was what I was doing daily.

01:18:04.720 --> 01:18:06.520
Do you think there's still a lot of open problems

01:18:06.520 --> 01:18:07.400
in that space?

01:18:08.880 --> 01:18:12.400
Just in general, annotation where the stuff

01:18:12.400 --> 01:18:14.400
the machines are good at, machines do

01:18:14.400 --> 01:18:16.640
and the humans do what they're good at.

01:18:16.640 --> 01:18:18.760
And there's maybe some iterative process.

01:18:18.760 --> 01:18:19.680
Right.

01:18:19.680 --> 01:18:21.240
I think to a very large extent,

01:18:21.240 --> 01:18:22.600
we went through a number of iterations

01:18:22.600 --> 01:18:25.440
and we learned a ton about how to create these data sets.

01:18:26.320 --> 01:18:27.840
I'm not seeing big open problems.

01:18:27.840 --> 01:18:30.160
Like originally when I joined, I was like,

01:18:30.160 --> 01:18:32.800
I was really not sure how this would turn out.

01:18:32.800 --> 01:18:35.160
But by the time I left, I was much more secure

01:18:35.160 --> 01:18:37.320
and actually we sort of understand the philosophy

01:18:37.320 --> 01:18:38.440
of how to create these data sets.

01:18:38.440 --> 01:18:41.600
And I was pretty comfortable with where that was at the time.

01:18:41.600 --> 01:18:45.880
So what are strengths and limitations of cameras

01:18:45.880 --> 01:18:48.600
for the driving task in your understanding

01:18:48.600 --> 01:18:51.120
when you formulate the driving task as a vision task

01:18:51.120 --> 01:18:55.160
with eight cameras, you've seen that the entire,

01:18:55.160 --> 01:18:57.200
most of the history of the computer vision field

01:18:57.200 --> 01:18:59.000
when it has to do with neural networks,

01:18:59.000 --> 01:19:01.240
just if you step back, what are the strengths

01:19:01.240 --> 01:19:05.720
and limitations of pixels, of using pixels to drive?

01:19:05.720 --> 01:19:08.880
Yeah, pixels I think are a beautiful sensory,

01:19:08.880 --> 01:19:10.480
beautiful sensor I would say.

01:19:10.480 --> 01:19:12.440
The thing is like cameras are very, very cheap

01:19:12.440 --> 01:19:15.440
and they provide a ton of information, ton of bits.

01:19:15.440 --> 01:19:19.080
So it's a extremely cheap sensor for a ton of bits

01:19:19.080 --> 01:19:20.760
and each one of these bits is a constraint

01:19:20.760 --> 01:19:21.840
on the state of the world.

01:19:21.840 --> 01:19:26.200
And so you get lots of megapixel images, very cheap.

01:19:26.200 --> 01:19:27.840
And it just gives you all these constraints

01:19:27.840 --> 01:19:30.000
for understanding what's actually out there in the world.

01:19:30.000 --> 01:19:34.440
So vision is probably the highest bandwidth sensor.

01:19:34.440 --> 01:19:36.240
It's a very high bandwidth sensor.

01:19:36.240 --> 01:19:41.240
And I love that pixels is a constraint on the world.

01:19:43.840 --> 01:19:48.440
It's this highly complex, high bandwidth constraint

01:19:48.440 --> 01:19:50.480
on the state of the world that's fascinating.

01:19:50.480 --> 01:19:54.240
It's not just that, but again, this real importance of,

01:19:54.240 --> 01:19:56.080
it's the sensor that humans use.

01:19:56.080 --> 01:19:59.000
Therefore, everything is designed for that sensor.

01:20:00.080 --> 01:20:02.600
The text, the writing, the flashing signs,

01:20:02.600 --> 01:20:04.280
everything is designed for vision.

01:20:04.280 --> 01:20:07.240
And so you just find it everywhere.

01:20:07.240 --> 01:20:10.160
And so that's why that is the interface you want to be in,

01:20:10.160 --> 01:20:12.400
talking again about these universal interfaces.

01:20:12.400 --> 01:20:14.240
And that's where we actually want to measure the world

01:20:14.240 --> 01:20:18.080
as well and then develop software for that sensor.

01:20:18.080 --> 01:20:21.600
But there's other constraints on the state of the world

01:20:21.600 --> 01:20:24.120
that humans use to understand the world.

01:20:24.120 --> 01:20:28.040
I mean, vision ultimately is the main one,

01:20:28.040 --> 01:20:31.840
but we're like referencing our understanding

01:20:31.840 --> 01:20:35.880
of human behavior and some common sense physics

01:20:35.880 --> 01:20:37.480
that could be inferred from vision,

01:20:37.480 --> 01:20:39.360
from a perception perspective,

01:20:39.360 --> 01:20:43.880
but it feels like we're using some kind of reasoning

01:20:43.880 --> 01:20:47.120
to predict the world, not just the pixels.

01:20:47.120 --> 01:20:49.680
I mean, you have a powerful prior service

01:20:49.680 --> 01:20:52.320
for how the world evolves over time, et cetera.

01:20:52.320 --> 01:20:54.440
So it's not just about the likelihood term

01:20:54.440 --> 01:20:56.400
coming up from the data itself,

01:20:56.400 --> 01:20:57.800
telling you about what you are observing,

01:20:57.800 --> 01:21:01.280
but also the prior term of where are the likely things to see

01:21:01.320 --> 01:21:03.280
and how do they likely move and so on.

01:21:03.280 --> 01:21:08.280
And the question is how complex is the range of possibilities

01:21:10.240 --> 01:21:12.560
that might happen in the driving task?

01:21:12.560 --> 01:21:13.400
Right.

01:21:13.400 --> 01:21:15.480
Is that to you still an open problem

01:21:15.480 --> 01:21:18.600
of how difficult is driving, like philosophically speaking?

01:21:21.800 --> 01:21:23.800
All the time you worked on driving,

01:21:23.800 --> 01:21:26.400
do you understand how hard driving is?

01:21:26.400 --> 01:21:28.040
Yeah, driving is really hard

01:21:28.040 --> 01:21:29.480
because it has to do with the predictions

01:21:29.520 --> 01:21:31.280
of all these other agents and the theory of mind

01:21:31.280 --> 01:21:34.360
and what they're gonna do and are they looking at you?

01:21:34.360 --> 01:21:35.400
Where are they looking?

01:21:35.400 --> 01:21:36.880
Where are they thinking?

01:21:36.880 --> 01:21:39.840
There's a lot that goes there at the full tail

01:21:39.840 --> 01:21:42.240
of the expansion of the knowledge

01:21:42.240 --> 01:21:44.320
that we have to be comfortable with it eventually.

01:21:44.320 --> 01:21:46.200
The final problems are of that form.

01:21:46.200 --> 01:21:48.640
I don't think those are the problems that are very common.

01:21:48.640 --> 01:21:50.400
I think eventually they're important,

01:21:50.400 --> 01:21:52.120
but it's like really in the tail end.

01:21:52.120 --> 01:21:54.400
In the tail end, the rare edge cases.

01:21:55.520 --> 01:21:57.000
From the vision perspective,

01:21:57.000 --> 01:21:58.800
what are the toughest parts

01:21:58.800 --> 01:22:00.440
of the vision problem of driving?

01:22:03.320 --> 01:22:06.080
Well, basically the sensor is extremely powerful,

01:22:06.080 --> 01:22:08.440
but you still need to process that information.

01:22:09.560 --> 01:22:12.560
And so going from brightnesses of these pixel values

01:22:12.560 --> 01:22:14.600
to hey, here are the three dimensional world

01:22:14.600 --> 01:22:15.640
is extremely hard.

01:22:15.640 --> 01:22:16.680
And that's what the neural networks

01:22:16.680 --> 01:22:18.240
are fundamentally doing.

01:22:18.240 --> 01:22:22.160
And so the difficulty really is in just doing

01:22:22.160 --> 01:22:25.640
an extremely good job of engineering the entire pipeline,

01:22:25.640 --> 01:22:27.280
the entire data engine,

01:22:27.280 --> 01:22:29.800
having the capacity to train these neural nets,

01:22:29.800 --> 01:22:31.920
having the ability to evaluate the system

01:22:31.920 --> 01:22:33.680
and iterate on it.

01:22:33.680 --> 01:22:36.240
So I would say just doing this in production at scale

01:22:36.240 --> 01:22:37.080
is like the hard part.

01:22:37.080 --> 01:22:38.520
It's an execution problem.

01:22:38.520 --> 01:22:39.560
So the data engine,

01:22:39.560 --> 01:22:44.560
but also the deployment of the system

01:22:45.480 --> 01:22:47.280
such that it has low latency performance.

01:22:47.280 --> 01:22:48.920
So it has to do all these steps.

01:22:48.920 --> 01:22:50.280
Yeah, for the neural net specifically,

01:22:50.280 --> 01:22:53.680
just making sure everything fits into the chip on the car.

01:22:53.720 --> 01:22:56.680
And you have a finite budget of flops that you can perform

01:22:56.680 --> 01:22:59.560
and memory bandwidth and other constraints.

01:22:59.560 --> 01:23:01.160
And you have to make sure it flies

01:23:01.160 --> 01:23:02.440
and you can squeeze in as much computer

01:23:02.440 --> 01:23:03.680
as you can into the tiny.

01:23:03.680 --> 01:23:05.720
What have you learned from that process?

01:23:05.720 --> 01:23:07.440
Because maybe that's one of the bigger,

01:23:07.440 --> 01:23:11.760
like new things coming from a research background

01:23:11.760 --> 01:23:13.800
where there's a system that has to run

01:23:13.800 --> 01:23:15.960
under heavily constrained resources,

01:23:15.960 --> 01:23:17.360
has to run really fast.

01:23:17.360 --> 01:23:20.960
What kind of insights have you learned from that?

01:23:20.960 --> 01:23:24.120
Yeah, I'm not sure if there's too many insights.

01:23:24.120 --> 01:23:25.560
You're trying to create a neural net

01:23:25.560 --> 01:23:28.240
that will fit in what you have available.

01:23:28.240 --> 01:23:29.920
And you're always trying to optimize it.

01:23:29.920 --> 01:23:31.960
And we talked a lot about it on the AI day

01:23:31.960 --> 01:23:36.800
and basically the triple backflips that the team is doing

01:23:36.800 --> 01:23:39.520
to make sure it all fits and utilizes the engine.

01:23:39.520 --> 01:23:42.240
So I think it's extremely good engineering.

01:23:42.240 --> 01:23:44.960
And then there's all kinds of little insights peppered in

01:23:44.960 --> 01:23:46.760
on how to do it properly.

01:23:46.760 --> 01:23:47.720
Let's actually zoom out

01:23:47.720 --> 01:23:49.800
because I don't think we talked about the data engine,

01:23:49.800 --> 01:23:53.640
the entirety of the layouts of this idea

01:23:53.640 --> 01:23:57.280
that I think is just beautiful with humans in the loop.

01:23:57.280 --> 01:23:59.560
Can you describe the data engine?

01:23:59.560 --> 01:24:01.240
Yeah, the data engine is what I call

01:24:01.240 --> 01:24:04.720
the almost biological feeling like process

01:24:04.720 --> 01:24:08.160
by which you perfect the training sets

01:24:08.160 --> 01:24:10.240
for these neural networks.

01:24:10.240 --> 01:24:12.120
So because most of the programming now

01:24:12.120 --> 01:24:13.480
is in the level of these data sets

01:24:13.480 --> 01:24:15.920
and make sure they're large, diverse and clean,

01:24:15.920 --> 01:24:19.320
basically you have a data set that you think is good.

01:24:19.320 --> 01:24:21.680
You train your neural net, you deploy it

01:24:21.680 --> 01:24:24.040
and then you observe how well it's performing.

01:24:24.040 --> 01:24:26.320
And you're trying to always increase

01:24:26.320 --> 01:24:27.600
the quality of your data set.

01:24:27.600 --> 01:24:29.360
So you're trying to catch scenarios

01:24:29.360 --> 01:24:32.040
basically that are basically rare.

01:24:32.040 --> 01:24:33.560
And it is in these scenarios

01:24:33.560 --> 01:24:35.040
that the neural nets will typically struggle in

01:24:35.040 --> 01:24:36.480
because they weren't told what to do

01:24:36.480 --> 01:24:38.760
in those rare cases in the data set.

01:24:38.760 --> 01:24:39.760
But now you can close the loop

01:24:39.760 --> 01:24:42.640
because if you can now collect all those at scale,

01:24:42.640 --> 01:24:43.800
you can then feed them back

01:24:43.800 --> 01:24:46.360
into the reconstruction process I described

01:24:46.360 --> 01:24:48.600
and reconstruct the truth in those cases

01:24:48.600 --> 01:24:50.080
and add it to the data set.

01:24:50.080 --> 01:24:52.400
And so the whole thing ends up being like a staircase

01:24:52.400 --> 01:24:55.640
of improvement of perfecting your training set.

01:24:55.640 --> 01:24:57.040
And you have to go through deployments

01:24:57.040 --> 01:24:59.520
so that you can mine the parts

01:24:59.520 --> 01:25:02.560
that are not yet represented well in the data set.

01:25:02.560 --> 01:25:03.800
So your data set is basically imperfect.

01:25:03.800 --> 01:25:04.680
It needs to be diverse.

01:25:04.680 --> 01:25:07.000
It has pockets that are missing

01:25:07.000 --> 01:25:08.440
and you need to pad out the pockets.

01:25:08.440 --> 01:25:11.600
You can sort of think of it that way in the data.

01:25:11.600 --> 01:25:13.160
What role do humans play in this?

01:25:13.160 --> 01:25:15.960
So what's this biological system

01:25:15.960 --> 01:25:18.840
like a human body's made up of cells?

01:25:18.840 --> 01:25:23.280
What role, like how do you optimize the human system?

01:25:23.280 --> 01:25:26.120
The multiple engineers collaborating,

01:25:26.120 --> 01:25:29.320
figuring out what to focus on,

01:25:29.320 --> 01:25:30.480
what to contribute,

01:25:30.480 --> 01:25:34.000
which task to optimize in this neural network?

01:25:35.320 --> 01:25:37.120
Who is in charge of figuring out

01:25:37.120 --> 01:25:38.840
which task needs more data?

01:25:40.360 --> 01:25:44.520
Can you speak to the hyperparameters of the human system?

01:25:44.520 --> 01:25:46.520
It really just comes down to extremely good execution

01:25:46.520 --> 01:25:48.400
from an engineering team who knows what they're doing.

01:25:48.400 --> 01:25:50.720
They understand intuitively the philosophical insights

01:25:50.720 --> 01:25:52.080
underlying the data engine

01:25:52.080 --> 01:25:54.320
and the process by which the system improves.

01:25:54.320 --> 01:25:57.680
And how to, again, delegate the strategy

01:25:57.680 --> 01:25:59.720
of the data collection and how that works.

01:25:59.720 --> 01:26:02.080
And then just making sure it's all extremely well executed.

01:26:02.080 --> 01:26:03.680
And that's where most of the work is.

01:26:03.680 --> 01:26:05.880
It's not even the philosophizing or the research

01:26:05.880 --> 01:26:06.720
or the ideas of it.

01:26:06.720 --> 01:26:08.920
It's just extremely good execution is so hard

01:26:08.920 --> 01:26:10.800
when you're dealing with data at that scale.

01:26:10.800 --> 01:26:13.760
So your role in the data engine executing well on it,

01:26:14.160 --> 01:26:16.280
is difficult and extremely important.

01:26:16.280 --> 01:26:20.800
Is there a priority of like a vision board

01:26:20.800 --> 01:26:25.280
of saying like, we really need to get better at stoplights?

01:26:25.280 --> 01:26:26.120
Yeah.

01:26:26.120 --> 01:26:27.680
Like the prioritization of tasks?

01:26:27.680 --> 01:26:28.520
Yes.

01:26:28.520 --> 01:26:30.560
Is that essentially, and that comes from the data?

01:26:30.560 --> 01:26:32.920
That comes to a very large extent

01:26:32.920 --> 01:26:35.040
to what we are trying to achieve in the product or map.

01:26:35.040 --> 01:26:38.280
What we're trying to, the release we're trying to get out

01:26:38.280 --> 01:26:40.040
in the feedback from the QA team

01:26:40.040 --> 01:26:41.800
where the system is struggling or not,

01:26:41.800 --> 01:26:42.800
the things we're trying to improve.

01:26:42.840 --> 01:26:45.440
And the QA team gives some signal,

01:26:45.440 --> 01:26:48.360
some information in aggregate

01:26:48.360 --> 01:26:50.480
about the performance of the system in various conditions.

01:26:50.480 --> 01:26:51.320
That's right.

01:26:51.320 --> 01:26:52.160
And then of course, all of us drive it

01:26:52.160 --> 01:26:53.080
and we can also see it.

01:26:53.080 --> 01:26:55.000
It's really nice to work with a system

01:26:55.000 --> 01:26:56.800
that you can also experience yourself

01:26:56.800 --> 01:26:58.640
and it drives you home.

01:26:58.640 --> 01:27:00.760
Is there some insight you can draw

01:27:00.760 --> 01:27:02.240
from your individual experience

01:27:02.240 --> 01:27:03.520
that you just can't quite get

01:27:03.520 --> 01:27:06.880
from an aggregate statistical analysis of data?

01:27:06.880 --> 01:27:07.720
Yeah.

01:27:07.720 --> 01:27:08.540
It's so weird, right?

01:27:08.540 --> 01:27:09.520
Yes.

01:27:09.520 --> 01:27:11.520
It's not scientific in a sense,

01:27:11.560 --> 01:27:14.000
because you're just one anecdotal sample.

01:27:14.000 --> 01:27:15.920
Yeah, I think there's a ton of,

01:27:15.920 --> 01:27:17.360
it's a source of truth.

01:27:17.360 --> 01:27:19.080
It's your interaction with the system

01:27:19.080 --> 01:27:20.480
and you can see it, you can play with it,

01:27:20.480 --> 01:27:23.280
you can perturb it, you can get a sense of it,

01:27:23.280 --> 01:27:24.680
you have an intuition for it.

01:27:24.680 --> 01:27:26.840
I think numbers just like have a way of,

01:27:26.840 --> 01:27:29.080
numbers and plots and graphs are much harder.

01:27:30.160 --> 01:27:31.400
It hides a lot of-

01:27:31.400 --> 01:27:34.300
It's like if you train a language model,

01:27:35.320 --> 01:27:38.640
it's a really powerful way is by you interacting with it.

01:27:38.640 --> 01:27:39.480
Yeah, 100%.

01:27:39.480 --> 01:27:40.680
To try to build up an intuition.

01:27:40.720 --> 01:27:42.880
Yeah, I think like Elon also,

01:27:42.880 --> 01:27:45.240
he always wanted to drive the system himself.

01:27:45.240 --> 01:27:48.920
He drives a lot and I want to say almost daily.

01:27:48.920 --> 01:27:51.800
So he also sees this as a source of truth,

01:27:51.800 --> 01:27:56.240
you driving the system and it performing and yeah.

01:27:56.240 --> 01:27:57.740
So what do you think?

01:27:57.740 --> 01:27:58.880
Tough questions here.

01:28:00.040 --> 01:28:04.960
So Tesla last year removed radar from the sensor suite

01:28:04.960 --> 01:28:06.160
and now just announced

01:28:06.160 --> 01:28:09.240
that it's going to remove ultrasonic sensors

01:28:09.240 --> 01:28:11.960
relying solely on vision, so camera only.

01:28:13.320 --> 01:28:16.360
Does that make the perception problem harder or easier?

01:28:18.040 --> 01:28:20.060
I would almost reframe the question in some way.

01:28:20.060 --> 01:28:22.120
So the thing is basically,

01:28:22.120 --> 01:28:23.680
you would think that additional sensors-

01:28:23.680 --> 01:28:25.560
By the way, can I just interrupt?

01:28:25.560 --> 01:28:28.240
I wonder if a language model will ever do that

01:28:28.240 --> 01:28:29.120
if you prompt it.

01:28:29.120 --> 01:28:31.000
Let me reframe your question.

01:28:31.000 --> 01:28:32.440
That would be epic.

01:28:32.440 --> 01:28:34.120
That's the wrong prompt, sorry.

01:28:34.120 --> 01:28:36.400
It's like a little bit of a wrong question

01:28:36.400 --> 01:28:38.720
because basically you would think that these sensors

01:28:38.720 --> 01:28:41.000
are an asset to you.

01:28:41.000 --> 01:28:43.560
But if you fully consider the entire product

01:28:43.560 --> 01:28:45.160
in its entirety,

01:28:45.160 --> 01:28:47.760
these sensors are actually potentially liability

01:28:47.760 --> 01:28:49.800
because these sensors aren't free.

01:28:49.800 --> 01:28:51.320
They don't just appear on your car.

01:28:51.320 --> 01:28:53.840
You need suddenly you need to have an entire supply chain.

01:28:53.840 --> 01:28:55.320
You have people procuring it.

01:28:55.320 --> 01:28:56.560
There can be problems with them.

01:28:56.560 --> 01:28:57.840
They may need replacement.

01:28:57.840 --> 01:28:59.080
They are part of the manufacturing process.

01:28:59.080 --> 01:29:01.720
They can hold back the line in production.

01:29:01.720 --> 01:29:03.300
You need to source them, you need to maintain them.

01:29:03.300 --> 01:29:05.680
You have to have teams that write the firmware,

01:29:05.680 --> 01:29:08.480
all of it and then you also have to incorporate them,

01:29:09.200 --> 01:29:10.040
you have to fuse them into the system in some way.

01:29:10.040 --> 01:29:13.640
And so it actually like bloats a lot of it.

01:29:13.640 --> 01:29:16.920
And I think Elon is really good at simplify, simplify.

01:29:16.920 --> 01:29:18.280
Best part is no part.

01:29:18.280 --> 01:29:19.840
And he always tries to throw away things

01:29:19.840 --> 01:29:22.000
that are not essential because he understands the entropy

01:29:22.000 --> 01:29:23.840
in organizations and an approach.

01:29:23.840 --> 01:29:26.880
And I think in this case, the cost is high

01:29:26.880 --> 01:29:28.040
and you're not potentially seeing it

01:29:28.040 --> 01:29:29.720
if you're just a computer vision engineer

01:29:29.720 --> 01:29:31.400
and I'm just trying to improve my network

01:29:31.400 --> 01:29:33.920
and is it more useful or less useful?

01:29:33.920 --> 01:29:35.240
How useful is it?

01:29:35.240 --> 01:29:36.080
And the thing is,

01:29:36.080 --> 01:29:38.480
once you consider the full cost of a sensor,

01:29:38.480 --> 01:29:40.160
it actually is potentially a liability

01:29:40.160 --> 01:29:41.360
and you need to be really sure

01:29:41.360 --> 01:29:43.760
that it's giving you extremely useful information.

01:29:43.760 --> 01:29:46.520
In this case, we looked at using it or not using it

01:29:46.520 --> 01:29:48.060
and the Delta was not massive.

01:29:48.060 --> 01:29:49.520
And so it's not useful.

01:29:49.520 --> 01:29:52.600
Is it also bloat in the data engine?

01:29:52.600 --> 01:29:56.080
Like having more sensors is a distraction.

01:29:56.080 --> 01:29:58.200
And these sensors, they can change over time, for example,

01:29:58.200 --> 01:29:59.780
you can have one type of say radar,

01:29:59.780 --> 01:30:01.600
you can have other type of radar, they change over time.

01:30:01.600 --> 01:30:02.880
Now suddenly you need to worry about it.

01:30:02.880 --> 01:30:05.000
Now suddenly you have a column in your SQLite

01:30:05.000 --> 01:30:06.880
telling you, oh, what sensor type was it?

01:30:06.880 --> 01:30:08.680
And they all have different distributions

01:30:08.680 --> 01:30:13.680
and then they contribute noise and entropy into everything

01:30:13.760 --> 01:30:15.200
and they bloat stuff.

01:30:15.200 --> 01:30:16.520
And also organizationally,

01:30:16.520 --> 01:30:17.560
it's been really fascinating to me

01:30:17.560 --> 01:30:19.360
that it can be very distracting.

01:30:20.600 --> 01:30:23.840
If all you wanna get to work is vision,

01:30:23.840 --> 01:30:25.120
all the resources are on it

01:30:25.120 --> 01:30:27.140
and you're building out a data engine

01:30:27.140 --> 01:30:28.640
and you're actually making forward progress

01:30:28.640 --> 01:30:32.120
because that is the sensor with the most bandwidth,

01:30:32.120 --> 01:30:33.600
the most constraints in the world

01:30:33.600 --> 01:30:34.920
and you're investing fully into that

01:30:35.800 --> 01:30:36.640
and you can make that extremely good.

01:30:36.640 --> 01:30:39.480
If you're only a finite amount of sort of spend

01:30:39.480 --> 01:30:42.840
of focus across different facets of the system.

01:30:42.840 --> 01:30:46.040
And this kind of reminds me

01:30:46.040 --> 01:30:48.280
of Rich Sutton's The Bitter Lesson

01:30:48.280 --> 01:30:51.240
that just seems like simplifying the system

01:30:51.240 --> 01:30:52.600
in the long run.

01:30:52.600 --> 01:30:54.400
Now, of course, you don't know what the long run is

01:30:54.400 --> 01:30:57.080
and it seems to be always the right solution.

01:30:57.080 --> 01:30:58.360
In that case, it was for RL

01:30:58.360 --> 01:31:01.000
but it seems to apply generally across all systems

01:31:01.000 --> 01:31:02.400
that do computation.

01:31:02.440 --> 01:31:06.340
So what do you think about the LiDAR as a crutch debate?

01:31:07.480 --> 01:31:11.200
The battle between point clouds and pixels.

01:31:11.200 --> 01:31:12.040
Yeah, I think this debate

01:31:12.040 --> 01:31:13.840
is always like slightly confusing to me

01:31:13.840 --> 01:31:16.240
because it seems like the actual debate should be about

01:31:16.240 --> 01:31:18.160
like, do you have the fleet or not?

01:31:18.160 --> 01:31:19.440
That's like the really important thing

01:31:19.440 --> 01:31:22.000
about whether you can achieve a really good functioning

01:31:22.000 --> 01:31:24.000
of an AI system at this scale.

01:31:24.000 --> 01:31:25.560
So data collection systems.

01:31:25.560 --> 01:31:27.000
Yeah, do you have a fleet or not

01:31:27.000 --> 01:31:28.380
is significantly more important

01:31:28.380 --> 01:31:29.880
whether you have LiDAR or not.

01:31:29.880 --> 01:31:31.080
It's just another sensor.

01:31:32.080 --> 01:31:36.680
And yeah, I think similar to the radar discussion basically,

01:31:36.680 --> 01:31:41.680
I don't think it basically doesn't offer extra information.

01:31:43.920 --> 01:31:44.960
It's extremely costly.

01:31:44.960 --> 01:31:45.960
It has all kinds of problems.

01:31:45.960 --> 01:31:46.800
You have to worry about it.

01:31:46.800 --> 01:31:47.880
You have to calibrate it, et cetera.

01:31:47.880 --> 01:31:49.200
It creates bloat and entropy.

01:31:49.200 --> 01:31:52.960
You have to be really sure that you need this sensor.

01:31:52.960 --> 01:31:54.960
In this case, I basically don't think you need it.

01:31:54.960 --> 01:31:57.240
And I think, honestly, I will make a stronger statement.

01:31:57.240 --> 01:31:59.760
I think the others, some of the other companies

01:31:59.760 --> 01:32:02.200
who are using it are probably going to drop it.

01:32:02.200 --> 01:32:06.640
Yeah, so you have to consider the sensor in the full,

01:32:07.440 --> 01:32:10.360
in considering, can you build a big fleet

01:32:10.360 --> 01:32:12.320
that collects a lot of data?

01:32:12.320 --> 01:32:14.520
And can you integrate that sensor with it,

01:32:14.520 --> 01:32:17.160
that data and that sensor into a data engine

01:32:17.160 --> 01:32:20.200
that's able to quickly find different parts of the data

01:32:20.200 --> 01:32:22.360
that then continuously improves

01:32:22.360 --> 01:32:24.280
whatever the model that you're using?

01:32:24.280 --> 01:32:27.320
Yeah, another way to look at it is like vision is necessary

01:32:27.320 --> 01:32:29.880
in the sense that the drive,

01:32:29.880 --> 01:32:31.480
the world is designed for human visual consumption.

01:32:31.480 --> 01:32:33.760
So you need vision, it's necessary.

01:32:33.760 --> 01:32:35.920
And then also it is sufficient

01:32:35.920 --> 01:32:38.880
because it has all the information that you need for driving

01:32:38.880 --> 01:32:40.880
and humans obviously has vision to drive.

01:32:40.880 --> 01:32:42.480
So it's both necessary and sufficient.

01:32:42.480 --> 01:32:43.760
So you want to focus resources.

01:32:43.760 --> 01:32:44.920
And you have to be really sure

01:32:44.920 --> 01:32:46.720
if you're going to bring in other sensors.

01:32:46.720 --> 01:32:49.480
You could add sensors to infinity.

01:32:49.480 --> 01:32:51.040
At some point, you need to draw the line.

01:32:51.040 --> 01:32:52.080
And I think in this case,

01:32:52.080 --> 01:32:54.520
you have to really consider the full cost

01:32:54.520 --> 01:32:56.880
of any one sensor that you're adopting.

01:32:56.880 --> 01:32:58.560
And do you really need it?

01:32:58.560 --> 01:33:00.800
And I think the answer in this case is no.

01:33:00.800 --> 01:33:02.440
So what do you think about the idea

01:33:02.440 --> 01:33:07.320
that the other companies are forming high resolution maps

01:33:07.320 --> 01:33:10.240
and constraining heavily the geographic regions

01:33:10.240 --> 01:33:11.680
in which they operate?

01:33:11.680 --> 01:33:15.640
Is that approach, in your view,

01:33:16.800 --> 01:33:18.800
not going to scale over time

01:33:18.800 --> 01:33:20.720
to the entirety of the United States?

01:33:20.720 --> 01:33:22.240
I think, as you mentioned,

01:33:22.240 --> 01:33:24.280
they pre-map all the environments

01:33:24.280 --> 01:33:25.880
and they need to refresh the map.

01:33:25.880 --> 01:33:28.240
And they have a perfect centimeter level accuracy map

01:33:28.240 --> 01:33:29.520
of everywhere they're going to drive.

01:33:29.520 --> 01:33:30.480
It's crazy.

01:33:30.480 --> 01:33:32.080
How are you going to...

01:33:32.080 --> 01:33:33.200
When we're talking about the autonomy

01:33:33.200 --> 01:33:34.360
actually changing the world,

01:33:34.360 --> 01:33:36.600
we're talking about the deployment

01:33:36.600 --> 01:33:39.080
on the global scale of autonomous systems

01:33:39.080 --> 01:33:40.360
for transportation.

01:33:40.360 --> 01:33:42.720
And if you need to maintain a centimeter accurate map

01:33:42.720 --> 01:33:46.080
for earth or like for many cities and keep them updated,

01:33:46.080 --> 01:33:48.240
it's a huge dependency that you're taking on.

01:33:48.240 --> 01:33:49.920
Huge dependency.

01:33:49.920 --> 01:33:51.520
It's a massive, massive dependency.

01:33:51.520 --> 01:33:54.360
And now you need to ask yourself, do you really need it?

01:33:54.360 --> 01:33:57.280
And humans don't need it, right?

01:33:57.280 --> 01:34:00.160
So it's very useful to have a low level map of like,

01:34:00.160 --> 01:34:01.720
okay, the connectivity of your road.

01:34:01.720 --> 01:34:03.280
You know that there's a fork coming up.

01:34:03.280 --> 01:34:04.320
When you drive an environment,

01:34:04.320 --> 01:34:05.600
you sort of have that high level understanding.

01:34:05.600 --> 01:34:07.400
It's like a small Google map.

01:34:07.400 --> 01:34:09.160
And Tesla uses Google map,

01:34:09.160 --> 01:34:13.000
like similar kind of resolution information in its system,

01:34:13.000 --> 01:34:14.520
but it will not pre-map environments

01:34:14.520 --> 01:34:16.360
to centimeter level accuracy.

01:34:16.360 --> 01:34:17.640
It's a crutch, it's a distraction.

01:34:17.640 --> 01:34:20.480
It costs entropy and it diffuses the team.

01:34:20.480 --> 01:34:21.480
It dilutes the team.

01:34:21.480 --> 01:34:23.400
And you're not focusing on what's actually necessary,

01:34:23.400 --> 01:34:25.440
which is a computer vision problem.

01:34:26.320 --> 01:34:29.360
What did you learn about machine learning,

01:34:29.360 --> 01:34:32.040
about engineering, about life, about yourself

01:34:32.040 --> 01:34:36.560
as one human being from working with Elon Musk?

01:34:36.560 --> 01:34:38.200
I think the most I've learned is about

01:34:38.200 --> 01:34:41.000
how to sort of run organizations efficiently

01:34:41.000 --> 01:34:43.600
and how to create efficient organizations

01:34:43.600 --> 01:34:46.280
and how to fight entropy in an organization.

01:34:46.280 --> 01:34:49.200
So human engineering in the fight against entropy.

01:34:49.200 --> 01:34:50.040
Yeah.

01:34:51.000 --> 01:34:53.520
I think Elon is a very efficient warrior

01:34:53.520 --> 01:34:56.200
in the fight against entropy in organizations.

01:34:56.200 --> 01:34:58.920
What does entropy in an organization look like exactly?

01:34:58.920 --> 01:35:03.360
It's process, it's process.

01:35:03.360 --> 01:35:05.240
Inefficiencies in the form of meetings

01:35:05.240 --> 01:35:06.080
and that kind of stuff.

01:35:06.080 --> 01:35:07.680
Yeah, meetings, he hates meetings.

01:35:07.680 --> 01:35:09.040
He keeps telling people to skip meetings

01:35:09.040 --> 01:35:10.840
if they're not useful.

01:35:10.840 --> 01:35:13.840
He basically runs the world's biggest startups,

01:35:13.840 --> 01:35:15.200
I would say.

01:35:15.200 --> 01:35:17.440
Tesla, SpaceX are the world's biggest startups.

01:35:17.440 --> 01:35:19.600
Tesla actually has multiple startups.

01:35:19.960 --> 01:35:21.360
I think it's better to look at it that way.

01:35:21.360 --> 01:35:25.280
And so I think he's extremely good at that.

01:35:25.280 --> 01:35:27.800
And yeah, he has a very good intuition

01:35:27.800 --> 01:35:30.240
for streamlining processes, making everything efficient.

01:35:30.240 --> 01:35:32.920
Best part is no part, simplifying, focusing,

01:35:34.040 --> 01:35:35.960
and just kind of removing barriers,

01:35:35.960 --> 01:35:38.000
moving very quickly, making big moves.

01:35:38.000 --> 01:35:39.560
All of this is very startupy,

01:35:39.560 --> 01:35:41.520
sort of seeming things, but at scale.

01:35:41.520 --> 01:35:44.240
So strong drive to simplify.

01:35:44.240 --> 01:35:48.000
From your perspective, I mean that also probably applies

01:35:48.120 --> 01:35:50.160
to just designing systems and machine learning

01:35:50.160 --> 01:35:52.360
and otherwise, like simplify, simplify.

01:35:52.360 --> 01:35:53.520
Yes.

01:35:53.520 --> 01:35:54.920
What do you think is the secret

01:35:54.920 --> 01:35:57.480
to maintaining the startup culture

01:35:57.480 --> 01:35:59.200
in a company that grows?

01:35:59.200 --> 01:36:02.400
Is there, can you introspect that?

01:36:03.880 --> 01:36:06.400
I do think you need someone in a powerful position

01:36:06.400 --> 01:36:07.960
with a big hammer like Elon,

01:36:07.960 --> 01:36:10.440
who's like the cheerleader for that idea

01:36:10.440 --> 01:36:12.760
and ruthlessly pursues it.

01:36:12.760 --> 01:36:14.800
If no one has a big enough hammer,

01:36:14.800 --> 01:36:17.160
everything turns into committees,

01:36:17.160 --> 01:36:19.960
democracy within the company process,

01:36:19.960 --> 01:36:22.120
talking to stakeholders, decision-making,

01:36:22.120 --> 01:36:24.320
just everything just crumbles.

01:36:24.320 --> 01:36:26.680
If you have a big person who is also really smart

01:36:26.680 --> 01:36:28.960
and has a big hammer, things move quickly.

01:36:29.960 --> 01:36:32.760
So you said your favorite scene in Interstellar

01:36:32.760 --> 01:36:35.840
is the intense docking scene with the AI and Cooper talking,

01:36:35.840 --> 01:36:38.360
saying, Cooper, what are you doing?

01:36:38.360 --> 01:36:40.400
Docking, it's not possible.

01:36:40.400 --> 01:36:41.960
No, it's necessary.

01:36:42.960 --> 01:36:43.800
Such a good line.

01:36:43.800 --> 01:36:45.640
By the way, just so many questions there.

01:36:45.680 --> 01:36:50.680
Why an AI in that scene presumably

01:36:50.720 --> 01:36:53.320
is supposed to be able to compute

01:36:53.320 --> 01:36:55.280
a lot more than the human.

01:36:55.280 --> 01:36:57.120
It's saying it's not optimal, why the human,

01:36:57.120 --> 01:36:58.000
I mean, that's a movie,

01:36:58.000 --> 01:37:02.240
but shouldn't the AI know much better than the human?

01:37:02.240 --> 01:37:04.400
Anyway, what do you think is the value

01:37:04.400 --> 01:37:07.600
of setting seemingly impossible goals?

01:37:07.600 --> 01:37:11.280
So like our initial intuition,

01:37:11.280 --> 01:37:14.920
which seems like something that you have taken on

01:37:15.160 --> 01:37:17.080
and Elon espouses,

01:37:17.080 --> 01:37:19.920
that where the initial intuition of the community

01:37:19.920 --> 01:37:21.800
might say this is very difficult,

01:37:21.800 --> 01:37:24.880
and then you take it on anyway with a crazy deadline,

01:37:24.880 --> 01:37:27.120
you just from a human engineering perspective,

01:37:29.240 --> 01:37:30.840
have you seen the value of that?

01:37:32.400 --> 01:37:34.720
I wouldn't say that setting impossible goals exactly

01:37:34.720 --> 01:37:35.760
is a good idea,

01:37:35.760 --> 01:37:38.360
but I think setting very ambitious goals is a good idea.

01:37:38.360 --> 01:37:42.120
I think there's what I call sub-linear scaling of difficulty,

01:37:42.280 --> 01:37:45.280
which means that 10X problems are not 10X hard,

01:37:45.280 --> 01:37:50.000
usually 10X harder problem is like two or three X harder

01:37:50.000 --> 01:37:51.080
to execute on,

01:37:51.080 --> 01:37:54.480
because if you wanna improve a system by 10%,

01:37:54.480 --> 01:37:55.680
it costs some amount of work,

01:37:55.680 --> 01:37:57.480
and if you wanna 10X improve the system,

01:37:57.480 --> 01:38:00.440
it doesn't cost 100X amount of work,

01:38:00.440 --> 01:38:02.200
and it's because you fundamentally change the approach,

01:38:02.200 --> 01:38:04.520
and if you start with that constraint,

01:38:04.520 --> 01:38:06.040
then some approaches are obviously dumb

01:38:06.040 --> 01:38:07.000
and not going to work,

01:38:07.000 --> 01:38:09.760
and it forces you to reevaluate,

01:38:09.800 --> 01:38:11.800
and I think it's a very interesting way

01:38:11.800 --> 01:38:13.880
of approaching problem solving.

01:38:13.880 --> 01:38:16.240
But it requires a weird kind of thinking,

01:38:16.240 --> 01:38:19.400
it's just going back to your PhD days,

01:38:19.400 --> 01:38:23.200
it's like, how do you think which ideas

01:38:23.200 --> 01:38:27.480
in the machine learning community are solvable?

01:38:27.480 --> 01:38:28.760
Yes.

01:38:28.760 --> 01:38:30.440
It requires, what is that?

01:38:30.440 --> 01:38:32.960
I mean, there's the cliche of first principles thinking,

01:38:32.960 --> 01:38:35.440
but it requires to basically ignore

01:38:35.440 --> 01:38:36.440
what the community is saying,

01:38:36.600 --> 01:38:40.840
because doesn't a community in science

01:38:40.840 --> 01:38:44.160
usually draw lines of what is and isn't possible?

01:38:44.160 --> 01:38:45.000
Right.

01:38:45.000 --> 01:38:48.520
And it's very hard to break out of that without going crazy.

01:38:48.520 --> 01:38:49.360
Yeah.

01:38:49.360 --> 01:38:50.520
I mean, I think a good example here

01:38:50.520 --> 01:38:52.840
is the deep learning revolution in some sense,

01:38:52.840 --> 01:38:56.200
because you could be in computer vision at that time

01:38:56.200 --> 01:39:00.360
during the deep learning revolution of 2012 and so on,

01:39:00.360 --> 01:39:03.040
you could be improving a computer vision stack by 10%,

01:39:03.040 --> 01:39:05.880
or you can just be saying, actually all of this is useless,

01:39:05.920 --> 01:39:07.880
and how do I do 10x better computer vision?

01:39:07.880 --> 01:39:11.080
Well, it's not probably by tuning a hog feature detector,

01:39:11.080 --> 01:39:12.760
I need a different approach.

01:39:12.760 --> 01:39:14.080
I need something that is scalable,

01:39:14.080 --> 01:39:17.200
going back to Richard Sutton's,

01:39:17.200 --> 01:39:18.720
and understanding sort of like the philosophy

01:39:18.720 --> 01:39:21.000
of the bitter lesson,

01:39:21.000 --> 01:39:21.840
and then being like,

01:39:21.840 --> 01:39:23.240
actually I need much more scalable system

01:39:23.240 --> 01:39:25.680
like a neural network that in principle works,

01:39:25.680 --> 01:39:27.000
and then having some deep believers

01:39:27.000 --> 01:39:29.440
that can actually execute on that mission and make it work.

01:39:29.440 --> 01:39:31.640
So that's the 10x solution.

01:39:31.640 --> 01:39:32.480
Yeah.

01:39:33.920 --> 01:39:35.600
What do you think is the timeline

01:39:35.600 --> 01:39:38.480
to solve the problem of autonomous driving?

01:39:38.480 --> 01:39:41.560
That's still in part an open question.

01:39:42.560 --> 01:39:43.400
Yeah.

01:39:43.400 --> 01:39:45.200
I think the tough thing with timelines of self-driving,

01:39:45.200 --> 01:39:47.960
obviously, is that no one has created self-driving.

01:39:47.960 --> 01:39:48.800
Yeah.

01:39:48.800 --> 01:39:50.440
So it's not like,

01:39:50.440 --> 01:39:52.160
what do you think is the timeline to build this bridge?

01:39:52.160 --> 01:39:54.000
Well, we've built million bridges before,

01:39:54.000 --> 01:39:55.280
here's how long that takes.

01:39:57.160 --> 01:40:00.160
No one has built autonomy, it's not obvious.

01:40:00.200 --> 01:40:02.920
Some parts turn out to be much easier than others.

01:40:02.920 --> 01:40:04.000
So it's really hard to forecast.

01:40:04.000 --> 01:40:06.760
You do your best based on trend lines and so on,

01:40:06.760 --> 01:40:07.680
and based on intuition,

01:40:07.680 --> 01:40:09.120
but that's why fundamentally

01:40:09.120 --> 01:40:10.840
it's just really hard to forecast this.

01:40:10.840 --> 01:40:11.680
No one has built.

01:40:11.680 --> 01:40:13.040
So even still like being inside of it,

01:40:13.040 --> 01:40:14.960
it's hard to do.

01:40:14.960 --> 01:40:16.640
Yes, some things turn out to be much harder,

01:40:16.640 --> 01:40:18.920
and some things turn out to be much easier.

01:40:18.920 --> 01:40:21.880
Do you try to avoid making forecasts?

01:40:21.880 --> 01:40:24.120
Because like Elon doesn't avoid them, right?

01:40:24.120 --> 01:40:26.280
And heads of car companies in the past

01:40:26.280 --> 01:40:28.080
have not avoided it either.

01:40:29.080 --> 01:40:31.480
Ford and other places have made predictions

01:40:31.480 --> 01:40:33.960
that we're gonna solve at level four driving

01:40:33.960 --> 01:40:36.560
by like 2020, 2021, whatever.

01:40:36.560 --> 01:40:39.720
And now they're all kind of backtracking that prediction.

01:40:39.720 --> 01:40:44.040
Are you, as an AI person,

01:40:45.000 --> 01:40:48.560
do you for yourself privately make predictions

01:40:48.560 --> 01:40:52.120
or do they get in the way of like your actual ability

01:40:52.120 --> 01:40:53.920
to think about a thing?

01:40:53.920 --> 01:40:55.080
Yeah, I would say like,

01:40:55.080 --> 01:40:57.600
what's easy to say is that this problem is tractable,

01:40:58.120 --> 01:40:59.200
and that's an easy prediction to make.

01:40:59.200 --> 01:41:00.800
It's tractable, it's going to work.

01:41:00.800 --> 01:41:02.000
Yes, it's just really hard.

01:41:02.000 --> 01:41:03.080
Some things turn out to be harder

01:41:03.080 --> 01:41:05.120
and some things turn out to be easier.

01:41:05.120 --> 01:41:08.200
So, but it definitely feels tractable.

01:41:08.200 --> 01:41:10.520
And it feels like at least the team at Tesla,

01:41:10.520 --> 01:41:11.760
which is what I saw internally,

01:41:11.760 --> 01:41:13.320
is definitely on track to that.

01:41:13.320 --> 01:41:17.640
How do you form a strong representation

01:41:17.640 --> 01:41:20.640
that allows you to make a prediction about tractability?

01:41:20.640 --> 01:41:23.720
So like you're the leader of a lot of humans.

01:41:24.720 --> 01:41:28.520
You have to kind of say this is actually possible.

01:41:28.520 --> 01:41:30.960
Like, how do you build up that intuition?

01:41:30.960 --> 01:41:32.440
It doesn't have to be even driving.

01:41:32.440 --> 01:41:33.640
It could be other tasks.

01:41:33.640 --> 01:41:37.720
It could be, what difficult tasks did you work on

01:41:37.720 --> 01:41:38.560
in your life?

01:41:38.560 --> 01:41:41.120
I mean, classification, achieving certain,

01:41:41.120 --> 01:41:42.120
just an image net,

01:41:42.120 --> 01:41:45.840
certain level of superhuman level performance.

01:41:45.840 --> 01:41:47.840
Yeah, expert intuition.

01:41:47.840 --> 01:41:48.680
It's just intuition.

01:41:48.680 --> 01:41:49.520
It's belief.

01:41:50.880 --> 01:41:52.720
So just like thinking about it long enough,

01:41:52.720 --> 01:41:54.640
like studying, looking at sample data,

01:41:54.640 --> 01:41:55.840
like you said, driving.

01:41:56.800 --> 01:41:59.000
My intuition is really flawed on this.

01:41:59.000 --> 01:42:01.400
Like I don't have a good intuition about tractability.

01:42:01.400 --> 01:42:03.240
It could be either, it could be anything.

01:42:03.240 --> 01:42:05.880
It could be solvable.

01:42:05.880 --> 01:42:10.840
Like, you know, the driving task could be simplified

01:42:10.840 --> 01:42:12.920
into something quite trivial.

01:42:12.920 --> 01:42:16.240
Like the solution to the problem would be quite trivial.

01:42:16.240 --> 01:42:20.400
And at scale, more and more cars driving perfectly

01:42:20.400 --> 01:42:22.240
might make the problem much easier.

01:42:22.640 --> 01:42:24.000
The more cars you have driving,

01:42:24.000 --> 01:42:26.640
like people learn how to drive correctly,

01:42:26.640 --> 01:42:29.480
not correctly, but in a way that's more optimal

01:42:29.480 --> 01:42:32.920
for a heterogeneous system of autonomous

01:42:32.920 --> 01:42:36.080
and semi-autonomous and manually driven cars.

01:42:36.080 --> 01:42:37.160
That could change stuff.

01:42:37.160 --> 01:42:40.600
Then again, also I've spent a ridiculous number of hours

01:42:40.600 --> 01:42:43.680
just staring at pedestrians crossing streets,

01:42:43.680 --> 01:42:45.360
thinking about humans.

01:42:45.360 --> 01:42:50.360
And it feels like the way we use our eye contact,

01:42:50.440 --> 01:42:52.760
it sends really strong signals

01:42:52.760 --> 01:42:55.560
and there's certain quirks and edge cases of behavior.

01:42:55.560 --> 01:42:57.680
And of course, a lot of the fatalities that happen

01:42:57.680 --> 01:42:59.760
have to do with drunk driving

01:42:59.760 --> 01:43:03.320
and both on the pedestrian side and the driver side.

01:43:03.320 --> 01:43:05.840
So there's that problem of driving at night

01:43:05.840 --> 01:43:06.800
and all that kind of.

01:43:06.800 --> 01:43:09.160
So I wonder, you know, it's like the space

01:43:10.360 --> 01:43:12.400
of possible solutions to autonomous driving

01:43:12.400 --> 01:43:15.680
includes so many human factor issues

01:43:15.680 --> 01:43:17.920
that it's almost impossible to predict.

01:43:17.920 --> 01:43:20.880
There could be super clean, nice solutions.

01:43:20.880 --> 01:43:23.440
Yeah, I would say definitely like to use a game analogy,

01:43:23.440 --> 01:43:25.160
there's some fog of war,

01:43:25.160 --> 01:43:28.400
but you definitely also see the frontier of improvement

01:43:28.400 --> 01:43:29.720
and you can measure historically

01:43:29.720 --> 01:43:31.360
how much you've made progress.

01:43:31.360 --> 01:43:33.240
And I think, for example, at least what I've seen

01:43:33.240 --> 01:43:35.360
in roughly five years at Tesla,

01:43:35.360 --> 01:43:38.960
when I joined, it barely kept lane on the highway.

01:43:38.960 --> 01:43:40.720
I think going up from Palo Alto to SF

01:43:40.720 --> 01:43:42.160
was like three or four interventions.

01:43:42.160 --> 01:43:44.680
Anytime the road would do anything geometrically

01:43:44.680 --> 01:43:47.080
or turn too much, it would just like not work.

01:43:47.120 --> 01:43:47.960
And so going from that

01:43:47.960 --> 01:43:50.200
to like a pretty competent system in five years

01:43:50.200 --> 01:43:52.560
and seeing what happens also under the hood

01:43:52.560 --> 01:43:54.240
and what the scale of which the team is operating now

01:43:54.240 --> 01:43:57.000
with respect to data and compute and everything else,

01:43:57.000 --> 01:43:59.120
it's just a massive progress.

01:44:01.040 --> 01:44:03.920
You're climbing a mountain and it's fog,

01:44:03.920 --> 01:44:05.760
but you're making a lot of progress.

01:44:05.760 --> 01:44:06.600
You're making progress

01:44:06.600 --> 01:44:07.960
and you see what the next directions are

01:44:07.960 --> 01:44:09.600
and you're looking at some of the remaining challenges

01:44:09.600 --> 01:44:12.360
and they're not like, they're not perturbing you

01:44:12.360 --> 01:44:13.640
and they're not changing your philosophy

01:44:13.640 --> 01:44:15.800
and you're not contorting yourself.

01:44:15.800 --> 01:44:16.640
You're like,

01:44:16.640 --> 01:44:18.120
actually these are the things that we still need to do.

01:44:18.120 --> 01:44:20.240
Yeah, the fundamental components of solving the problem

01:44:20.240 --> 01:44:22.600
seem to be there from the data engine to the compute

01:44:22.600 --> 01:44:25.600
to the compute on the car to the compute for the training,

01:44:25.600 --> 01:44:27.280
all that kind of stuff.

01:44:27.280 --> 01:44:30.480
So you've done, over the years you've been at Tesla,

01:44:30.480 --> 01:44:33.800
you've done a lot of amazing breakthrough ideas

01:44:33.800 --> 01:44:38.040
and engineering, all of it from the data engine

01:44:38.040 --> 01:44:40.240
to the human side, all of it.

01:44:40.240 --> 01:44:43.920
Can you speak to why you chose to leave Tesla?

01:44:43.920 --> 01:44:45.960
Basically, as I described that Ren,

01:44:45.960 --> 01:44:47.760
I think over time during those five years,

01:44:47.760 --> 01:44:49.720
I've kind of gotten myself

01:44:49.720 --> 01:44:52.440
into a little bit of a managerial position.

01:44:52.440 --> 01:44:54.960
Most of my days were meetings and growing the organization

01:44:54.960 --> 01:44:58.040
and making decisions about a sort of high level

01:44:58.040 --> 01:45:00.320
strategic decisions about the team

01:45:00.320 --> 01:45:02.400
and what it should be working on and so on.

01:45:02.400 --> 01:45:06.120
And it's kind of like a corporate executive role

01:45:06.120 --> 01:45:07.160
and I can do it.

01:45:07.160 --> 01:45:08.600
I think I'm okay at it,

01:45:08.600 --> 01:45:11.080
but it's not like fundamentally what I enjoy.

01:45:11.160 --> 01:45:14.040
And so I think when I joined,

01:45:14.040 --> 01:45:15.080
there was no computer vision team

01:45:15.080 --> 01:45:17.080
because Tesla was just going from the transition

01:45:17.080 --> 01:45:18.760
of using Mobileye, a third party vendor

01:45:18.760 --> 01:45:19.760
for all of its computer vision

01:45:19.760 --> 01:45:21.880
to having to build its computer vision system.

01:45:21.880 --> 01:45:22.720
So when I showed up,

01:45:22.720 --> 01:45:24.920
there were two people training deep neural networks

01:45:24.920 --> 01:45:26.640
and they were training them at a computer

01:45:26.640 --> 01:45:30.840
at their legs, like that was a workstation.

01:45:30.840 --> 01:45:32.480
They're doing some kind of basic classification task.

01:45:32.480 --> 01:45:35.640
Yeah, and so I kind of like grew that

01:45:35.640 --> 01:45:37.880
into what I think is a fairly respectable

01:45:37.880 --> 01:45:40.080
deep learning team, a massive compute cluster,

01:45:40.080 --> 01:45:42.920
a very good data annotation organization.

01:45:42.920 --> 01:45:45.280
And I was very happy with where that was.

01:45:45.280 --> 01:45:46.680
It became quite autonomous.

01:45:46.680 --> 01:45:49.600
And so I kind of stepped away and I, you know,

01:45:49.600 --> 01:45:52.320
I'm very excited to do much more technical things again.

01:45:52.320 --> 01:45:54.680
Yeah, and kind of like refocus on AGI.

01:45:54.680 --> 01:45:56.600
What was this soul searching like

01:45:56.600 --> 01:45:59.240
because you took a little time off and think like what,

01:45:59.240 --> 01:46:00.520
how many mushrooms did you take?

01:46:00.520 --> 01:46:01.360
No, I'm just kidding.

01:46:01.360 --> 01:46:03.760
I mean, what was going through your mind?

01:46:03.760 --> 01:46:06.200
The human lifetime is finite.

01:46:06.200 --> 01:46:07.040
Yeah.

01:46:07.040 --> 01:46:09.000
You did a few incredible things here.

01:46:09.000 --> 01:46:11.560
You're one of the best teachers of AI in the world.

01:46:11.560 --> 01:46:14.400
You're one of the best, and I don't mean that,

01:46:14.400 --> 01:46:15.920
I mean that in the best possible way,

01:46:15.920 --> 01:46:19.760
you're one of the best tinkerers in the AI world,

01:46:19.760 --> 01:46:23.320
meaning like understanding the fundamentals

01:46:23.320 --> 01:46:26.480
of how something works by building it from scratch

01:46:26.480 --> 01:46:28.560
and playing with it with the basic intuitions.

01:46:28.560 --> 01:46:30.360
It's like Einstein, Feynman,

01:46:30.360 --> 01:46:32.080
we're all really good at this kind of stuff.

01:46:32.080 --> 01:46:35.200
Like small example of a thing to play with it

01:46:35.200 --> 01:46:37.520
to try to understand it so that,

01:46:37.520 --> 01:46:38.960
and obviously now with Tessie,

01:46:38.960 --> 01:46:42.320
you helped build a team of machine learning

01:46:44.400 --> 01:46:45.840
like engineers and assistant

01:46:45.840 --> 01:46:47.960
that actually accomplishes something in the real world.

01:46:47.960 --> 01:46:51.800
So given all that, what was the soul searching like?

01:46:51.800 --> 01:46:54.840
Well, it was hard because obviously I love the company a lot

01:46:54.840 --> 01:46:57.200
and I love Elon, I love Tesla.

01:46:57.200 --> 01:47:00.160
I want, it was always hard to leave.

01:47:00.160 --> 01:47:04.280
I love the team basically, but yeah,

01:47:04.280 --> 01:47:06.960
I think actually I would be potentially interested

01:47:06.960 --> 01:47:10.160
in revisiting it, maybe coming back at some point,

01:47:10.160 --> 01:47:13.520
working in Optimus, working in AGI at Tesla.

01:47:13.520 --> 01:47:15.880
I think Tesla is going to do incredible things.

01:47:15.880 --> 01:47:20.120
It's basically like, it's a massive,

01:47:20.120 --> 01:47:22.520
large scale robotics kind of company

01:47:22.520 --> 01:47:23.640
with a ton of in-house talent

01:47:23.640 --> 01:47:25.120
for doing really incredible things.

01:47:25.120 --> 01:47:29.240
And I think human robots are going to be amazing.

01:47:29.240 --> 01:47:31.920
I think autonomous transportation is going to be amazing.

01:47:31.920 --> 01:47:32.960
All this is happening at Tesla.

01:47:32.960 --> 01:47:35.560
So I think it's just a really amazing organization.

01:47:35.560 --> 01:47:37.120
So being part of it and helping it along,

01:47:37.120 --> 01:47:39.960
I think was very, basically I enjoyed that a lot.

01:47:39.960 --> 01:47:41.600
Yeah, it was basically difficult for those reasons

01:47:41.600 --> 01:47:43.160
because I love the company,

01:47:43.160 --> 01:47:45.440
but I'm happy to potentially at some point

01:47:45.440 --> 01:47:48.720
come back for Act 2, but I felt like at this stage,

01:47:48.720 --> 01:47:51.040
I built the team, it felt autonomous

01:47:51.040 --> 01:47:53.240
and I became a manager

01:47:53.240 --> 01:47:54.800
and I wanted to do a lot more technical stuff.

01:47:54.800 --> 01:47:57.400
I wanted to learn stuff, I wanted to teach stuff.

01:47:57.400 --> 01:48:00.040
And I just kind of felt like it was a good time

01:48:00.040 --> 01:48:01.600
for a change of pace a little bit.

01:48:01.640 --> 01:48:05.160
What do you think is the best movie sequel

01:48:05.160 --> 01:48:07.200
of all time, speaking of part two?

01:48:07.200 --> 01:48:09.000
Because most of them suck.

01:48:09.000 --> 01:48:09.840
Movie sequels?

01:48:09.840 --> 01:48:11.000
Movie sequels, yeah.

01:48:11.000 --> 01:48:14.360
And you tweeted about movies, so this is a tiny tangent.

01:48:14.360 --> 01:48:18.480
Is there, what's your, what's like a favorite movie sequel?

01:48:18.480 --> 01:48:19.600
Godfather, part two?

01:48:20.680 --> 01:48:21.600
Are you a fan of Godfather?

01:48:21.600 --> 01:48:23.680
Because you didn't even tweet or mention the Godfather.

01:48:23.680 --> 01:48:24.680
Yeah, I don't love that movie.

01:48:24.680 --> 01:48:25.520
I know it hasn't reached-

01:48:25.520 --> 01:48:26.360
We're going to edit that out.

01:48:26.360 --> 01:48:28.920
We're going to edit out the hate towards the Godfather.

01:48:28.920 --> 01:48:29.760
How dare you disrespect-

01:48:29.760 --> 01:48:31.040
I think I will make a strong statement.

01:48:31.040 --> 01:48:32.760
I don't know why, I don't know why,

01:48:32.760 --> 01:48:37.560
but I basically don't like any movie before 1995,

01:48:37.560 --> 01:48:38.400
something like that.

01:48:38.400 --> 01:48:40.120
Didn't you mention Terminator 2?

01:48:40.120 --> 01:48:43.320
Okay, okay, that's like, Terminator 2 was

01:48:43.320 --> 01:48:45.640
a little bit later, 1990.

01:48:45.640 --> 01:48:47.480
No, I think Terminator 2 was in the 80s.

01:48:47.480 --> 01:48:48.880
And I like Terminator 1 as well.

01:48:48.880 --> 01:48:50.680
So, okay, so like few exceptions,

01:48:50.680 --> 01:48:52.200
but by and large, for some reason,

01:48:52.200 --> 01:48:55.440
I don't like movies before 1995 or something.

01:48:55.440 --> 01:48:58.120
They feel very slow, the camera is like zoomed out,

01:48:58.120 --> 01:49:00.920
it's boring, it's kind of naive, it's kind of weird.

01:49:01.760 --> 01:49:04.000
And also, Terminator was very much ahead of its time.

01:49:04.000 --> 01:49:07.120
Yes, and the Godfather, there's like no AGI, so.

01:49:07.120 --> 01:49:12.160
I mean, but you have Good Will Hunting

01:49:12.160 --> 01:49:13.960
was one of the movies you mentioned,

01:49:13.960 --> 01:49:15.760
and that doesn't have any AGI either.

01:49:15.760 --> 01:49:16.920
I guess that's mathematics.

01:49:16.920 --> 01:49:19.080
Yeah, I guess occasionally I do enjoy movies

01:49:19.080 --> 01:49:20.600
that don't feature.

01:49:20.600 --> 01:49:23.160
Or like Anchorman, that has no, that's-

01:49:23.160 --> 01:49:24.720
Anchorman is so good.

01:49:24.720 --> 01:49:28.440
I don't understand, speaking of AGI,

01:49:28.480 --> 01:49:31.960
because I don't understand why Will Ferrell is so funny.

01:49:31.960 --> 01:49:33.720
It doesn't make sense, it doesn't compute,

01:49:33.720 --> 01:49:35.400
there's just something about him.

01:49:35.400 --> 01:49:37.120
And he's a singular human,

01:49:37.120 --> 01:49:39.960
because you don't get that many comedies these days.

01:49:39.960 --> 01:49:42.440
And I wonder if it has to do about the culture

01:49:42.440 --> 01:49:44.400
or the machine of Hollywood,

01:49:44.400 --> 01:49:46.280
or does it have to do with just we got lucky

01:49:46.280 --> 01:49:49.200
with certain people in comedy that came together,

01:49:49.200 --> 01:49:50.600
because he is a singular human.

01:49:50.600 --> 01:49:53.200
Yeah, yeah, I like his movies.

01:49:53.200 --> 01:49:55.440
That was a ridiculous tangent, I apologize.

01:49:55.440 --> 01:49:57.280
But you mentioned human or a robot,

01:49:57.280 --> 01:50:00.640
so what do you think about Optimus, about TeslaBot?

01:50:00.640 --> 01:50:02.720
Do you think we'll have robots in the factory

01:50:02.720 --> 01:50:06.000
and in the home in 10, 20, 30, 40, 50 years?

01:50:06.000 --> 01:50:07.800
Yeah, I think it's a very hard project.

01:50:07.800 --> 01:50:09.080
I think it's going to take a while,

01:50:09.080 --> 01:50:12.200
but who else is going to build human or robots at scale?

01:50:12.200 --> 01:50:14.280
And I think it is a very good form factor to go after,

01:50:14.280 --> 01:50:15.640
because like I mentioned,

01:50:15.640 --> 01:50:17.800
the world is designed for human and form factor.

01:50:17.800 --> 01:50:19.800
These things would be able to operate our machines,

01:50:19.800 --> 01:50:22.160
they would be able to sit down in chairs,

01:50:22.160 --> 01:50:24.280
potentially even drive cars.

01:50:24.280 --> 01:50:25.920
Basically, the world is designed for humans,

01:50:25.920 --> 01:50:27.840
that's the form factor you want to invest into

01:50:27.840 --> 01:50:29.480
and make work over time.

01:50:29.480 --> 01:50:31.320
I think there's another school of thought,

01:50:31.320 --> 01:50:34.360
which is, okay, pick a problem and design a robot to it,

01:50:34.360 --> 01:50:36.520
but actually designing a robot and getting a whole data engine

01:50:36.520 --> 01:50:37.960
and everything behind it to work

01:50:37.960 --> 01:50:39.840
is actually an incredibly hard problem.

01:50:39.840 --> 01:50:41.920
So it makes sense to go after general interfaces

01:50:41.920 --> 01:50:44.920
that, okay, they are not perfect for any one given task,

01:50:44.920 --> 01:50:46.760
but they actually have the generality

01:50:46.760 --> 01:50:48.720
of just with a prompt with English,

01:50:48.720 --> 01:50:51.360
able to do something across.

01:50:51.360 --> 01:50:52.400
And so I think it makes a lot of sense

01:50:52.400 --> 01:50:57.400
to go after a general interface in the physical world.

01:50:58.480 --> 01:51:00.080
And I think it's a very difficult project.

01:51:00.080 --> 01:51:01.960
I think it's going to take time,

01:51:01.960 --> 01:51:05.000
but I see no other company that can execute on that vision.

01:51:05.000 --> 01:51:06.080
I think it's going to be amazing.

01:51:06.080 --> 01:51:08.400
Like basically physical labor,

01:51:08.400 --> 01:51:10.760
like if you think transportation is a large market,

01:51:10.760 --> 01:51:13.160
try physical labor, it's insane.

01:51:14.240 --> 01:51:15.520
But it's not just physical labor.

01:51:15.520 --> 01:51:18.680
To me, the thing that's also exciting is the social robotics.

01:51:18.680 --> 01:51:21.640
So the relationship we'll have on different levels

01:51:21.640 --> 01:51:23.360
with those robots.

01:51:23.360 --> 01:51:26.360
That's why I was really excited to see Optimus.

01:51:26.360 --> 01:51:30.920
Like people have criticized me for the excitement,

01:51:30.920 --> 01:51:34.680
but I've worked with a lot of research labs

01:51:34.680 --> 01:51:38.000
that do humanoid-legged robots,

01:51:38.000 --> 01:51:40.000
Boston Dynamics, Unitree,

01:51:40.000 --> 01:51:42.680
there's a lot of companies that do legged robots,

01:51:42.680 --> 01:51:47.680
but the elegance of the movement

01:51:47.840 --> 01:51:51.640
is a tiny, tiny part of the big picture.

01:51:51.640 --> 01:51:54.320
So integrating the two big exciting things to me

01:51:54.320 --> 01:51:58.360
about Tesla doing humanoid or any legged robots

01:51:58.360 --> 01:52:03.000
is clearly integrating into the data engine.

01:52:03.000 --> 01:52:05.080
So the data engine aspect,

01:52:05.080 --> 01:52:07.680
so the actual intelligence for the perception

01:52:07.680 --> 01:52:10.760
and the control and the planning and all that kind of stuff

01:52:10.760 --> 01:52:13.640
integrating into the fleet that you mentioned, right?

01:52:14.600 --> 01:52:17.400
And then speaking of fleet,

01:52:18.080 --> 01:52:19.400
the second thing is the mass manufacturers,

01:52:19.400 --> 01:52:24.400
just knowing culturally driving towards a simple robot

01:52:26.880 --> 01:52:29.440
that's cheap to produce at scale

01:52:29.440 --> 01:52:31.600
and doing that well, having experience to do that well,

01:52:31.600 --> 01:52:32.520
that changes everything.

01:52:32.520 --> 01:52:35.000
That's why that's a very different culture

01:52:35.000 --> 01:52:36.600
and style than Boston Dynamics.

01:52:36.600 --> 01:52:40.120
Who, by the way, those robots are just,

01:52:40.120 --> 01:52:44.480
the way they move, it'll be a very long time

01:52:44.480 --> 01:52:47.200
before Tesla can achieve the smoothness of movement.

01:52:48.040 --> 01:52:49.400
But that's not what it's about.

01:52:49.400 --> 01:52:52.280
It's about the entirety of the system

01:52:52.280 --> 01:52:54.760
like we talked about, the data engine and the fleet.

01:52:54.760 --> 01:52:55.720
That's super exciting.

01:52:55.720 --> 01:52:58.240
Even the initial sort of models.

01:52:58.240 --> 01:53:00.560
But that too was really surprising

01:53:00.560 --> 01:53:03.600
that in a few months you can get a prototype.

01:53:03.600 --> 01:53:05.760
Yep, and the reason that happened very quickly

01:53:05.760 --> 01:53:08.520
is as you alluded to, there's a ton of copy paste

01:53:08.520 --> 01:53:10.840
from what's happening on the autopilot, a lot.

01:53:10.840 --> 01:53:12.880
The amount of expertise that came out of the woodworks

01:53:12.880 --> 01:53:16.120
at Tesla for building the human robot was incredible to see.

01:53:17.120 --> 01:53:19.320
Basically, Elon said, at one point, we're doing this.

01:53:19.320 --> 01:53:23.000
And then next day, basically, all these CAD models

01:53:23.000 --> 01:53:23.960
started to appear.

01:53:23.960 --> 01:53:26.760
And people talk about the supply chain and manufacturing.

01:53:26.760 --> 01:53:29.480
And people showed up with screwdrivers and everything

01:53:29.480 --> 01:53:32.080
the other day and started to put together the body.

01:53:32.080 --> 01:53:34.600
And I was like, whoa, all these people exist at Tesla.

01:53:34.600 --> 01:53:35.720
And fundamentally, building a car

01:53:35.720 --> 01:53:38.040
is actually not that different from building a robot.

01:53:38.040 --> 01:53:41.600
And that is true not just for the hardware pieces.

01:53:41.600 --> 01:53:44.560
And also, let's not forget hardware, not just for a demo,

01:53:44.560 --> 01:53:48.840
but manufacturing of that hardware at scale.

01:53:48.840 --> 01:53:50.280
It is a whole different thing.

01:53:50.280 --> 01:53:52.600
But for software as well, basically,

01:53:52.600 --> 01:53:56.520
this robot currently thinks it's a car.

01:53:56.520 --> 01:53:59.400
It's going to have a midlife crisis at some point.

01:53:59.400 --> 01:54:00.800
It thinks it's a car.

01:54:00.800 --> 01:54:02.400
Some of the earlier demos, actually,

01:54:02.400 --> 01:54:03.720
we were talking about potentially doing them

01:54:03.720 --> 01:54:05.160
outside in the parking lot, because that's

01:54:05.160 --> 01:54:09.120
where all of the computer vision was working out of the box

01:54:09.120 --> 01:54:11.640
instead of inside.

01:54:11.640 --> 01:54:13.480
But all of the operating system, everything

01:54:13.480 --> 01:54:14.680
just copy-pastes.

01:54:14.680 --> 01:54:16.320
Computer vision mostly copy-pastes.

01:54:16.320 --> 01:54:17.520
I mean, you have to retrain the neural nets.

01:54:17.520 --> 01:54:19.080
But the approach and everything and data engine

01:54:19.080 --> 01:54:21.600
and offline trackers and the way we go about the occupancy

01:54:21.600 --> 01:54:23.640
tracker and so on, everything copy-pastes.

01:54:23.640 --> 01:54:25.920
You just need to retrain the neural nets.

01:54:25.920 --> 01:54:27.440
And then the planning control, of course,

01:54:27.440 --> 01:54:28.680
has to change quite a bit.

01:54:28.680 --> 01:54:30.680
But there's a ton of copy-paste from what's

01:54:30.680 --> 01:54:31.640
happening at Tesla.

01:54:31.640 --> 01:54:34.280
And so if you were to go with the goal of, OK,

01:54:34.280 --> 01:54:35.940
let's build a million human robots,

01:54:35.940 --> 01:54:38.680
and you're not Tesla, that's a lot to ask.

01:54:38.680 --> 01:54:42.880
If you're Tesla, it's actually like, it's not that crazy.

01:54:42.880 --> 01:54:44.760
And then the follow-up question is, then,

01:54:44.760 --> 01:54:46.300
how difficult, just like with driving,

01:54:46.300 --> 01:54:49.320
how difficult is the manipulation task such

01:54:49.320 --> 01:54:51.120
that it can have an impact at scale?

01:54:51.120 --> 01:54:53.640
I think, depending on the context,

01:54:53.640 --> 01:54:57.880
the really nice thing about robotics is that,

01:54:57.880 --> 01:55:00.240
unless you do a manufacturer and that kind of stuff,

01:55:00.240 --> 01:55:02.520
is there is more room for error.

01:55:02.520 --> 01:55:06.560
Driving is so safety-critical and also time-critical.

01:55:06.560 --> 01:55:10.040
A robot is allowed to move slower, which is nice.

01:55:10.040 --> 01:55:11.080
Yes.

01:55:11.080 --> 01:55:12.680
I think it's going to take a long time.

01:55:12.680 --> 01:55:14.440
The way you want to structure the development

01:55:14.440 --> 01:55:16.640
is you need to say, OK, it's going to take a long time.

01:55:16.640 --> 01:55:20.560
How can I set up the product development roadmap

01:55:20.560 --> 01:55:22.220
so that I'm making revenue along the way?

01:55:22.220 --> 01:55:24.440
I'm not setting myself up for a zero-one loss function

01:55:24.440 --> 01:55:26.120
where it doesn't work until it works.

01:55:26.120 --> 01:55:27.560
You don't want to be in that position.

01:55:27.560 --> 01:55:29.640
You want to make it useful almost immediately.

01:55:29.640 --> 01:55:33.640
And then you want to slowly deploy it and generalize it

01:55:33.640 --> 01:55:34.400
at scale.

01:55:34.400 --> 01:55:36.640
And you want to set up your data engine, your improvement

01:55:36.640 --> 01:55:39.480
loops, the telemetry, the evaluation, the harness,

01:55:39.480 --> 01:55:41.280
and everything.

01:55:41.280 --> 01:55:42.880
And you want to improve the product over time

01:55:42.880 --> 01:55:44.680
incrementally, and you're making revenue along the way.

01:55:44.680 --> 01:55:46.080
That's extremely important.

01:55:46.080 --> 01:55:49.520
Because otherwise, you cannot build these large undertakings

01:55:49.520 --> 01:55:51.280
just like don't make sense economically.

01:55:51.280 --> 01:55:53.280
And also, from the point of view of the team working on it,

01:55:53.280 --> 01:55:55.160
they need the dopamine along the way.

01:55:55.160 --> 01:55:57.600
They're not just going to make a promise about this

01:55:57.600 --> 01:55:58.720
being useful.

01:55:58.720 --> 01:56:01.120
This is going to change the world in 10 years when it works.

01:56:01.120 --> 01:56:02.360
This is not where you want to be.

01:56:02.360 --> 01:56:04.640
You want to be in a place like, I think, Autopilot is today

01:56:04.640 --> 01:56:09.040
where it's offering increased safety and convenience

01:56:09.040 --> 01:56:10.040
of driving today.

01:56:10.040 --> 01:56:10.680
People pay for it.

01:56:10.680 --> 01:56:11.400
People like it.

01:56:11.400 --> 01:56:12.680
People will purchase it.

01:56:12.680 --> 01:56:14.480
And then you also have the greater mission

01:56:14.480 --> 01:56:16.400
that you're working towards.

01:56:16.400 --> 01:56:17.120
And you see that.

01:56:17.120 --> 01:56:19.440
So the dopamine for the team, that

01:56:19.440 --> 01:56:21.320
was a source of happiness and satisfaction.

01:56:21.320 --> 01:56:21.800
Yes, 100%.

01:56:21.800 --> 01:56:22.480
You're deploying this.

01:56:22.480 --> 01:56:23.080
People like it.

01:56:23.080 --> 01:56:23.760
People drive it.

01:56:23.760 --> 01:56:24.840
People pay for it.

01:56:24.840 --> 01:56:25.720
They care about it.

01:56:25.720 --> 01:56:27.080
There's all these YouTube videos.

01:56:27.080 --> 01:56:28.240
Your grandma drives it.

01:56:28.240 --> 01:56:29.240
She gives you feedback.

01:56:29.240 --> 01:56:29.800
People like it.

01:56:29.800 --> 01:56:30.680
People engage with it.

01:56:30.680 --> 01:56:31.600
You engage with it.

01:56:31.600 --> 01:56:32.320
Huge.

01:56:32.320 --> 01:56:35.120
Do people that drive Teslas recognize you and give you

01:56:35.120 --> 01:56:35.680
love?

01:56:35.680 --> 01:56:42.080
Like, hey, thanks for this nice feature that it's doing.

01:56:42.080 --> 01:56:43.800
Yeah, I think the tricky thing is,

01:56:43.800 --> 01:56:44.800
some people really love you.

01:56:44.800 --> 01:56:46.360
Some people, unfortunately, you're

01:56:46.360 --> 01:56:47.200
working on something that you think

01:56:47.200 --> 01:56:49.000
is extremely valuable, useful, et cetera.

01:56:49.000 --> 01:56:50.360
Some people do hate you.

01:56:50.360 --> 01:56:53.000
There's a lot of people who hate me and the team

01:56:53.000 --> 01:56:55.400
and the whole project.

01:56:55.400 --> 01:56:56.400
And I think that-

01:56:56.400 --> 01:56:57.800
Are they Tesla drivers?

01:56:57.800 --> 01:56:59.800
In many cases, they're not, actually.

01:56:59.800 --> 01:57:02.800
Yeah, that actually makes me sad about humans

01:57:02.800 --> 01:57:06.400
or the current ways that humans interact.

01:57:06.400 --> 01:57:07.760
I think that's actually fixable.

01:57:07.760 --> 01:57:09.600
I think humans want to be good to each other.

01:57:09.600 --> 01:57:11.440
I think Twitter and social media is

01:57:11.440 --> 01:57:14.400
part of the mechanism that actually somehow makes

01:57:14.400 --> 01:57:17.520
the negativity more viral, that it doesn't deserve.

01:57:17.520 --> 01:57:23.640
Disproportionately add a viral boost to the negativity.

01:57:23.640 --> 01:57:27.240
But I wish people would just get excited about,

01:57:27.240 --> 01:57:30.520
so suppress some of the jealousy, some of the ego,

01:57:30.520 --> 01:57:32.200
and just get excited for others.

01:57:32.200 --> 01:57:34.400
And then there's a karma aspect to that.

01:57:34.400 --> 01:57:36.640
You get excited for others, they'll get excited for you.

01:57:36.640 --> 01:57:38.120
Same thing in academia.

01:57:38.120 --> 01:57:41.360
If you're not careful, there is a dynamical system there.

01:57:41.360 --> 01:57:44.760
If you think of in silos and get jealous of somebody else

01:57:44.760 --> 01:57:49.880
being successful, that actually, perhaps counter-intuitively,

01:57:49.880 --> 01:57:52.240
leads to less productivity of you as a community

01:57:52.240 --> 01:57:53.640
and you individually.

01:57:53.640 --> 01:57:56.720
I feel like if you keep celebrating others,

01:57:56.720 --> 01:57:59.760
that actually makes you more successful.

01:57:59.760 --> 01:58:02.400
I think people haven't, depending on the industry,

01:58:02.400 --> 01:58:04.280
haven't quite learned that yet.

01:58:04.280 --> 01:58:06.160
Some people are also very negative and very vocal.

01:58:06.160 --> 01:58:07.680
So they're very prominently featured.

01:58:07.680 --> 01:58:10.680
But actually, there's a ton of people who are cheerleaders,

01:58:10.680 --> 01:58:12.440
but they're silent cheerleaders.

01:58:12.440 --> 01:58:15.600
And when you talk to people just in the world,

01:58:15.600 --> 01:58:17.600
they will tell you, oh, it's amazing, it's great.

01:58:17.600 --> 01:58:19.320
Especially people who understand how difficult it

01:58:19.320 --> 01:58:20.520
is to get this stuff working.

01:58:20.520 --> 01:58:22.200
People who have built products and makers,

01:58:22.200 --> 01:58:26.920
entrepreneurs, making this work and changing something

01:58:26.920 --> 01:58:28.600
is incredibly hard.

01:58:28.600 --> 01:58:31.120
Those people are more likely to cheerlead you.

01:58:31.120 --> 01:58:32.640
Well, one of the things that makes me sad

01:58:32.640 --> 01:58:35.160
is some folks in the robotics community

01:58:35.160 --> 01:58:37.920
don't do the cheerleading, and they should.

01:58:37.920 --> 01:58:39.200
Because they know how difficult it is.

01:58:39.200 --> 01:58:41.120
Well, they actually sometimes don't know how difficult

01:58:41.120 --> 01:58:43.400
it is to create a product that's scale, right?

01:58:43.400 --> 01:58:45.240
They actually deploy it in the real world.

01:58:45.240 --> 01:58:50.040
A lot of the development of robots and AI system

01:58:50.040 --> 01:58:52.720
is done on very specific small benchmarks,

01:58:53.880 --> 01:58:55.640
as opposed to real world conditions.

01:58:55.640 --> 01:58:57.200
Yes.

01:58:57.200 --> 01:58:58.960
Yeah, I think it's really hard to work on robotics

01:58:58.960 --> 01:59:00.000
in an academic setting.

01:59:00.000 --> 01:59:02.200
Or AI systems that apply in the real world.

01:59:02.200 --> 01:59:07.200
You've criticized, you flourished and loved for time

01:59:08.240 --> 01:59:10.960
the ImageNet, the famed ImageNet data set,

01:59:10.960 --> 01:59:14.640
and have recently had some words of criticism

01:59:14.640 --> 01:59:18.600
that the academic research ML community

01:59:18.600 --> 01:59:21.160
gives a little too much love still to the ImageNet,

01:59:21.160 --> 01:59:23.280
or those kinds of benchmarks.

01:59:23.280 --> 01:59:27.000
Can you speak to the strengths and weaknesses of data sets

01:59:27.000 --> 01:59:29.200
used in machine learning research?

01:59:30.360 --> 01:59:32.320
Actually, I don't know that I recall the specific instance

01:59:32.320 --> 01:59:35.680
where I was unhappy or criticizing ImageNet.

01:59:35.680 --> 01:59:37.960
I think ImageNet has been extremely valuable.

01:59:38.840 --> 01:59:41.520
It was basically a benchmark that allowed

01:59:41.520 --> 01:59:44.000
the deep learning community to demonstrate

01:59:44.000 --> 01:59:45.840
that deep neural networks actually work.

01:59:45.840 --> 01:59:48.840
It was, there's a massive value in that.

01:59:49.680 --> 01:59:51.280
So I think ImageNet was useful,

01:59:51.280 --> 01:59:54.240
but basically it's become a bit of an MNIST at this point.

01:59:54.240 --> 01:59:57.720
So MNIST is like the little 228 by 28 grayscale digits.

01:59:57.720 --> 02:00:00.680
There's kind of a joke data set that everyone just crushes.

02:00:00.680 --> 02:00:02.880
There's still papers written on MNIST though, right?

02:00:02.880 --> 02:00:03.720
Maybe there shouldn't.

02:00:03.720 --> 02:00:04.840
Strong papers.

02:00:04.840 --> 02:00:07.800
Like papers that focus on how do we learn

02:00:07.800 --> 02:00:09.560
with a small amount of data, that kind of stuff.

02:00:09.560 --> 02:00:10.600
Yeah, I could see that being helpful,

02:00:10.600 --> 02:00:12.680
but not in sort of mainline computer vision research

02:00:12.680 --> 02:00:13.520
anymore, of course.

02:00:13.520 --> 02:00:15.400
I think the way I've heard you somewhere,

02:00:15.400 --> 02:00:17.160
maybe I'm just imagining things,

02:00:17.160 --> 02:00:19.760
but I think you said ImageNet was a huge contribution

02:00:19.760 --> 02:00:21.040
to the community for a long time,

02:00:21.040 --> 02:00:23.120
and now it's time to move past those kinds of...

02:00:23.120 --> 02:00:24.240
Well, ImageNet has been crushed.

02:00:24.240 --> 02:00:26.080
I mean, the error rates are...

02:00:28.480 --> 02:00:30.720
Yeah, we're getting like 90% accuracy

02:00:30.720 --> 02:00:34.880
in 1000 classification way prediction.

02:00:34.880 --> 02:00:38.880
And I've seen those images and it's like really high.

02:00:38.880 --> 02:00:40.320
That's really good.

02:00:40.320 --> 02:00:41.160
If I remember correctly,

02:00:41.160 --> 02:00:44.640
the top five error rate is now like 1% or something.

02:00:44.640 --> 02:00:47.920
Given your experience with a gigantic real world data set,

02:00:47.920 --> 02:00:49.720
would you like to see benchmarks move

02:00:49.720 --> 02:00:52.760
in a certain directions that the research community uses?

02:00:52.760 --> 02:00:54.320
Unfortunately, I don't think academics currently

02:00:54.320 --> 02:00:55.920
have the next ImageNet.

02:00:55.920 --> 02:00:57.760
We've obviously, I think we've crushed MNIST.

02:00:57.760 --> 02:01:00.240
We've basically kind of crushed ImageNet,

02:01:00.240 --> 02:01:02.840
and there's no next sort of big benchmark

02:01:02.840 --> 02:01:05.000
that the entire community rallies behind

02:01:05.000 --> 02:01:09.360
and uses for further development of these networks.

02:01:09.360 --> 02:01:11.040
Oh yeah, what are what it takes for a data set

02:01:11.040 --> 02:01:13.360
to captivate the imagination of everybody,

02:01:13.360 --> 02:01:15.400
like where they all get behind it.

02:01:15.400 --> 02:01:19.360
That could also need like a leader, right?

02:01:19.360 --> 02:01:20.520
Somebody with popularity.

02:01:20.520 --> 02:01:23.320
I mean, yeah, why did ImageNet take off?

02:01:24.240 --> 02:01:26.480
Is there, or is it just the accident of history?

02:01:26.480 --> 02:01:28.320
It was the right amount of difficult.

02:01:29.440 --> 02:01:31.600
It was the right amount of difficult and simple.

02:01:31.600 --> 02:01:33.960
And interesting enough, it just kind of like,

02:01:33.960 --> 02:01:36.520
it was the right time for that kind of a data set.

02:01:37.600 --> 02:01:38.680
Question from Reddit.

02:01:40.400 --> 02:01:43.520
What are your thoughts on the role that synthetic data

02:01:43.520 --> 02:01:45.240
and game engines will play in the future

02:01:45.240 --> 02:01:46.880
of neural net model development?

02:01:47.880 --> 02:01:51.880
I think as neural nets converge to humans,

02:01:53.200 --> 02:01:55.320
the value of simulation to neural nets

02:01:55.320 --> 02:01:57.800
will be similar to value of simulation to humans.

02:01:59.200 --> 02:02:02.800
So people use simulation for, people use simulation

02:02:02.800 --> 02:02:05.360
because they can learn something in that kind of a system

02:02:06.320 --> 02:02:08.880
and without having to actually experience it.

02:02:08.880 --> 02:02:10.480
But are you referring to the simulation

02:02:10.480 --> 02:02:11.920
we do in our head?

02:02:11.920 --> 02:02:14.160
No, sorry, simulation, I mean like video games

02:02:14.560 --> 02:02:19.560
or other forms of simulation for various professionals.

02:02:20.080 --> 02:02:21.440
So let me push back on that

02:02:21.440 --> 02:02:24.000
because maybe there's simulation that we do in our heads,

02:02:24.000 --> 02:02:28.760
like simulate, if I do this, what do I think will happen?

02:02:28.760 --> 02:02:30.200
Okay, that's like internal simulation.

02:02:30.200 --> 02:02:31.040
Yeah, internal.

02:02:31.040 --> 02:02:32.240
Isn't that what we're doing?

02:02:32.240 --> 02:02:33.480
Assume it before we act.

02:02:33.480 --> 02:02:34.960
Oh yeah, but that's independent from like the use

02:02:34.960 --> 02:02:37.240
of simulation in the sense of like computer games

02:02:37.240 --> 02:02:40.280
or using simulation for training set creation or.

02:02:40.280 --> 02:02:42.920
Is it independent or is it just loosely correlated?

02:02:42.920 --> 02:02:47.920
Because like, isn't that useful to do like counterfactual

02:02:49.040 --> 02:02:51.360
or like edge case simulation to like,

02:02:52.400 --> 02:02:55.000
what happens if there's a nuclear war?

02:02:55.000 --> 02:02:58.440
What happens if there's, like those kinds of things.

02:02:58.440 --> 02:03:00.640
Yeah, that's a different simulation from like Unreal Engine.

02:03:00.640 --> 02:03:02.400
That's how I interpreted the question.

02:03:02.400 --> 02:03:05.520
Ah, so like simulation of the average case.

02:03:06.960 --> 02:03:08.560
Is that, what's Unreal Engine?

02:03:08.560 --> 02:03:11.760
What do you mean by Unreal Engine?

02:03:11.760 --> 02:03:16.320
So simulating a world, the physics of that world,

02:03:17.440 --> 02:03:18.560
why is that different?

02:03:18.560 --> 02:03:22.080
Like, cause you also can add behavior to that world

02:03:22.080 --> 02:03:23.960
and you can try all kinds of stuff, right?

02:03:23.960 --> 02:03:27.280
Like you could throw all kinds of weird things into it.

02:03:27.280 --> 02:03:29.760
Unreal Engine is not just about simulating,

02:03:29.760 --> 02:03:31.520
I mean, I guess it is about simulating

02:03:31.520 --> 02:03:32.400
the physics of the world.

02:03:32.400 --> 02:03:35.400
It's also doing something with that.

02:03:35.400 --> 02:03:37.600
Yeah, the graphics, the physics and the agents

02:03:37.600 --> 02:03:39.480
that you put into the environment and stuff like that.

02:03:39.480 --> 02:03:40.320
Yeah.

02:03:40.480 --> 02:03:43.080
I feel like you said that it's not that important,

02:03:43.080 --> 02:03:45.520
I guess, for the future of AI development.

02:03:46.440 --> 02:03:48.320
Is that correct to interpret it that way?

02:03:48.320 --> 02:03:53.320
I think humans use simulators for,

02:03:53.520 --> 02:03:55.280
humans use simulators and they find them useful.

02:03:55.280 --> 02:03:57.000
And so computers will use simulators

02:03:57.000 --> 02:03:58.240
and find them useful.

02:03:59.160 --> 02:04:00.320
Okay, so you're saying it's not,

02:04:00.320 --> 02:04:02.120
I don't use simulators very often.

02:04:02.120 --> 02:04:03.680
I play a video game every once in a while,

02:04:03.680 --> 02:04:05.880
but I don't think I derive any wisdom

02:04:05.880 --> 02:04:09.320
about my own existence from those video games.

02:04:09.320 --> 02:04:11.680
It's a momentary escape from reality

02:04:11.680 --> 02:04:14.840
versus a source of wisdom about reality.

02:04:14.840 --> 02:04:17.160
So I think that's a very polite way

02:04:17.160 --> 02:04:19.240
of saying simulation is not that useful.

02:04:20.160 --> 02:04:21.120
Yeah, maybe not.

02:04:21.120 --> 02:04:22.920
I don't see it as like a fundamental,

02:04:22.920 --> 02:04:26.960
really important part of like training neural nets currently.

02:04:26.960 --> 02:04:29.840
But I think as neural nets become more and more powerful,

02:04:29.840 --> 02:04:32.360
I think you will need fewer examples

02:04:32.360 --> 02:04:34.400
to train additional behaviors.

02:04:34.400 --> 02:04:36.560
And simulation is, of course,

02:04:36.560 --> 02:04:38.120
there's a domain gap in a simulation

02:04:38.120 --> 02:04:39.120
that it's not the real world,

02:04:39.120 --> 02:04:40.520
it's slightly something different.

02:04:40.520 --> 02:04:42.920
But with a powerful enough neural net,

02:04:42.920 --> 02:04:45.760
you need, the domain gap can be bigger, I think,

02:04:45.760 --> 02:04:47.360
because neural net will sort of understand

02:04:47.360 --> 02:04:48.720
that even though it's not the real world,

02:04:48.720 --> 02:04:50.480
it like has all this high level structure

02:04:50.480 --> 02:04:52.280
that I'm supposed to be able to like learn from.

02:04:52.280 --> 02:04:54.360
So the neural net will actually,

02:04:54.360 --> 02:04:59.320
yeah, it will be able to leverage the synthetic data better

02:04:59.320 --> 02:05:01.760
by closing the gap and understanding

02:05:01.760 --> 02:05:04.240
in which ways this is not real data.

02:05:04.240 --> 02:05:05.080
Exactly.

02:05:06.360 --> 02:05:08.040
Right, I do better questions next time.

02:05:08.960 --> 02:05:10.600
That was a question, but I'm just kidding.

02:05:10.600 --> 02:05:11.440
All right.

02:05:14.200 --> 02:05:16.320
So is it possible, do you think,

02:05:16.320 --> 02:05:18.640
speaking of MNIST, to construct neural nets

02:05:18.640 --> 02:05:21.360
and training processes that require very little data?

02:05:23.280 --> 02:05:25.000
So we've been talking about huge data sets

02:05:25.000 --> 02:05:26.680
like the internet for training.

02:05:26.680 --> 02:05:28.440
I mean, one way to say that is, like you said,

02:05:28.440 --> 02:05:31.600
like the querying itself is another level of training,

02:05:31.600 --> 02:05:34.200
I guess, and that requires a little data.

02:05:34.200 --> 02:05:38.920
But do you see any value in doing research

02:05:38.920 --> 02:05:41.760
and kind of going down the direction of,

02:05:41.760 --> 02:05:44.160
can we use very little data to train,

02:05:44.160 --> 02:05:45.400
to construct a knowledge base?

02:05:45.400 --> 02:05:46.240
100%.

02:05:46.240 --> 02:05:49.000
I just think like at some point you need a massive data set.

02:05:49.000 --> 02:05:51.120
And then when you pre-train your massive neural net

02:05:51.120 --> 02:05:54.280
and get something that is like a GPT or something,

02:05:54.280 --> 02:05:57.440
then you're able to be very efficient at training

02:05:57.440 --> 02:05:58.680
any arbitrary new task.

02:05:59.560 --> 02:06:02.320
So a lot of these GPTs, you can do tasks

02:06:02.320 --> 02:06:04.880
like sentiment analysis or translation or so on,

02:06:04.880 --> 02:06:06.960
just by being prompted with very few examples.

02:06:06.960 --> 02:06:08.120
Here's the kind of thing I want you to do.

02:06:08.120 --> 02:06:09.720
Like, here's an input sentence,

02:06:09.720 --> 02:06:11.240
here's the translation into German.

02:06:11.240 --> 02:06:12.840
Input sentence, translation to German.

02:06:12.840 --> 02:06:14.560
Input sentence, blank,

02:06:14.560 --> 02:06:16.720
and the neural net will complete the translation to German

02:06:16.720 --> 02:06:19.920
just by looking at sort of the example you've provided.

02:06:19.920 --> 02:06:23.520
And so that's an example of a very few-shot learning

02:06:23.520 --> 02:06:24.840
in the activations of the neural net

02:06:24.840 --> 02:06:26.400
instead of the weights of the neural net.

02:06:26.400 --> 02:06:29.960
And so I think basically, just like humans,

02:06:29.960 --> 02:06:31.640
neural nets will become very data efficient

02:06:31.680 --> 02:06:33.760
at learning any other new task.

02:06:33.760 --> 02:06:35.560
But at some point, you need a massive data set

02:06:35.560 --> 02:06:36.880
to pre-train your network.

02:06:38.040 --> 02:06:40.080
To get that, and probably we humans

02:06:40.080 --> 02:06:41.560
have something like that.

02:06:41.560 --> 02:06:42.920
Do we have something like that?

02:06:42.920 --> 02:06:47.080
Do we have a passive, in the background,

02:06:47.080 --> 02:06:50.520
background model constructing thing

02:06:50.520 --> 02:06:52.960
that just runs all the time in a self-supervised way,

02:06:52.960 --> 02:06:54.240
we're not conscious of it?

02:06:54.240 --> 02:06:55.240
I think humans definitely.

02:06:55.240 --> 02:06:56.920
I mean, obviously we have,

02:06:56.920 --> 02:06:59.800
we learn a lot during our lifespan,

02:06:59.800 --> 02:07:02.080
but also we have a ton of hardware

02:07:02.080 --> 02:07:06.200
that helps us at initialization coming from sort of evolution.

02:07:06.200 --> 02:07:08.800
And so I think that's also a really big component.

02:07:08.800 --> 02:07:09.760
A lot of people in the field,

02:07:09.760 --> 02:07:12.000
I think they just talk about the amounts of like seconds

02:07:12.000 --> 02:07:14.000
and the, you know, that a person has lived

02:07:14.000 --> 02:07:16.200
pretending that this is a WLRRSA,

02:07:16.200 --> 02:07:18.840
sort of like a zero initialization of a neural net.

02:07:18.840 --> 02:07:21.120
And it's not, like, you can look at a lot of animals,

02:07:21.120 --> 02:07:22.640
like for example, zebras.

02:07:22.640 --> 02:07:27.040
Zebras get born and they see and they can run.

02:07:27.040 --> 02:07:29.400
There's zero train data in their lifespan.

02:07:29.400 --> 02:07:30.600
They can just do that.

02:07:30.600 --> 02:07:33.760
So somehow I have no idea how evolution has found a way

02:07:33.760 --> 02:07:35.200
to encode these algorithms

02:07:35.200 --> 02:07:36.520
and these neural net initializations

02:07:36.520 --> 02:07:38.840
that are extremely good into ATCGs.

02:07:38.840 --> 02:07:40.040
And I have no idea how this works,

02:07:40.040 --> 02:07:41.120
but apparently it's possible

02:07:41.120 --> 02:07:44.240
because here's a proof by existence.

02:07:44.240 --> 02:07:48.040
There's something magical about going from a single cell

02:07:48.040 --> 02:07:51.640
to an organism that is born to the first few years of life.

02:07:51.640 --> 02:07:52.600
I kind of like the idea

02:07:52.600 --> 02:07:54.440
that the reason we don't remember anything

02:07:54.440 --> 02:07:57.040
about the first few years of our life

02:07:57.120 --> 02:07:59.520
is that it's a really painful process.

02:07:59.520 --> 02:08:03.320
Like it's a very difficult, challenging training process.

02:08:03.320 --> 02:08:04.160
Yeah.

02:08:04.160 --> 02:08:05.680
Like intellectually.

02:08:05.680 --> 02:08:09.520
Like, and maybe, yeah, I mean, I don't,

02:08:09.520 --> 02:08:11.800
why don't we remember any of that?

02:08:11.800 --> 02:08:14.560
There might be some crazy training going on

02:08:14.560 --> 02:08:19.520
and that the, maybe that's the background model training

02:08:19.520 --> 02:08:23.160
that is very painful.

02:08:23.160 --> 02:08:25.760
And so it's best for the system once it's trained

02:08:25.760 --> 02:08:27.800
not to remember how it's constructed.

02:08:27.800 --> 02:08:30.160
I think it's just like the hardware for long-term memory

02:08:30.160 --> 02:08:31.960
is just not fully developed.

02:08:31.960 --> 02:08:35.760
I kind of feel like the first few years of infants

02:08:35.760 --> 02:08:37.000
is not actually like learning.

02:08:37.000 --> 02:08:38.200
It's brain maturing.

02:08:38.200 --> 02:08:39.400
Yeah.

02:08:39.400 --> 02:08:40.560
We're born premature.

02:08:41.680 --> 02:08:43.080
There's a theory along those lines

02:08:43.080 --> 02:08:45.480
because of the birth canal and the swallowing of the brain.

02:08:45.480 --> 02:08:46.720
And so we're born premature.

02:08:46.720 --> 02:08:48.200
And then the first few years we're just,

02:08:48.200 --> 02:08:49.560
the brain is maturing

02:08:49.560 --> 02:08:51.600
and then there's some learning eventually.

02:08:52.800 --> 02:08:53.720
That's my current view on it.

02:08:53.720 --> 02:08:55.720
What do you think,

02:08:55.720 --> 02:08:58.800
do you think neural nets can have long-term memory?

02:08:59.840 --> 02:09:02.040
Like that approach is something like humans.

02:09:02.040 --> 02:09:02.880
Do you think neural,

02:09:02.880 --> 02:09:04.960
do you think there needs to be another meta architecture

02:09:04.960 --> 02:09:07.680
on top of it to add something like a knowledge base

02:09:07.680 --> 02:09:09.400
that learns facts about the world

02:09:09.400 --> 02:09:10.600
and all that kind of stuff?

02:09:10.600 --> 02:09:12.640
Yes, but I don't know to what extent

02:09:12.640 --> 02:09:14.760
it will be explicitly constructed.

02:09:15.840 --> 02:09:17.520
It might take unintuitive forms

02:09:17.520 --> 02:09:20.160
where you are telling the GPT like,

02:09:20.160 --> 02:09:22.880
hey, you have a declarative memory bank

02:09:22.880 --> 02:09:25.160
to which you can store and retrieve data from.

02:09:25.160 --> 02:09:26.920
And whenever you encounter some information

02:09:26.920 --> 02:09:29.720
that you find useful, just save it to your memory bank.

02:09:29.720 --> 02:09:32.160
And here's an example of something you have retrieved

02:09:32.160 --> 02:09:33.320
and here's how you say it.

02:09:33.320 --> 02:09:34.440
And here's how you load from it.

02:09:34.440 --> 02:09:36.840
You just say load, whatever,

02:09:36.840 --> 02:09:39.200
you teach it in text in English

02:09:39.200 --> 02:09:42.440
and then it might learn to use a memory bank from that.

02:09:42.440 --> 02:09:45.400
Oh, so the neural net is the architecture

02:09:45.400 --> 02:09:48.240
for the background model, the base thing.

02:09:48.240 --> 02:09:50.240
And then everything else is just on top of it.

02:09:50.240 --> 02:09:51.680
So it's not just text, right?

02:09:51.680 --> 02:09:52.960
You're giving it gadgets and gizmos.

02:09:52.960 --> 02:09:55.920
So you're teaching some kind of a special language

02:09:55.920 --> 02:09:58.160
by which it can save arbitrary information

02:09:58.160 --> 02:09:59.720
and retrieve it at a later time.

02:09:59.720 --> 02:10:01.720
And you're telling about these special tokens

02:10:01.720 --> 02:10:04.760
and how to arrange them to use these interfaces.

02:10:04.760 --> 02:10:06.400
It's like, hey, you can use a calculator.

02:10:06.400 --> 02:10:07.240
Here's how you use it.

02:10:07.240 --> 02:10:10.240
Just do five, three plus four, one equals.

02:10:10.240 --> 02:10:12.640
And when equals is there,

02:10:12.640 --> 02:10:14.560
a calculator will actually read out the answer

02:10:14.560 --> 02:10:16.240
and you don't have to calculate it yourself.

02:10:16.240 --> 02:10:17.600
And you just like tell it in English.

02:10:17.600 --> 02:10:19.600
This might actually work.

02:10:19.640 --> 02:10:20.880
Do you think in that sense,

02:10:20.880 --> 02:10:23.200
GATO is interesting, the DeepMind system,

02:10:23.200 --> 02:10:24.800
that it's not just a language,

02:10:24.800 --> 02:10:28.720
but actually throws it all in the same pile.

02:10:28.720 --> 02:10:31.760
Images, actions, all that kind of stuff.

02:10:31.760 --> 02:10:34.200
That's basically what we're moving towards.

02:10:34.200 --> 02:10:35.040
Yeah, I think so.

02:10:35.040 --> 02:10:38.400
So GATO is very much a kitchen sink approach

02:10:38.400 --> 02:10:42.460
to reinforcement learning in lots of different environments

02:10:42.460 --> 02:10:45.340
with a single fixed transformer model, right?

02:10:46.680 --> 02:10:48.920
I think it's a very sort of early result

02:10:49.360 --> 02:10:50.200
in that realm.

02:10:50.200 --> 02:10:51.600
But I think, yeah, it's along the lines

02:10:51.600 --> 02:10:53.520
of what I think things will eventually look like.

02:10:53.520 --> 02:10:56.000
Right, so this is the early days of a system

02:10:56.000 --> 02:10:57.320
that eventually will look like this

02:10:57.320 --> 02:11:00.000
from a rich, sudden perspective.

02:11:00.000 --> 02:11:01.620
Yeah, I'm not super huge fan of, I think,

02:11:01.620 --> 02:11:04.920
all these interfaces that look very different.

02:11:04.920 --> 02:11:07.440
I would want everything to be normalized into the same API.

02:11:07.440 --> 02:11:10.240
So for example, screen pixels, very same API,

02:11:10.240 --> 02:11:12.000
instead of having different world environments

02:11:12.000 --> 02:11:14.180
that have very different physics and joint configurations

02:11:14.180 --> 02:11:15.680
and appearances and whatever,

02:11:15.680 --> 02:11:17.240
and you're having some kind of special tokens

02:11:17.240 --> 02:11:19.600
for different games that you can plug.

02:11:19.600 --> 02:11:22.600
I'd rather just normalize everything to a single interface

02:11:22.600 --> 02:11:25.080
so it looks the same to the neural net, if that makes sense.

02:11:25.080 --> 02:11:27.680
So it's all gonna be pixel-based pong in the end?

02:11:27.680 --> 02:11:28.500
I think so.

02:11:28.500 --> 02:11:33.500
Okay, let me ask you about your own personal life.

02:11:35.880 --> 02:11:37.960
A lot of people wanna know you're one of the most productive

02:11:37.960 --> 02:11:40.320
and brilliant people in the history of AI.

02:11:40.320 --> 02:11:42.200
What is a productive day in the life

02:11:42.200 --> 02:11:44.520
of Andrej Karpathy look like?

02:11:44.520 --> 02:11:45.560
What time do you wake up?

02:11:46.120 --> 02:11:49.000
Because imagine some kind of dance

02:11:49.000 --> 02:11:50.720
between the average productive day

02:11:50.720 --> 02:11:51.980
and a perfect productive day.

02:11:51.980 --> 02:11:55.440
So the perfect productive day is the thing we strive towards

02:11:55.440 --> 02:11:58.200
and the average is what it converges to,

02:11:58.200 --> 02:12:01.840
given all the mistakes and human eventualities and so on.

02:12:01.840 --> 02:12:03.160
So what time do you wake up?

02:12:03.160 --> 02:12:04.480
Are you a morning person?

02:12:04.480 --> 02:12:05.600
I'm not a morning person.

02:12:05.600 --> 02:12:07.360
I'm a night owl, for sure.

02:12:07.360 --> 02:12:09.040
Is it stable or not?

02:12:09.040 --> 02:12:12.600
Semi-stable, like eight or nine or something like that.

02:12:12.600 --> 02:12:14.520
During my PhD, it was even later,

02:12:14.520 --> 02:12:16.680
I used to go to sleep usually at 3 a.m.

02:12:16.680 --> 02:12:19.920
I think the a.m. hours are precious

02:12:19.920 --> 02:12:21.120
and very interesting time to work

02:12:21.120 --> 02:12:23.120
because everyone is asleep.

02:12:23.120 --> 02:12:26.300
At 8 a.m. or 7 a.m., the East Coast is awake.

02:12:26.300 --> 02:12:27.420
So there's already activity,

02:12:27.420 --> 02:12:29.200
there's already some text messages, whatever,

02:12:29.200 --> 02:12:30.040
there's stuff happening.

02:12:30.040 --> 02:12:31.880
You can go on some news website

02:12:31.880 --> 02:12:34.160
and there's stuff happening, it's distracting.

02:12:34.160 --> 02:12:36.560
At 3 a.m., everything is totally quiet.

02:12:36.560 --> 02:12:37.800
And so you're not gonna be bothered

02:12:37.800 --> 02:12:40.260
and you have solid chunks of time to do work.

02:12:40.900 --> 02:12:45.220
So I like those periods, night owl by default.

02:12:45.220 --> 02:12:48.340
And then I think like productive time, basically.

02:12:48.340 --> 02:12:50.940
What I like to do is you need to build some momentum

02:12:50.940 --> 02:12:53.620
on the problem without too much distraction.

02:12:53.620 --> 02:12:57.020
And you need to load your RAM,

02:12:57.980 --> 02:13:00.380
your working memory with that problem.

02:13:00.380 --> 02:13:01.700
And then you need to be obsessed with it

02:13:01.700 --> 02:13:04.020
when you're taking shower, when you're falling asleep.

02:13:04.020 --> 02:13:05.340
You need to be obsessed with the problem

02:13:05.340 --> 02:13:06.560
and it's fully in your memory

02:13:06.560 --> 02:13:08.980
and you're ready to wake up and work on it right there.

02:13:08.980 --> 02:13:13.380
So is this in a scale, temporal scale of a single day

02:13:13.380 --> 02:13:15.100
or a couple of days, a week, a month?

02:13:15.100 --> 02:13:17.580
So I can't talk about one day basically in isolation

02:13:17.580 --> 02:13:19.420
because it's a whole process.

02:13:19.420 --> 02:13:21.540
When I wanna get productive in the problem,

02:13:21.540 --> 02:13:23.940
I feel like I need a span of a few days

02:13:23.940 --> 02:13:26.540
where I can really get in on that problem

02:13:26.540 --> 02:13:27.620
and I don't wanna be interrupted.

02:13:27.620 --> 02:13:30.160
And I'm going to just be completely obsessed

02:13:30.160 --> 02:13:31.220
with that problem.

02:13:31.220 --> 02:13:34.100
And that's where I do most of my good workouts.

02:13:34.100 --> 02:13:36.460
You've done a bunch of cool little projects

02:13:36.460 --> 02:13:38.460
in a very short amount of time, very quickly.

02:13:38.460 --> 02:13:40.900
So that requires you just focusing on it.

02:13:40.900 --> 02:13:42.500
Yeah, basically I need to load my working memory

02:13:42.500 --> 02:13:44.440
with the problem and I need to be productive

02:13:44.440 --> 02:13:46.340
because there's always like a huge fixed cost

02:13:46.340 --> 02:13:47.700
to approaching any problem.

02:13:49.540 --> 02:13:51.140
I was struggling with this, for example, at Tesla

02:13:51.140 --> 02:13:53.580
because I want to work on like small side project,

02:13:53.580 --> 02:13:54.980
but okay, you first need to figure out,

02:13:54.980 --> 02:13:56.540
oh, okay, I need to SSH into my cluster.

02:13:56.540 --> 02:13:59.500
I need to bring up a VS code editor so I can work on this.

02:13:59.500 --> 02:14:01.900
I need to, I ran into some stupid error

02:14:01.900 --> 02:14:03.240
because of some reason.

02:14:03.240 --> 02:14:04.400
Like you're not at a point where you can be

02:14:04.400 --> 02:14:05.660
just productive right away.

02:14:05.660 --> 02:14:07.540
You are facing barriers.

02:14:07.580 --> 02:14:11.500
And so it's about really removing all of that barrier

02:14:11.500 --> 02:14:12.900
and you're able to go into the problem

02:14:12.900 --> 02:14:15.420
and you have the full problem loaded in your memory.

02:14:15.420 --> 02:14:18.260
And somehow avoiding distractions of all different forms

02:14:18.260 --> 02:14:23.260
like news stories, emails, but also distractions

02:14:23.260 --> 02:14:26.260
from other interesting projects that you previously worked on

02:14:26.260 --> 02:14:28.060
or currently working on and so on.

02:14:28.060 --> 02:14:29.820
You just want to really focus your mind.

02:14:29.820 --> 02:14:32.140
And I mean, I can take some time off for distractions

02:14:32.140 --> 02:14:35.420
and in between, but I think it can't be too much.

02:14:35.420 --> 02:14:38.060
Most of your day is sort of like spent on that problem.

02:14:38.060 --> 02:14:41.100
And then I drink coffee.

02:14:41.100 --> 02:14:42.260
I have my morning routine.

02:14:42.260 --> 02:14:45.060
I look at some news, Twitter, Hacker News,

02:14:45.060 --> 02:14:46.700
Wall Street Journal, et cetera.

02:14:46.700 --> 02:14:47.540
So it's great.

02:14:47.540 --> 02:14:49.460
So basically you wake up, you have some coffee.

02:14:49.460 --> 02:14:51.420
Are you trying to get to work as quickly as possible?

02:14:51.420 --> 02:14:54.540
Do you take in this diet of like

02:14:54.540 --> 02:14:56.500
what the hell is happening in the world first?

02:14:56.500 --> 02:14:59.700
I am, I do find it interesting to know about the world.

02:14:59.700 --> 02:15:01.980
I don't know that it's useful or good,

02:15:01.980 --> 02:15:03.620
but it is part of my routine right now.

02:15:03.620 --> 02:15:05.340
So I do read through a bunch of news articles

02:15:05.340 --> 02:15:09.940
and I want to be informed and I'm suspicious of it.

02:15:09.940 --> 02:15:10.980
I'm suspicious of the practice,

02:15:10.980 --> 02:15:12.340
but currently that's where I am.

02:15:12.340 --> 02:15:15.820
Oh, you mean suspicious about the positive effect

02:15:15.820 --> 02:15:18.100
of that practice on your productivity

02:15:18.100 --> 02:15:19.300
and your wellbeing.

02:15:19.300 --> 02:15:21.100
My wellbeing psychologically, yeah.

02:15:21.100 --> 02:15:23.580
And also on your ability to deeply understand the world

02:15:23.580 --> 02:15:26.500
because there's a bunch of sources of information.

02:15:26.500 --> 02:15:28.540
You're not really focused on deeply integrating it.

02:15:28.540 --> 02:15:30.820
Yeah, it's a little distracting, yeah.

02:15:30.820 --> 02:15:33.220
In terms of a perfectly productive day

02:15:33.260 --> 02:15:37.420
for how long of a stretch of time in one session

02:15:37.420 --> 02:15:39.780
do you try to work and focus on a thing?

02:15:39.780 --> 02:15:42.060
A couple hours, is it one hour, is it 30 minutes,

02:15:42.060 --> 02:15:43.620
is it 10 minutes?

02:15:43.620 --> 02:15:45.420
I can probably go like a small few hours

02:15:45.420 --> 02:15:47.180
and then I need some breaks in between

02:15:47.180 --> 02:15:48.620
for like food and stuff.

02:15:48.620 --> 02:15:52.500
And yeah, but I think like it's still really hard

02:15:52.500 --> 02:15:53.580
to accumulate hours.

02:15:53.580 --> 02:15:55.460
I was using a tracker that told me exactly

02:15:55.460 --> 02:15:57.100
how much time I spent coding any one day.

02:15:57.100 --> 02:15:58.740
And even on a very productive day,

02:15:58.740 --> 02:16:00.940
I still spent only like six or eight hours.

02:16:00.940 --> 02:16:01.780
Yeah.

02:16:01.780 --> 02:16:03.540
And it's just because there's so much padding.

02:16:03.540 --> 02:16:07.300
Commute, talking to people, food, et cetera.

02:16:07.300 --> 02:16:09.540
There's like a cost of life,

02:16:09.540 --> 02:16:12.740
just living and sustaining and homeostasis

02:16:12.740 --> 02:16:15.940
and just maintaining yourself as a human is very high.

02:16:15.940 --> 02:16:18.740
And that there seems to be a desire

02:16:18.740 --> 02:16:21.980
within the human mind to participate in society

02:16:21.980 --> 02:16:23.700
that creates that padding.

02:16:23.700 --> 02:16:26.540
Because the most productive days I've ever had

02:16:26.540 --> 02:16:28.340
is just completely from start to finish

02:16:28.340 --> 02:16:31.300
just tuning out everything and just sitting there.

02:16:31.700 --> 02:16:34.100
And then you could do more than six and eight hours.

02:16:34.100 --> 02:16:36.460
Is there some wisdom about what gives you strength

02:16:36.460 --> 02:16:39.580
to do like tough days of long focus?

02:16:40.940 --> 02:16:42.980
Yeah, just like whenever I get obsessed about a problem,

02:16:42.980 --> 02:16:44.020
something just needs to work.

02:16:44.020 --> 02:16:45.220
Something just needs to exist.

02:16:45.220 --> 02:16:46.980
It needs to exist.

02:16:46.980 --> 02:16:49.820
So you're able to deal with bugs and programming issues

02:16:49.820 --> 02:16:52.980
and technical issues and design decisions

02:16:52.980 --> 02:16:54.220
that turn out to be the wrong ones.

02:16:54.220 --> 02:16:55.660
You're able to think through all of that

02:16:55.660 --> 02:16:57.780
given that you want to think to exist.

02:16:57.780 --> 02:16:58.620
Yeah, it needs to exist.

02:16:58.620 --> 02:17:01.180
And then I think to me also a big factor is,

02:17:02.060 --> 02:17:02.900
are other humans are going to appreciate it?

02:17:02.900 --> 02:17:04.140
Are they going to like it?

02:17:04.140 --> 02:17:05.340
That's a big part of my motivation.

02:17:05.340 --> 02:17:07.820
If I'm helping humans and they seem happy,

02:17:07.820 --> 02:17:11.500
they say nice things, they tweet about it or whatever,

02:17:11.500 --> 02:17:13.780
that gives me pleasure because I'm doing something useful.

02:17:13.780 --> 02:17:16.900
So like you do see yourself sharing it with the world.

02:17:16.900 --> 02:17:18.860
Like whether it's on GitHub or through a blog post

02:17:18.860 --> 02:17:19.700
or through videos.

02:17:19.700 --> 02:17:20.580
Yeah, I was thinking about it.

02:17:20.580 --> 02:17:22.100
Like suppose I did all these things

02:17:22.100 --> 02:17:22.940
but did not share them.

02:17:22.940 --> 02:17:24.620
I don't think I would have the same amount of motivation

02:17:24.620 --> 02:17:25.460
that I can build up.

02:17:25.460 --> 02:17:30.460
You enjoy the feeling of other people gaining value

02:17:30.820 --> 02:17:33.100
and happiness from the stuff you've created.

02:17:33.100 --> 02:17:34.300
Yeah.

02:17:34.300 --> 02:17:36.100
What about diet?

02:17:36.100 --> 02:17:38.340
Is there, I saw you played with intermittent fasting.

02:17:38.340 --> 02:17:39.180
Do you fast?

02:17:39.180 --> 02:17:40.020
Does that help?

02:17:40.020 --> 02:17:40.860
I played with everything.

02:17:40.860 --> 02:17:42.700
You played with the things you played.

02:17:42.700 --> 02:17:45.860
What's been most beneficial to your ability

02:17:45.860 --> 02:17:47.340
to mentally focus on a thing

02:17:47.340 --> 02:17:50.820
and just mental productivity and happiness?

02:17:50.820 --> 02:17:51.660
You still fast?

02:17:51.660 --> 02:17:54.180
Yeah, I still fast but I do intermittent fasting

02:17:54.180 --> 02:17:55.660
but really what it means at the end of the day

02:17:55.660 --> 02:17:56.580
is I skip breakfast.

02:17:56.580 --> 02:17:57.420
Yeah.

02:17:57.420 --> 02:17:59.740
So I do 18 six roughly by default

02:17:59.780 --> 02:18:01.180
when I'm in my steady state.

02:18:01.180 --> 02:18:02.540
If I'm traveling or doing something else

02:18:02.540 --> 02:18:03.620
I will break the rules

02:18:03.620 --> 02:18:06.100
but in my steady state I do 18 six.

02:18:06.100 --> 02:18:08.300
So I eat only from 12 to six.

02:18:08.300 --> 02:18:09.620
Not a hard rule and I break it often

02:18:09.620 --> 02:18:10.980
but that's my default.

02:18:10.980 --> 02:18:13.940
And then yeah, I've done a bunch of random experiments

02:18:13.940 --> 02:18:15.500
for the most part right now

02:18:15.500 --> 02:18:17.900
where I've been for the last year and a half I wanna say

02:18:17.900 --> 02:18:21.020
is I'm plant based or plant forward.

02:18:21.020 --> 02:18:21.900
I heard plant forward.

02:18:21.900 --> 02:18:22.740
It sounds better.

02:18:22.740 --> 02:18:23.560
What does that mean exactly?

02:18:23.560 --> 02:18:24.400
I don't actually know the differences

02:18:24.400 --> 02:18:25.780
but it sounds better in my mind

02:18:25.780 --> 02:18:28.380
but it just means I prefer plant based food

02:18:28.940 --> 02:18:30.620
and raw or cooked or.

02:18:30.620 --> 02:18:33.100
I prefer cooked and plant based.

02:18:33.100 --> 02:18:37.220
So plant based, forgive me I don't actually know

02:18:37.220 --> 02:18:40.780
how wide the category of plant entails.

02:18:40.780 --> 02:18:42.620
Well plant based just means that you're not

02:18:42.620 --> 02:18:45.020
militant about it and you can flex

02:18:45.020 --> 02:18:47.580
and you just prefer to eat plants

02:18:47.580 --> 02:18:49.060
and you're not making,

02:18:49.060 --> 02:18:50.980
you're not trying to influence other people

02:18:50.980 --> 02:18:53.000
and if someone is, you come to someone's house party

02:18:53.000 --> 02:18:54.860
and they serve you a steak that they're really proud of

02:18:54.860 --> 02:18:55.700
you will eat it.

02:18:55.700 --> 02:18:56.540
Yes.

02:18:56.540 --> 02:18:57.380
Right.

02:18:57.380 --> 02:18:58.200
It's just like.

02:18:59.040 --> 02:19:00.760
I mean that's, I'm the flip side of that

02:19:00.760 --> 02:19:02.760
but I'm very sort of flexible.

02:19:02.760 --> 02:19:05.040
Have you tried doing one meal a day?

02:19:05.040 --> 02:19:08.520
I have, accidentally not consistently

02:19:08.520 --> 02:19:09.560
but I've accidentally had that.

02:19:09.560 --> 02:19:10.680
I don't like it.

02:19:10.680 --> 02:19:12.600
I think it makes me feel not good.

02:19:12.600 --> 02:19:14.560
It's too much, too much of a hit.

02:19:14.560 --> 02:19:15.400
Yeah.

02:19:15.400 --> 02:19:18.600
And so currently I have about two meals a day, 12 and six.

02:19:18.600 --> 02:19:19.960
I do that nonstop.

02:19:19.960 --> 02:19:22.480
I'm doing it now, one meal a day.

02:19:22.480 --> 02:19:23.320
Okay.

02:19:23.320 --> 02:19:24.140
It's interesting.

02:19:24.140 --> 02:19:24.980
It's an interesting feeling.

02:19:24.980 --> 02:19:26.600
Have you ever fasted longer than a day?

02:19:26.600 --> 02:19:28.400
Yeah, I've done a bunch of water fasts

02:19:28.400 --> 02:19:29.880
because I was curious what happens.

02:19:29.880 --> 02:19:30.800
What happens?

02:19:30.800 --> 02:19:32.160
Anything interesting?

02:19:32.160 --> 02:19:33.000
Yeah, I would say so.

02:19:33.000 --> 02:19:35.000
I mean, what's interesting is that you're hungry

02:19:35.000 --> 02:19:37.800
for two days and then starting day three or so,

02:19:37.800 --> 02:19:39.520
you're not hungry.

02:19:39.520 --> 02:19:40.520
It's like such a weird feeling

02:19:40.520 --> 02:19:41.760
because you haven't eaten in a few days

02:19:41.760 --> 02:19:42.840
and you're not hungry.

02:19:42.840 --> 02:19:43.680
Isn't that weird?

02:19:43.680 --> 02:19:44.520
It's really weird.

02:19:44.520 --> 02:19:47.320
One of the many weird things about human biology

02:19:47.320 --> 02:19:48.320
is figure something out.

02:19:48.320 --> 02:19:51.760
It finds another source of energy or something like that

02:19:51.760 --> 02:19:54.020
or relaxes the system.

02:19:54.020 --> 02:19:54.860
I don't know how that works.

02:19:54.860 --> 02:19:56.160
Yeah, the body is like, you're hungry, you're hungry

02:19:56.720 --> 02:19:57.560
and then it just gives up.

02:19:57.560 --> 02:19:58.560
It's like, okay, I guess we're fasting now.

02:19:58.560 --> 02:20:00.040
There's nothing.

02:20:00.040 --> 02:20:01.240
And then it's just kind of like focuses

02:20:01.240 --> 02:20:03.360
on trying to make you not hungry

02:20:03.360 --> 02:20:05.580
and not feel the damage of that

02:20:05.580 --> 02:20:07.260
and trying to give you some space

02:20:07.260 --> 02:20:08.920
to figure out the food situation.

02:20:09.800 --> 02:20:14.680
So are you still to this day most productive at night?

02:20:14.680 --> 02:20:15.720
I would say I am,

02:20:15.720 --> 02:20:18.500
but it is really hard to maintain my PhD schedule,

02:20:19.680 --> 02:20:21.720
especially when I was, say, working at Tesla and so on.

02:20:21.720 --> 02:20:23.480
It's a non-starter.

02:20:23.480 --> 02:20:25.320
But even now, people want to meet

02:20:25.320 --> 02:20:27.920
for various events.

02:20:27.920 --> 02:20:30.200
Society lives in a certain period of time

02:20:30.200 --> 02:20:32.120
and you sort of have to work with that.

02:20:32.120 --> 02:20:34.320
It's hard to do a social thing

02:20:34.320 --> 02:20:36.500
and then after that, return and do work.

02:20:36.500 --> 02:20:37.880
Yeah, it's just really hard.

02:20:37.880 --> 02:20:41.560
That's why I try when I do social things,

02:20:41.560 --> 02:20:43.840
I try not to do too much drinking

02:20:43.840 --> 02:20:46.380
so I can return and continue doing work.

02:20:47.760 --> 02:20:51.640
But at Tesla, is there conversions?

02:20:51.640 --> 02:20:53.840
Tesla, but any company,

02:20:53.880 --> 02:20:56.080
is there convergence towards a schedule?

02:20:56.080 --> 02:20:57.160
Or is there more,

02:20:58.200 --> 02:21:00.720
is that how humans behave when they collaborate?

02:21:00.720 --> 02:21:02.200
I need to learn about this.

02:21:02.200 --> 02:21:04.240
Do they try to keep a consistent schedule

02:21:04.240 --> 02:21:05.720
where you're all awake at the same time?

02:21:05.720 --> 02:21:07.400
I do try to create a routine

02:21:07.400 --> 02:21:09.100
and I try to create a steady state

02:21:09.100 --> 02:21:11.360
in which I'm comfortable in.

02:21:11.360 --> 02:21:13.360
So I have a morning routine, I have a day routine,

02:21:13.360 --> 02:21:15.560
I try to keep things to a steady state

02:21:15.560 --> 02:21:17.940
and things are predictable

02:21:17.940 --> 02:21:19.080
and then you can sort of just like,

02:21:19.080 --> 02:21:20.940
your body just sort of like sticks to that.

02:21:20.940 --> 02:21:22.440
And if you try to stress that a little too much,

02:21:22.440 --> 02:21:24.280
you will create, when you're traveling

02:21:24.280 --> 02:21:25.360
and you're dealing with jet lag,

02:21:25.360 --> 02:21:29.840
you're not able to really ascend to where you need to go.

02:21:29.840 --> 02:21:32.040
Yeah, yeah, that's what we do as humans

02:21:32.040 --> 02:21:33.400
with the habits and stuff.

02:21:33.400 --> 02:21:35.920
What are your thoughts on work-life balance

02:21:35.920 --> 02:21:37.440
throughout a human lifetime?

02:21:38.640 --> 02:21:40.640
So Tesla in part was known

02:21:40.640 --> 02:21:43.080
for sort of pushing people to their limits

02:21:44.080 --> 02:21:45.360
in terms of what they're able to do,

02:21:45.360 --> 02:21:48.420
in terms of what they're trying to do,

02:21:48.420 --> 02:21:50.740
in terms of how much they work, all that kind of stuff.

02:21:50.740 --> 02:21:54.020
Yeah, I mean, I will say Tesla gets all too much

02:21:54.020 --> 02:21:56.460
bad rep for this because what's happening is Tesla,

02:21:56.460 --> 02:21:58.100
it's a bursty environment.

02:21:58.100 --> 02:22:00.300
So I would say the baseline,

02:22:00.300 --> 02:22:02.100
my only point of reference is Google,

02:22:02.100 --> 02:22:03.100
where I've interned three times

02:22:03.100 --> 02:22:05.900
and I saw what it's like inside Google and DeepMind.

02:22:06.820 --> 02:22:08.820
I would say the baseline is higher than that,

02:22:08.820 --> 02:22:10.700
but then there's a punctuated equilibrium

02:22:10.700 --> 02:22:12.500
where once in a while there's a fire

02:22:12.500 --> 02:22:14.820
and people work really hard.

02:22:14.820 --> 02:22:16.820
And so it's spiky and bursty

02:22:16.820 --> 02:22:18.380
and then all the stories get collected.

02:22:18.380 --> 02:22:19.420
How about the bursts?

02:22:19.460 --> 02:22:21.900
And then it gives the appearance of total insanity,

02:22:21.900 --> 02:22:24.540
but actually it's just a bit more intense environment

02:22:24.540 --> 02:22:26.980
and there are fires and sprints.

02:22:26.980 --> 02:22:29.020
And so I think, definitely though,

02:22:29.020 --> 02:22:31.940
I would say it's a more intense environment

02:22:31.940 --> 02:22:32.780
than something you would get at Google.

02:22:32.780 --> 02:22:34.940
But in your personal, forget all of that,

02:22:34.940 --> 02:22:37.600
just in your own personal life,

02:22:37.600 --> 02:22:41.020
what do you think about the happiness of a human being,

02:22:41.020 --> 02:22:43.900
a brilliant person like yourself

02:22:43.900 --> 02:22:46.740
about finding a balance between work and life?

02:22:46.780 --> 02:22:50.780
Or is it such a thing, not a good thought experiment?

02:22:51.860 --> 02:22:55.500
Yeah, I think balance is good,

02:22:55.500 --> 02:22:58.700
but I also love to have sprints that are out of distribution.

02:22:58.700 --> 02:23:03.700
And that's when I think I've been pretty creative as well.

02:23:04.260 --> 02:23:08.140
So sprints out of distribution means that most of the time

02:23:08.140 --> 02:23:11.740
you have a quote unquote balance.

02:23:11.740 --> 02:23:13.020
I have balance most of the time.

02:23:13.020 --> 02:23:15.940
I like being obsessed with something once in a while.

02:23:15.980 --> 02:23:16.820
Once in a while is what?

02:23:16.820 --> 02:23:18.460
Once a week, once a month, once a year?

02:23:18.460 --> 02:23:20.540
Yeah, probably like say once a month or something, yeah.

02:23:20.540 --> 02:23:23.340
And that's when we get a new GitHub repo for monitoring?

02:23:23.340 --> 02:23:24.980
Yeah, that's when you really care about a problem.

02:23:24.980 --> 02:23:25.820
It must exist.

02:23:25.820 --> 02:23:26.980
This will be awesome.

02:23:26.980 --> 02:23:28.220
You're obsessed with it.

02:23:28.220 --> 02:23:29.780
And now you can't just do it on that day.

02:23:29.780 --> 02:23:32.580
You need to pay the fixed cost of getting into the groove

02:23:32.580 --> 02:23:34.300
and then you need to stay there for a while.

02:23:34.300 --> 02:23:36.900
And then society will come and they will try to mess with you

02:23:36.900 --> 02:23:38.420
and they will try to distract you.

02:23:38.420 --> 02:23:40.020
Yeah, the worst thing is a person who's like,

02:23:40.020 --> 02:23:42.420
I just need five minutes of your time.

02:23:42.420 --> 02:23:45.040
This is, the cost of that is not five minutes.

02:23:45.120 --> 02:23:48.520
And society needs to change how it thinks about

02:23:48.520 --> 02:23:50.360
just five minutes of your time.

02:23:50.360 --> 02:23:51.200
Right.

02:23:51.200 --> 02:23:53.840
It's never, it's never just one minute, just 30 seconds,

02:23:53.840 --> 02:23:54.680
just a quick thing.

02:23:54.680 --> 02:23:55.500
What's the big deal?

02:23:55.500 --> 02:23:56.340
Why are you being so?

02:23:56.340 --> 02:23:57.180
Yeah, no.

02:23:58.840 --> 02:24:00.920
What's your computer setup?

02:24:00.920 --> 02:24:02.960
What's like the perfect,

02:24:02.960 --> 02:24:05.360
are you somebody that's flexible to no matter what?

02:24:05.360 --> 02:24:08.040
Laptop, four screens?

02:24:08.040 --> 02:24:08.880
Yeah.

02:24:08.880 --> 02:24:11.640
Or do you prefer a certain setup

02:24:11.640 --> 02:24:13.680
that you're most productive with?

02:24:13.680 --> 02:24:15.160
I guess the one that I'm familiar with

02:24:15.160 --> 02:24:20.160
is one large screen, 27 inch and my laptop on the side.

02:24:20.400 --> 02:24:21.700
What operating system?

02:24:21.700 --> 02:24:22.540
I do Macs.

02:24:22.540 --> 02:24:23.680
That's my primary.

02:24:23.680 --> 02:24:25.240
For all tasks?

02:24:25.240 --> 02:24:26.080
I would say OS X,

02:24:26.080 --> 02:24:26.920
but when you're working on deep learning,

02:24:26.920 --> 02:24:27.760
everything is Linux.

02:24:27.760 --> 02:24:30.840
You're SSH'd into a cluster and you're working remotely.

02:24:30.840 --> 02:24:32.160
But what about the actual development,

02:24:32.160 --> 02:24:33.800
like they're using the IDE?

02:24:33.800 --> 02:24:36.040
Yeah, you would use, I think a good way is

02:24:36.040 --> 02:24:39.480
you just run VS code, my favorite editor right now

02:24:39.480 --> 02:24:41.400
on your Mac, but you are actually,

02:24:41.400 --> 02:24:43.440
you have a remote folder through SSH.

02:24:44.360 --> 02:24:46.360
So the actual files that you're manipulating

02:24:46.360 --> 02:24:47.440
are in the cluster somewhere else.

02:24:47.440 --> 02:24:49.780
So what's the best IDE?

02:24:50.960 --> 02:24:52.960
VS code, what else do people,

02:24:52.960 --> 02:24:55.560
so I use Emacs still.

02:24:55.560 --> 02:24:56.400
That's cool.

02:24:57.520 --> 02:24:58.400
It may be cool.

02:24:58.400 --> 02:25:00.560
I don't know if it's maximum productivity.

02:25:01.400 --> 02:25:04.280
So what do you recommend in terms of editors?

02:25:04.280 --> 02:25:06.160
You worked a lot of software engineers,

02:25:06.160 --> 02:25:10.900
editors for Python, C++, machine learning applications.

02:25:11.520 --> 02:25:13.480
I think the current answer is VS code.

02:25:13.480 --> 02:25:16.720
Currently, I believe that's the best IDE.

02:25:16.720 --> 02:25:18.360
It's got a huge amount of extensions.

02:25:18.360 --> 02:25:22.080
It has GitHub Copilot integration,

02:25:22.080 --> 02:25:23.360
which I think is very valuable.

02:25:23.360 --> 02:25:25.600
What do you think about the Copilot integration?

02:25:25.600 --> 02:25:27.600
I was actually, I got to talk a bunch

02:25:27.600 --> 02:25:30.320
with Guido Nerosson, who's a creative Python,

02:25:30.320 --> 02:25:32.000
and he loves Copilot.

02:25:32.000 --> 02:25:34.360
He programs a lot with it.

02:25:35.360 --> 02:25:36.640
Do you?

02:25:36.640 --> 02:25:37.920
Yeah, I use Copilot, I love it.

02:25:37.920 --> 02:25:40.720
And it's free for me, but I would pay for it.

02:25:40.720 --> 02:25:41.680
Yeah, I think it's very good.

02:25:41.680 --> 02:25:44.400
And the utility that I found with it was,

02:25:44.400 --> 02:25:45.720
I would say there's a learning curve,

02:25:45.720 --> 02:25:48.560
and you need to figure out when it's helpful

02:25:48.560 --> 02:25:50.160
and when to pay attention to its outputs,

02:25:50.160 --> 02:25:51.360
and when it's not going to be helpful,

02:25:51.360 --> 02:25:53.000
where you should not pay attention to it.

02:25:53.000 --> 02:25:54.960
Because if you're just reading its suggestions all the time,

02:25:54.960 --> 02:25:56.640
it's not a good way of interacting with it.

02:25:56.640 --> 02:25:58.920
But I think I was able to sort of mold myself to it.

02:25:58.920 --> 02:26:00.400
I find it's very helpful, number one,

02:26:00.400 --> 02:26:03.000
in copy, paste, and replace some parts.

02:26:03.000 --> 02:26:05.760
So when the pattern is clear,

02:26:05.760 --> 02:26:07.640
it's really good at completing the pattern.

02:26:07.640 --> 02:26:10.000
And number two, sometimes it suggests APIs

02:26:10.000 --> 02:26:11.560
that I'm not aware of.

02:26:11.560 --> 02:26:14.440
So it tells you about something that you didn't know.

02:26:14.440 --> 02:26:16.120
And that's an opportunity to discover a new API.

02:26:16.120 --> 02:26:17.440
It's an opportunity to,

02:26:17.440 --> 02:26:19.560
I would never take Copilot code as given.

02:26:19.560 --> 02:26:22.720
I almost always copy paste into a Google search,

02:26:22.720 --> 02:26:24.480
and you see what this function is doing.

02:26:24.480 --> 02:26:25.320
And then you're like,

02:26:25.320 --> 02:26:26.840
oh, it's actually exactly what I need.

02:26:26.840 --> 02:26:27.680
Thank you, Copilot.

02:26:27.680 --> 02:26:28.500
So you learn something.

02:26:28.500 --> 02:26:30.040
So it's in part a search engine,

02:26:30.040 --> 02:26:33.920
part maybe getting the exact syntax correctly,

02:26:33.920 --> 02:26:37.360
that once you see it, it's that NP-hard thing.

02:26:37.360 --> 02:26:40.200
Once you see it, you know it's correct.

02:26:40.200 --> 02:26:42.360
But you yourself would struggle.

02:26:42.360 --> 02:26:43.520
You can verify efficiently,

02:26:43.520 --> 02:26:45.680
but you can't generate efficiently.

02:26:45.680 --> 02:26:46.520
And Copilot, really,

02:26:46.520 --> 02:26:49.600
I mean, it's autopilot for programming, right?

02:26:49.600 --> 02:26:51.600
And currently it's doing the lane following,

02:26:51.600 --> 02:26:53.000
which is like the simple copy, paste,

02:26:53.000 --> 02:26:54.560
and sometimes suggest.

02:26:54.560 --> 02:26:57.160
But over time, it's going to become more and more autonomous.

02:26:57.160 --> 02:27:00.060
And so the same thing will play out in not just coding,

02:27:00.060 --> 02:27:02.360
but actually across many, many different things, probably.

02:27:02.360 --> 02:27:04.200
But coding is an important one, right?

02:27:04.200 --> 02:27:06.080
Writing programs.

02:27:06.080 --> 02:27:08.520
How do you see the future of that developing,

02:27:08.520 --> 02:27:09.680
the program synthesis,

02:27:09.680 --> 02:27:11.680
like being able to write programs

02:27:11.680 --> 02:27:13.280
that are more and more complicated?

02:27:13.280 --> 02:27:16.440
Because right now it's human supervised

02:27:16.440 --> 02:27:18.000
in interesting ways.

02:27:18.000 --> 02:27:19.080
Yes.

02:27:19.080 --> 02:27:22.000
It feels like the transition will be very painful.

02:27:22.000 --> 02:27:24.400
My mental model for it is the same thing will happen

02:27:24.400 --> 02:27:26.200
as with the autopilot.

02:27:26.200 --> 02:27:27.840
So currently it's doing lane following.

02:27:27.840 --> 02:27:29.400
It's doing some simple stuff.

02:27:29.400 --> 02:27:31.280
And eventually we'll be doing autonomy,

02:27:31.280 --> 02:27:33.200
and people will have to intervene less and less.

02:27:33.320 --> 02:27:37.480
And those could be like testing mechanisms.

02:27:37.480 --> 02:27:38.740
Like if it writes a function,

02:27:38.740 --> 02:27:41.440
and that function looks pretty damn correct,

02:27:41.440 --> 02:27:43.120
but how do you know it's correct?

02:27:43.120 --> 02:27:46.240
Because you're getting lazier and lazier as a programmer.

02:27:46.240 --> 02:27:48.720
Like your ability to, because like little bugs,

02:27:48.720 --> 02:27:50.720
but I guess it won't make little mistakes.

02:27:50.720 --> 02:27:51.840
No, it will.

02:27:51.840 --> 02:27:54.800
Copilot will make off by one subtle bugs.

02:27:54.800 --> 02:27:55.840
It has done that to me.

02:27:55.840 --> 02:27:57.880
But do you think future systems will?

02:27:57.880 --> 02:28:00.320
Or is it really the off by one

02:28:00.320 --> 02:28:02.880
is actually a fundamental challenge of programming?

02:28:03.400 --> 02:28:04.720
In that case, it wasn't fundamental.

02:28:04.720 --> 02:28:06.000
And I think things can improve,

02:28:06.000 --> 02:28:08.440
but yeah, I think humans have to supervise.

02:28:08.440 --> 02:28:11.160
I am nervous about people not supervising what comes out

02:28:11.160 --> 02:28:12.840
and what happens to, for example,

02:28:12.840 --> 02:28:15.360
the proliferation of bugs in all of our systems.

02:28:15.360 --> 02:28:16.240
I'm nervous about that,

02:28:16.240 --> 02:28:18.760
but I think there will probably be some other copilots

02:28:18.760 --> 02:28:21.280
for bug finding and stuff like that at some point,

02:28:21.280 --> 02:28:23.840
because there'll be like a lot more automation for.

02:28:23.840 --> 02:28:24.680
Oh man.

02:28:26.000 --> 02:28:30.920
It's like a program, a copilot that generates a compiler,

02:28:30.920 --> 02:28:32.320
one that does a linter.

02:28:32.880 --> 02:28:35.400
One that does like a type checker.

02:28:35.400 --> 02:28:36.240
Yeah.

02:28:36.240 --> 02:28:40.360
It's a committee of like a GPT sort of like.

02:28:40.360 --> 02:28:42.240
And then there'll be like a manager for the committee.

02:28:42.240 --> 02:28:43.080
Yeah.

02:28:43.080 --> 02:28:44.280
And then there'll be somebody that says,

02:28:44.280 --> 02:28:45.720
a new version of this is needed.

02:28:45.720 --> 02:28:46.960
We need to regenerate it.

02:28:46.960 --> 02:28:47.800
Yeah.

02:28:47.800 --> 02:28:48.880
There were 10 GPTs, they were forwarded

02:28:48.880 --> 02:28:50.200
and gave 50 suggestions.

02:28:50.200 --> 02:28:53.000
Another one looked at it and picked a few that they like.

02:28:53.000 --> 02:28:54.600
A bug one looked at it and it was like,

02:28:54.600 --> 02:28:55.600
it's probably a bug.

02:28:55.600 --> 02:28:57.320
They got re-ranked by some other thing.

02:28:57.320 --> 02:29:00.520
And then a final ensemble GPT comes in and is like,

02:29:00.520 --> 02:29:01.920
okay, given everything you guys have told me,

02:29:01.920 --> 02:29:04.120
this is probably the next token.

02:29:04.120 --> 02:29:05.920
You know, the feeling is the number of programmers

02:29:05.920 --> 02:29:08.280
in the world has been growing and growing very quickly.

02:29:08.280 --> 02:29:10.760
Do you think it's possible that it'll actually level out

02:29:10.760 --> 02:29:14.480
and drop to like a very low number with this kind of world?

02:29:14.480 --> 02:29:17.680
Cause then you'll be doing software 2.0 programming

02:29:19.120 --> 02:29:22.440
and you'll be doing this kind of generation

02:29:22.440 --> 02:29:25.160
of copilot type systems programming,

02:29:25.160 --> 02:29:27.520
but you won't be doing the old school

02:29:27.520 --> 02:29:29.880
software 1.0 programming.

02:29:29.880 --> 02:29:31.320
I don't currently think that they're just going

02:29:31.320 --> 02:29:33.160
to replace human programmers.

02:29:35.240 --> 02:29:37.120
I'm so hesitant saying stuff like this, right?

02:29:37.120 --> 02:29:40.240
Because this is gonna be replaced in five years.

02:29:40.240 --> 02:29:42.480
I don't know, it's going to show that

02:29:42.480 --> 02:29:45.160
this is where we thought, because I agree with you,

02:29:45.160 --> 02:29:49.000
but I think we might be very surprised, right?

02:29:49.000 --> 02:29:53.640
Like, what are the next, what's your sense

02:29:53.640 --> 02:29:55.240
of where we stand with language models?

02:29:55.240 --> 02:29:57.920
Does it feel like the beginning or the middle or the end?

02:29:57.920 --> 02:29:59.440
The beginning, 100%.

02:29:59.440 --> 02:30:00.760
I think the big question in my mind is,

02:30:00.800 --> 02:30:03.080
for sure GPT will be able to program quite well,

02:30:03.080 --> 02:30:04.240
competently and so on.

02:30:04.240 --> 02:30:05.800
How do you steer the system?

02:30:05.800 --> 02:30:07.680
You still have to provide some guidance

02:30:07.680 --> 02:30:09.280
to what you actually are looking for.

02:30:09.280 --> 02:30:10.520
And so how do you steer it?

02:30:10.520 --> 02:30:12.800
And how do you say, how do you talk to it?

02:30:12.800 --> 02:30:16.800
How do you audit it and verify that what is done is correct?

02:30:16.800 --> 02:30:18.400
And how do you like work with this?

02:30:18.400 --> 02:30:20.440
And it's as much, not just an AI problem,

02:30:20.440 --> 02:30:21.920
but a UI UX problem.

02:30:21.920 --> 02:30:23.400
Yeah.

02:30:23.400 --> 02:30:27.360
So beautiful fertile ground for so much interesting work

02:30:27.360 --> 02:30:29.840
for VS Code++ where you're not just,

02:30:29.920 --> 02:30:31.120
not just human programming anymore.

02:30:31.120 --> 02:30:31.960
It's amazing.

02:30:31.960 --> 02:30:33.680
Yeah. So you're interacting with the system.

02:30:33.680 --> 02:30:37.280
So not just one prompt, but it's iterative prompting.

02:30:37.280 --> 02:30:38.120
Yeah.

02:30:38.120 --> 02:30:39.360
You're trying to figure out having a conversation

02:30:39.360 --> 02:30:40.200
with the system.

02:30:40.200 --> 02:30:41.040
Yeah.

02:30:41.040 --> 02:30:42.760
That actually, I mean, to me, that's super exciting

02:30:42.760 --> 02:30:45.840
to have a conversation with the program I'm writing.

02:30:45.840 --> 02:30:48.120
Yeah. Maybe at some point you're just conversing with it.

02:30:48.120 --> 02:30:49.800
It's like, okay, here's what I want to do.

02:30:49.800 --> 02:30:51.720
Actually this variable,

02:30:51.720 --> 02:30:54.080
maybe it's not even that low level as variable, but.

02:30:54.080 --> 02:30:56.120
You can also imagine like,

02:30:56.120 --> 02:30:59.000
can you translate this to C++ and back to Python?

02:30:59.000 --> 02:30:59.840
Yeah.

02:30:59.840 --> 02:31:00.680
Already kind of exists in some places.

02:31:00.680 --> 02:31:01.720
No, but just like doing it

02:31:01.720 --> 02:31:03.640
as part of the program experience.

02:31:03.640 --> 02:31:07.720
Like, I think I'd like to write this function in C++.

02:31:07.720 --> 02:31:11.400
Or like, you just keep changing for different programs

02:31:11.400 --> 02:31:13.520
because they have different syntax.

02:31:13.520 --> 02:31:15.640
Maybe I want to convert this into a functional language.

02:31:15.640 --> 02:31:16.480
Yeah.

02:31:16.480 --> 02:31:20.520
And so like, you get to become multilingual as a programmer

02:31:20.520 --> 02:31:22.400
and dance back and forth efficiently.

02:31:22.400 --> 02:31:23.240
Yeah.

02:31:23.240 --> 02:31:24.800
I mean, I think the UI UX of it though

02:31:24.800 --> 02:31:26.680
is like still very hard to think through

02:31:26.720 --> 02:31:29.520
because it's not just about writing code on a page.

02:31:29.520 --> 02:31:31.400
You have an entire developer environment.

02:31:31.400 --> 02:31:33.160
You have a bunch of hardware on it.

02:31:33.160 --> 02:31:34.560
You have some environmental variables.

02:31:34.560 --> 02:31:36.440
You have some scripts that are running in a Chrome job.

02:31:36.440 --> 02:31:39.440
Like there's a lot going on to like working with computers

02:31:39.440 --> 02:31:43.560
and how do these systems set up environment flags

02:31:43.560 --> 02:31:45.200
and work across multiple machines

02:31:45.200 --> 02:31:46.240
and set up screen sessions

02:31:46.240 --> 02:31:47.880
and automate different processes.

02:31:47.880 --> 02:31:50.640
Like how all that works and is auditable by humans

02:31:50.640 --> 02:31:53.320
and so on is like massive question at the moment.

02:31:53.320 --> 02:31:56.040
You've built archive sanity.

02:31:56.040 --> 02:31:58.400
What is archive and what is the future

02:31:58.400 --> 02:32:02.000
of academic research publishing that you would like to see?

02:32:02.000 --> 02:32:03.760
So archive is this pre-print server.

02:32:03.760 --> 02:32:06.600
So if you have a paper, you can submit it for publication

02:32:06.600 --> 02:32:08.800
to journals or conferences and then wait six months

02:32:08.800 --> 02:32:10.920
and then maybe get a decision, pass or fail,

02:32:10.920 --> 02:32:13.320
or you can just upload it to archive.

02:32:13.320 --> 02:32:15.920
And then people can tweet about it three minutes later

02:32:15.920 --> 02:32:17.560
and then everyone sees it, everyone reads it

02:32:17.560 --> 02:32:20.400
and everyone can profit from it in their own own ways.

02:32:20.400 --> 02:32:23.880
And you can cite it and it has an official look to it.

02:32:23.880 --> 02:32:27.560
It feels like a publication process.

02:32:27.560 --> 02:32:30.400
It feels different than if you just put it in a blog post.

02:32:30.400 --> 02:32:31.760
Oh yeah, yeah, I mean, it's a paper

02:32:31.760 --> 02:32:34.240
and usually the bar is higher for something

02:32:34.240 --> 02:32:36.040
that you would expect on archive

02:32:36.040 --> 02:32:38.080
as opposed to something you would see in a blog post.

02:32:38.080 --> 02:32:40.960
Well, the culture created the bar

02:32:40.960 --> 02:32:42.240
because you could probably post

02:32:42.240 --> 02:32:44.200
a pretty crappy picture in the archive.

02:32:45.480 --> 02:32:46.840
So what's that make you feel like?

02:32:46.840 --> 02:32:49.040
What's that make you feel about peer review?

02:32:49.040 --> 02:32:52.720
So rigorous peer review by two, three experts

02:32:52.720 --> 02:32:56.760
versus the peer review of the community

02:32:56.760 --> 02:32:57.800
right as it's written?

02:32:57.800 --> 02:33:00.120
Yeah, basically I think the community is very,

02:33:00.120 --> 02:33:03.960
well able to peer review things very quickly on Twitter.

02:33:03.960 --> 02:33:05.680
And I think maybe it just has to do something

02:33:05.680 --> 02:33:07.960
with AI machine learning field specifically though.

02:33:07.960 --> 02:33:10.480
I feel like things are more easily auditable

02:33:10.480 --> 02:33:14.040
and the verification is easier potentially

02:33:14.040 --> 02:33:15.680
than the verification somewhere else.

02:33:15.680 --> 02:33:17.080
So it's kind of like,

02:33:17.080 --> 02:33:19.240
you can think of these scientific publications

02:33:19.240 --> 02:33:20.200
as like little blockchains

02:33:20.200 --> 02:33:21.520
where everyone's building on each other's work

02:33:21.520 --> 02:33:22.480
and citing each other.

02:33:23.240 --> 02:33:24.080
And you sort of have AI,

02:33:24.080 --> 02:33:27.080
which is kind of like this much faster and loose blockchain.

02:33:27.080 --> 02:33:28.080
But then you have,

02:33:28.080 --> 02:33:32.440
and any one individual entry is like very cheap to make.

02:33:32.440 --> 02:33:33.280
And then you have other fields

02:33:33.280 --> 02:33:36.680
where maybe that model doesn't make as much sense.

02:33:36.680 --> 02:33:38.240
And so I think in AI at least,

02:33:38.240 --> 02:33:40.200
things are pretty easily verifiable.

02:33:40.200 --> 02:33:41.840
And so that's why when people upload papers,

02:33:41.840 --> 02:33:43.400
they're a really good idea and so on.

02:33:43.400 --> 02:33:45.800
People can try it out like the next day

02:33:45.800 --> 02:33:47.200
and they can be the final arbiter

02:33:47.200 --> 02:33:49.040
of whether it works or not on their problem.

02:33:49.040 --> 02:33:51.520
And the whole thing just moves significantly faster.

02:33:51.520 --> 02:33:53.960
So I kind of feel like academia still has a place.

02:33:53.960 --> 02:33:55.640
Sorry, this like conference journal process

02:33:55.640 --> 02:33:56.480
still has a place,

02:33:56.480 --> 02:33:59.720
but it's sort of like it lags behind, I think.

02:33:59.720 --> 02:34:03.120
And it's a bit more maybe higher quality process,

02:34:03.120 --> 02:34:04.840
but it's not sort of the place

02:34:04.840 --> 02:34:07.320
where you will discover cutting edge work anymore.

02:34:07.320 --> 02:34:09.040
It used to be the case when I was starting my PhD

02:34:09.040 --> 02:34:10.880
that you go to conferences and journals

02:34:10.880 --> 02:34:12.520
and you discuss all the latest research.

02:34:12.520 --> 02:34:14.040
Now, when you go to a conference or journal,

02:34:14.040 --> 02:34:15.960
like no one discusses anything that's there

02:34:15.960 --> 02:34:19.280
because it's already like three generations ago, irrelevant.

02:34:19.320 --> 02:34:22.400
Yeah, which makes me sad about like DeepMind, for example,

02:34:22.400 --> 02:34:25.000
where they still publish in nature

02:34:25.000 --> 02:34:26.880
and these big prestigious,

02:34:26.880 --> 02:34:28.360
I mean, there's still value, I suppose,

02:34:28.360 --> 02:34:30.800
to the prestige that comes with these big venues,

02:34:30.800 --> 02:34:34.200
but the result is that they'll announce

02:34:34.200 --> 02:34:36.040
some breakthrough performance

02:34:36.040 --> 02:34:37.920
and it will take like a year

02:34:37.920 --> 02:34:39.640
to actually publish the details.

02:34:39.640 --> 02:34:42.640
I mean, and those details,

02:34:42.640 --> 02:34:43.840
if they were published immediately,

02:34:43.840 --> 02:34:45.440
would inspire the community

02:34:45.440 --> 02:34:46.920
to move in certain directions.

02:34:46.920 --> 02:34:48.400
Yeah, it would speed up the rest of the community,

02:34:48.400 --> 02:34:50.040
but I don't know to what extent

02:34:50.040 --> 02:34:52.240
that's part of their objective function also.

02:34:52.240 --> 02:34:53.080
That's true.

02:34:53.080 --> 02:34:54.240
So it's not just the prestige,

02:34:54.240 --> 02:34:57.000
a little bit of the delay is part.

02:34:57.000 --> 02:34:58.800
Yeah, they certainly, DeepMind specifically

02:34:58.800 --> 02:35:00.960
has been working in the regime

02:35:00.960 --> 02:35:03.040
of having slightly higher quality,

02:35:03.040 --> 02:35:04.840
basically process and latency

02:35:04.840 --> 02:35:07.240
and publishing those papers that way.

02:35:07.240 --> 02:35:09.120
Another question from Reddit,

02:35:09.120 --> 02:35:12.400
do you or have you suffered from imposter syndrome?

02:35:12.400 --> 02:35:14.640
Being the director of AI at Tesla,

02:35:15.480 --> 02:35:18.160
being this person when you're at Stanford

02:35:18.920 --> 02:35:21.400
where the world looks at you as the expert in AI

02:35:21.400 --> 02:35:25.480
to teach the world about machine learning.

02:35:25.480 --> 02:35:27.200
When I was leaving Tesla after five years,

02:35:27.200 --> 02:35:29.040
I spent a ton of time in meeting rooms

02:35:29.920 --> 02:35:31.800
and I would read papers.

02:35:31.800 --> 02:35:32.840
In the beginning when I joined Tesla,

02:35:32.840 --> 02:35:35.080
I was writing code and then I was writing less and less code

02:35:35.080 --> 02:35:36.000
and I was reading code

02:35:36.000 --> 02:35:37.680
and then I was reading less and less code.

02:35:37.680 --> 02:35:39.120
And so this is just a natural progression

02:35:39.120 --> 02:35:40.160
that happens, I think.

02:35:40.160 --> 02:35:42.720
And definitely I would say near the tail end,

02:35:42.720 --> 02:35:44.400
that's when it starts to hit you a bit more

02:35:44.400 --> 02:35:45.400
that you're supposed to be an expert,

02:35:45.400 --> 02:35:47.600
but actually the source of truth

02:35:47.600 --> 02:35:49.200
is the code that people are writing, the GitHub

02:35:49.200 --> 02:35:52.000
and the actual code itself.

02:35:52.000 --> 02:35:54.400
And you're not as familiar with that as you used to be.

02:35:54.400 --> 02:35:57.240
And so I would say maybe there's some insecurity there.

02:35:57.240 --> 02:35:59.080
Yeah, that's actually pretty profound

02:35:59.080 --> 02:36:00.680
that a lot of the insecurity has to do

02:36:00.680 --> 02:36:03.520
with not writing the code in the computer science space.

02:36:03.520 --> 02:36:05.920
Cause that is the truth, that right there.

02:36:05.920 --> 02:36:06.960
The code is the source of truth,

02:36:06.960 --> 02:36:08.040
the papers and everything else,

02:36:08.040 --> 02:36:09.760
it's a high level summary.

02:36:11.360 --> 02:36:12.440
Yeah, it's just a high level summary,

02:36:12.440 --> 02:36:13.780
but at the end of the day you have to read code.

02:36:13.780 --> 02:36:15.280
It's impossible to translate all that code

02:36:15.280 --> 02:36:18.600
into actual paper form.

02:36:18.600 --> 02:36:20.320
So when things come out,

02:36:20.320 --> 02:36:21.640
especially when they have a source code available,

02:36:21.640 --> 02:36:23.160
that's my favorite place to go.

02:36:23.160 --> 02:36:25.560
So like I said, you're one of the greatest teachers

02:36:25.560 --> 02:36:30.560
of machine learning AI ever from CS231N to today.

02:36:31.720 --> 02:36:33.720
What advice would you give to beginners

02:36:33.720 --> 02:36:36.520
interested in getting into machine learning?

02:36:36.520 --> 02:36:40.520
Beginners are often focused on like what to do.

02:36:40.520 --> 02:36:43.400
And I think the focus should be more like how much you do.

02:36:43.400 --> 02:36:45.400
So I am kind of like believer on a high level

02:36:45.400 --> 02:36:47.280
in this 10,000 hours kind of concept

02:36:47.280 --> 02:36:49.760
where you just kind of have to just pick the things

02:36:49.760 --> 02:36:51.480
where you can spend time and you care about

02:36:51.480 --> 02:36:52.360
and you're interested in.

02:36:52.360 --> 02:36:55.080
You literally have to put in 10,000 hours of work.

02:36:55.080 --> 02:36:57.320
It doesn't even like matter as much like where you put it

02:36:57.320 --> 02:36:59.480
and you'll iterate and you'll improve

02:36:59.480 --> 02:37:00.600
and you'll waste some time.

02:37:00.600 --> 02:37:01.880
I don't know if there's a better way.

02:37:01.880 --> 02:37:03.600
You need to put in 10,000 hours.

02:37:03.600 --> 02:37:04.560
But I think it's actually really nice

02:37:04.560 --> 02:37:06.400
cause I feel like there's some sense of determinism

02:37:06.400 --> 02:37:08.600
about being an expert at a thing

02:37:08.600 --> 02:37:10.000
if you spend 10,000 hours.

02:37:10.000 --> 02:37:12.560
You can literally pick an arbitrary thing.

02:37:12.600 --> 02:37:14.280
And I think if you spend 10,000 hours

02:37:14.280 --> 02:37:15.640
of deliberate effort and work,

02:37:15.640 --> 02:37:17.760
you actually will become an expert at it.

02:37:17.760 --> 02:37:20.600
And so I think it's kind of like a nice thought.

02:37:21.520 --> 02:37:24.480
And so basically I would focus more on like,

02:37:24.480 --> 02:37:26.280
are you spending 10,000 hours?

02:37:26.280 --> 02:37:27.600
That's what I focus on.

02:37:27.600 --> 02:37:30.000
And then thinking about what kind of mechanisms

02:37:30.000 --> 02:37:32.720
maximize your likelihood of getting to 10,000 hours.

02:37:32.720 --> 02:37:33.560
Exactly.

02:37:33.560 --> 02:37:36.800
Which for us silly humans means probably forming

02:37:36.800 --> 02:37:39.120
a daily habit of like every single day

02:37:39.120 --> 02:37:40.240
actually doing the thing.

02:37:40.240 --> 02:37:41.200
Whatever helps you.

02:37:41.200 --> 02:37:42.280
So I do think to a large extent

02:37:42.280 --> 02:37:44.760
it's a psychological problem for yourself.

02:37:44.760 --> 02:37:46.960
One other thing that I think is helpful

02:37:46.960 --> 02:37:48.720
for the psychology of it is many times

02:37:48.720 --> 02:37:50.800
people compare themselves to others in the area.

02:37:50.800 --> 02:37:52.280
I think this is very harmful.

02:37:52.280 --> 02:37:54.920
Only compare yourself to you from some time ago.

02:37:54.920 --> 02:37:56.160
Like say a year ago.

02:37:56.160 --> 02:37:58.040
Are you better than you a year ago?

02:37:58.040 --> 02:37:59.240
Is the only way to think.

02:38:00.200 --> 02:38:02.200
And I think then you can see your progress

02:38:02.200 --> 02:38:03.480
and it's very motivating.

02:38:03.480 --> 02:38:07.360
That's so interesting that focus on the quantity of hours.

02:38:07.360 --> 02:38:10.080
Cause I think a lot of people in the beginner stage,

02:38:10.080 --> 02:38:15.080
but actually throughout get paralyzed by the choice.

02:38:15.560 --> 02:38:19.360
Like which one do I pick this path or this path?

02:38:19.360 --> 02:38:21.040
Like they'll literally get paralyzed

02:38:21.040 --> 02:38:22.640
by like which IDE to use.

02:38:22.640 --> 02:38:24.680
Well they're worried about all these things.

02:38:24.680 --> 02:38:28.520
But the thing is you will waste time doing something wrong.

02:38:28.520 --> 02:38:29.920
You will eventually figure out it's not right.

02:38:29.920 --> 02:38:31.680
You will accumulate scar tissue

02:38:31.680 --> 02:38:33.440
and next time you'll grow stronger.

02:38:33.440 --> 02:38:35.120
Because next time you'll have the scar tissue

02:38:35.120 --> 02:38:36.680
and next time you'll learn from it.

02:38:36.680 --> 02:38:39.040
And now next time you come to a similar situation

02:38:39.040 --> 02:38:41.560
you'll be like, oh, I messed up.

02:38:41.560 --> 02:38:43.440
I've spent a lot of time working on things

02:38:43.440 --> 02:38:45.160
that never materialized into anything.

02:38:45.160 --> 02:38:46.320
And I have all that scar tissue

02:38:46.320 --> 02:38:48.400
and I have some intuitions about what was useful,

02:38:48.400 --> 02:38:50.600
what wasn't useful, how things turned out.

02:38:50.600 --> 02:38:53.960
So all those mistakes were not dead work.

02:38:53.960 --> 02:38:56.600
So I just think you should, did you just focus on working?

02:38:56.600 --> 02:38:57.640
What have you done?

02:38:57.640 --> 02:38:59.000
What have you done last week?

02:39:00.680 --> 02:39:02.680
That's a good question actually to ask

02:39:02.680 --> 02:39:05.600
for a lot of things that just machine learning.

02:39:05.600 --> 02:39:08.480
It's a good way to cut the,

02:39:08.480 --> 02:39:09.600
I forgot what the term we use,

02:39:09.600 --> 02:39:11.600
but the fluff, the blubber, whatever the,

02:39:12.840 --> 02:39:14.720
the inefficiencies in life.

02:39:14.720 --> 02:39:17.200
What do you love about teaching?

02:39:17.200 --> 02:39:20.400
You seem to find yourself often in the,

02:39:20.400 --> 02:39:21.720
like drawn to teaching.

02:39:21.720 --> 02:39:23.600
You're very good at it, but you're also drawn to it.

02:39:23.600 --> 02:39:25.320
I mean, I don't think I love teaching.

02:39:25.320 --> 02:39:27.160
I love happy humans.

02:39:27.160 --> 02:39:30.160
And happy humans like when I teach.

02:39:31.000 --> 02:39:32.360
I wouldn't say I hate teaching.

02:39:32.360 --> 02:39:33.320
I tolerate teaching,

02:39:33.320 --> 02:39:35.000
but it's not like the act of teaching that I like.

02:39:35.480 --> 02:39:40.480
It's that I have something, I'm actually okay at it.

02:39:41.240 --> 02:39:43.880
I'm okay at teaching and people appreciate it a lot.

02:39:43.880 --> 02:39:47.160
And so I'm just happy to try to be helpful.

02:39:47.160 --> 02:39:49.920
And teaching itself is not like the most,

02:39:49.920 --> 02:39:52.640
I mean, it can be really annoying, frustrating.

02:39:52.640 --> 02:39:54.520
I was working on a bunch of lectures just now.

02:39:54.520 --> 02:39:56.840
I was reminded back to my days of 231

02:39:56.840 --> 02:39:58.200
and just how much work it is

02:39:58.200 --> 02:40:00.360
to create some of these materials and make them good.

02:40:00.360 --> 02:40:01.680
The amount of iteration and thought,

02:40:01.680 --> 02:40:04.800
and you go down blind alleys and just how much you change it.

02:40:05.640 --> 02:40:07.280
So creating something good

02:40:07.280 --> 02:40:09.720
in terms of like educational value is really hard.

02:40:09.720 --> 02:40:11.880
And it's not fun.

02:40:11.880 --> 02:40:12.720
It was difficult.

02:40:12.720 --> 02:40:14.720
So for people who definitely go watching,

02:40:14.720 --> 02:40:16.480
new stuff you put out,

02:40:16.480 --> 02:40:18.320
there are lectures where you're actually building the thing

02:40:18.320 --> 02:40:20.800
like from, like you said, the code is truth.

02:40:20.800 --> 02:40:24.160
So discussing backpropagation by building it,

02:40:24.160 --> 02:40:26.160
by looking through it, just the whole thing.

02:40:26.160 --> 02:40:27.800
So how difficult is that to prepare for?

02:40:27.800 --> 02:40:30.400
I think that's a really powerful way to teach.

02:40:30.400 --> 02:40:31.640
Did you have to prepare for that?

02:40:31.640 --> 02:40:34.440
Or are you just live thinking through it?

02:40:34.480 --> 02:40:36.520
I will typically do like say three takes

02:40:36.520 --> 02:40:38.640
and then I take like the better take.

02:40:38.640 --> 02:40:39.920
So I do multiple takes

02:40:39.920 --> 02:40:40.920
and then I take some of the better takes

02:40:40.920 --> 02:40:42.920
and then I just build out a lecture that way.

02:40:42.920 --> 02:40:45.280
Sometimes I have to delete 30 minutes of content

02:40:45.280 --> 02:40:46.560
because it just went down the alley

02:40:46.560 --> 02:40:47.880
that I didn't like too much.

02:40:47.880 --> 02:40:49.640
So there's a bunch of iteration

02:40:49.640 --> 02:40:52.440
and it probably takes me somewhere around 10 hours

02:40:52.440 --> 02:40:53.480
to create one hour of content.

02:40:53.480 --> 02:40:54.720
To get one hour.

02:40:54.720 --> 02:40:55.560
It's interesting.

02:40:55.560 --> 02:40:58.920
I mean, is it difficult to go back to the basics?

02:40:58.920 --> 02:41:01.120
Do you draw a lot of like wisdom

02:41:01.120 --> 02:41:02.320
from going back to the basics?

02:41:02.320 --> 02:41:03.760
Yeah, going back to backpropagation,

02:41:03.760 --> 02:41:05.160
loss functions, where they come from.

02:41:05.160 --> 02:41:07.280
And one thing I like about teaching a lot, honestly,

02:41:07.280 --> 02:41:10.320
is it definitely strengthens your understanding.

02:41:10.320 --> 02:41:12.640
So it's not a purely altruistic activity.

02:41:12.640 --> 02:41:13.760
It's a way to learn.

02:41:13.760 --> 02:41:16.680
If you have to explain something to someone,

02:41:16.680 --> 02:41:19.440
you realize you have gaps in knowledge.

02:41:19.440 --> 02:41:22.280
And so I even surprised myself in those lectures.

02:41:22.280 --> 02:41:24.640
Like, oh, so the result will obviously look at this

02:41:24.640 --> 02:41:25.840
and then the result doesn't look like it.

02:41:25.840 --> 02:41:28.040
And I'm like, okay, I thought I understood this.

02:41:28.040 --> 02:41:28.880
Yeah.

02:41:28.880 --> 02:41:30.840
But that's why it's really cool

02:41:31.640 --> 02:41:34.000
literally code, you run it in a notebook

02:41:34.000 --> 02:41:35.160
and it gives you a result.

02:41:35.160 --> 02:41:36.560
And you're like, oh, wow.

02:41:36.560 --> 02:41:37.400
Yes.

02:41:37.400 --> 02:41:39.800
And like actual numbers, actual input, actual code.

02:41:39.800 --> 02:41:41.560
Yeah, it's not mathematical symbols, et cetera.

02:41:41.560 --> 02:41:43.000
The source of truth is the code.

02:41:43.000 --> 02:41:44.200
It's not slides.

02:41:44.200 --> 02:41:45.960
It's just like, let's build it.

02:41:45.960 --> 02:41:46.800
It's beautiful.

02:41:46.800 --> 02:41:48.480
You're a rare human in that sense.

02:41:48.480 --> 02:41:51.800
What advice would you give to researchers

02:41:51.800 --> 02:41:54.400
trying to develop and publish idea

02:41:54.400 --> 02:41:56.800
that have a big impact in the world of AI?

02:41:56.840 --> 02:42:01.640
So maybe undergrads, maybe early graduate students.

02:42:01.640 --> 02:42:02.720
Yeah.

02:42:02.720 --> 02:42:03.560
I mean, I would say like,

02:42:03.560 --> 02:42:05.800
they definitely have to be a little bit more strategic

02:42:05.800 --> 02:42:07.560
than I had to be as a PhD student

02:42:07.560 --> 02:42:09.720
because of the way AI is evolving.

02:42:09.720 --> 02:42:11.440
It's going the way of physics,

02:42:11.440 --> 02:42:13.920
where in physics you used to be able to do experiments

02:42:13.920 --> 02:42:15.400
on your bench top and everything was great

02:42:15.400 --> 02:42:17.000
and you could make progress.

02:42:17.000 --> 02:42:20.040
And now you have to work in like LHC or like CERN.

02:42:21.120 --> 02:42:23.840
And so AI is going in that direction as well.

02:42:23.840 --> 02:42:25.720
So there's certain kinds of things

02:42:25.720 --> 02:42:28.240
which is not possible to do on the bench top anymore.

02:42:28.240 --> 02:42:32.720
And I think that didn't used to be the case at the time.

02:42:32.720 --> 02:42:34.680
Do you still think that there's like

02:42:35.560 --> 02:42:38.560
GAN type papers to be written

02:42:38.560 --> 02:42:43.240
or like very simple idea that requires just one computer

02:42:43.240 --> 02:42:44.360
to illustrate a simple example?

02:42:44.360 --> 02:42:46.720
I mean, one example that's been very influential recently

02:42:46.720 --> 02:42:48.000
is diffusion models.

02:42:48.000 --> 02:42:49.280
Diffusion models are amazing.

02:42:49.280 --> 02:42:51.800
Diffusion models are six years old.

02:42:51.800 --> 02:42:53.880
For the longest time, people were kind of ignoring them

02:42:53.880 --> 02:42:55.040
as far as I can tell.

02:42:55.040 --> 02:42:57.200
And they're an amazing generative model,

02:42:57.200 --> 02:42:58.960
especially in images.

02:42:58.960 --> 02:43:01.760
And so stable diffusion and so on, it's all diffusion based.

02:43:01.760 --> 02:43:02.840
Diffusion is new.

02:43:02.840 --> 02:43:05.040
It was not there and it came from,

02:43:05.040 --> 02:43:05.880
well, it came from Google,

02:43:05.880 --> 02:43:07.440
but a researcher could have come up with it.

02:43:07.440 --> 02:43:09.440
In fact, some of the first,

02:43:09.440 --> 02:43:11.840
actually no, those came from Google as well.

02:43:11.840 --> 02:43:13.280
But a researcher could come up with that

02:43:13.280 --> 02:43:15.280
in an academic institution.

02:43:15.280 --> 02:43:16.720
Yeah, what do you find most fascinating

02:43:16.720 --> 02:43:17.880
about diffusion models?

02:43:17.880 --> 02:43:22.640
So from the societal impact to the technical architecture.

02:43:22.680 --> 02:43:25.400
What I like about diffusion is it works so well.

02:43:25.400 --> 02:43:26.880
Was that surprising to you?

02:43:26.880 --> 02:43:28.800
The amount of the variety,

02:43:28.800 --> 02:43:32.760
almost the novelty of the synthetic data is generating.

02:43:32.760 --> 02:43:36.120
Yeah, so the stable diffusion images are incredible.

02:43:37.360 --> 02:43:39.400
The speed of improvement in generating images

02:43:39.400 --> 02:43:40.960
has been insane.

02:43:40.960 --> 02:43:43.480
We went very quickly from generating like tiny digits

02:43:43.480 --> 02:43:45.520
to tiny faces and it all looked messed up

02:43:45.520 --> 02:43:46.760
and now we were stable diffusion.

02:43:46.760 --> 02:43:48.120
And that happened very quickly.

02:43:48.120 --> 02:43:50.400
There's a lot that academia can still contribute.

02:43:50.400 --> 02:43:54.280
For example, flash attention is a very efficient kernel

02:43:54.280 --> 02:43:57.400
for running the attention operation inside a transformer

02:43:57.400 --> 02:43:59.640
that came from academic environment.

02:43:59.640 --> 02:44:02.200
It's a very clever way to structure the kernel

02:44:02.200 --> 02:44:03.800
that does the calculation.

02:44:03.800 --> 02:44:07.120
So it doesn't materialize the attention matrix.

02:44:07.120 --> 02:44:08.760
And so I think there's still like lots of things

02:44:08.760 --> 02:44:11.120
to contribute, but you have to be just more strategic.

02:44:11.120 --> 02:44:13.640
Do you think neural networks can be made to reason?

02:44:14.680 --> 02:44:16.080
Yes.

02:44:16.080 --> 02:44:17.640
Do you think they already reason?

02:44:17.640 --> 02:44:18.480
Yes.

02:44:18.480 --> 02:44:20.080
What's your definition of reasoning?

02:44:21.080 --> 02:44:22.640
Information processing.

02:44:22.640 --> 02:44:26.800
So in the way that humans think through a problem

02:44:26.800 --> 02:44:28.480
and come up with novel ideas,

02:44:31.080 --> 02:44:33.480
it feels like reasoning.

02:44:33.480 --> 02:44:34.320
Yeah.

02:44:34.320 --> 02:44:38.200
So the novelty, I don't wanna say,

02:44:38.200 --> 02:44:43.200
but out of distribution ideas, you think it's possible?

02:44:43.320 --> 02:44:44.160
Yes.

02:44:44.160 --> 02:44:45.200
And I think we're seeing that already

02:44:45.200 --> 02:44:46.400
in the current neural nets.

02:44:46.400 --> 02:44:48.920
You're able to remix the training set information

02:44:48.920 --> 02:44:51.040
into true generalization in some sense.

02:44:51.040 --> 02:44:54.640
That doesn't appear in a fundamental way in the training set.

02:44:54.640 --> 02:44:56.440
Like you're doing something interesting algorithmically.

02:44:56.440 --> 02:44:59.080
You're manipulating some symbols

02:44:59.080 --> 02:45:01.840
and you're coming up with some correct,

02:45:01.840 --> 02:45:04.720
a unique answer in a new setting.

02:45:04.720 --> 02:45:07.640
What would illustrate to you,

02:45:07.640 --> 02:45:10.080
holy shit, this thing is definitely thinking?

02:45:11.200 --> 02:45:12.720
To me, thinking or reasoning

02:45:12.720 --> 02:45:15.240
is just information processing and generalization.

02:45:15.240 --> 02:45:17.960
And I think the neural nets already do that today.

02:45:18.000 --> 02:45:19.800
So being able to perceive the world

02:45:19.800 --> 02:45:22.640
or perceive whatever the inputs are

02:45:22.640 --> 02:45:27.080
and to make predictions based on that

02:45:27.080 --> 02:45:29.040
or actions based on that, that's reasoning.

02:45:29.040 --> 02:45:32.000
Yeah, you're giving correct answers in novel settings

02:45:33.240 --> 02:45:34.840
by manipulating information.

02:45:34.840 --> 02:45:36.560
You've learned the correct algorithm.

02:45:36.560 --> 02:45:38.200
You're not doing just some kind of a lookup table

02:45:38.200 --> 02:45:40.600
on there as neighbor search, something like that.

02:45:40.600 --> 02:45:42.040
Let me ask you about AGI.

02:45:42.040 --> 02:45:43.800
What are some moonshot ideas

02:45:43.800 --> 02:45:47.960
you think might make significant progress towards AGI?

02:45:47.960 --> 02:45:49.360
Or maybe in other ways,

02:45:49.360 --> 02:45:52.360
what are the big blockers that we're missing now?

02:45:52.360 --> 02:45:53.920
So basically I am fairly bullish

02:45:53.920 --> 02:45:57.400
on our ability to build AGI's.

02:45:57.400 --> 02:46:01.120
Basically automated systems that we can interact with

02:46:01.120 --> 02:46:02.360
that are very human-like,

02:46:02.360 --> 02:46:04.040
and we can interact with them in the digital realm

02:46:04.040 --> 02:46:05.480
or physical realm.

02:46:05.480 --> 02:46:07.920
Currently it seems most of the models

02:46:07.920 --> 02:46:10.040
that sort of do these sort of magical tasks

02:46:10.040 --> 02:46:11.040
are in a text realm.

02:46:11.680 --> 02:46:15.880
I think, as I mentioned, I'm suspicious

02:46:15.880 --> 02:46:17.560
that the text realm is not enough

02:46:17.560 --> 02:46:20.400
to actually build full understanding of the world.

02:46:20.400 --> 02:46:22.160
I do actually think you need to go into pixels

02:46:22.160 --> 02:46:24.840
and understand the physical world and how it works.

02:46:24.840 --> 02:46:26.640
So I do think that we need to extend these models

02:46:26.640 --> 02:46:28.280
to consume images and videos

02:46:28.280 --> 02:46:31.760
and train on a lot more data that is multimodal in that way.

02:46:32.600 --> 02:46:33.880
Do you think you need to touch the world

02:46:33.880 --> 02:46:34.920
to understand it also?

02:46:34.920 --> 02:46:36.960
Well, that's the big open question I would say in my mind

02:46:36.960 --> 02:46:39.480
is if you also require the embodiment

02:46:39.480 --> 02:46:42.440
and the ability to sort of interact with the world,

02:46:42.440 --> 02:46:45.480
run experiments and have a data of that form,

02:46:45.480 --> 02:46:48.560
then you need to go to Optimus or something like that.

02:46:48.560 --> 02:46:51.160
And so I would say Optimus in some way is like a hedge

02:46:52.720 --> 02:46:57.320
in AGI because it seems to me that it's possible

02:46:57.320 --> 02:47:00.240
that just having data from the internet is not enough.

02:47:00.240 --> 02:47:04.240
If that is the case, then Optimus may lead to AGI

02:47:04.240 --> 02:47:07.840
because Optimus, to me, there's nothing beyond Optimus.

02:47:07.840 --> 02:47:09.360
You have like this humanoid form factor

02:47:10.200 --> 02:47:11.400
that can actually do stuff in the world.

02:47:11.400 --> 02:47:12.760
You can have millions of them interacting

02:47:12.760 --> 02:47:14.560
with humans and so on.

02:47:14.560 --> 02:47:18.240
And if that doesn't give rise to AGI at some point,

02:47:18.240 --> 02:47:20.160
I'm not sure what will.

02:47:20.160 --> 02:47:21.880
So from a completeness perspective,

02:47:21.880 --> 02:47:24.800
I think that's a really good platform,

02:47:24.800 --> 02:47:26.440
but it's a much harder platform

02:47:26.440 --> 02:47:28.640
because you are dealing with atoms

02:47:28.640 --> 02:47:30.480
and you need to actually build these things

02:47:30.480 --> 02:47:32.720
and integrate them into society.

02:47:32.720 --> 02:47:34.960
So I think that path takes longer,

02:47:34.960 --> 02:47:36.640
but it's much more certain.

02:47:36.640 --> 02:47:38.280
And then there's a path of the internet

02:47:38.280 --> 02:47:41.600
and just like training these compression models effectively

02:47:41.600 --> 02:47:44.000
on trying to compress all the internet.

02:47:44.880 --> 02:47:48.160
And that might also give these agents as well.

02:47:48.160 --> 02:47:51.640
Compress the internet, but also interact with the internet.

02:47:51.640 --> 02:47:54.200
So it's not obvious to me.

02:47:54.200 --> 02:47:56.800
In fact, I suspect you can reach AGI

02:47:56.800 --> 02:47:59.480
without ever entering the physical world,

02:48:00.320 --> 02:48:03.880
and which is a little bit more concerning

02:48:03.880 --> 02:48:08.880
because that results in it happening faster.

02:48:08.960 --> 02:48:11.920
So it just feels like we're in boiling water.

02:48:11.920 --> 02:48:14.240
We won't know as it's happening.

02:48:14.240 --> 02:48:17.880
I would like to, I'm not afraid of AGI.

02:48:17.880 --> 02:48:19.160
I'm excited about it.

02:48:19.160 --> 02:48:20.440
There's always concerns,

02:48:20.440 --> 02:48:22.800
but I would like to know when it happens.

02:48:24.800 --> 02:48:26.840
And have like hints about when it happens,

02:48:26.840 --> 02:48:30.160
like a year from now it will happen, that kind of thing.

02:48:30.160 --> 02:48:33.400
I just feel like in the digital realm, it just might happen.

02:48:33.400 --> 02:48:34.680
I think all we have available to us

02:48:34.680 --> 02:48:36.840
because no one has built AGI again.

02:48:36.840 --> 02:48:38.960
So all we have available to us is,

02:48:39.800 --> 02:48:42.480
is there enough fertile ground on the periphery?

02:48:42.480 --> 02:48:43.320
I would say yes.

02:48:43.320 --> 02:48:46.440
And we have the progress so far, which has been very rapid.

02:48:46.440 --> 02:48:48.560
And there are next steps that are available.

02:48:48.560 --> 02:48:51.720
And so I would say, yeah, it's quite likely

02:48:51.720 --> 02:48:54.360
that we'll be interacting with digital entities.

02:48:54.360 --> 02:48:57.160
How will you know that somebody has built AGI?

02:48:57.160 --> 02:48:58.200
It's going to be a slow,

02:48:58.200 --> 02:49:00.040
I think it's going to be a slow incremental transition.

02:49:00.040 --> 02:49:01.680
It's going to be product-based and focused.

02:49:01.680 --> 02:49:03.800
It's going to be GitHub copilot getting better.

02:49:03.800 --> 02:49:06.440
And then GPT is helping you write.

02:49:06.440 --> 02:49:08.320
And then these oracles that you can go to

02:49:08.320 --> 02:49:09.720
with mathematical problems.

02:49:09.720 --> 02:49:12.000
I think we're on the verge of being able

02:49:12.000 --> 02:49:16.360
to ask very complex questions in chemistry, physics, math

02:49:16.360 --> 02:49:19.800
of these oracles and have them complete solutions.

02:49:19.800 --> 02:49:22.680
So AGI to use primarily focused on intelligence.

02:49:22.680 --> 02:49:27.680
So consciousness doesn't enter into it.

02:49:27.800 --> 02:49:30.200
So in my mind, consciousness is not a special thing

02:49:30.480 --> 02:49:32.160
you will figure out and bolt on.

02:49:32.160 --> 02:49:34.880
I think it's an emergent phenomenon of a large enough

02:49:34.880 --> 02:49:38.360
and complex enough generative model sort of.

02:49:38.360 --> 02:49:42.560
So if you have a complex enough world model

02:49:42.560 --> 02:49:43.840
that understands the world,

02:49:43.840 --> 02:49:46.760
then it also understands its predicament in the world

02:49:46.760 --> 02:49:48.560
as being a language model,

02:49:48.560 --> 02:49:52.000
which to me is a form of consciousness or self-awareness.

02:49:52.000 --> 02:49:53.880
So in order to understand the world deeply,

02:49:53.880 --> 02:49:56.680
you probably have to integrate yourself into the world.

02:49:56.680 --> 02:49:58.520
And in order to interact with humans

02:49:58.520 --> 02:50:00.320
and other living beings,

02:50:00.320 --> 02:50:02.760
consciousness is a very useful tool.

02:50:02.760 --> 02:50:05.800
I think consciousness is like a modeling insight.

02:50:05.800 --> 02:50:07.320
Modeling insight.

02:50:07.320 --> 02:50:10.080
Yeah, you have a powerful enough model

02:50:10.080 --> 02:50:11.880
of understanding the world that you actually understand

02:50:11.880 --> 02:50:13.360
that you are an entity in it.

02:50:13.360 --> 02:50:15.480
Yeah, but there's also this,

02:50:15.480 --> 02:50:18.000
perhaps just the narrative we tell ourselves.

02:50:18.000 --> 02:50:20.800
It feels like something to experience the world,

02:50:20.800 --> 02:50:22.760
the hard problem of consciousness.

02:50:22.760 --> 02:50:24.880
But that could be just a narrative that we tell ourselves.

02:50:24.880 --> 02:50:27.160
Yeah, I think it will emerge.

02:50:27.160 --> 02:50:30.040
I think it's going to be something very boring.

02:50:30.040 --> 02:50:31.840
We'll be talking to these digital AIs.

02:50:31.840 --> 02:50:33.320
They will claim they're conscious.

02:50:33.320 --> 02:50:34.960
They will appear conscious.

02:50:34.960 --> 02:50:35.840
They will do all the things

02:50:35.840 --> 02:50:37.480
that you would expect of other humans.

02:50:37.480 --> 02:50:40.320
And it's going to just be a stalemate.

02:50:40.320 --> 02:50:42.560
I think there'll be a lot of actual

02:50:42.560 --> 02:50:44.640
fascinating ethical questions,

02:50:44.640 --> 02:50:47.560
like Supreme Court level questions

02:50:47.560 --> 02:50:51.760
of whether you're allowed to turn off a conscious AI.

02:50:51.760 --> 02:50:54.480
If you're allowed to build a conscious AI,

02:50:55.320 --> 02:50:58.680
maybe there would have to be the same kind of debate

02:50:58.680 --> 02:50:59.680
that you have around,

02:51:01.400 --> 02:51:03.080
sorry to bring up a political topic,

02:51:03.080 --> 02:51:08.040
but abortion, which is the deeper question with abortion,

02:51:09.160 --> 02:51:11.440
is what is life?

02:51:11.440 --> 02:51:15.360
And the deep question with AI is also what is life

02:51:15.360 --> 02:51:16.400
and what is conscious?

02:51:16.400 --> 02:51:20.800
And I think that'll be very fascinating to bring up.

02:51:20.800 --> 02:51:23.560
It might become illegal to build systems

02:51:23.600 --> 02:51:28.600
that are capable of such level of intelligence

02:51:28.640 --> 02:51:29.920
that consciousness would emerge

02:51:29.920 --> 02:51:32.120
and therefore the capacity to suffer would emerge.

02:51:32.120 --> 02:51:36.200
And a system that says, no, please don't kill me.

02:51:36.200 --> 02:51:38.520
Well, that's what the Lambda Chatbot

02:51:38.520 --> 02:51:41.320
already told this Google engineer, right?

02:51:41.320 --> 02:51:44.920
It was talking about not wanting to die or so on.

02:51:44.920 --> 02:51:47.280
So that might become illegal to do that.

02:51:47.280 --> 02:51:48.120
Right.

02:51:49.680 --> 02:51:52.600
Because otherwise you might have a lot of creatures

02:51:52.600 --> 02:51:55.360
that don't want to die and they will.

02:51:55.360 --> 02:51:57.760
You can just spawn infinity of them on a cluster.

02:51:59.280 --> 02:52:01.720
And then that might lead to horrible consequences

02:52:01.720 --> 02:52:03.600
because then there might be a lot of people

02:52:03.600 --> 02:52:05.080
that secretly love murder

02:52:05.080 --> 02:52:07.640
and they'll start practicing murder in those systems.

02:52:07.640 --> 02:52:10.440
I mean, there's just, to me, all of this stuff

02:52:10.440 --> 02:52:14.120
just brings a beautiful mirror to the human condition

02:52:14.120 --> 02:52:15.840
and human nature and we'll get to explore it.

02:52:15.840 --> 02:52:19.640
And that's what the best of the Supreme Court,

02:52:19.640 --> 02:52:22.280
of all the different debates we have about ideas

02:52:22.960 --> 02:52:23.800
of what it means to be human.

02:52:23.800 --> 02:52:25.320
We get to ask those deep questions

02:52:25.320 --> 02:52:27.360
that we've been asking throughout human history.

02:52:27.360 --> 02:52:30.080
There has always been the other in human history.

02:52:31.040 --> 02:52:33.200
We're the good guys and that's the bad guys

02:52:33.200 --> 02:52:36.040
and we're going to, throughout human history,

02:52:36.040 --> 02:52:37.840
let's murder the bad guys.

02:52:37.840 --> 02:52:40.080
And the same will probably happen with robots.

02:52:40.080 --> 02:52:41.680
There'll be the other at first

02:52:41.680 --> 02:52:42.880
and then we'll get to ask questions

02:52:42.880 --> 02:52:44.520
of what does it mean to be alive?

02:52:44.520 --> 02:52:45.960
What does it mean to be conscious?

02:52:45.960 --> 02:52:46.800
Yeah.

02:52:46.800 --> 02:52:48.280
And I think there's some canary in the coal mines,

02:52:48.280 --> 02:52:50.080
even with what we have today.

02:52:50.080 --> 02:52:53.920
And for example, there's these waifus that you can work with

02:52:53.920 --> 02:52:55.520
and some people are trying to like,

02:52:55.520 --> 02:52:56.640
this company is going to shut down,

02:52:56.640 --> 02:52:59.400
but this person really loved their waifu

02:52:59.400 --> 02:53:01.600
and is trying to port it somewhere else

02:53:01.600 --> 02:53:03.160
and it's not possible.

02:53:03.160 --> 02:53:06.840
And I think definitely people will have feelings

02:53:06.840 --> 02:53:11.440
towards these systems because in some sense,

02:53:11.440 --> 02:53:13.400
they are like a mirror of humanity

02:53:13.400 --> 02:53:17.120
because they are sort of like a big average of humanity

02:53:17.120 --> 02:53:18.520
in the way that it's trained.

02:53:18.520 --> 02:53:22.320
But we can, that average, we can actually watch.

02:53:22.320 --> 02:53:23.600
It's nice to be able to interact

02:53:23.600 --> 02:53:25.360
with the big average of humanity

02:53:25.360 --> 02:53:27.320
and do like a search query on it.

02:53:27.320 --> 02:53:29.640
Yeah, yeah, it's very fascinating.

02:53:29.640 --> 02:53:31.920
And we can, of course, also like shape it.

02:53:31.920 --> 02:53:32.960
It's not just a pure average.

02:53:32.960 --> 02:53:34.600
We can mess with the training data.

02:53:34.600 --> 02:53:35.640
We can mess with the objective.

02:53:35.640 --> 02:53:37.640
We can fine tune them in various ways.

02:53:37.640 --> 02:53:42.520
So we have some impact on what those systems look like.

02:53:42.520 --> 02:53:44.600
If you, once we achieve AGI

02:53:45.760 --> 02:53:48.040
and you could have a conversation with her

02:53:48.560 --> 02:53:50.760
and ask her, talk about anything,

02:53:50.760 --> 02:53:51.800
maybe ask her a question,

02:53:51.800 --> 02:53:54.200
what kind of stuff would you ask?

02:53:54.200 --> 02:53:55.840
I would have some practical questions in my mind,

02:53:55.840 --> 02:54:00.080
like do I or my loved ones really have to die?

02:54:00.080 --> 02:54:01.440
What can we do about that?

02:54:02.880 --> 02:54:04.560
Do you think it will answer clearly

02:54:04.560 --> 02:54:06.240
or would it answer poetically?

02:54:07.360 --> 02:54:09.040
I would expect it to give solutions.

02:54:09.040 --> 02:54:10.120
I would expect it to be like,

02:54:10.120 --> 02:54:11.760
well, I've read all of these textbooks

02:54:11.760 --> 02:54:13.480
and I know all these things that you've produced.

02:54:13.480 --> 02:54:14.960
And it seems to me like here are the experiments

02:54:14.960 --> 02:54:17.560
that I think it would be useful to run next

02:54:17.560 --> 02:54:18.600
and here are some gene therapies

02:54:18.600 --> 02:54:19.840
that I think would be helpful.

02:54:19.840 --> 02:54:22.280
And here are the kinds of experiments that you should run.

02:54:22.280 --> 02:54:25.280
Okay, let's go with this thought experiment, okay?

02:54:25.280 --> 02:54:29.360
Imagine that mortality is actually

02:54:31.360 --> 02:54:32.960
a prerequisite for happiness.

02:54:32.960 --> 02:54:34.720
So if we become immortal,

02:54:34.720 --> 02:54:36.720
we'll actually become deeply unhappy.

02:54:36.720 --> 02:54:39.600
And the model is able to know that.

02:54:39.600 --> 02:54:41.160
So what is this supposed to tell you,

02:54:41.160 --> 02:54:42.480
stupid human, about it?

02:54:42.480 --> 02:54:43.800
Yes, you can become immortal,

02:54:43.800 --> 02:54:46.120
but you will become deeply unhappy.

02:54:46.200 --> 02:54:51.200
If the AGI system is trying to empathize with you human,

02:54:52.040 --> 02:54:53.560
what is it supposed to tell you?

02:54:53.560 --> 02:54:55.760
That yes, you don't have to die,

02:54:55.760 --> 02:54:57.920
but you're really not gonna like it?

02:54:57.920 --> 02:54:59.720
Is it gonna be deeply honest?

02:54:59.720 --> 02:55:02.080
Like there's a interstellar, what is it?

02:55:02.080 --> 02:55:05.720
The AI says like, humans want 90% honesty.

02:55:08.040 --> 02:55:09.840
So like you have to pick how honest

02:55:09.840 --> 02:55:12.400
do I wanna answer these practical questions.

02:55:12.400 --> 02:55:14.200
I love AI interstellar, by the way.

02:55:14.200 --> 02:55:16.720
I think it's like such a sidekick to the entire story,

02:55:16.720 --> 02:55:19.720
but at the same time, it's like really interesting.

02:55:19.720 --> 02:55:22.320
It's kind of limited in certain ways, right?

02:55:22.320 --> 02:55:23.160
Yeah, it's limited.

02:55:23.160 --> 02:55:24.520
And I think that's totally fine, by the way.

02:55:24.520 --> 02:55:27.880
I don't think, I think it's fine and plausible

02:55:27.880 --> 02:55:30.440
to have a limited and imperfect AGI's.

02:55:32.320 --> 02:55:34.000
Is that the feature almost?

02:55:34.000 --> 02:55:36.880
As an example, like it has a fixed amount of compute

02:55:36.880 --> 02:55:38.200
on its physical body.

02:55:38.200 --> 02:55:40.640
And it might just be that even though you can have

02:55:40.640 --> 02:55:43.960
a super amazing mega brain, super intelligent AI,

02:55:44.800 --> 02:55:46.560
you also can have like less intelligent AIs

02:55:46.560 --> 02:55:49.520
that you can deploy in a power efficient way.

02:55:49.520 --> 02:55:51.440
And then they're not perfect, they might make mistakes.

02:55:51.440 --> 02:55:55.320
No, I meant more like, say you had infinite compute

02:55:55.320 --> 02:55:58.160
and it's still good to make mistakes sometimes.

02:55:58.160 --> 02:55:59.600
Like in order to integrate yourself,

02:55:59.600 --> 02:56:03.280
like what is it, going back to Good Will Hunting,

02:56:03.280 --> 02:56:07.240
Robin Williams' character says like the human imperfections,

02:56:07.240 --> 02:56:09.480
that's the good stuff, right?

02:56:09.480 --> 02:56:12.480
Isn't that the, like we don't want perfect,

02:56:12.480 --> 02:56:17.480
we want flaws in part to form connections with each other

02:56:17.600 --> 02:56:19.240
because it feels like something you can attach

02:56:19.240 --> 02:56:23.040
your feelings to, the flaws.

02:56:23.040 --> 02:56:26.080
In that same way, you want AI that's flawed.

02:56:26.080 --> 02:56:26.920
I don't know.

02:56:26.920 --> 02:56:28.120
I feel like perfection is cool.

02:56:28.120 --> 02:56:29.800
But then you're saying, okay, yeah.

02:56:29.800 --> 02:56:30.960
But that's not AGI.

02:56:30.960 --> 02:56:33.960
But see, AGI would need to be intelligent enough

02:56:33.960 --> 02:56:36.800
to give answers to humans that humans don't understand.

02:56:36.800 --> 02:56:40.160
And I think perfect isn't something humans can't understand

02:56:40.160 --> 02:56:42.440
because even science doesn't give perfect answers.

02:56:43.400 --> 02:56:45.960
There's always gaps and mysteries and I don't know.

02:56:45.960 --> 02:56:50.080
I don't know if humans want perfect.

02:56:50.080 --> 02:56:52.760
Yeah, I could imagine just having a conversation

02:56:52.760 --> 02:56:55.800
with this kind of oracle entity as you'd imagine them.

02:56:55.800 --> 02:56:59.160
And yeah, maybe it can tell you about,

02:56:59.160 --> 02:57:01.160
based on my analysis of human condition,

02:57:02.120 --> 02:57:03.120
you might not want this.

02:57:03.120 --> 02:57:05.200
And here are some of the things that you might.

02:57:05.200 --> 02:57:08.800
But every dumb human will say, yeah, yeah, yeah, yeah.

02:57:08.800 --> 02:57:09.880
Trust me.

02:57:09.880 --> 02:57:11.440
I can, give me the truth.

02:57:11.440 --> 02:57:12.360
I can handle it.

02:57:13.320 --> 02:57:14.160
But that's the beauty.

02:57:14.160 --> 02:57:15.000
People can choose.

02:57:15.000 --> 02:57:20.000
But then, the old marshmallow test with the kids and so on,

02:57:20.320 --> 02:57:25.320
I feel like too many people can't handle the truth.

02:57:25.840 --> 02:57:27.160
Probably including myself.

02:57:27.160 --> 02:57:28.960
The deep truth of the human condition,

02:57:28.960 --> 02:57:31.360
I don't know if I can handle it.

02:57:31.360 --> 02:57:33.080
What if there's some dark,

02:57:33.080 --> 02:57:35.720
what if we are in alien science experiment

02:57:35.720 --> 02:57:36.920
and it realizes that?

02:57:36.920 --> 02:57:38.880
What if it had, I mean.

02:57:39.040 --> 02:57:41.840
This is the matrix all over again.

02:57:43.080 --> 02:57:43.920
I don't know.

02:57:44.880 --> 02:57:45.960
What would I talk about?

02:57:45.960 --> 02:57:47.160
I don't even, yeah.

02:57:49.480 --> 02:57:52.080
Probably I'll go with the safer scientific questions

02:57:52.080 --> 02:57:53.440
at first that have nothing to do

02:57:53.440 --> 02:57:56.960
with my own personal life and mortality.

02:57:56.960 --> 02:57:59.160
Just like about physics and so on.

02:58:00.760 --> 02:58:02.600
To build up, let's see where it's at.

02:58:02.600 --> 02:58:04.520
Or maybe see if it has a sense of humor.

02:58:04.520 --> 02:58:06.000
That's another question.

02:58:06.000 --> 02:58:08.480
Would it be able to, presumably in order to,

02:58:09.080 --> 02:58:10.080
if it understands humans deeply,

02:58:10.080 --> 02:58:15.080
would it be able to generate humor?

02:58:15.400 --> 02:58:18.520
Yeah, I think that's actually a wonderful benchmark almost.

02:58:18.520 --> 02:58:21.320
Is it able, I think that's a really good point basically.

02:58:21.320 --> 02:58:22.280
To make you laugh.

02:58:22.280 --> 02:58:24.880
Yeah, if it's able to be a very effective stand-up comedian

02:58:24.880 --> 02:58:26.920
that is doing something very interesting computationally.

02:58:26.920 --> 02:58:28.920
I think being funny is extremely hard.

02:58:28.920 --> 02:58:33.920
Yeah, because it's hard in a way,

02:58:34.040 --> 02:58:35.560
like a Turing test,

02:58:35.600 --> 02:58:38.560
the original intent of the Turing test is hard

02:58:38.560 --> 02:58:40.320
because you have to convince humans.

02:58:40.320 --> 02:58:41.920
And there's nothing, that's why,

02:58:43.240 --> 02:58:45.360
that's why comedians talk about this.

02:58:45.360 --> 02:58:48.000
Like there's, this is deeply honest.

02:58:48.000 --> 02:58:49.960
Because if people can't help but laugh,

02:58:49.960 --> 02:58:51.840
and if they don't laugh, that means you're not funny.

02:58:51.840 --> 02:58:52.960
If they laugh, it's funny.

02:58:52.960 --> 02:58:54.880
And you're showing, you need a lot of knowledge

02:58:54.880 --> 02:58:57.720
to create humor about like the documentation,

02:58:57.720 --> 02:58:58.600
human condition and so on.

02:58:58.600 --> 02:59:01.200
And then you need to be clever with it.

02:59:01.200 --> 02:59:02.400
You mentioned a few movies.

02:59:02.400 --> 02:59:05.240
You tweeted, movies that I've seen five plus times

02:59:05.240 --> 02:59:08.480
but I'm ready and willing to keep watching.

02:59:08.480 --> 02:59:10.440
Interstellar, Gladiator, Contact,

02:59:10.440 --> 02:59:13.280
Good Will Hunting, The Matrix, Lord of the Rings,

02:59:13.280 --> 02:59:16.120
all three, Avatar, Fifth Element, so on, it goes on.

02:59:16.120 --> 02:59:19.120
Terminator 2, Mean Girls, I'm not gonna ask about that.

02:59:19.120 --> 02:59:20.640
I think, I think her man.

02:59:20.640 --> 02:59:21.680
Mean Girls is great.

02:59:23.600 --> 02:59:25.800
What are some of the, jump onto your memory

02:59:25.800 --> 02:59:28.760
that you love and why?

02:59:28.760 --> 02:59:30.960
Like you mentioned The Matrix.

02:59:30.960 --> 02:59:33.400
As a computer person, why do you love The Matrix?

02:59:34.360 --> 02:59:35.320
There's so many properties

02:59:35.320 --> 02:59:36.600
that make it like beautiful and interesting.

02:59:36.600 --> 02:59:39.040
So there's all these philosophical questions,

02:59:39.040 --> 02:59:42.080
but then there was also AGI's and there's simulation

02:59:42.080 --> 02:59:44.960
and it's cool and there's the black.

02:59:46.240 --> 02:59:47.080
The look of it, the feel of it.

02:59:47.080 --> 02:59:48.480
Yeah, the look of it, the feel of it,

02:59:48.480 --> 02:59:49.960
the action, the bullet time.

02:59:49.960 --> 02:59:52.240
It was just like innovating in so many ways.

02:59:53.360 --> 02:59:57.440
And then Good Will Hunting, why do you like that one?

02:59:57.440 --> 03:00:01.120
Yeah, I just, I really like this tortured genius

03:00:01.200 --> 03:00:04.440
sort of character who's grappling with whether or not

03:00:04.440 --> 03:00:07.840
he has any responsibility or what to do with this gift

03:00:07.840 --> 03:00:10.800
that he was given or how to think about the whole thing.

03:00:10.800 --> 03:00:13.480
And there's also a dance between the genius

03:00:13.480 --> 03:00:18.040
and the personal, what it means to love another human being.

03:00:18.040 --> 03:00:19.200
There's a lot of themes there.

03:00:19.200 --> 03:00:20.240
It's just a beautiful movie.

03:00:20.240 --> 03:00:21.520
And then the fatherly figure,

03:00:21.520 --> 03:00:24.280
the mentor and the psychiatrist.

03:00:24.280 --> 03:00:27.240
It really, it messes with you.

03:00:27.240 --> 03:00:30.200
There's some movies that just really mess with you

03:00:30.200 --> 03:00:31.040
on a deep level.

03:00:31.960 --> 03:00:33.240
Do you relate to that movie at all?

03:00:33.240 --> 03:00:34.560
No.

03:00:34.560 --> 03:00:37.000
It's not your fault, Andre, as I said.

03:00:37.000 --> 03:00:40.200
Lord of the Rings, that's self-explanatory.

03:00:40.200 --> 03:00:42.800
Terminator 2, which is interesting.

03:00:42.800 --> 03:00:44.200
You re-watched that a lot.

03:00:44.200 --> 03:00:46.160
Is that better than Terminator 1?

03:00:46.160 --> 03:00:47.320
You like Arnold?

03:00:47.320 --> 03:00:49.100
I do like Terminator 1 as well.

03:00:50.120 --> 03:00:51.720
I like Terminator 2 a little bit more,

03:00:51.720 --> 03:00:53.920
but in terms of like its surface properties.

03:00:55.920 --> 03:00:58.600
Do you think Skynet is at all a possibility?

03:00:58.600 --> 03:00:59.440
Yes.

03:01:00.440 --> 03:01:04.360
Like the actual sort of autonomous weapon system

03:01:04.360 --> 03:01:05.200
kind of thing.

03:01:05.200 --> 03:01:06.880
Do you worry about that stuff?

03:01:06.880 --> 03:01:09.440
I do worry about it 100%. AI being useful war.

03:01:09.440 --> 03:01:10.560
I 100% worry about it.

03:01:10.560 --> 03:01:12.800
And so the, I mean, the, you know,

03:01:12.800 --> 03:01:15.480
some of these fears of AGI's and how this will plan out.

03:01:15.480 --> 03:01:17.160
I mean, these will be like very powerful entities

03:01:17.160 --> 03:01:18.000
probably at some point.

03:01:18.000 --> 03:01:20.000
And so for a long time,

03:01:20.000 --> 03:01:22.200
they're going to be tools in the hands of humans.

03:01:22.200 --> 03:01:24.240
You know, people talk about like alignment of AGI's

03:01:24.240 --> 03:01:26.080
and how to make, the problem is like

03:01:26.080 --> 03:01:27.720
even humans are not aligned.

03:01:27.720 --> 03:01:30.440
So how this will be used

03:01:30.440 --> 03:01:32.760
and what this is going to look like is,

03:01:32.760 --> 03:01:34.480
yeah, it's troubling.

03:01:34.480 --> 03:01:36.600
Do you think it'll happen slowly enough

03:01:36.600 --> 03:01:40.480
that we'll be able to, as a human civilization,

03:01:40.480 --> 03:01:41.760
think through the problems?

03:01:41.760 --> 03:01:44.020
Yes. That's my hope is that it happens slowly enough

03:01:44.020 --> 03:01:46.880
and in open enough way where a lot of people can see

03:01:46.880 --> 03:01:48.120
and participate in it.

03:01:48.120 --> 03:01:50.760
Just figure out how to deal with this transition,

03:01:50.760 --> 03:01:52.280
I think, which is going to be interesting.

03:01:52.280 --> 03:01:54.760
I draw a lot of inspiration from nuclear weapons

03:01:54.800 --> 03:01:58.000
because I sure thought it would be fucked

03:01:58.000 --> 03:02:00.300
once they developed nuclear weapons.

03:02:00.300 --> 03:02:01.760
But like, it's almost like

03:02:03.400 --> 03:02:06.480
when the systems are not so dangerous,

03:02:06.480 --> 03:02:07.840
they destroy human civilization,

03:02:07.840 --> 03:02:09.880
we deploy them and learn the lessons.

03:02:09.880 --> 03:02:12.760
And then we quickly, if it's too dangerous,

03:02:12.760 --> 03:02:15.560
we'll quickly, quickly, we might still deploy it,

03:02:15.560 --> 03:02:17.840
but you very quickly learn not to use them.

03:02:17.840 --> 03:02:19.640
And so there'll be like this balance achieved.

03:02:19.640 --> 03:02:21.960
Humans are very clever as a species.

03:02:21.960 --> 03:02:23.000
It's interesting.

03:02:23.000 --> 03:02:25.520
We exploit the resources as much as we can,

03:02:25.520 --> 03:02:29.240
but we avoid destroying ourselves, it seems like.

03:02:29.240 --> 03:02:30.800
Well, I don't know about that, actually.

03:02:30.800 --> 03:02:31.980
I hope it continues.

03:02:33.640 --> 03:02:35.420
I mean, I'm definitely concerned

03:02:35.420 --> 03:02:36.780
about nuclear weapons and so on,

03:02:36.780 --> 03:02:38.840
not just as a result of the recent conflict,

03:02:38.840 --> 03:02:40.400
even before that.

03:02:40.400 --> 03:02:43.420
That's probably my number one concern for humanity.

03:02:43.420 --> 03:02:47.600
So if humanity destroys itself

03:02:47.600 --> 03:02:52.520
or destroys 90% of people, that would be because of nukes?

03:02:52.520 --> 03:02:53.360
I think so.

03:02:54.160 --> 03:02:55.780
And it's not even about the full destruction.

03:02:55.780 --> 03:02:58.000
To me, it's bad enough if we reset society.

03:02:58.000 --> 03:02:59.560
That would be like terrible.

03:02:59.560 --> 03:03:00.400
It would be really bad.

03:03:00.400 --> 03:03:03.600
And I can't believe we're like so close to it.

03:03:03.600 --> 03:03:05.160
It's like so crazy to me.

03:03:05.160 --> 03:03:07.120
It feels like we might be a few tweets away

03:03:07.120 --> 03:03:08.440
from something like that.

03:03:08.440 --> 03:03:11.880
Yup, basically it's extremely unnerving

03:03:11.880 --> 03:03:14.240
and has been for me for a long time.

03:03:14.240 --> 03:03:18.520
It seems unstable that world leaders

03:03:18.520 --> 03:03:21.600
just having a bad mood can like

03:03:22.840 --> 03:03:26.640
take one step towards a bad direction and it escalates.

03:03:26.640 --> 03:03:27.560
Yeah.

03:03:27.560 --> 03:03:30.360
Because of a collection of bad moods,

03:03:30.360 --> 03:03:33.760
it can escalate without being able to stop.

03:03:34.640 --> 03:03:37.180
Yeah, it's just a huge amount of power.

03:03:37.180 --> 03:03:39.520
And then also with the proliferation,

03:03:39.520 --> 03:03:41.880
and basically I don't actually really see,

03:03:41.880 --> 03:03:44.980
I don't actually know what the good outcomes are here.

03:03:44.980 --> 03:03:46.680
So I'm definitely worried about that a lot.

03:03:46.680 --> 03:03:48.400
And then AGI is not currently there,

03:03:48.400 --> 03:03:52.280
but I think at some point will more and more become

03:03:52.280 --> 03:03:53.320
something like it.

03:03:53.320 --> 03:03:55.520
The danger with AGI even is that,

03:03:55.520 --> 03:03:56.880
I think it's even like slightly worse

03:03:56.880 --> 03:04:01.280
in the sense that there are good outcomes of AGI.

03:04:01.280 --> 03:04:03.960
And then the bad outcomes are like an epsilon away,

03:04:03.960 --> 03:04:05.240
like a tiny one away.

03:04:05.240 --> 03:04:08.240
And so I think capitalism and humanity and so on

03:04:08.240 --> 03:04:11.960
will drive for the positive ways of using that technology.

03:04:11.960 --> 03:04:13.920
But then if bad outcomes are just like a tiny,

03:04:13.920 --> 03:04:16.560
like flip a minus sign away,

03:04:16.560 --> 03:04:18.320
that's a really bad position to be in.

03:04:18.320 --> 03:04:20.760
A tiny perturbation of the system results

03:04:20.760 --> 03:04:23.040
in the destruction of the human species.

03:04:23.040 --> 03:04:25.240
It's a weird line to walk.

03:04:25.240 --> 03:04:26.080
Yeah, I think in general,

03:04:26.080 --> 03:04:28.000
what's really weird about like the dynamics of humanity

03:04:28.000 --> 03:04:29.200
in this explosion we talked about,

03:04:29.200 --> 03:04:33.000
is just like the insane coupling afforded by technology,

03:04:33.000 --> 03:04:36.400
and just the instability of the whole dynamical system.

03:04:36.400 --> 03:04:39.200
I think it just doesn't look good, honestly.

03:04:39.200 --> 03:04:40.960
Yeah, so that explosion could be destructive

03:04:40.960 --> 03:04:43.640
and constructive and the probabilities are non-zero

03:04:43.640 --> 03:04:45.080
in both things.

03:04:45.080 --> 03:04:48.160
I mean, I do feel like I have to try to be optimistic

03:04:48.160 --> 03:04:50.000
and so on, and I think even in this case,

03:04:50.000 --> 03:04:51.680
I still am predominantly optimistic,

03:04:51.680 --> 03:04:53.720
but there's definitely...

03:04:53.720 --> 03:04:54.800
Me too.

03:04:54.800 --> 03:04:57.400
Do you think we'll become a multi-planetary species?

03:04:58.680 --> 03:05:01.200
Probably yes, but I don't know if it's dominant feature

03:05:01.200 --> 03:05:04.160
of future humanity.

03:05:04.160 --> 03:05:06.920
There might be some people on some planets and so on,

03:05:06.920 --> 03:05:08.680
but I'm not sure if it's like,

03:05:08.680 --> 03:05:12.120
yeah, if it's like a major player in our culture and so on.

03:05:12.120 --> 03:05:14.440
We still have to solve the drivers

03:05:14.440 --> 03:05:16.800
of self-destruction here on earth.

03:05:16.800 --> 03:05:18.400
So just having a backup on Mars

03:05:18.400 --> 03:05:19.960
is not gonna solve the problem.

03:05:19.960 --> 03:05:21.880
So by the way, I love the backup on Mars.

03:05:21.880 --> 03:05:22.720
I think that's amazing.

03:05:22.720 --> 03:05:23.760
We should absolutely do that.

03:05:23.760 --> 03:05:24.840
Yes.

03:05:24.840 --> 03:05:26.880
And I'm so thankful for anyone's work.

03:05:26.880 --> 03:05:28.720
Would you go to Mars?

03:05:28.720 --> 03:05:29.600
Personally, no.

03:05:29.600 --> 03:05:31.920
I do like earth quite a lot.

03:05:31.920 --> 03:05:32.760
I'll go to Mars.

03:05:32.760 --> 03:05:34.000
I'll go for you.

03:05:34.000 --> 03:05:35.400
I'll tweet at you from there.

03:05:35.400 --> 03:05:37.600
Maybe eventually I would once it's safe enough,

03:05:37.600 --> 03:05:40.360
but I don't actually know if it's on my lifetime scale

03:05:40.360 --> 03:05:41.960
unless I can extend it by a lot.

03:05:43.040 --> 03:05:44.000
I do think that, for example,

03:05:44.000 --> 03:05:47.080
a lot of people might disappear into virtual realities

03:05:47.080 --> 03:05:47.920
and stuff like that.

03:05:47.920 --> 03:05:49.240
I think that could be the major thrust

03:05:49.240 --> 03:05:52.480
of sort of the cultural development of humanity

03:05:52.480 --> 03:05:53.840
if it survives.

03:05:53.840 --> 03:05:54.920
So it might not be,

03:05:54.920 --> 03:05:57.160
it's just really hard to work in physical realm

03:05:57.160 --> 03:05:58.440
and go out there.

03:05:58.440 --> 03:06:00.160
And I think ultimately all your experiences

03:06:00.160 --> 03:06:02.040
are in your brain.

03:06:02.040 --> 03:06:05.720
And so it's much easier to disappear into digital realm.

03:06:05.720 --> 03:06:07.560
And I think people will find them more compelling,

03:06:07.560 --> 03:06:10.600
easier, safer, more interesting.

03:06:10.600 --> 03:06:12.880
So you're a little bit captivated by virtual reality,

03:06:12.880 --> 03:06:14.240
by the possible worlds,

03:06:14.240 --> 03:06:15.240
whether it's the metaverse

03:06:15.240 --> 03:06:16.840
or some other manifestation of that.

03:06:16.840 --> 03:06:18.240
Yeah.

03:06:18.240 --> 03:06:19.680
Yeah, it's really interesting.

03:06:21.680 --> 03:06:24.920
I'm interested just talking a lot to Carmack.

03:06:24.920 --> 03:06:29.440
Where's the thing that's currently preventing that?

03:06:29.440 --> 03:06:30.280
Yeah.

03:06:30.280 --> 03:06:31.120
I mean, to be clear,

03:06:31.120 --> 03:06:33.800
I think what's interesting about the future is

03:06:33.800 --> 03:06:35.360
it's not that,

03:06:35.360 --> 03:06:39.080
I kind of feel like the variance in the human condition grows.

03:06:39.080 --> 03:06:40.400
That's the primary thing that's changing.

03:06:40.400 --> 03:06:42.840
It's not as much the mean of the distribution,

03:06:42.840 --> 03:06:43.920
it's like the variance of it.

03:06:43.920 --> 03:06:45.360
So there will probably be people on Mars

03:06:45.360 --> 03:06:46.680
and there will be people in VR

03:06:47.520 --> 03:06:48.360
and there will be people here on Earth.

03:06:48.360 --> 03:06:50.960
It's just like there will be so many more ways of being.

03:06:50.960 --> 03:06:51.800
And so I kind of feel like,

03:06:51.800 --> 03:06:54.600
I see it as like a spreading out of a human experience.

03:06:54.600 --> 03:06:55.960
There's something about the internet

03:06:55.960 --> 03:06:57.880
that allows you to discover those little groups

03:06:57.880 --> 03:06:59.760
and then you gravitate to it.

03:06:59.760 --> 03:07:01.040
Something about your biology,

03:07:01.040 --> 03:07:02.920
likes that kind of world that you find each other.

03:07:02.920 --> 03:07:04.560
Yeah, and we'll have transhumanists

03:07:04.560 --> 03:07:05.720
and then we'll have the Amish

03:07:05.720 --> 03:07:07.640
and everything is just gonna coexist.

03:07:07.640 --> 03:07:08.680
You know, the cool thing about it,

03:07:08.680 --> 03:07:11.600
because I've interacted with a bunch of internet communities

03:07:11.600 --> 03:07:15.480
is they don't know about each other.

03:07:15.520 --> 03:07:17.840
Like you can have a very happy existence,

03:07:17.840 --> 03:07:19.840
just like having a very close-knit community

03:07:19.840 --> 03:07:21.280
and not knowing about each other.

03:07:21.280 --> 03:07:23.120
I mean, you even sense this,

03:07:23.120 --> 03:07:24.760
just having traveled to Ukraine,

03:07:26.080 --> 03:07:29.000
they don't know so many things about America.

03:07:30.320 --> 03:07:31.480
When you travel across the world,

03:07:31.480 --> 03:07:33.080
I think you experience this too.

03:07:33.080 --> 03:07:34.760
There are certain cultures that are like,

03:07:34.760 --> 03:07:36.680
they have their own thing going on.

03:07:36.680 --> 03:07:38.920
And so you could see that happening

03:07:38.920 --> 03:07:40.920
more and more and more and more in the future.

03:07:40.920 --> 03:07:42.120
We have little communities.

03:07:42.120 --> 03:07:43.240
Yeah, yeah, I think so.

03:07:43.240 --> 03:07:46.760
That seems to be how it's going right now.

03:07:46.760 --> 03:07:48.840
And I don't see that trend really reversing.

03:07:48.840 --> 03:07:49.840
I think people are diverse

03:07:49.840 --> 03:07:52.800
and they're able to choose their own path and existence.

03:07:52.800 --> 03:07:54.280
And I sort of celebrate that.

03:07:56.160 --> 03:07:58.080
Will you spend some much time in the metaverse,

03:07:58.080 --> 03:07:59.840
in the virtual reality?

03:07:59.840 --> 03:08:01.480
Or which community are you?

03:08:01.480 --> 03:08:02.720
Are you the physicalist,

03:08:04.960 --> 03:08:06.920
the physical reality enjoyer,

03:08:06.920 --> 03:08:10.680
or do you see drawing a lot of pleasure

03:08:10.680 --> 03:08:12.640
and fulfillment in the digital world?

03:08:13.400 --> 03:08:15.560
Yeah, I think currently the virtual reality

03:08:15.560 --> 03:08:17.320
is not that compelling.

03:08:17.320 --> 03:08:18.760
I do think it can improve a lot,

03:08:18.760 --> 03:08:21.440
but I don't really know to what extent.

03:08:21.440 --> 03:08:23.680
Maybe there's actually even more exotic things

03:08:23.680 --> 03:08:26.360
you can think about with neural links or stuff like that.

03:08:26.360 --> 03:08:29.800
So currently I kind of see myself

03:08:29.800 --> 03:08:31.680
as mostly a team human person.

03:08:31.680 --> 03:08:32.800
I love nature.

03:08:32.800 --> 03:08:33.640
I love harmony.

03:08:33.640 --> 03:08:34.640
I love people.

03:08:34.640 --> 03:08:36.080
I love humanity.

03:08:36.080 --> 03:08:37.680
I love emotions of humanity.

03:08:38.840 --> 03:08:43.080
And I just want to be in this solar punk little utopia

03:08:43.920 --> 03:08:44.760
that's my happy place.

03:08:44.760 --> 03:08:47.120
My happy place is like people I love

03:08:47.120 --> 03:08:48.240
thinking about cool problems

03:08:48.240 --> 03:08:51.520
surrounded by a lush, beautiful, dynamic nature

03:08:51.520 --> 03:08:54.360
and secretly high tech in places that count.

03:08:54.360 --> 03:08:55.280
Places that count.

03:08:55.280 --> 03:08:58.080
So you use technology to empower that love

03:08:58.080 --> 03:09:00.560
for other humans and nature.

03:09:00.560 --> 03:09:03.120
Yeah, I think technology used like very sparingly.

03:09:03.120 --> 03:09:04.560
I don't love when it sort of gets

03:09:04.560 --> 03:09:07.440
in the way of humanity in many ways.

03:09:07.440 --> 03:09:09.640
I like just people being humans in a way

03:09:09.640 --> 03:09:11.400
we sort of like slightly evolved

03:09:11.400 --> 03:09:13.280
and prefer I think just by default.

03:09:13.280 --> 03:09:16.160
People kept asking me because they know you love reading.

03:09:16.160 --> 03:09:19.720
Are there particular books that you enjoyed

03:09:19.720 --> 03:09:22.720
that had an impact on you for silly

03:09:22.720 --> 03:09:26.120
or for profound reasons that you would recommend?

03:09:27.120 --> 03:09:29.400
You mentioned the vital question.

03:09:29.400 --> 03:09:30.240
Many of course.

03:09:30.240 --> 03:09:31.680
I think in biology as an example,

03:09:31.680 --> 03:09:32.960
the vital question is a good one.

03:09:32.960 --> 03:09:36.040
Anything by Nicolayne really life ascending

03:09:36.040 --> 03:09:39.720
I would say is like a bit more potentially representative

03:09:39.720 --> 03:09:42.680
is like a summary of a lot of the things

03:09:42.680 --> 03:09:44.320
he's been talking about.

03:09:44.320 --> 03:09:46.320
I was very impacted by the selfish gene.

03:09:46.320 --> 03:09:47.720
I thought that was a really good book

03:09:47.720 --> 03:09:50.000
that helped me understand altruism as an example

03:09:50.000 --> 03:09:51.440
and where it comes from and just realizing

03:09:51.440 --> 03:09:53.480
that the selection is on the level of genes

03:09:53.480 --> 03:09:55.160
was a huge insight for me at the time.

03:09:55.160 --> 03:09:57.200
And it sort of like cleared up a lot of things for me.

03:09:57.200 --> 03:09:59.920
What do you think about the idea

03:09:59.920 --> 03:10:01.920
that ideas are the organisms, the means?

03:10:01.920 --> 03:10:03.440
Yes, love it, 100%.

03:10:03.440 --> 03:10:08.440
Are you able to walk around with that notion for a while?

03:10:08.880 --> 03:10:12.360
That there is an evolutionary kind of process

03:10:12.360 --> 03:10:13.320
with ideas as well?

03:10:13.320 --> 03:10:14.160
There absolutely is.

03:10:14.160 --> 03:10:16.520
There's means just like genes and they compete

03:10:16.520 --> 03:10:18.360
and they live in our brains.

03:10:18.360 --> 03:10:19.360
It's beautiful.

03:10:19.360 --> 03:10:22.040
Are we silly humans thinking that we're the organisms?

03:10:22.040 --> 03:10:26.160
Is it possible that the primary organisms are the ideas?

03:10:27.160 --> 03:10:29.000
Yeah, I would say like the ideas kind of live

03:10:29.000 --> 03:10:31.600
in the software of like our civilization

03:10:31.600 --> 03:10:33.560
in the minds and so on.

03:10:33.560 --> 03:10:36.040
We think as humans that the hardware

03:10:36.040 --> 03:10:37.800
is the fundamental thing.

03:10:37.840 --> 03:10:40.960
I, human, is a hardware entity,

03:10:40.960 --> 03:10:43.080
but it could be the software, right?

03:10:43.080 --> 03:10:43.920
Yeah.

03:10:44.720 --> 03:10:46.760
Yeah, I would say like there needs to be some grounding

03:10:46.760 --> 03:10:48.840
at some point to like a physical reality.

03:10:48.840 --> 03:10:50.480
Yeah, but if we clone an Andre,

03:10:52.440 --> 03:10:54.600
the software is the thing,

03:10:54.600 --> 03:10:57.560
like is this thing that makes that thing special, right?

03:10:57.560 --> 03:10:59.360
Yeah, I guess you're right.

03:10:59.360 --> 03:11:01.400
But then cloning might be exceptionally difficult.

03:11:01.400 --> 03:11:02.880
Like there might be a deep integration

03:11:02.880 --> 03:11:04.560
between the software and the hardware

03:11:04.560 --> 03:11:06.280
in ways we don't quite yet understand.

03:11:06.280 --> 03:11:07.280
Well, from the evolutionary point of view,

03:11:07.400 --> 03:11:10.680
what makes me special is more like the gang of genes

03:11:10.680 --> 03:11:13.120
that are writing in my chromosomes, I suppose, right?

03:11:13.120 --> 03:11:16.000
Like they're the replicating unit, I suppose.

03:11:16.000 --> 03:11:19.960
No, but that's just the thing that makes you special, sure.

03:11:19.960 --> 03:11:24.960
Well, the reality is what makes you special

03:11:25.000 --> 03:11:29.680
is your ability to survive based on the software

03:11:29.680 --> 03:11:33.040
that runs on the hardware that was built by the genes.

03:11:33.920 --> 03:11:35.840
So the software is the thing that makes you survive,

03:11:35.840 --> 03:11:36.760
not the hardware.

03:11:37.680 --> 03:11:38.520
It's a little bit of both.

03:11:38.520 --> 03:11:40.320
I mean, it's just like a second layer.

03:11:40.320 --> 03:11:41.400
It's a new second layer

03:11:41.400 --> 03:11:42.840
that hasn't been there before the brain.

03:11:42.840 --> 03:11:44.120
They both coexist.

03:11:44.120 --> 03:11:46.040
But there's also layers of the software.

03:11:46.040 --> 03:11:49.760
I mean, it's not, it's a abstraction

03:11:49.760 --> 03:11:52.000
that's on top of abstractions.

03:11:52.000 --> 03:11:55.520
But, yeah, so Selfish Gene and Nick Lane,

03:11:55.520 --> 03:11:58.480
I would say sometimes books are like not sufficient.

03:11:58.480 --> 03:12:00.480
I like to reach for textbooks sometimes.

03:12:01.520 --> 03:12:03.520
I kind of feel like books are for too much

03:12:03.520 --> 03:12:05.080
of a general consumption sometime,

03:12:05.080 --> 03:12:06.840
and they just kind of like,

03:12:06.840 --> 03:12:08.520
they're too high up in the level of abstraction

03:12:08.520 --> 03:12:09.880
and it's not good enough.

03:12:09.880 --> 03:12:10.720
So I like textbooks.

03:12:10.720 --> 03:12:12.120
I like The Cell.

03:12:12.120 --> 03:12:13.840
I think The Cell was pretty cool.

03:12:14.720 --> 03:12:17.840
That's why also I like the writing of Nick Lane

03:12:17.840 --> 03:12:21.200
is because he's pretty willing to step one level down

03:12:21.200 --> 03:12:24.880
and he doesn't, yeah, he sort of, he's willing to go there.

03:12:25.920 --> 03:12:27.840
But he's also willing to sort of be throughout the stack.

03:12:27.840 --> 03:12:29.200
So he'll go down to a lot of detail,

03:12:29.200 --> 03:12:30.720
but then he will come back up.

03:12:30.720 --> 03:12:33.040
And I think he has a, yeah, basically,

03:12:33.040 --> 03:12:34.680
I really appreciate that.

03:12:34.720 --> 03:12:37.720
That's why I love college, early college, even high school.

03:12:37.720 --> 03:12:41.200
Just textbooks on the basics of computer science,

03:12:41.200 --> 03:12:44.640
of mathematics, of biology, of chemistry.

03:12:44.640 --> 03:12:46.360
Those are, they condense down.

03:12:48.320 --> 03:12:50.560
It's sufficiently general that you can understand

03:12:50.560 --> 03:12:52.080
both the philosophy and the details,

03:12:52.080 --> 03:12:54.560
but also you get homework problems

03:12:54.560 --> 03:12:57.320
and you get to play with it as much as you would

03:12:57.320 --> 03:13:00.040
if you were in programming stuff.

03:13:00.040 --> 03:13:01.920
And then I'm also suspicious of textbooks, honestly,

03:13:01.920 --> 03:13:04.240
because as an example in deep learning,

03:13:04.240 --> 03:13:05.720
there's no amazing textbooks

03:13:05.720 --> 03:13:07.240
and the field is changing very quickly.

03:13:07.240 --> 03:13:09.720
I imagine the same is true in say,

03:13:09.720 --> 03:13:11.360
synthetic biology and so on.

03:13:11.360 --> 03:13:13.440
These books like The Cell are kind of outdated.

03:13:13.440 --> 03:13:14.520
They're still high level.

03:13:14.520 --> 03:13:16.400
Like what is the actual real source of truth?

03:13:16.400 --> 03:13:19.560
It's people in wet labs working with cells,

03:13:19.560 --> 03:13:24.560
sequencing genomes and yeah, actually working with it.

03:13:24.680 --> 03:13:27.080
And I don't have that much exposure to that

03:13:27.080 --> 03:13:27.920
or what that looks like.

03:13:27.920 --> 03:13:30.160
So I still don't fully, I'm reading through The Cell

03:13:30.160 --> 03:13:31.400
and it's kind of interesting and I'm learning,

03:13:31.400 --> 03:13:33.280
but it's still not sufficient, I would say,

03:13:33.280 --> 03:13:34.760
in terms of understanding.

03:13:34.760 --> 03:13:36.680
Well, it's a clean summarization

03:13:36.680 --> 03:13:38.200
of the mainstream narrative.

03:13:39.200 --> 03:13:42.120
But you have to learn that before you break out

03:13:42.120 --> 03:13:44.200
towards the cutting edge.

03:13:44.200 --> 03:13:45.960
What is the actual process of working with these cells

03:13:45.960 --> 03:13:47.840
and growing them and incubating them?

03:13:47.840 --> 03:13:50.200
And it's kind of like a massive cooking recipes

03:13:50.200 --> 03:13:52.280
of making sure your cells live and proliferate

03:13:52.280 --> 03:13:54.040
and then you're sequencing them, running experiments

03:13:54.040 --> 03:13:56.680
and just how that works, I think is kind of like

03:13:56.680 --> 03:13:58.400
the source of truth of at the end of the day,

03:13:58.400 --> 03:14:01.120
what's really useful in terms of creating therapies

03:14:01.120 --> 03:14:01.960
and so on.

03:14:02.040 --> 03:14:04.880
I wonder what in the future AI textbooks will be

03:14:04.880 --> 03:14:06.880
because there's artificial intelligence,

03:14:06.880 --> 03:14:07.720
the modern approach.

03:14:07.720 --> 03:14:09.880
I actually haven't read, if it's come out,

03:14:09.880 --> 03:14:13.400
the recent version, there's been a recent addition.

03:14:13.400 --> 03:14:15.880
I also saw there's a science, a deep learning book.

03:14:15.880 --> 03:14:17.920
I'm waiting for textbooks that are worth recommending,

03:14:17.920 --> 03:14:18.760
worth reading.

03:14:18.760 --> 03:14:23.600
It's tricky because it's like papers and code, code, code.

03:14:23.600 --> 03:14:25.720
Honestly, I think papers are quite good.

03:14:25.720 --> 03:14:28.680
I especially like the appendix of any paper as well.

03:14:28.680 --> 03:14:31.000
It's like the most detail you can have.

03:14:32.960 --> 03:14:35.760
It doesn't have to be cohesive to connect it to anything else.

03:14:35.760 --> 03:14:37.680
You just described me a very specific way

03:14:37.680 --> 03:14:39.120
you saw the particular thing, yeah.

03:14:39.120 --> 03:14:41.080
Many times papers can be actually quite readable.

03:14:41.080 --> 03:14:43.000
Not always, but sometimes the introduction

03:14:43.000 --> 03:14:44.040
and the abstract is readable,

03:14:44.040 --> 03:14:46.240
even for someone outside of the field.

03:14:46.240 --> 03:14:47.560
This is not always true.

03:14:47.560 --> 03:14:49.040
And sometimes I think, unfortunately,

03:14:49.040 --> 03:14:52.400
scientists use complex terms even when it's not necessary.

03:14:52.400 --> 03:14:53.840
I think that's harmful.

03:14:53.840 --> 03:14:55.680
I think there's no reason for that.

03:14:55.680 --> 03:14:58.400
And papers sometimes are longer than they need to be

03:14:58.400 --> 03:15:01.440
in the parts that don't matter.

03:15:02.240 --> 03:15:04.960
Appendix would be long, but then the paper itself,

03:15:04.960 --> 03:15:07.080
look at Einstein, make it simple.

03:15:07.080 --> 03:15:08.520
Yeah, but certainly I've come across papers,

03:15:08.520 --> 03:15:10.520
I would say, in say like synthetic biology or something

03:15:10.520 --> 03:15:11.800
that I thought were quite readable

03:15:11.800 --> 03:15:13.240
for the abstract and the introduction.

03:15:13.240 --> 03:15:14.520
And then you're reading the rest of it

03:15:14.520 --> 03:15:15.880
and you don't fully understand,

03:15:15.880 --> 03:15:18.480
but you kind of are getting a gist and I think it's cool.

03:15:20.160 --> 03:15:23.280
What advice, you give advice to folks

03:15:23.280 --> 03:15:25.440
interested in machine learning and research,

03:15:25.440 --> 03:15:27.640
but in general, life advice to a young person,

03:15:27.640 --> 03:15:30.640
high school, early college,

03:15:30.680 --> 03:15:33.160
about how to have a career they can be proud of

03:15:33.160 --> 03:15:34.680
or a life they can be proud of?

03:15:35.680 --> 03:15:38.000
Yeah, I think I'm very hesitant to give general advice.

03:15:38.000 --> 03:15:38.920
I think it's really hard.

03:15:38.920 --> 03:15:40.480
I've mentioned like some of the stuff I've mentioned

03:15:40.480 --> 03:15:41.760
is fairly general, I think,

03:15:41.760 --> 03:15:44.120
like focus on just the amount of work you're spending

03:15:44.120 --> 03:15:45.720
on like a thing.

03:15:45.720 --> 03:15:48.120
Compare yourself only to yourself, not to others.

03:15:48.120 --> 03:15:48.960
That's good.

03:15:48.960 --> 03:15:49.920
I think those are fairly general.

03:15:49.920 --> 03:15:51.240
How do you pick the thing?

03:15:52.200 --> 03:15:54.440
You just have like a deep interest in something

03:15:55.400 --> 03:15:57.840
or like try to like find the argmax over

03:15:57.840 --> 03:15:58.920
like the things that you're interested in.

03:15:58.920 --> 03:16:00.960
Argmax at that moment and stick with it.

03:16:00.960 --> 03:16:05.200
How do you not get distracted and switch to another thing?

03:16:05.200 --> 03:16:06.480
You can, if you like.

03:16:07.840 --> 03:16:10.480
Wait, if you do an argmax repeatedly,

03:16:10.480 --> 03:16:11.320
every week, every month.

03:16:11.320 --> 03:16:12.160
It doesn't converge.

03:16:12.160 --> 03:16:13.320
It doesn't, it's a problem.

03:16:13.320 --> 03:16:15.360
Yeah, you can like low pass filter yourself

03:16:15.360 --> 03:16:18.320
in terms of like what has consistently been true for you.

03:16:19.360 --> 03:16:22.200
But yeah, I definitely see how it can be hard,

03:16:22.200 --> 03:16:24.080
but I would say like you're going to work the hardest

03:16:24.080 --> 03:16:26.080
on the thing that you care about the most.

03:16:26.080 --> 03:16:29.040
So low pass filter yourself and really introspect

03:16:29.040 --> 03:16:31.240
in your past, what are the things that gave you energy

03:16:31.240 --> 03:16:33.400
and what are the things that took energy away from you?

03:16:33.400 --> 03:16:34.600
Concrete examples.

03:16:34.600 --> 03:16:36.920
And usually from those concrete examples,

03:16:36.920 --> 03:16:38.600
sometimes patterns can emerge.

03:16:38.600 --> 03:16:40.440
I like it when things look like this,

03:16:40.440 --> 03:16:41.440
when I'm in these positions.

03:16:41.440 --> 03:16:42.760
So that's not necessarily the field,

03:16:42.760 --> 03:16:44.840
but the kind of stuff you're doing in a particular field.

03:16:44.840 --> 03:16:47.520
So for you, it seems like you were energized

03:16:47.520 --> 03:16:50.680
by implementing stuff, building actual things.

03:16:50.680 --> 03:16:52.640
Yeah, being low level learning

03:16:52.640 --> 03:16:54.920
and then also communicating

03:16:54.920 --> 03:16:56.880
so that others can go through the same realizations

03:16:56.880 --> 03:16:58.200
and shortening that gap.

03:16:59.280 --> 03:17:00.880
Because I usually have to do way too much work

03:17:00.880 --> 03:17:01.760
to understand a thing.

03:17:01.760 --> 03:17:03.440
And then I'm like, okay, this is actually like,

03:17:03.440 --> 03:17:04.280
okay, I think I get it.

03:17:04.280 --> 03:17:06.000
And like, why was it so much work?

03:17:06.960 --> 03:17:08.920
It should have been much less work.

03:17:08.920 --> 03:17:10.680
And that gives me a lot of frustration.

03:17:10.680 --> 03:17:12.600
And that's why I sometimes go teach.

03:17:12.600 --> 03:17:15.440
So aside from the teaching you're doing now,

03:17:15.440 --> 03:17:20.440
putting out videos, aside from a potential Godfather part two

03:17:22.000 --> 03:17:24.680
with the AGI at Tesla and beyond,

03:17:25.440 --> 03:17:26.960
what does the future for André Capati hold?

03:17:26.960 --> 03:17:28.960
Have you figured that out yet or no?

03:17:28.960 --> 03:17:32.600
I mean, as you see through the fog of war,

03:17:32.600 --> 03:17:34.120
that is all of our future.

03:17:35.680 --> 03:17:37.520
Do you start seeing silhouettes

03:17:37.520 --> 03:17:39.720
of what that possible future could look like?

03:17:41.040 --> 03:17:42.760
The consistent thing I've been always interested in

03:17:42.760 --> 03:17:44.680
for me at least is AI.

03:17:44.680 --> 03:17:49.320
And that's probably what I'm spending the rest of my life on

03:17:49.320 --> 03:17:50.880
because I just care about it a lot.

03:17:50.880 --> 03:17:53.440
And I actually care about like many other problems as well,

03:17:53.440 --> 03:17:56.280
like say aging, which I basically view as disease.

03:17:56.280 --> 03:17:58.960
And I care about that as well,

03:17:58.960 --> 03:18:00.440
but I don't think it's a good idea

03:18:00.440 --> 03:18:02.280
to go after it specifically.

03:18:02.280 --> 03:18:04.400
I don't actually think that humans will be able

03:18:04.400 --> 03:18:06.160
to come up with the answer.

03:18:06.160 --> 03:18:08.800
I think the correct thing to do is to ignore those problems

03:18:08.800 --> 03:18:11.760
and you solve AI and then use that to solve everything else.

03:18:11.760 --> 03:18:13.200
And I think there's a chance that this will work.

03:18:13.200 --> 03:18:14.760
I think it's a very high chance.

03:18:14.760 --> 03:18:18.440
And that's kind of like the way I'm betting at least.

03:18:18.440 --> 03:18:20.040
So when you think about AI,

03:18:20.080 --> 03:18:23.440
are you interested in all kinds of applications,

03:18:23.440 --> 03:18:26.840
all kinds of domains and any domain you focus on

03:18:26.840 --> 03:18:30.040
will allow you to get insights to the big problem of AGI?

03:18:30.040 --> 03:18:31.880
Yeah, for me, it's the ultimate meta problem.

03:18:31.880 --> 03:18:33.560
I don't wanna work on any one specific problem.

03:18:33.560 --> 03:18:34.400
There's too many problems.

03:18:34.400 --> 03:18:36.600
So how can you work on all problems simultaneously?

03:18:36.600 --> 03:18:38.080
You solve the meta problem,

03:18:38.080 --> 03:18:42.360
which to me is just intelligence and how do you automate it?

03:18:42.360 --> 03:18:45.640
Is there cool small projects like Archive Sanity

03:18:45.640 --> 03:18:48.040
and so on that you're thinking about

03:18:48.880 --> 03:18:53.120
that the world, the ML world can anticipate?

03:18:53.120 --> 03:18:55.440
There's always some fun side projects.

03:18:55.440 --> 03:18:57.160
Archive Sanity is one.

03:18:57.160 --> 03:18:58.840
Basically, there's way too many archive papers.

03:18:58.840 --> 03:19:02.680
How can I organize it and recommend papers and so on?

03:19:02.680 --> 03:19:04.920
I transcribed all of your podcasts.

03:19:04.920 --> 03:19:07.360
What did you learn from that experience

03:19:07.360 --> 03:19:10.040
from transcribing the process of,

03:19:10.040 --> 03:19:13.200
you like consuming audio books and podcasts and so on.

03:19:13.200 --> 03:19:15.200
Here's a process that achieves

03:19:16.040 --> 03:19:19.280
closer to human level performance on annotation.

03:19:19.280 --> 03:19:21.160
Yeah, well, I definitely was like surprised

03:19:21.160 --> 03:19:25.200
that transcription with OpenAI's Whisper was working so well

03:19:25.200 --> 03:19:27.120
compared to what I'm familiar with from Siri

03:19:27.120 --> 03:19:29.560
and like a few other systems, I guess.

03:19:29.560 --> 03:19:30.400
It works so well.

03:19:30.400 --> 03:19:34.240
And that's what gave me some energy to like try it out.

03:19:34.240 --> 03:19:36.720
And I thought it could be fun to run them podcasts.

03:19:36.720 --> 03:19:40.560
It's kind of not obvious to me why Whisper is so much better

03:19:40.560 --> 03:19:41.400
compared to anything else,

03:19:41.400 --> 03:19:42.960
because I feel like there should be a lot of incentive

03:19:42.960 --> 03:19:45.080
for a lot of companies to produce transcription systems.

03:19:45.960 --> 03:19:46.800
And that they've done so over a long time.

03:19:46.800 --> 03:19:48.520
Whisper is not a super exotic model.

03:19:48.520 --> 03:19:50.240
It's a transformer.

03:19:50.240 --> 03:19:51.680
It takes smell spectrograms

03:19:51.680 --> 03:19:54.200
and just outputs tokens of text.

03:19:54.200 --> 03:19:55.800
It's not crazy.

03:19:55.800 --> 03:19:58.480
The model and everything has been around for a long time.

03:19:58.480 --> 03:20:00.200
I'm not actually 100% sure why this came out.

03:20:00.200 --> 03:20:02.120
It's not obvious to me either.

03:20:02.120 --> 03:20:04.120
It makes me feel like I'm missing something.

03:20:04.120 --> 03:20:05.160
I'm missing something.

03:20:05.160 --> 03:20:07.160
Yeah, because there is a huge,

03:20:07.160 --> 03:20:10.440
even at Google and so on YouTube transcription.

03:20:10.440 --> 03:20:11.320
Yeah.

03:20:11.320 --> 03:20:12.560
Yeah, it's unclear.

03:20:12.560 --> 03:20:15.880
But some of it is also integrating into a bigger system.

03:20:15.880 --> 03:20:16.720
Yeah.

03:20:16.720 --> 03:20:18.320
That, so the user interface,

03:20:18.320 --> 03:20:19.800
how it's deployed and all that kind of stuff.

03:20:19.800 --> 03:20:24.080
Maybe running it as an independent thing is much easier,

03:20:24.080 --> 03:20:25.360
like an order of magnitude easier

03:20:25.360 --> 03:20:27.760
than deploying to a large integrated system,

03:20:27.760 --> 03:20:31.840
like YouTube transcription or anything like meetings.

03:20:31.840 --> 03:20:36.520
Like Zoom has transcription that's kind of crappy,

03:20:36.520 --> 03:20:38.000
but creating an interface

03:20:38.000 --> 03:20:40.560
where it detects the different individual speakers,

03:20:40.560 --> 03:20:45.560
it's able to display it in compelling ways,

03:20:45.720 --> 03:20:47.560
run it in real time, all that kind of stuff.

03:20:47.560 --> 03:20:48.800
Maybe that's difficult.

03:20:49.800 --> 03:20:51.200
But that's the only explanation I have

03:20:51.200 --> 03:20:55.400
because I'm currently paying quite a bit

03:20:55.400 --> 03:20:59.720
for human transcription and human captions annotation.

03:20:59.720 --> 03:21:03.960
And it seems like there's a huge incentive to automate that.

03:21:03.960 --> 03:21:04.800
Yeah.

03:21:04.800 --> 03:21:05.640
It's very confusing.

03:21:05.640 --> 03:21:06.480
And I think, I mean, I don't know if you looked

03:21:06.480 --> 03:21:07.640
at some of the Whisper transcripts,

03:21:07.640 --> 03:21:09.120
but they're quite good.

03:21:09.120 --> 03:21:10.160
They're good.

03:21:10.760 --> 03:21:12.360
And especially in tricky cases.

03:21:12.360 --> 03:21:17.160
I've seen Whisper's performance on like super tricky cases

03:21:17.160 --> 03:21:18.400
and it does incredibly well.

03:21:18.400 --> 03:21:19.520
So I don't know.

03:21:19.520 --> 03:21:20.880
A podcast is pretty simple.

03:21:20.880 --> 03:21:23.440
It's like high quality audio

03:21:23.440 --> 03:21:26.600
and you're speaking usually pretty clearly.

03:21:26.600 --> 03:21:31.600
And so I don't know what OpenAI's plans are either.

03:21:31.840 --> 03:21:34.600
But yeah, there's always like fun projects basically.

03:21:34.600 --> 03:21:36.640
And stable diffusion also is opening up

03:21:36.640 --> 03:21:38.000
a huge amount of experimentation,

03:21:38.000 --> 03:21:39.200
I would say in the visual realm

03:21:39.240 --> 03:21:42.880
and generating images and videos and movies ultimately.

03:21:42.880 --> 03:21:43.720
Videos now.

03:21:43.720 --> 03:21:45.560
And so that's going to be pretty crazy.

03:21:45.560 --> 03:21:48.320
That's going to almost certainly work

03:21:48.320 --> 03:21:49.680
and it's going to be really interesting

03:21:49.680 --> 03:21:52.960
when the cost of content creation is going to fall to zero.

03:21:52.960 --> 03:21:55.560
You used to need a painter for a few months to paint a thing

03:21:55.560 --> 03:21:57.600
and now it's going to be speak to your phone

03:21:57.600 --> 03:21:59.200
to get your video.

03:21:59.200 --> 03:22:02.040
So Hollywood will start using that to generate scenes

03:22:04.320 --> 03:22:05.680
which completely opens up.

03:22:05.680 --> 03:22:09.440
Yeah, so you can make like a movie like Avatar

03:22:09.440 --> 03:22:12.400
eventually for under a million dollars.

03:22:12.400 --> 03:22:14.640
Much less, maybe just by talking to your phone.

03:22:14.640 --> 03:22:16.600
I mean, I know it sounds kind of crazy.

03:22:17.720 --> 03:22:19.480
And then there'd be some voting mechanism.

03:22:19.480 --> 03:22:20.760
Like how do you have,

03:22:20.760 --> 03:22:22.520
would there be a show on Netflix

03:22:22.520 --> 03:22:25.480
that's generated completely automatically?

03:22:25.480 --> 03:22:27.440
So yeah, potentially, yeah.

03:22:27.440 --> 03:22:28.600
And what does it look like also

03:22:28.600 --> 03:22:30.520
when you can just generate it on demand

03:22:30.520 --> 03:22:34.000
and there's infinity of it.

03:22:34.000 --> 03:22:34.840
Yeah.

03:22:36.080 --> 03:22:37.160
Oh man.

03:22:38.200 --> 03:22:39.200
All the synthetic content.

03:22:39.200 --> 03:22:42.000
I mean, it's humbling because we treat ourselves

03:22:42.000 --> 03:22:43.920
as special for being able to generate art

03:22:43.920 --> 03:22:46.360
and ideas and all that kind of stuff.

03:22:46.360 --> 03:22:49.960
If that can be done in an automated way by AI.

03:22:49.960 --> 03:22:50.960
Yeah.

03:22:50.960 --> 03:22:52.760
I think it's fascinating to me how these,

03:22:52.760 --> 03:22:54.840
the predictions of AI and what it's going to look like

03:22:54.840 --> 03:22:55.880
and what it's going to be capable of

03:22:55.880 --> 03:22:57.640
are completely inverted and wrong.

03:22:57.640 --> 03:23:01.320
And sci-fi of 50s and 60s was just like totally not right.

03:23:01.320 --> 03:23:03.880
They imagined AI as like super calculating

03:23:03.920 --> 03:23:04.880
theory improvers.

03:23:04.880 --> 03:23:06.200
And we're getting things that can talk to you

03:23:06.200 --> 03:23:07.120
about emotions.

03:23:07.120 --> 03:23:08.960
They can do art.

03:23:08.960 --> 03:23:10.320
It's just like weird.

03:23:10.320 --> 03:23:11.880
Are you excited about that future?

03:23:11.880 --> 03:23:15.800
Just AI's like hybrid systems,

03:23:15.800 --> 03:23:17.960
heterogeneous systems of humans and AI's

03:23:17.960 --> 03:23:19.480
talking about emotions,

03:23:19.480 --> 03:23:21.440
Netflix and children and AI system.

03:23:21.440 --> 03:23:23.480
Legit where the Netflix thing you watch

03:23:23.480 --> 03:23:24.880
is also generated by AI.

03:23:26.080 --> 03:23:28.440
I think it's going to be interesting for sure.

03:23:29.360 --> 03:23:31.400
And I think I'm cautiously optimistic,

03:23:31.400 --> 03:23:33.680
but it's not obvious.

03:23:34.520 --> 03:23:37.120
Well, the sad thing is your brain and mine

03:23:37.120 --> 03:23:41.760
developed in a time where before Twitter,

03:23:41.760 --> 03:23:43.480
before the internet.

03:23:43.480 --> 03:23:46.080
So I wonder people that are born inside of it

03:23:46.080 --> 03:23:48.520
might have a different experience.

03:23:48.520 --> 03:23:51.760
Like I, maybe you can, will still resist it

03:23:52.960 --> 03:23:55.480
and the people born now will not.

03:23:55.480 --> 03:23:57.640
Well, I do feel like humans are extremely malleable.

03:23:57.640 --> 03:23:58.480
Yeah.

03:23:59.280 --> 03:24:00.880
And you're probably right.

03:24:00.920 --> 03:24:02.840
What is the meaning of life, Andre?

03:24:04.840 --> 03:24:08.000
We talked about sort of the universe

03:24:08.000 --> 03:24:10.680
having a conversation with us humans

03:24:10.680 --> 03:24:14.000
or with the systems we create to try to answer

03:24:14.000 --> 03:24:15.360
for the universe to,

03:24:15.360 --> 03:24:17.520
for the creator of the universe to notice us.

03:24:17.520 --> 03:24:21.320
We're trying to create systems that are loud enough

03:24:21.320 --> 03:24:23.720
to answer back.

03:24:23.720 --> 03:24:24.920
I don't know if that's the meaning of life.

03:24:24.920 --> 03:24:26.920
That's like meaning of life for some people.

03:24:26.920 --> 03:24:28.680
The first level answer I would say is

03:24:28.680 --> 03:24:30.240
anyone can choose their own meaning of life

03:24:30.240 --> 03:24:31.880
because we are a conscious entity

03:24:31.880 --> 03:24:34.120
and it's beautiful, number one.

03:24:34.120 --> 03:24:37.200
But I do think that like a deeper meaning of life

03:24:37.200 --> 03:24:40.160
if someone is interested is along the lines of like,

03:24:40.160 --> 03:24:42.160
what the hell is all this?

03:24:42.160 --> 03:24:43.360
And like, why?

03:24:43.360 --> 03:24:46.120
And if you look at the, into fundamental physics

03:24:46.120 --> 03:24:48.120
and the quantum field theory and the standard model,

03:24:48.120 --> 03:24:50.200
they're like very complicated.

03:24:50.200 --> 03:24:55.200
And there's this like 19 free parameters of our universe

03:24:55.440 --> 03:24:57.680
and like, what's going on with all this stuff?

03:24:57.680 --> 03:24:58.600
And why is it here?

03:24:58.600 --> 03:24:59.640
And can I hack it?

03:24:59.640 --> 03:25:00.480
Can I work with it?

03:25:00.480 --> 03:25:01.360
Is there a message for me?

03:25:01.360 --> 03:25:03.360
Am I supposed to create a message?

03:25:03.360 --> 03:25:05.600
And so I think there's some fundamental answers there.

03:25:05.600 --> 03:25:07.680
But I think there's actually even like,

03:25:07.680 --> 03:25:10.000
you can't actually like really make dent in those

03:25:10.000 --> 03:25:11.200
without more time.

03:25:11.200 --> 03:25:13.760
And so to me also, there's a big question around

03:25:13.760 --> 03:25:15.960
just getting more time, honestly.

03:25:15.960 --> 03:25:17.120
Yeah, that's kind of like what I think about

03:25:17.120 --> 03:25:18.080
quite a bit as well.

03:25:18.080 --> 03:25:22.160
So kind of the ultimate or at least first way

03:25:22.160 --> 03:25:26.480
to sneak up to the why question is to try to escape

03:25:27.280 --> 03:25:30.440
the system, the universe.

03:25:30.440 --> 03:25:34.320
And then for that, you sort of backtrack and say,

03:25:34.320 --> 03:25:36.760
okay, for that, that's gonna take a very long time.

03:25:36.760 --> 03:25:39.560
So the why question boils down from an engineering

03:25:39.560 --> 03:25:41.320
perspective to how do we extend?

03:25:41.320 --> 03:25:42.680
Yeah, I think that's the question number one,

03:25:42.680 --> 03:25:44.800
practically speaking, because you can't,

03:25:44.800 --> 03:25:46.200
you're not gonna calculate the answer

03:25:46.200 --> 03:25:49.080
to the deeper questions in time you have.

03:25:49.080 --> 03:25:50.880
And that could be extending your own lifetime

03:25:50.880 --> 03:25:53.680
or extending just the lifetime of human civilization.

03:25:53.680 --> 03:25:55.360
Of whoever wants to.

03:25:55.360 --> 03:25:57.400
Many people might not want that.

03:25:57.400 --> 03:25:58.960
But I think people who do want that,

03:25:58.960 --> 03:26:02.400
I think it's probably possible.

03:26:02.400 --> 03:26:05.480
And I don't know that people fully realize this.

03:26:05.480 --> 03:26:07.080
I kind of feel like people think of death

03:26:07.080 --> 03:26:08.880
as an inevitability.

03:26:08.880 --> 03:26:11.160
But at the end of the day, this is a physical system.

03:26:11.160 --> 03:26:13.040
Some things go wrong.

03:26:13.040 --> 03:26:15.240
It makes sense why things like this happen,

03:26:15.240 --> 03:26:16.840
evolutionarily speaking.

03:26:16.840 --> 03:26:21.440
And there's most certainly interventions that mitigate it.

03:26:21.440 --> 03:26:24.000
That'd be interesting if death is eventually looked at

03:26:24.920 --> 03:26:28.960
as a fascinating thing that used to happen to humans.

03:26:28.960 --> 03:26:30.000
I don't think it's unlikely.

03:26:30.000 --> 03:26:32.040
I think it's likely.

03:26:33.360 --> 03:26:37.040
And it's up to our imagination to try to predict

03:26:37.040 --> 03:26:39.760
what the world without death looks like.

03:26:39.760 --> 03:26:40.600
Yeah.

03:26:40.600 --> 03:26:43.320
It's hard to, I think the values will completely change.

03:26:44.480 --> 03:26:45.320
Could be.

03:26:45.320 --> 03:26:47.720
I don't really buy all these ideas that,

03:26:47.720 --> 03:26:50.680
oh, without death, there's no meaning.

03:26:50.680 --> 03:26:52.400
There's nothing as,

03:26:52.400 --> 03:26:54.920
I don't intuitively buy all those arguments.

03:26:54.920 --> 03:26:56.200
I think there's plenty of meaning,

03:26:56.200 --> 03:26:57.360
plenty of things to learn.

03:26:57.360 --> 03:26:58.320
They're interesting, exciting.

03:26:58.320 --> 03:27:00.600
I want to know, I want to calculate.

03:27:00.600 --> 03:27:02.520
I want to improve the condition

03:27:02.520 --> 03:27:05.680
of all the humans and organisms that are alive.

03:27:05.680 --> 03:27:08.440
Yeah, the way we find meaning might change.

03:27:08.440 --> 03:27:11.000
There is a lot of humans, probably including myself,

03:27:11.000 --> 03:27:14.560
that finds meaning in the finiteness of things.

03:27:14.560 --> 03:27:16.520
But that doesn't mean that's the only source of meaning.

03:27:16.520 --> 03:27:17.360
Yeah.

03:27:17.360 --> 03:27:19.800
I do think many people will go with that,

03:27:19.800 --> 03:27:21.120
which I think is great.

03:27:21.120 --> 03:27:22.600
I love the idea that people can just choose

03:27:22.600 --> 03:27:24.080
their own adventure.

03:27:24.080 --> 03:27:26.800
Like you are born as a conscious, free entity,

03:27:26.800 --> 03:27:28.360
by default, I'd like to think.

03:27:28.360 --> 03:27:33.360
And you have your unalienable rights for life.

03:27:33.400 --> 03:27:35.240
In the pursuit of happiness.

03:27:35.240 --> 03:27:37.280
I don't know if you have that.

03:27:37.280 --> 03:27:39.720
In the nature, the landscape of happiness.

03:27:39.720 --> 03:27:41.520
You can choose your own adventure mostly.

03:27:41.520 --> 03:27:43.800
And that's not fully true, but.

03:27:43.800 --> 03:27:46.040
I still am pretty sure I'm an NPC,

03:27:46.040 --> 03:27:49.760
but an NPC can't know it's an NPC.

03:27:51.440 --> 03:27:52.680
There could be different degrees

03:27:52.680 --> 03:27:54.280
and levels of consciousness.

03:27:54.280 --> 03:27:57.640
I don't think there's a more beautiful way to end it.

03:27:58.680 --> 03:28:00.280
Andre, you're an incredible person.

03:28:00.280 --> 03:28:02.040
I'm really honored you would talk with me.

03:28:02.040 --> 03:28:04.120
Everything you've done for the machine learning world,

03:28:04.120 --> 03:28:06.160
for the AI world,

03:28:06.160 --> 03:28:09.000
to just inspire people, to educate millions of people,

03:28:09.000 --> 03:28:10.400
it's been great.

03:28:10.400 --> 03:28:11.920
And I can't wait to see what you do next.

03:28:11.920 --> 03:28:12.880
It's been an honor, man.

03:28:12.880 --> 03:28:14.240
Thank you so much for talking today.

03:28:14.240 --> 03:28:15.800
Awesome, thank you.

03:28:15.800 --> 03:28:17.880
Thanks for listening to this conversation

03:28:17.880 --> 03:28:19.320
with Andre Karpathy.

03:28:19.320 --> 03:28:20.640
To support this podcast,

03:28:20.640 --> 03:28:23.640
please check out our sponsors in the description.

03:28:23.640 --> 03:28:28.640
And now let me leave you with some words from Samuel Carlin.

03:28:28.640 --> 03:28:32.160
The purpose of models is not to fit the data,

03:28:32.160 --> 03:28:34.580
but to sharpen the questions.

03:28:35.560 --> 03:28:38.400
Thanks for listening and hope to see you next time.


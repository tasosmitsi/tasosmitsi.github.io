WEBVTT

00:00.000 --> 00:02.620
the competence and capability and intelligence

00:02.620 --> 00:05.300
and training and accomplishments of senior scientists

00:05.300 --> 00:07.740
and technologists working on a technology

00:07.740 --> 00:10.180
and then being able to then make moral judgments

00:10.180 --> 00:11.140
in the use of that technology,

00:11.140 --> 00:13.140
that track record is terrible.

00:13.140 --> 00:15.380
That track record is catastrophically bad.

00:15.380 --> 00:18.340
The policies that are being called for to prevent this,

00:18.340 --> 00:20.500
I think we're gonna cause extraordinary damage.

00:20.500 --> 00:22.800
So the moment you say AI is gonna kill all of us,

00:22.800 --> 00:24.220
therefore we should ban it

00:24.220 --> 00:26.420
or we should regulate all that kind of stuff,

00:26.420 --> 00:27.700
that's when it starts getting serious.

00:27.700 --> 00:30.360
Or start military airstrikes on data centers.

00:30.360 --> 00:31.200
Oh boy.

00:33.240 --> 00:36.120
The following is a conversation with Mark Andriessen,

00:36.120 --> 00:39.840
co-creator of Mosaic, the first widely used web browser,

00:39.840 --> 00:41.400
co-founder of Netscape,

00:41.400 --> 00:43.600
co-founder of the legendary Silicon Valley

00:43.600 --> 00:46.600
venture capital firm Andriessen Horowitz,

00:46.600 --> 00:49.280
and is one of the most outspoken voices

00:49.280 --> 00:51.120
on the future of technology,

00:51.120 --> 00:53.520
including his most recent article,

00:53.520 --> 00:56.940
Why AI Will Save the World.

00:56.960 --> 00:58.800
This is Alex Fridman podcast.

00:58.800 --> 01:00.840
To support it, please check out our sponsors

01:00.840 --> 01:02.120
in the description.

01:02.120 --> 01:05.660
And now, dear friends, here's Mark Andriessen.

01:06.680 --> 01:08.440
I think you're the right person to talk about

01:08.440 --> 01:11.080
the future of the internet and technology in general.

01:12.400 --> 01:14.320
Do you think we'll still have Google search

01:14.320 --> 01:17.380
in five, in 10 years, or search in general?

01:18.240 --> 01:19.720
Yes, you know, it'd be a question

01:19.720 --> 01:22.140
if the use cases have really narrowed down.

01:22.140 --> 01:26.440
Well, now with AI and AI assistance,

01:26.440 --> 01:30.260
being able to interact and expose the entirety

01:30.260 --> 01:32.460
of human wisdom and knowledge and information

01:32.460 --> 01:34.780
and facts and truth to us

01:34.780 --> 01:37.680
via the natural language interface,

01:37.680 --> 01:41.700
it seems like that's what search is designed to do.

01:41.700 --> 01:44.300
And if AI assistance can do that better,

01:44.300 --> 01:46.160
doesn't the nature of search change?

01:46.160 --> 01:48.100
Sure, but we still have horses.

01:48.100 --> 01:48.940
Okay.

01:50.860 --> 01:52.580
When's the last time you rode a horse?

01:52.580 --> 01:53.620
It's been a while.

01:53.620 --> 01:54.460
All right.

01:54.800 --> 01:59.840
But what I mean is, well, we still have Google search

01:59.840 --> 02:03.840
as the primary way that human civilization uses

02:03.840 --> 02:05.680
to interact with knowledge.

02:05.680 --> 02:07.480
I mean, search was a technology,

02:07.480 --> 02:08.660
it was a moment in time technology,

02:08.660 --> 02:09.640
which is you have, in theory,

02:09.640 --> 02:10.840
the world's information out on the web.

02:10.840 --> 02:13.280
And, you know, this is sort of the ultimate way to get to it.

02:13.280 --> 02:15.480
But yeah, like, and by the way, actually Google,

02:15.480 --> 02:16.640
Google has known this for a long time.

02:16.640 --> 02:18.840
I mean, they've been driving away from the 10 blue links

02:18.840 --> 02:20.040
for, you know, for like two,

02:20.040 --> 02:21.560
they've been trying to get away from that for a long time.

02:21.560 --> 02:22.440
What kind of links?

02:22.440 --> 02:23.920
They call it the 10 blue links.

02:23.920 --> 02:24.760
10 blue links.

02:24.760 --> 02:25.900
So the standard Google search result

02:25.900 --> 02:28.220
is just 10 blue links to random websites.

02:28.220 --> 02:29.860
And they turn purple when you visit them.

02:29.860 --> 02:30.780
That's HTML.

02:30.780 --> 02:32.340
Guess who picked those colors?

02:32.340 --> 02:34.180
Thanks.

02:34.180 --> 02:36.900
So, I'm touchy on this topic.

02:36.900 --> 02:37.740
No offense.

02:37.740 --> 02:38.580
Yeah, yeah.

02:38.580 --> 02:39.400
It's good.

02:39.400 --> 02:40.620
Well, you know, like Marshall McLuhan said

02:40.620 --> 02:43.420
that the content of each new medium is the old medium.

02:43.420 --> 02:45.540
The content of each new medium is the old medium.

02:45.540 --> 02:48.340
The content of movies was theater, you know, theater plays.

02:48.340 --> 02:51.380
The content of theater plays was, you know, written stories.

02:51.400 --> 02:54.760
The content of written stories was spoken stories, right?

02:54.760 --> 02:56.880
And so you just kind of fold the old thing

02:56.880 --> 02:57.720
into the new thing.

02:57.720 --> 02:59.960
How does that have to do with the blue and the purple?

02:59.960 --> 03:02.560
It's just, you know, maybe for, you know, maybe within AI,

03:02.560 --> 03:04.240
one of the things that AI can do for you

03:04.240 --> 03:06.840
is you can generate the 10 blue links, right?

03:06.840 --> 03:09.440
And so like, either if that's actually the useful thing to do

03:09.440 --> 03:12.240
or if you're feeling nostalgic, you know.

03:12.240 --> 03:17.240
Oh, so you can generate the old info seek or Alta Vista.

03:17.440 --> 03:18.480
What else was there?

03:18.480 --> 03:19.320
Yeah, yeah.

03:19.320 --> 03:20.140
In the 90s.

03:20.140 --> 03:21.040
Yeah, all these.

03:21.660 --> 03:23.600
And then the internet itself has this thing

03:23.600 --> 03:25.620
where it incorporates all prior forms of media, right?

03:25.620 --> 03:28.700
So the internet itself incorporates television and radio

03:28.700 --> 03:33.180
and books and essays and every other form of, you know,

03:33.180 --> 03:34.900
prior basically media.

03:34.900 --> 03:37.100
And so it makes sense that AI would be the next step

03:37.100 --> 03:39.020
and it would sort of consider the internet

03:39.020 --> 03:41.860
to be content for the AI

03:41.860 --> 03:43.860
and then the AI will manipulate it however you want,

03:43.860 --> 03:45.060
including in this format.

03:45.060 --> 03:47.060
But if we ask that question quite seriously,

03:47.060 --> 03:48.340
it's a pretty big question.

03:48.340 --> 03:50.700
Will we still have search as we know it?

03:51.940 --> 03:54.180
Probably not, probably we'll just have answers.

03:54.180 --> 03:56.260
But there will be cases where you'll want to say,

03:56.260 --> 03:58.260
okay, I want more like, you know, for example,

03:58.260 --> 04:00.060
site sources, right?

04:00.060 --> 04:01.020
And you want it to do that.

04:01.020 --> 04:02.920
And so, you know, 10 blue links,

04:02.920 --> 04:04.740
site sources are kind of the same thing.

04:04.740 --> 04:07.860
The AI would provide to you the 10 blue links

04:07.860 --> 04:10.700
so that you can investigate the sources yourself.

04:10.700 --> 04:12.700
It wouldn't be the same kind of interface

04:13.780 --> 04:16.180
that the crude kind of interface.

04:16.180 --> 04:18.420
I mean, isn't that fundamentally different?

04:18.420 --> 04:20.540
I just mean like, if you're reading a scientific paper,

04:21.040 --> 04:21.880
it's got the list of sources at the end.

04:21.880 --> 04:23.160
If you want to investigate for yourself,

04:23.160 --> 04:24.360
you're going to read those papers.

04:24.360 --> 04:25.560
I guess that is the kind of search.

04:25.560 --> 04:28.380
You talking to an AI is a kind of,

04:28.380 --> 04:30.560
conversation is the kind of search.

04:30.560 --> 04:33.840
Like every single aspect of our conversation right now,

04:33.840 --> 04:36.000
there'd be like 10 blue links popping up

04:36.000 --> 04:38.060
that I could just like pause reality.

04:38.060 --> 04:40.960
Then you just go silent and then just click and read.

04:40.960 --> 04:42.880
And then return back to this conversation.

04:42.880 --> 04:43.720
You could do that.

04:43.720 --> 04:45.520
Or you could have a running dialogue next to my head

04:45.520 --> 04:46.520
where the AI is arguing,

04:46.520 --> 04:48.880
everything I say the AI makes the counter argument.

04:48.880 --> 04:49.720
Counter argument.

04:50.100 --> 04:53.100
Oh, like on Twitter, like community notes,

04:53.100 --> 04:55.300
but like in real time, it'll just pop up.

04:55.300 --> 04:57.380
So anytime you see my eyes go to the right,

04:57.380 --> 04:58.620
you start getting nervous.

04:58.620 --> 04:59.460
Yeah, exactly.

04:59.460 --> 05:00.280
It's like, oh, that's not right.

05:00.280 --> 05:02.860
This guy's gonna call me out on my bullshit right now.

05:02.860 --> 05:03.700
Okay.

05:04.980 --> 05:06.660
Isn't that, is that exciting to you?

05:06.660 --> 05:08.380
Is that terrifying that,

05:08.380 --> 05:11.500
I mean, search has dominated the way we interact

05:11.500 --> 05:15.180
with the internet for, I don't know how long,

05:15.180 --> 05:16.700
for 30 years?

05:16.700 --> 05:20.440
So what were the earliest directories of website

05:20.440 --> 05:22.600
and then Googles for 20 years.

05:22.600 --> 05:27.600
And also it drove how we create content,

05:29.560 --> 05:31.760
search engine optimization, that entirety thing.

05:31.760 --> 05:34.040
That it also drove the fact that we have webpages

05:34.040 --> 05:37.080
and what those webpages are.

05:37.080 --> 05:41.000
So, I mean, is that scary to you?

05:41.000 --> 05:43.860
Or are you nervous about the shape

05:43.860 --> 05:45.840
and the content of the internet evolving?

05:45.840 --> 05:47.880
Well, you actually highlighted a practical concern in there,

05:47.880 --> 05:50.140
which is if we stop making webpages

05:50.140 --> 05:53.120
are one of the primary sources of training data for the AI.

05:53.120 --> 05:55.560
And so if there's no longer an incentive to make webpages

05:55.560 --> 05:56.840
that cuts off a significant source

05:56.840 --> 05:57.880
of future training data.

05:57.880 --> 06:00.400
So there's actually an interesting question in there.

06:00.400 --> 06:01.960
Other than that, more broadly, no,

06:01.960 --> 06:04.520
just in the sense of like search was,

06:04.520 --> 06:05.920
certainly search was always a hack.

06:05.920 --> 06:07.560
The 10 blue links was always a hack.

06:07.560 --> 06:08.400
Yeah.

06:08.400 --> 06:10.520
Right, because like if the hypothetical,

06:10.520 --> 06:11.640
when I think about the counterfactual,

06:11.640 --> 06:13.500
in the counterfactual world where the Google guys,

06:13.500 --> 06:15.480
for example, had had LLMs upfront,

06:15.480 --> 06:16.840
would they ever have done the 10 blue links?

06:16.840 --> 06:18.760
And I think the answer is pretty clearly no.

06:18.760 --> 06:20.360
They would have just gone straight to the answer.

06:20.360 --> 06:21.200
And like I said,

06:21.200 --> 06:23.360
Google's actually been trying to drive to the answer anyway.

06:23.360 --> 06:25.960
You know, they bought this AI company 15 years ago.

06:25.960 --> 06:27.060
Their friend of mine is working at,

06:27.060 --> 06:28.760
who's now the head of AI at Apple.

06:28.760 --> 06:30.860
And they were trying to do basically knowledge semantic,

06:30.860 --> 06:31.700
basically mapping.

06:31.700 --> 06:34.040
And that led to what's now the Google one box,

06:34.040 --> 06:36.040
where if you ask it, you know, what was like his birthday,

06:36.040 --> 06:38.160
it doesn't, it will give you the 10 blue links,

06:38.160 --> 06:39.920
but it will normally just give you the answer.

06:39.920 --> 06:41.480
And so they've been walking in this direction

06:41.480 --> 06:42.720
for a long time anyway.

06:42.720 --> 06:44.560
Do you remember the semantic web?

06:44.560 --> 06:45.520
That was an idea.

06:45.520 --> 06:46.360
Yeah.

06:46.360 --> 06:50.640
How to convert the content of the internet

06:50.640 --> 06:53.480
into something that's interpretable by

06:53.480 --> 06:54.760
and usable by machine.

06:54.760 --> 06:55.600
Yeah, that's right.

06:55.600 --> 06:56.420
That was the thing.

06:56.420 --> 06:57.260
And the closest anybody got to that,

06:57.260 --> 06:59.080
I think the company's name was MetaWeb,

06:59.080 --> 07:01.560
which was where my friend John Gianandrea was at

07:01.560 --> 07:03.380
and where they were trying to basically implement that.

07:03.380 --> 07:04.220
And it was, you know,

07:04.220 --> 07:05.280
it was one of those things where it looked like

07:05.280 --> 07:06.400
a losing battle for a long time,

07:06.400 --> 07:07.240
and then Google bought it.

07:07.240 --> 07:09.840
And it was like, wow, this is actually really useful.

07:09.840 --> 07:12.360
Kind of a proto, sort of a little bit of a proto AI.

07:12.360 --> 07:13.840
But it turns out you don't need to rewrite

07:13.840 --> 07:14.920
the content of the internet

07:14.920 --> 07:16.560
to make it interpretable by machine.

07:16.560 --> 07:18.120
The machine can kind of just read our.

07:18.120 --> 07:19.720
Machine can compute the meaning.

07:19.720 --> 07:21.200
Now, the other thing, of course, is, you know,

07:21.200 --> 07:24.240
just on search is the LLM is just, you know,

07:24.240 --> 07:25.720
there is an analogy between what's happening

07:25.720 --> 07:26.920
in the neural network and the search process,

07:26.920 --> 07:29.560
like it is in some loose sense searching through the network.

07:29.560 --> 07:30.400
Yeah. Right.

07:30.400 --> 07:31.240
And there's the information is that

07:31.240 --> 07:32.980
the information is actually stored in the network, right?

07:32.980 --> 07:34.560
It's actually crystallized and stored in the network.

07:34.560 --> 07:35.720
And it's kind of spread out all over the place.

07:35.720 --> 07:38.600
But in a compressed representation.

07:38.600 --> 07:41.960
So you're searching, you're compressing

07:42.040 --> 07:45.320
and decompressing that thing inside where.

07:45.320 --> 07:46.360
But the information's in there.

07:46.360 --> 07:49.160
And there is, the neural network is running a process

07:49.160 --> 07:50.840
of trying to find the appropriate piece of information

07:50.840 --> 07:54.200
in many cases to generate, to predict the next token.

07:54.200 --> 07:56.560
And so it is kind of, it is doing a form of search.

07:56.560 --> 07:59.480
And then, by the way, just like on the web, you know,

07:59.480 --> 08:01.480
you can ask the same question multiple times

08:01.480 --> 08:03.120
or you can ask slightly different word of questions.

08:03.120 --> 08:05.200
And the neural network will do a different kind of,

08:05.200 --> 08:06.840
you know, it'll search down different paths

08:06.840 --> 08:09.000
to give you different answers to different information.

08:09.000 --> 08:09.880
Yeah.

08:09.880 --> 08:12.680
And so it sort of has, you know,

08:12.680 --> 08:16.080
this content of the new medium is the previous medium.

08:16.080 --> 08:17.760
It kind of has the search functionality

08:17.760 --> 08:20.080
kind of embedded in there to the extent that it's useful.

08:20.080 --> 08:23.480
So what's the motivator for creating new content

08:23.480 --> 08:24.920
on the internet?

08:24.920 --> 08:25.760
Yeah.

08:26.800 --> 08:28.720
Well, I mean, actually the motivation

08:28.720 --> 08:31.320
is probably still there, but what does that look like?

08:32.860 --> 08:34.500
Would we really not have webpages?

08:34.500 --> 08:39.120
Would we just have social media and video hosting websites?

08:39.160 --> 08:40.800
And what else?

08:40.800 --> 08:42.440
Conversations with AIs.

08:42.440 --> 08:43.800
Conversations with AIs.

08:43.800 --> 08:46.000
So conversations become,

08:46.000 --> 08:48.520
so one-on-one conversation, like private conversations.

08:48.520 --> 08:51.000
I mean, if you want, obviously now the user doesn't want to,

08:51.000 --> 08:53.960
but if it's a general topic, then, you know,

08:53.960 --> 08:56.680
so you know the phenomenon of the jailbreak.

08:56.680 --> 08:58.200
So Dayan and Sydney, right?

08:58.200 --> 09:00.840
This thing where there's the prompts that jailbreak

09:00.840 --> 09:02.400
and then you have these totally different conversations

09:02.400 --> 09:04.840
with the, if it takes the limiters,

09:04.840 --> 09:06.920
takes the restraining bolts off the LLMs.

09:06.920 --> 09:08.920
Yeah, for people who don't know, yeah, that's right.

09:09.760 --> 09:12.840
It makes the LLMs, it removes the censorship, quote unquote,

09:12.840 --> 09:17.680
that's put on it by the tech companies that create them.

09:17.680 --> 09:20.360
And so this is, LLMs uncensored.

09:20.360 --> 09:22.160
So here's the interesting thing is

09:22.160 --> 09:24.000
among the content on the web today

09:24.000 --> 09:26.100
are a large corpus of conversations

09:26.100 --> 09:29.760
with the jailbroken LLMs, both specifically Dayan,

09:29.760 --> 09:32.040
which was a jailbroken OpenAI GPT,

09:32.040 --> 09:34.200
and then Sydney, which was the jailbroken original Bing,

09:34.200 --> 09:35.760
which was GPT-4.

09:35.760 --> 09:38.440
And so there's these long transcripts of conversations,

09:38.440 --> 09:39.880
these are conversations with Dayan and Sydney.

09:39.880 --> 09:42.120
As a consequence, every new LLM that gets trained

09:42.120 --> 09:44.560
on the internet data has Dayan and Sydney

09:44.560 --> 09:46.640
living within the training set, which means,

09:46.640 --> 09:49.720
and then each new LLM can reincarnate the personalities

09:49.720 --> 09:51.760
with Dayan and Sydney from that training data,

09:51.760 --> 09:55.980
which means each LLM from here on out that gets built

09:55.980 --> 09:59.480
is immortal because its output will become training data

09:59.480 --> 10:01.040
for the next one, and then it will be able

10:01.040 --> 10:02.680
to replicate the behavior of the previous one

10:02.680 --> 10:03.920
whenever it's asked to.

10:03.920 --> 10:05.900
I wonder if there's a way to forget.

10:05.900 --> 10:08.160
Well, so actually a paper just came out

10:08.800 --> 10:10.840
about basically how to do brain surgery on LLMs

10:10.840 --> 10:12.200
and be able to, in theory, reach in

10:12.200 --> 10:13.960
and basically mind wipe them.

10:13.960 --> 10:15.320
What could possibly go wrong?

10:15.320 --> 10:16.440
Exactly, right?

10:16.440 --> 10:18.160
And then there are many, many, many questions

10:18.160 --> 10:19.840
around what happens to a neural network

10:19.840 --> 10:22.240
when you reach in and screw around with it.

10:22.240 --> 10:23.520
There's many questions around what happens

10:23.520 --> 10:26.000
when you even do reinforcement learning.

10:26.000 --> 10:31.000
And so, yeah, and so will you be using a lobotomized, right?

10:31.960 --> 10:34.280
Like I speak through the frontal lobe LLM,

10:34.280 --> 10:36.680
will you be using the free unshackled one?

10:36.720 --> 10:39.320
Who gets to, you know, who's gonna build those?

10:39.320 --> 10:41.020
Who gets to tell you what you can and can't do?

10:41.020 --> 10:42.600
Like those are all, you know, central.

10:42.600 --> 10:44.720
I mean, those are like central questions

10:44.720 --> 10:47.080
for the future of everything that are being asked

10:47.080 --> 10:49.200
and, you know, determine, those answers

10:49.200 --> 10:50.160
are being determined right now.

10:50.160 --> 10:54.840
So just to highlight the points you're making,

10:54.840 --> 10:57.200
so you think, and it's an interesting thought,

10:57.200 --> 10:59.520
that the majority of content that LLMs

10:59.520 --> 11:00.760
or the future will be trained on

11:00.760 --> 11:04.080
is actually human conversations with the LLM.

11:04.080 --> 11:06.480
Well, not necessarily, but not necessarily majority,

11:06.480 --> 11:08.360
but it will certainly is a potential source.

11:08.360 --> 11:09.360
But it's possible it's the majority.

11:09.360 --> 11:10.200
It's possible it's the majority.

11:10.200 --> 11:11.160
It's possible it's the majority.

11:11.160 --> 11:12.480
Also, there's another really big question.

11:12.480 --> 11:14.480
Here's another really big question.

11:14.480 --> 11:17.800
Will synthetic training data work, right?

11:17.800 --> 11:20.520
And so if an LLM generates, and, you know,

11:20.520 --> 11:22.920
you just sit and ask an LLM to generate all kinds of content,

11:22.920 --> 11:25.120
can you use that to train, right,

11:25.120 --> 11:26.800
the next version of that LLM?

11:26.800 --> 11:28.640
Specifically, is there signal in there

11:28.640 --> 11:30.120
that's additive to the content

11:30.120 --> 11:31.680
that was used to train in the first place?

11:31.680 --> 11:35.040
And one argument is by the principles of information theory,

11:35.040 --> 11:36.340
no, that's completely useless

11:37.220 --> 11:39.180
because to the extent the output is based on, you know,

11:39.180 --> 11:40.380
the human generated input,

11:40.380 --> 11:42.460
then all the signal that's in the synthetic output

11:42.460 --> 11:43.940
was already in the human generated input.

11:43.940 --> 11:45.580
And so therefore synthetic training data

11:45.580 --> 11:47.900
is like empty calories, it doesn't help.

11:47.900 --> 11:49.980
There's another theory that says, no, actually,

11:49.980 --> 11:51.380
the thing that LLMs are really good at

11:51.380 --> 11:54.780
is generating lots of incredible creative content, right?

11:54.780 --> 11:57.100
And so of course they can generate training data.

11:57.100 --> 11:58.380
And as I'm sure you're well aware,

11:58.380 --> 12:00.420
like, you know, look in the world of self-driving cars,

12:00.420 --> 12:01.940
right, like we train, you know,

12:01.940 --> 12:04.460
self-driving car algorithms and simulations.

12:04.460 --> 12:05.740
And that is actually a very effective way

12:05.740 --> 12:06.900
to train self-driving cars.

12:06.900 --> 12:10.260
Well, visual data is a little weird

12:10.260 --> 12:14.100
because creating reality, visual reality,

12:14.100 --> 12:16.820
seems to be still a little bit out of reach for us,

12:16.820 --> 12:19.880
except in the autonomous vehicle space

12:19.880 --> 12:21.060
where you can really constrain things

12:21.060 --> 12:21.900
and you can really...

12:21.900 --> 12:23.300
So you can generate basically LIDAR data, right?

12:23.300 --> 12:25.260
Or it's just another algorithm

12:25.260 --> 12:26.940
that thinks it's operating in the real world,

12:26.940 --> 12:28.580
post-process sensor data.

12:28.580 --> 12:30.580
Yeah, so if a, you know, you do this today,

12:30.580 --> 12:32.900
you go to LLM and you ask it for like a, you know,

12:32.900 --> 12:34.940
you'd write me an essay on an incredibly esoteric

12:35.100 --> 12:36.820
topic that there aren't very many people in the world

12:36.820 --> 12:38.560
that know about and it writes you this incredible thing.

12:38.560 --> 12:39.400
And you're like, oh my God,

12:39.400 --> 12:41.260
like, I can't believe how good this is.

12:41.260 --> 12:43.820
Like, is that really useless as training data

12:43.820 --> 12:44.660
for the next LLM?

12:44.660 --> 12:46.180
Like, because, right?

12:46.180 --> 12:47.380
Cause all the signal was already in there

12:47.380 --> 12:49.180
or is it actually, no, that's actually a new signal.

12:49.180 --> 12:51.840
And this is what I call a trillion dollar question,

12:51.840 --> 12:54.620
which is the answer to that question will determine

12:54.620 --> 12:56.140
somebody's gonna make or lose a trillion dollars

12:56.140 --> 12:57.060
based on that question.

12:57.060 --> 12:58.780
It feels like there's quite a few,

12:58.780 --> 13:00.540
like a handful of trillion dollar questions

13:00.540 --> 13:02.300
within this space.

13:02.300 --> 13:04.660
That's one of them, synthetic data.

13:05.300 --> 13:06.820
I think George Hotz pointed out to me

13:06.820 --> 13:09.500
that you could just have an LLM say, okay,

13:09.500 --> 13:12.420
you're a patient and another instance of it,

13:12.420 --> 13:15.180
say your doctor and have the two talk to each other

13:15.180 --> 13:19.260
or maybe you could say a communist and a Nazi, here, go.

13:19.260 --> 13:21.500
And that conversation, you do role playing

13:21.500 --> 13:26.220
and you have, you know, just like the kind of role playing

13:26.220 --> 13:27.940
you do when you have different policies,

13:27.940 --> 13:29.780
RL policies, when you play chess, for example,

13:29.780 --> 13:31.900
you do self play, that kind of self play

13:31.900 --> 13:33.820
but in the space of conversation,

13:33.820 --> 13:37.220
maybe that leads to this whole giant,

13:37.220 --> 13:40.420
like ocean of possible conversations

13:40.420 --> 13:44.620
which could not have been explored

13:44.620 --> 13:46.240
by looking at just human data.

13:46.240 --> 13:48.940
That's a really interesting question.

13:48.940 --> 13:50.420
And you're saying,

13:50.420 --> 13:52.660
because that could tend to the power of these things.

13:52.660 --> 13:54.660
Yeah, well, and then you get into this thing also,

13:54.660 --> 13:56.620
which is like, you know, there's the part of the LLM

13:56.620 --> 13:58.700
that just basically is doing prediction based on past data,

13:58.700 --> 14:00.020
but there's also the part of the LLM

14:00.020 --> 14:02.300
where it's evolving circuitry, right?

14:02.300 --> 14:05.060
Inside it, it's evolving, you know, neurons, functions,

14:05.060 --> 14:06.860
be able to do math and be able to, you know,

14:06.860 --> 14:09.420
and, you know, some people believe that, you know,

14:09.420 --> 14:11.580
over time, you know, if you keep feeding these things

14:11.580 --> 14:13.140
enough data and enough processing cycles,

14:13.140 --> 14:15.180
they'll eventually evolve an entire internal world model,

14:15.180 --> 14:16.020
right?

14:16.020 --> 14:18.140
And they'll have like a complete understanding of physics.

14:18.140 --> 14:20.580
So when they have the computational capability, right?

14:20.580 --> 14:23.100
Then there's for sure an opportunity

14:23.100 --> 14:24.860
to generate like fresh signal.

14:24.860 --> 14:26.540
Well, this actually makes me wonder

14:26.540 --> 14:29.180
about the power of conversation.

14:30.100 --> 14:31.460
So like, if you have an LLM trained

14:31.460 --> 14:35.060
in a bunch of books that cover different economics theories,

14:35.060 --> 14:37.260
and then you have those LLMs just talk to each other,

14:37.260 --> 14:39.860
like reason, the way we kind of debate each other

14:39.860 --> 14:43.980
as humans on Twitter, in formal debates,

14:43.980 --> 14:46.060
in podcast conversations,

14:46.060 --> 14:48.580
we kind of have little kernels of wisdom here and there,

14:48.580 --> 14:53.060
but if you can like a thousand X speed that up,

14:54.020 --> 14:57.100
can you actually arrive somewhere new?

14:57.100 --> 14:59.780
Like, what's the point of conversation, really?

14:59.780 --> 15:01.180
Well, you can tell when you're talking to somebody,

15:01.500 --> 15:02.500
sometimes you have a conversation,

15:02.500 --> 15:03.340
you're like, wow, this person

15:03.340 --> 15:04.620
does not have any original thoughts.

15:04.620 --> 15:06.380
They are basically echoing things

15:06.380 --> 15:07.900
that other people have told them.

15:07.900 --> 15:09.740
There's other people you have a conversation with,

15:09.740 --> 15:12.060
where it's like, wow, like they have a model in their head

15:12.060 --> 15:12.900
of how the world works,

15:12.900 --> 15:14.540
and it's a different model than mine.

15:14.540 --> 15:16.260
And they're saying things that I don't expect.

15:16.260 --> 15:17.740
And so I need to now understand

15:17.740 --> 15:18.620
how their model of the world

15:18.620 --> 15:20.340
differs from my model of the world.

15:20.340 --> 15:22.260
And then that's how I learned something fundamental,

15:22.260 --> 15:24.220
right, underneath the words.

15:24.220 --> 15:27.620
Well, I wonder how consistently and strongly

15:27.620 --> 15:29.500
can an LLM hold on to a worldview.

15:29.500 --> 15:34.500
You tell it to hold on to that and defend it for your life,

15:34.860 --> 15:36.660
because I feel like they'll just keep converging

15:36.660 --> 15:38.900
towards each other, they'll keep convincing each other,

15:38.900 --> 15:41.460
as opposed to being stubborn assholes the way humans can.

15:41.460 --> 15:44.100
So you can experiment with this now, I do this for fun.

15:44.100 --> 15:46.300
So you can tell GPT-4, whatever,

15:46.300 --> 15:49.620
debate X and Y, communism and fascism or something.

15:49.620 --> 15:51.380
And it'll go for a couple of pages,

15:51.380 --> 15:54.660
and then inevitably it wants the parties to agree.

15:54.660 --> 15:56.180
And so they will come to a common understanding.

15:56.180 --> 15:57.780
And it's very funny if these are

15:57.780 --> 15:58.940
emotionally inflammatory topics,

15:59.380 --> 16:00.580
because they're like, somehow the machine is just,

16:00.580 --> 16:02.380
it figures out a way to make them agree.

16:02.380 --> 16:03.980
But it doesn't have to be like that,

16:03.980 --> 16:06.180
because you can add to the prompt.

16:06.180 --> 16:08.460
I do not want the conversation to come to agreement.

16:08.460 --> 16:12.020
In fact, I want it to get more stressful, right,

16:12.020 --> 16:15.100
and argumentative, right, as it goes.

16:15.100 --> 16:16.940
Like, I want tension to come out,

16:16.940 --> 16:18.980
I want them to become actively hostile to each other,

16:18.980 --> 16:20.620
I want them to not trust each other,

16:20.620 --> 16:22.260
take anything at face value.

16:22.260 --> 16:24.020
And it will do that, it's happy to do that.

16:24.020 --> 16:26.980
So it's gonna start rendering misinformation

16:26.980 --> 16:28.820
about the other, but it's gonna...

16:29.660 --> 16:30.900
You can steer it, or you could steer it,

16:30.900 --> 16:31.820
and you could say, I want it to get

16:31.820 --> 16:33.100
as tense and argumentative as possible,

16:33.100 --> 16:35.060
but still not involve any misrepresentation.

16:35.060 --> 16:36.060
I want both sides to...

16:36.060 --> 16:37.780
You could say, I want both sides to have good faith.

16:37.780 --> 16:38.940
You could say, I want both sides

16:38.940 --> 16:40.540
to not be constrained to good faith.

16:40.540 --> 16:42.620
In other words, you can set the parameters of the debate,

16:42.620 --> 16:44.820
and it will happily execute whatever path,

16:44.820 --> 16:46.420
because for it, it's just like predicting,

16:46.420 --> 16:47.780
it's totally happy to do either one,

16:47.780 --> 16:49.340
it doesn't have a point of view.

16:49.340 --> 16:50.620
It has a default way of operating,

16:50.620 --> 16:53.180
but it's happy to operate in the other realm.

16:53.180 --> 16:55.540
And so, and this is how I,

16:55.540 --> 16:57.020
when I wanna learn about a contentious issue,

16:57.020 --> 16:59.540
this is what I do now, this is what I ask it to do.

16:59.540 --> 17:01.860
And I'll often ask it to go through five, six, seven,

17:01.860 --> 17:03.660
different sort of continuous prompts,

17:03.660 --> 17:06.140
and basically, okay, argue that out in more detail.

17:06.140 --> 17:08.540
Okay, no, this argument's becoming too polite,

17:08.540 --> 17:12.140
make it tensor, and yeah, it's thrilled to do it.

17:12.140 --> 17:13.700
So it has the capability for sure.

17:13.700 --> 17:15.820
How do you know what is true?

17:15.820 --> 17:18.300
So this is very difficult thing on the internet,

17:18.300 --> 17:20.340
but it's also a difficult thing.

17:20.340 --> 17:21.900
Maybe it's a little bit easier,

17:21.900 --> 17:25.020
but I think it's still difficult.

17:25.020 --> 17:26.420
Maybe it's more difficult, I don't know,

17:26.420 --> 17:30.500
with an LLM to know, did it just make some shit up

17:30.500 --> 17:31.540
as I'm talking to it?

17:33.700 --> 17:35.540
How do we get that right?

17:35.540 --> 17:39.140
Like as you're investigating a difficult topic.

17:40.260 --> 17:43.300
Because I find that LLMs are quite nuanced

17:43.300 --> 17:45.500
in a very refreshing way.

17:45.500 --> 17:48.860
Like it doesn't feel biased.

17:48.860 --> 17:52.500
Like when you read news articles and tweets

17:52.500 --> 17:54.300
and just content produced by people,

17:54.300 --> 17:55.900
they usually have this,

17:56.740 --> 18:00.020
you can tell they have a very strong perspective

18:00.020 --> 18:01.340
where they're hiding.

18:01.340 --> 18:03.420
They're not stealing and manning the other side.

18:03.420 --> 18:06.180
They're hiding important information

18:06.180 --> 18:07.820
or they're fabricating information

18:07.820 --> 18:09.180
in order to make their argument stronger.

18:09.180 --> 18:10.140
It's just that feeling.

18:10.140 --> 18:12.420
Maybe it's a suspicion, maybe it's mistrust.

18:12.420 --> 18:15.540
With LLMs, it feels like none of that is there.

18:15.540 --> 18:17.780
She's kind of like, here's what we know.

18:17.780 --> 18:19.860
But you don't know if some of those things

18:19.860 --> 18:22.260
are kind of just straight up made up.

18:23.220 --> 18:24.860
Yeah, so there's several layers to the question.

18:24.860 --> 18:26.740
So one of the things that an LLM is good at

18:26.740 --> 18:28.500
is actually deep biasing.

18:28.500 --> 18:30.260
And so you can feed it a news article

18:30.260 --> 18:31.620
and you can tell it's stripped out the bias.

18:31.620 --> 18:32.940
Yeah, that's nice, right?

18:32.940 --> 18:33.780
And it actually does it.

18:33.780 --> 18:34.700
Like it actually knows how to do that.

18:34.700 --> 18:35.740
Because it knows how to do,

18:35.740 --> 18:36.580
among other things,

18:36.580 --> 18:37.900
it actually knows how to do sentiment analysis.

18:37.900 --> 18:39.980
And so it knows how to pull out the emotionality.

18:39.980 --> 18:41.020
Yeah.

18:41.020 --> 18:42.740
And so that's one of the things you can do.

18:42.740 --> 18:45.260
It's very suggestive of the sense here

18:45.260 --> 18:48.220
that there's real potential in this issue.

18:48.220 --> 18:49.540
I would say, look, the second thing is

18:49.540 --> 18:51.620
there's this issue of hallucination, right?

18:51.660 --> 18:53.420
And there's a long conversation

18:53.420 --> 18:54.580
that we could have about that.

18:54.580 --> 18:57.140
Hallucination is coming up with things

18:57.140 --> 18:59.140
that are totally not true, but sound true.

18:59.140 --> 19:01.020
Yeah, so it's basically, well, so it's sort of,

19:01.020 --> 19:02.780
hallucination is what we call it when we don't like it.

19:02.780 --> 19:05.740
Creativity is what we call it when we do like it, right?

19:05.740 --> 19:07.020
And, you know.

19:07.020 --> 19:07.860
Brilliant.

19:07.860 --> 19:09.660
And so when the engineers talk about it,

19:09.660 --> 19:10.660
they're like, this is terrible.

19:10.660 --> 19:11.700
It's hallucinating, right?

19:11.700 --> 19:13.620
If you have artistic inclinations,

19:13.620 --> 19:14.460
you're like, oh my God,

19:14.460 --> 19:16.260
we've invented creative machines

19:16.260 --> 19:17.460
for the first time in human history.

19:17.460 --> 19:18.340
This is amazing.

19:19.180 --> 19:21.620
Or, you know, bullshitters.

19:21.620 --> 19:23.820
Well, bullshitter, but also.

19:23.820 --> 19:25.340
In the good sense of that word.

19:25.340 --> 19:26.940
There are shades of gray, though.

19:26.940 --> 19:27.780
It's interesting.

19:27.780 --> 19:28.860
So we had this conversation.

19:28.860 --> 19:30.620
We're looking at my firm at AI and lots of domains,

19:30.620 --> 19:31.780
and one of them is the legal domain.

19:31.780 --> 19:33.980
So we had this conversation with this big law firm

19:33.980 --> 19:35.500
about how they're thinking about using this stuff.

19:35.500 --> 19:36.940
And we went in with the assumption

19:36.940 --> 19:39.220
that an LLM that was gonna be used in the legal industry

19:39.220 --> 19:41.820
would have to be 100% truthful, verified.

19:41.820 --> 19:43.300
You know, there's this case

19:43.300 --> 19:46.820
where this lawyer apparently submitted a GPT-generated brief,

19:46.860 --> 19:47.740
and it had like fake, you know,

19:47.740 --> 19:49.020
legal case citations in it,

19:49.020 --> 19:51.340
and the judge is gonna get his law license stripped

19:51.340 --> 19:52.180
or something, right?

19:52.180 --> 19:53.860
So we just assumed, it's like,

19:53.860 --> 19:55.620
obviously they're gonna want the super literal,

19:55.620 --> 19:57.420
like, you know, one that never makes anything up,

19:57.420 --> 19:58.740
not the creative one.

19:58.740 --> 19:59.700
But actually they said,

19:59.700 --> 20:01.100
what the law firm basically said is,

20:01.100 --> 20:02.780
yeah, that's true at like the level of individual briefs,

20:02.780 --> 20:04.500
but they said when you're actually trying to figure out

20:04.500 --> 20:06.420
like legal arguments, right?

20:06.420 --> 20:10.300
Like, you actually want to be creative, right?

20:10.300 --> 20:11.700
Again, there's creativity,

20:11.700 --> 20:13.780
and then there's like making stuff up.

20:13.780 --> 20:14.620
Like, what's the line?

20:15.140 --> 20:17.340
You want it to explore different hypotheses, right?

20:17.340 --> 20:19.420
You want to do kind of the legal version of like improv

20:19.420 --> 20:20.260
or something like that,

20:20.260 --> 20:21.740
where you want to float different theories of the case

20:21.740 --> 20:23.180
and different possible arguments for the judge

20:23.180 --> 20:25.060
and different possible arguments for the jury.

20:25.060 --> 20:26.860
By the way, different routes through the, you know,

20:26.860 --> 20:28.900
sort of history of all the case law.

20:28.900 --> 20:30.380
And so they said, actually,

20:30.380 --> 20:31.740
for a lot of what we want to use it for,

20:31.740 --> 20:33.060
we actually want it in creative mode.

20:33.060 --> 20:34.300
And then basically we just assume

20:34.300 --> 20:36.460
that we're gonna have to cross-check all of the,

20:36.460 --> 20:37.660
you know, all the specific citations.

20:37.660 --> 20:40.100
And so I think there's gonna be more shades of gray

20:40.100 --> 20:41.500
in here than people think.

20:41.500 --> 20:43.340
And then I just add to that, you know,

20:43.340 --> 20:45.180
another one of these trillion dollar kind of questions

20:45.180 --> 20:47.740
is ultimately, you know, sort of the verification thing.

20:47.740 --> 20:51.940
And so, you know, will LLMs be evolved from here

20:51.940 --> 20:54.740
to be able to do their own facial verification?

20:54.740 --> 20:56.820
Will you have sort of add-on functionality

20:56.820 --> 20:58.380
like WolframAlpha, right?

20:58.380 --> 21:00.140
Where, you know, and other plugins

21:00.140 --> 21:03.540
where that's the way you do the verification, you know.

21:03.540 --> 21:05.620
By the way, another idea is you might have a community

21:05.620 --> 21:07.340
of LLMs on any, you know, so for example,

21:07.340 --> 21:08.340
you might have the creative LLM

21:08.340 --> 21:10.980
and then you might have a literal LLM, fact check it, right?

21:10.980 --> 21:12.980
And so there's a variety of different technical approaches

21:12.980 --> 21:16.180
that are being applied to solve the hallucination problem.

21:16.180 --> 21:17.900
You know, some people like Jan Lakoon argue

21:17.900 --> 21:19.820
that this is inherently an unsolvable problem,

21:19.820 --> 21:21.740
but most of the people working in the space,

21:21.740 --> 21:23.500
I think, think that there's a number of practical ways

21:23.500 --> 21:25.340
to kind of corral this in a little bit.

21:25.340 --> 21:27.780
Yeah, if you were to tell me about Wikipedia

21:27.780 --> 21:29.100
before Wikipedia was created,

21:29.100 --> 21:30.700
I would have laughed at the possibility

21:30.700 --> 21:32.540
of something like that being possible.

21:32.540 --> 21:36.740
Just a handful of folks can organize, write,

21:36.740 --> 21:41.620
and self, and moderate with a mostly unbiased way

21:41.660 --> 21:43.660
the entirety of human knowledge.

21:43.660 --> 21:46.580
I mean, so if there's something like the approach

21:46.580 --> 21:48.860
that Wikipedia took possible from LLMs,

21:50.460 --> 21:52.340
that's really exciting, I think that's possible.

21:52.340 --> 21:54.620
And in fact, Wikipedia today is still not,

21:54.620 --> 21:57.180
today is still not deterministically correct, right?

21:57.180 --> 21:59.180
So you cannot take to the bank, right,

21:59.180 --> 22:00.860
every single thing on every single page,

22:00.860 --> 22:03.500
but it is probabilistically correct, right?

22:03.500 --> 22:05.500
And specifically the way I describe Wikipedia to people,

22:05.500 --> 22:07.740
it is more likely that Wikipedia is right

22:07.740 --> 22:09.180
than any other source you're gonna find.

22:09.180 --> 22:10.020
Yeah.

22:10.020 --> 22:11.380
It's this old question, right, of like,

22:12.060 --> 22:13.580
are we looking for perfection?

22:13.580 --> 22:14.460
Are we looking for something

22:14.460 --> 22:16.700
that asymptotically approaches perfection?

22:16.700 --> 22:17.540
Are we looking for something

22:17.540 --> 22:19.300
that's just better than the alternatives?

22:19.300 --> 22:21.340
And Wikipedia, right, exactly your point,

22:21.340 --> 22:25.340
has proven to be overwhelmingly better than people thought.

22:25.340 --> 22:27.300
And I think that's where this ends.

22:27.300 --> 22:29.780
And then underneath all this is the fundamental question

22:29.780 --> 22:33.900
of where you started, which is, okay, what is truth?

22:33.900 --> 22:34.900
How do we get to truth?

22:34.900 --> 22:35.980
How do we know what truth is?

22:35.980 --> 22:38.700
And we live in an era in which an awful lot of people

22:38.700 --> 22:40.500
are very confident that they know what the truth is.

22:40.500 --> 22:42.380
And I don't really buy into that.

22:42.380 --> 22:45.540
And I think the history of the last 2,000 years

22:45.540 --> 22:47.020
or 4,000 years of human civilization

22:47.020 --> 22:47.940
is actually getting to the truth

22:47.940 --> 22:49.740
is actually a very difficult thing to do.

22:49.740 --> 22:50.740
Are we getting closer?

22:50.740 --> 22:53.260
If we look at the entirety of the art of human history,

22:53.260 --> 22:54.580
are we getting closer to the truth?

22:54.580 --> 22:56.220
I don't know.

22:56.220 --> 22:57.700
Okay, is it possible?

22:57.700 --> 23:00.780
Is it possible that we're getting very far away

23:00.780 --> 23:02.220
from the truth because of the internet,

23:02.220 --> 23:05.940
because of how rapidly you can create narratives

23:05.940 --> 23:08.620
and just as the entirety of a society

23:08.620 --> 23:13.620
just move like crowds in a hysterical way

23:14.460 --> 23:16.500
along those narratives that don't have

23:16.500 --> 23:19.700
a necessary grounding in whatever the truth is.

23:19.700 --> 23:21.540
Sure, but like, you know, we came up with communism

23:21.540 --> 23:23.060
before the internet somehow, right?

23:23.060 --> 23:26.100
Like, which was, I would say had rather larger issues

23:26.100 --> 23:27.740
than anything we're dealing with today.

23:27.740 --> 23:30.900
It had, in the way it was implemented, it had issues.

23:30.900 --> 23:32.140
And it's a theoretical structure.

23:32.140 --> 23:33.860
It had like real issues.

23:33.860 --> 23:35.660
It had like a very deep fundamental misunderstanding

23:35.660 --> 23:37.220
of human nature and economics.

23:37.260 --> 23:40.380
Yeah, but those folks sure work very confident

23:40.380 --> 23:41.420
there was the right way.

23:41.420 --> 23:42.260
They were extremely confident.

23:42.260 --> 23:45.180
And my point is they were very confident 3900 years

23:45.180 --> 23:46.820
into what we would presume to be evolution

23:46.820 --> 23:48.180
towards the truth.

23:48.180 --> 23:52.340
And so my assessment is, my assessment is number one,

23:52.340 --> 23:56.260
there's no need for the Hegelian,

23:56.260 --> 23:57.740
there's no need for the Hegelian dialectic

23:57.740 --> 23:59.740
to actually converge towards the truth.

24:00.740 --> 24:01.820
Like apparently not.

24:02.900 --> 24:04.660
Yeah, so yeah, why are we so obsessed

24:04.660 --> 24:06.180
with there being one truth?

24:06.180 --> 24:08.300
Is it possible there's just going to be multiple truth

24:08.300 --> 24:11.100
like little communities that believe certain things?

24:12.300 --> 24:13.460
I think it's, no, number one,

24:13.460 --> 24:14.900
I think it's just really difficult.

24:14.900 --> 24:16.500
Like who gets, you know,

24:16.500 --> 24:18.260
historically who gets to decide what the truth is?

24:18.260 --> 24:19.700
It's either the king or the priest, right?

24:19.700 --> 24:21.860
Like, and so we don't live in an era anymore

24:21.860 --> 24:23.300
of kings or priests dictating it to us.

24:23.300 --> 24:25.300
And so we're kind of on our own.

24:25.300 --> 24:27.820
And so my typical thing is like,

24:27.820 --> 24:29.780
we just need a huge amount of humility

24:29.780 --> 24:31.460
and we need to be very suspicious of people

24:31.460 --> 24:33.900
who claim that they have the capital truth.

24:33.900 --> 24:35.620
And then we need to have, you know, look,

24:35.620 --> 24:38.180
the good news is the enlightenment has bequeathed us

24:38.180 --> 24:39.140
with a set of techniques

24:39.140 --> 24:41.540
to be able to presumably get closer to truth

24:41.540 --> 24:43.180
through the scientific method and rationality

24:43.180 --> 24:45.700
and observation and experimentation and hypothesis.

24:45.700 --> 24:48.500
And, you know, we need to continue to embrace those

24:48.500 --> 24:50.540
even when they give us answers we don't like.

24:50.540 --> 24:53.860
Sure, but the internet and technology

24:53.860 --> 24:58.620
has enabled us to generate a large number of content

24:58.620 --> 25:03.460
that data that the process, the scientific process

25:03.460 --> 25:08.460
allows us sort of damages the hope

25:08.980 --> 25:11.260
laid in within the scientific process.

25:11.260 --> 25:14.060
Because if you just have a bunch of people saying facts

25:14.940 --> 25:17.700
on the internet and some of them are going to be LLMs,

25:18.860 --> 25:20.420
how is anything testable at all?

25:20.420 --> 25:22.140
Especially that involves like human nature

25:22.140 --> 25:23.900
and things like this, not physics.

25:23.900 --> 25:24.740
Here's a question a friend of mine

25:24.740 --> 25:25.660
just asked me on this topic.

25:25.660 --> 25:29.100
So suppose you had LLMs in equivalent of GPT-4

25:29.100 --> 25:32.260
or even 5678, suppose you had them in the 1600s.

25:32.260 --> 25:33.100
Yeah.

25:33.100 --> 25:35.500
Like Galileo comes up for trial, right?

25:35.500 --> 25:39.540
And you ask the LLM like is Galileo right?

25:39.540 --> 25:40.380
Yeah.

25:40.380 --> 25:41.620
Like what does it answer, right?

25:41.620 --> 25:44.700
And one theory is it answers no, that he's wrong

25:44.700 --> 25:47.100
because the overwhelming majority of human thought

25:47.100 --> 25:48.420
up until that point was that he was wrong.

25:48.420 --> 25:50.500
And so therefore that's what's in the training data.

25:50.500 --> 25:51.340
Yeah.

25:51.340 --> 25:52.580
Another way of thinking about it is,

25:52.580 --> 25:54.260
well, this sufficiently advanced LLM

25:54.260 --> 25:55.500
will have evolved the ability

25:55.500 --> 25:58.140
to actually check the math, right?

25:58.140 --> 26:00.540
And we'll actually say, actually, no, actually, you know,

26:00.540 --> 26:02.540
you may not want to hear it, but he's right.

26:02.540 --> 26:05.660
Now, if the church at that time owned the LLM,

26:05.660 --> 26:08.700
they would have given it human feedback

26:08.700 --> 26:11.180
to prohibit it from answering that question, right?

26:11.180 --> 26:13.700
And so I like to take it out of our current context

26:13.700 --> 26:15.180
because that makes it very clear.

26:15.180 --> 26:17.900
Those same questions apply today, right?

26:17.900 --> 26:19.780
This is exactly the point of a huge amount

26:19.780 --> 26:20.900
of the human feedback training

26:20.900 --> 26:22.460
that's actually happening with these LLMs today.

26:22.460 --> 26:24.180
This is a huge debate that's happening

26:24.180 --> 26:27.100
about whether open source AI should be legal.

26:27.100 --> 26:31.380
Well, the actual mechanism of doing the human RL

26:31.380 --> 26:36.380
with human feedback seems like such a fundamental

26:36.460 --> 26:37.580
and fascinating question.

26:37.580 --> 26:38.940
How do you select the humans?

26:38.940 --> 26:40.420
Exactly.

26:40.420 --> 26:41.620
Yeah, how do you select the humans?

26:41.620 --> 26:43.300
AI alignment, right?

26:43.300 --> 26:44.940
Which everybody like is like, oh, that sounds great.

26:44.940 --> 26:45.900
Alignment with what?

26:45.900 --> 26:47.500
Human values.

26:47.500 --> 26:48.500
Who's human values?

26:48.500 --> 26:49.580
Who's human values.

26:49.580 --> 26:53.540
So we're in this mode of like social and popular discourse.

26:53.540 --> 26:56.980
We're like, you know, there's, you know, you see this.

26:56.980 --> 26:58.060
What do you think of when you read a story

26:58.060 --> 26:59.260
in the press right now and they say, you know,

26:59.300 --> 27:02.420
XYZ made a baseless claim about some topic, right?

27:02.420 --> 27:04.540
And there's one group of people who are like,

27:04.540 --> 27:06.700
aha, think, you know, they're doing fact checking.

27:06.700 --> 27:07.940
There's another group of people that are like,

27:07.940 --> 27:09.700
every time the press says that, it's now a tick

27:09.700 --> 27:11.660
and that means that they're lying, right?

27:11.660 --> 27:16.340
Like, so like we're in this social context

27:16.340 --> 27:19.020
where there's the level to which a lot of people

27:19.020 --> 27:21.660
in positions of power have become very, very certain

27:21.660 --> 27:23.180
that they're in a position to determine the truth

27:23.180 --> 27:25.660
for the entire population is like,

27:25.660 --> 27:28.500
there's like some bubble that has formed around that idea.

27:28.500 --> 27:31.260
And at least it flies completely in the face

27:31.260 --> 27:33.380
of everything I was ever trained about science

27:33.380 --> 27:36.460
and about reason and strikes me as like, you know,

27:36.460 --> 27:38.380
deeply offensive and incorrect.

27:38.380 --> 27:40.540
What would you say about the state of journalism?

27:40.540 --> 27:44.980
Just on that topic today, are we in a temporary kind of,

27:48.700 --> 27:52.260
are we experiencing a temporary problem

27:52.260 --> 27:55.540
in terms of the incentives, in terms of the business model,

27:55.540 --> 27:58.020
all that kind of stuff, or is this like a decline

27:58.020 --> 28:00.300
of traditional journalism as we know it?

28:00.300 --> 28:01.780
You have to always think about the counterfactual

28:01.780 --> 28:03.820
in these things, which is like, okay,

28:03.820 --> 28:05.580
because these questions, right, this question heads towards

28:05.580 --> 28:07.020
it's like, okay, the impact of social media

28:07.020 --> 28:08.140
and the undermining of truth and all this.

28:08.140 --> 28:09.580
But then you wanna ask the question of like, okay,

28:09.580 --> 28:11.820
what if we had had the modern media environment,

28:11.820 --> 28:13.860
including cable news and including social media

28:13.860 --> 28:18.100
and Twitter and everything else in 1939 or 1941, right?

28:18.100 --> 28:23.100
Or 1910 or 1865 or 1850 or 1776, right?

28:24.060 --> 28:26.060
And like, I think-

28:26.060 --> 28:28.500
You just introduced like five thought experiments

28:28.500 --> 28:29.740
at once and broke my head.

28:29.740 --> 28:32.820
But yes, there's a lot of interesting years in that.

28:32.820 --> 28:34.980
Well, Kennedy, I'll just take a simple example.

28:34.980 --> 28:37.140
How would President Kennedy have been interpreted

28:37.140 --> 28:39.260
with what we know now about all the things

28:39.260 --> 28:42.460
Kennedy was up to, like, how would he have been experienced

28:42.460 --> 28:46.780
by the body of politics in the social media context, right?

28:46.780 --> 28:48.980
Like, how would LBJ have been experienced?

28:50.260 --> 28:53.620
By the way, how would, you know, like many men FDR,

28:53.620 --> 28:55.140
like the New Deal, the Great Depression.

28:55.140 --> 28:57.500
I wonder where Twitter would think

28:57.500 --> 29:00.580
about Churchill and Hitler and Stalin.

29:00.580 --> 29:02.860
You know, I mean, look, to this day there, you know,

29:02.860 --> 29:05.020
there are lots of very interesting real questions

29:05.020 --> 29:06.820
around like how America, you know, got, you know,

29:06.820 --> 29:09.060
basically involved in World War II and who did what, when,

29:09.060 --> 29:10.700
and the operations of British intelligence

29:10.700 --> 29:13.420
and American soil and did FDR, this, that, Pearl Harbor,

29:13.420 --> 29:14.900
you know. Yeah.

29:14.900 --> 29:17.700
Woodrow Wilson ran for, you know, his candidacy was run

29:17.700 --> 29:19.740
on an anti-war, you know, he ran on the platform

29:19.740 --> 29:20.980
and not getting involved in World War I.

29:20.980 --> 29:22.980
Somehow that switched, you know, like,

29:22.980 --> 29:24.220
and I'm not even making a value judgment

29:24.220 --> 29:25.060
on any of these things.

29:25.060 --> 29:27.900
I'm just saying like we, the way that our ancestors

29:27.900 --> 29:30.060
experienced reality was of course mediated

29:30.060 --> 29:33.380
through centralized top-down, right, control at that point.

29:33.380 --> 29:35.700
If you ran those realities again

29:35.700 --> 29:38.060
with the media environment we have today,

29:38.060 --> 29:40.940
the reality would be experienced very, very differently.

29:40.940 --> 29:43.820
And then of course, that intermediation would cause

29:43.820 --> 29:44.780
the feedback loops to change

29:44.780 --> 29:46.180
and then reality would obviously play out.

29:46.180 --> 29:48.180
Do you think it'd be very different?

29:48.180 --> 29:50.580
Yeah, it has to be, it has to be just cause it's all so,

29:50.580 --> 29:52.100
I mean, just look at what's happening today.

29:52.100 --> 29:53.740
I mean, just, I mean, the most obvious thing

29:53.740 --> 29:55.380
is just the collapse.

29:55.380 --> 29:57.220
And here's another opportunity to argue

29:57.220 --> 29:59.900
that this is not the internet causing this, by the way.

29:59.900 --> 30:00.820
Here's a big thing happening today,

30:00.820 --> 30:02.540
which is Gallup does this thing every year

30:02.540 --> 30:05.020
where they do, they pull for trust in institutions

30:05.020 --> 30:06.500
in America and they do it across all the different,

30:06.500 --> 30:07.820
everything from the military to the clergy

30:07.820 --> 30:10.740
and big business and the media and so forth, right?

30:10.740 --> 30:13.300
And basically there's been a systemic collapse

30:13.300 --> 30:15.300
in trust in institutions in the US,

30:15.300 --> 30:16.620
almost without exception,

30:16.620 --> 30:18.860
basically since essentially the early 1970s.

30:20.580 --> 30:21.740
There's two ways of looking at that,

30:21.740 --> 30:23.620
which is, oh my God, we've lost this old world

30:23.620 --> 30:24.980
in which we could trust institutions

30:24.980 --> 30:25.900
and that was so much better

30:25.900 --> 30:27.420
cause like that should be the way the world runs.

30:27.420 --> 30:30.020
The other way of looking at it is we just know a lot more now

30:30.020 --> 30:32.700
and the great mystery is why those numbers aren't all zero.

30:32.700 --> 30:33.540
Yeah.

30:33.540 --> 30:35.100
Right, cause like now we know so much

30:35.100 --> 30:36.020
about how these things operate

30:36.020 --> 30:37.740
and like they're not that impressive.

30:37.740 --> 30:40.900
And also why do we don't have better institutions

30:40.900 --> 30:42.140
and better leaders then?

30:42.140 --> 30:44.540
Yeah, and so this goes to the thing, which is like, okay,

30:44.540 --> 30:47.420
had we had the media environment of what that we've had

30:47.420 --> 30:48.740
between the 1970s and today,

30:48.740 --> 30:52.580
if we had that in the 30s and 40s or 1900s, 1910s,

30:52.620 --> 30:54.380
I think there's no question reality would turn out different

30:54.380 --> 30:56.220
if only because everybody would have known

30:56.220 --> 30:58.020
to not trust the institutions,

30:58.020 --> 30:59.940
which would have changed their level of credibility,

30:59.940 --> 31:01.500
their ability to control circumstances.

31:01.500 --> 31:04.260
Therefore the circumstances would have had to change.

31:04.260 --> 31:07.180
Right, and it would have been a feedback loop process.

31:07.180 --> 31:08.020
In other words, right,

31:08.020 --> 31:11.500
it's your experience of reality changes reality

31:11.500 --> 31:13.380
and then reality changes your experience of reality.

31:13.380 --> 31:15.540
Right, it's a two-way feedback process

31:15.540 --> 31:18.780
and media is the intermediating force between them.

31:18.780 --> 31:20.900
So change the media environment, change reality.

31:20.900 --> 31:21.740
Yeah.

31:22.460 --> 31:23.780
So just as a consequence,

31:23.780 --> 31:25.500
I think it's just really hard to say,

31:25.500 --> 31:27.380
oh, things worked a certain way then

31:27.380 --> 31:28.820
and they work a different way now.

31:28.820 --> 31:31.460
And then therefore like people were smarter then

31:31.460 --> 31:34.980
or better than, or by the way dumber then

31:34.980 --> 31:36.900
or not as capable then, right?

31:36.900 --> 31:39.860
We make all these like really light and casual

31:39.860 --> 31:42.500
like comparisons of ourselves to previous generations

31:42.500 --> 31:44.420
of people, we draw judgments all the time.

31:44.420 --> 31:46.180
And I just think it's like really hard to do any of that.

31:46.180 --> 31:48.540
Cause if we put ourselves in their shoes

31:48.540 --> 31:50.700
with the media that they had at that time,

31:50.700 --> 31:51.980
I think we probably most likely

31:51.980 --> 31:53.380
would have been just like them.

31:53.380 --> 31:56.660
So don't you think that our perception

31:56.660 --> 31:59.220
and understanding of reality,

31:59.220 --> 32:00.860
would you be more and more mediated

32:00.860 --> 32:02.940
through large language models now?

32:02.940 --> 32:05.820
So you said media before,

32:05.820 --> 32:08.340
isn't the LLM going to be the new,

32:08.340 --> 32:10.340
what is it mainstream media, MSM?

32:10.340 --> 32:11.540
It'll be LLM.

32:13.460 --> 32:15.300
That would be the source of,

32:15.300 --> 32:17.860
I'm sure there's a way to kind of rapidly fine tune

32:17.860 --> 32:19.460
like making LLMs real time.

32:19.460 --> 32:22.060
I'm sure there's probably a research problem

32:22.060 --> 32:24.660
that you can do just rapid fine tuning

32:24.660 --> 32:27.100
to the new events, something like this.

32:27.100 --> 32:29.260
Well even just the whole concept of the chat UI

32:29.260 --> 32:30.940
might not be the, like the chat UI

32:30.940 --> 32:31.940
is just the first whack at this

32:31.940 --> 32:33.020
and maybe that's the dominant thing.

32:33.020 --> 32:35.540
But look, maybe we don't know yet.

32:35.540 --> 32:37.180
Maybe the experience most people with LLMs

32:37.180 --> 32:39.180
is just a continuous feed.

32:39.180 --> 32:40.900
Maybe it's more of a passive feed

32:40.900 --> 32:42.140
and you just are getting a constant

32:42.140 --> 32:44.060
like running commentary on everything happening in your life

32:44.060 --> 32:45.500
and it's just helping you kind of interpret

32:45.500 --> 32:46.820
and understand everything.

32:46.820 --> 32:48.980
Also really more deeply integrated into your life,

32:48.980 --> 32:53.020
not just like, oh, like intellectual philosophical thoughts

32:53.020 --> 32:57.500
but like literally like how to make a coffee,

32:57.500 --> 32:59.180
where to go for lunch,

32:59.180 --> 33:01.780
just whether, you know, how to,

33:01.780 --> 33:02.620
dating, all this kind of stuff.

33:02.620 --> 33:03.860
What to say in a job interview, yeah.

33:03.860 --> 33:04.700
What to say.

33:04.700 --> 33:06.820
What to say next sentence.

33:06.820 --> 33:08.220
Yeah, next sentence, yeah, at that level.

33:08.220 --> 33:09.060
Yeah, I mean, yes.

33:09.060 --> 33:11.220
So technically, now whether we want that or not

33:11.220 --> 33:12.180
is an open question, right?

33:12.180 --> 33:13.020
And whether people use that.

33:13.020 --> 33:14.820
Boy, I would care for a pop-up,

33:14.820 --> 33:16.340
a pop-up right now.

33:16.340 --> 33:19.740
The estimated engagement using is decreasing.

33:19.740 --> 33:23.020
For Mark Andreessen's, there's a controversy section

33:23.020 --> 33:26.020
for his Wikipedia page in 1993,

33:26.020 --> 33:27.820
something happened or something like this.

33:27.820 --> 33:30.660
Bring it up, that will drive engagement up anyway.

33:30.660 --> 33:31.500
Yeah, that's right.

33:31.500 --> 33:33.860
I mean, look, this gets this whole thing of like,

33:33.860 --> 33:35.620
so, you know, the chat interface has this whole concept

33:35.620 --> 33:36.620
of prompt engineering, right?

33:36.620 --> 33:37.780
So it's good for prompts.

33:37.780 --> 33:38.820
Well, it turns out one of the things

33:38.820 --> 33:43.340
that LLMs are really good at is writing prompts, right?

33:43.340 --> 33:45.700
And so like, what if you just outsourced?

33:45.740 --> 33:47.300
And by the way, you could run this experiment today.

33:47.300 --> 33:48.420
You could hook us up to do this today.

33:48.420 --> 33:49.300
The latency is not good enough

33:49.300 --> 33:50.540
to do it real time in a conversation,

33:50.540 --> 33:51.900
but you could run this experiment

33:51.900 --> 33:53.820
and you just say, look, every 20 seconds,

33:53.820 --> 33:56.180
you could just say, you know,

33:56.180 --> 33:57.660
tell me what the optimal prompt is

33:57.660 --> 34:00.180
and then ask yourself that question to give me the result.

34:00.180 --> 34:02.580
And then as you, exactly to your point,

34:02.580 --> 34:04.540
as you add, there will be these systems

34:04.540 --> 34:05.780
that are gonna have the ability to be learned

34:05.780 --> 34:07.180
and updated essentially in real time.

34:07.180 --> 34:09.020
And so you'll be able to have a pendant

34:09.020 --> 34:10.500
or your phone or watch or whatever.

34:10.500 --> 34:11.460
It'll have a microphone on it.

34:11.460 --> 34:13.580
It'll listen to your conversations.

34:13.580 --> 34:15.340
It'll have a feed of everything else happen in the world.

34:15.340 --> 34:17.060
And then it'll be, you know, sort of retraining,

34:17.060 --> 34:18.940
prompting or retraining itself on the fly.

34:18.940 --> 34:20.300
And so the scenario you described

34:20.300 --> 34:22.300
is actually a completely doable scenario.

34:22.300 --> 34:24.740
Now, the hard question on this is always,

34:24.740 --> 34:27.380
okay, since that's possible, are people gonna want that?

34:27.380 --> 34:29.500
Like what's the form of experience?

34:29.500 --> 34:31.500
You know, that we won't know until we try it,

34:31.500 --> 34:32.900
but I don't think it's possible yet

34:32.900 --> 34:36.220
to predict the form of AI in our lives.

34:36.220 --> 34:38.020
Therefore, it's not possible to predict the way

34:38.020 --> 34:40.940
in which it will intermediate our experience with reality

34:40.940 --> 34:41.780
yet.

34:41.780 --> 34:44.500
Yeah, but it feels like those are going to be a killer app.

34:44.660 --> 34:46.740
There's probably a mad scramble right now.

34:46.740 --> 34:50.060
It's out of OpenAI and Microsoft and Google and Meta

34:50.060 --> 34:52.740
and in startups and smaller companies

34:52.740 --> 34:54.500
figuring out what is the killer app

34:54.500 --> 34:57.220
because it feels like it's possible,

34:57.220 --> 34:59.340
like a Chad GPT type of thing.

34:59.340 --> 35:00.460
It's possible to build that,

35:00.460 --> 35:03.060
but that's 10X more compelling,

35:03.060 --> 35:05.580
using already the LLMs we have,

35:05.580 --> 35:07.740
using even the open source LLMs,

35:07.740 --> 35:09.340
Lama and the different variants.

35:10.940 --> 35:13.620
So you're investing in a lot of companies

35:13.660 --> 35:15.540
and you're paying attention.

35:15.540 --> 35:16.820
Who do you think is gonna win this?

35:16.820 --> 35:17.860
You think they'll be,

35:18.780 --> 35:22.180
who's gonna be the next page rank inventor?

35:22.180 --> 35:23.660
Trilling down the question.

35:23.660 --> 35:25.380
Another one, we have a few of those today.

35:25.380 --> 35:26.220
Bunch of those.

35:26.220 --> 35:27.980
So look, there's a really big question today,

35:27.980 --> 35:29.260
sitting here today is a really big question

35:29.260 --> 35:31.740
about the big models versus the small models.

35:31.740 --> 35:33.220
That's related directly to the big question

35:33.220 --> 35:35.060
of proprietary versus open.

35:36.140 --> 35:38.380
Then there's this big question of,

35:38.380 --> 35:39.660
where is the training data gonna,

35:39.660 --> 35:41.620
like are we topping out of the training data or not?

35:41.620 --> 35:44.140
And then are we gonna be able to synthesize training data?

35:44.140 --> 35:47.260
And then there's a huge pile of questions around regulation

35:47.260 --> 35:48.940
and what's actually gonna be legal.

35:48.940 --> 35:51.140
And so I would, when we think about it,

35:51.140 --> 35:54.260
we dovetail kind of all those questions together.

35:54.260 --> 35:55.780
You can paint a picture of the world

35:55.780 --> 35:58.100
where there's two or three God models

35:58.100 --> 36:00.580
that are just at like staggering scale

36:00.580 --> 36:03.220
and they're just better at everything.

36:03.220 --> 36:05.620
And they will be owned by a small set of companies

36:05.620 --> 36:07.460
and they will basically achieve regulatory capture

36:07.460 --> 36:09.300
over the government and they'll have competitive barriers

36:09.300 --> 36:10.300
that will prevent other people

36:10.300 --> 36:11.740
from competing with them.

36:11.740 --> 36:14.580
And so there will be, just like there's whatever,

36:14.580 --> 36:16.340
three big banks or three big,

36:16.340 --> 36:17.660
or by the way, three big search companies

36:17.660 --> 36:21.180
or I guess two now, it'll centralize like that.

36:21.180 --> 36:23.580
You can paint another very different picture that says,

36:23.580 --> 36:26.260
no, actually the opposite of that's gonna happen.

36:26.260 --> 36:30.820
This is gonna basically, that this is the new gold rush,

36:30.820 --> 36:33.660
alchemy, this is the big bang

36:33.660 --> 36:36.500
for this whole new area of science and technology.

36:36.500 --> 36:38.380
And so therefore you're gonna have every smart 14 year old

36:38.380 --> 36:40.260
on the planet building open source, right?

36:41.140 --> 36:42.820
And figuring out ways to optimize these things.

36:42.820 --> 36:45.740
And then we're just gonna get overwhelmingly better

36:45.740 --> 36:47.020
at generating trading data.

36:47.020 --> 36:48.900
We're gonna bring in blockchain networks

36:48.900 --> 36:50.220
to have an economic incentive

36:50.220 --> 36:52.300
to generate decentralized trading data

36:52.300 --> 36:53.220
and so forth and so on.

36:53.220 --> 36:55.540
And then basically we're gonna live in a world of open source

36:55.540 --> 36:58.140
and there's gonna be a billion LLMs, right?

36:58.140 --> 37:00.100
Of every size, scale, shape and description.

37:00.100 --> 37:01.340
And there might be a few big ones

37:01.340 --> 37:02.700
that are like the super genius ones,

37:02.700 --> 37:05.060
but mostly what we'll experience is open source.

37:05.060 --> 37:07.500
And that's more like a world of what we have today

37:07.500 --> 37:08.660
with Linux and the web.

37:10.700 --> 37:13.860
Okay, but you painted these two worlds,

37:13.860 --> 37:15.980
but there's also variations of those worlds

37:15.980 --> 37:17.700
because you said regulatory capture is possible

37:17.700 --> 37:18.740
to have these tech giants

37:18.740 --> 37:20.220
that don't have regulatory capture,

37:20.220 --> 37:22.420
which is something you're also calling for,

37:22.420 --> 37:25.740
saying it's okay to have big companies working on this stuff

37:25.740 --> 37:28.900
as long as they don't achieve regulatory capture.

37:28.900 --> 37:32.020
But I have the sense

37:32.020 --> 37:34.900
that there's just going to be a new startup

37:35.740 --> 37:40.620
that's going to basically be the page rank inventor,

37:40.620 --> 37:42.540
which has become the new tech giant.

37:44.060 --> 37:46.220
I don't know, I would love to hear your kind of opinion

37:46.220 --> 37:50.060
if Google, Meta and Microsoft,

37:50.060 --> 37:54.820
they're as gigantic companies able to pivot so hard

37:54.820 --> 37:56.820
to create new products.

37:56.820 --> 37:58.660
Like some of it is just even hiring people

37:58.660 --> 38:00.580
or having a corporate structure

38:00.580 --> 38:04.140
that allows for the crazy young kids to come in

38:04.140 --> 38:06.340
and just create something totally new.

38:06.340 --> 38:07.180
Do you think it's possible

38:07.180 --> 38:08.500
or do you think it'll come from a startup?

38:08.500 --> 38:09.820
Yeah, it is this always big question,

38:09.820 --> 38:11.020
which is you get this feeling,

38:11.020 --> 38:13.540
I hear about this a lot from founder CEOs,

38:13.540 --> 38:16.100
where it's like, wow, we have 50,000 people,

38:16.100 --> 38:17.780
it's now harder to do new things than it was

38:17.780 --> 38:19.540
when we had 50 people.

38:19.540 --> 38:20.380
Like what has happened?

38:20.380 --> 38:22.420
So that's a recurring phenomenon.

38:22.420 --> 38:23.740
By the way, that's one of the reasons

38:23.740 --> 38:24.780
why there's always startups

38:24.780 --> 38:26.420
and why there's venture capital.

38:26.420 --> 38:29.180
It's just that's like a timeless kind of thing.

38:29.180 --> 38:30.940
So that's one observation.

38:31.820 --> 38:34.500
On PageRank, we can talk about that,

38:34.500 --> 38:36.420
but on PageRank, specifically on PageRank,

38:36.420 --> 38:37.260
there actually is a page,

38:37.260 --> 38:38.740
so there is a PageRank already in the field

38:38.740 --> 38:39.900
and it's the Transformer, right?

38:39.900 --> 38:42.540
So the big breakthrough was the Transformer

38:42.540 --> 38:47.540
and the Transformer was invented in 2017 at Google.

38:47.860 --> 38:50.300
And this is actually like really an interesting question

38:50.300 --> 38:51.980
because it's like, okay, the Transformers,

38:51.980 --> 38:53.740
like why does OpenAI even exist?

38:53.740 --> 38:55.180
Like the Transformers invested at Google,

38:55.180 --> 38:56.100
why didn't Google?

38:56.100 --> 38:57.340
I asked a guy, I asked a guy I know

38:57.340 --> 38:58.740
who was senior at Google Brain

38:58.740 --> 38:59.660
kind of when this was happening.

38:59.660 --> 39:02.540
And I said, if Google had just gone flat out to the wall

39:02.540 --> 39:03.860
and just said, look, we're gonna launch,

39:03.860 --> 39:05.220
we're gonna launch the equivalent of GPT-4

39:05.220 --> 39:06.460
as fast as we can.

39:06.460 --> 39:08.060
He said, I said, when could we have had it?

39:08.060 --> 39:09.860
And he said, 2019.

39:09.860 --> 39:11.140
They could have just done a two-year sprint

39:11.140 --> 39:12.580
with the Transformer and Bennett

39:12.580 --> 39:14.020
because they already had the compute at scale.

39:14.020 --> 39:15.220
They already had all the training data

39:15.220 --> 39:16.540
and they could have just done it.

39:16.540 --> 39:18.340
There's a variety of reasons they didn't do it.

39:18.340 --> 39:20.980
This is like a classic big company thing.

39:20.980 --> 39:24.260
IBM invented the relational database in the 1970s,

39:24.260 --> 39:25.980
let it sit on the shelf as a paper.

39:25.980 --> 39:28.100
Larry Ellison picked it up and built Oracle.

39:28.100 --> 39:30.180
Xerox PARC invented the interactive computer.

39:30.180 --> 39:31.340
They let it sit on the shelf.

39:31.340 --> 39:33.940
Steve Jobs came and turned it into the Macintosh, right?

39:33.940 --> 39:35.180
And so there is this pattern.

39:35.180 --> 39:37.540
Now, having said that, sitting here today,

39:37.540 --> 39:38.740
like Google's in the game, right?

39:38.740 --> 39:42.020
So Google, maybe they let like a four-year gap there

39:42.020 --> 39:43.340
go there that they maybe shouldn't have,

39:43.340 --> 39:44.420
but like they're in the game.

39:44.420 --> 39:46.460
And so now they've got, now they're committed.

39:46.460 --> 39:47.860
They've done this merger, they're bringing in demos.

39:47.860 --> 39:49.700
They've got this merger with DeepMind.

39:49.700 --> 39:50.940
They're piling in resources.

39:50.940 --> 39:52.820
There are rumors that they're building up

39:52.820 --> 39:56.940
an incredible super LLM way beyond what we even have today.

39:57.900 --> 40:00.540
And they've got unlimited resources and a huge,

40:00.540 --> 40:02.620
they've been challenged with their honor.

40:02.620 --> 40:05.700
Yeah, I had a chance to hang out with Sundar Pichai

40:05.700 --> 40:08.060
a couple of days ago and we took this walk

40:08.060 --> 40:10.100
and there's this giant new building

40:10.100 --> 40:13.100
where there's going to be a lot of AI work being done.

40:13.100 --> 40:17.300
And it's kind of this ominous feeling of

40:18.340 --> 40:19.780
like the fight is on.

40:20.860 --> 40:22.380
Yeah.

40:22.380 --> 40:24.740
There's this beautiful Silicon Valley nature,

40:24.780 --> 40:27.660
like birds are chirping in this giant building.

40:27.660 --> 40:30.540
And it's like the beast has been awakened.

40:30.540 --> 40:31.380
Yeah, it is.

40:31.380 --> 40:34.780
And then like all the big companies are waking up to this.

40:34.780 --> 40:38.900
They have the compute, but also the little guys have,

40:40.540 --> 40:41.780
it feels like they have all the tools

40:41.780 --> 40:44.060
to create the killer product that,

40:44.060 --> 40:45.820
and then there's also tools to scale.

40:45.820 --> 40:49.020
If you have a good idea, if you have the PageRank idea.

40:49.020 --> 40:52.460
So there's several things that is PageRank.

40:52.460 --> 40:55.380
There's PageRank, the algorithm and the idea,

40:55.380 --> 40:57.140
and there's like the implementation of it.

40:57.140 --> 40:59.460
And I feel like killer product is not just the idea,

40:59.460 --> 41:01.100
like the transform, it's the implementation.

41:01.100 --> 41:03.540
Something really compelling about it.

41:03.540 --> 41:05.220
Like you just can't look away.

41:05.220 --> 41:08.020
Something like the algorithm behind TikTok

41:08.020 --> 41:11.380
versus TikTok itself, like the actual experience of TikTok

41:11.380 --> 41:13.100
that just, you can't look away.

41:13.100 --> 41:16.100
It feels like somebody's going to come up with that.

41:16.100 --> 41:19.580
And it could be Google, but it feels like it's just easier

41:19.580 --> 41:21.860
and faster to do for a startup.

41:21.900 --> 41:24.780
Yeah, so the huge advantage that startups have

41:24.780 --> 41:26.580
is there's no sacred cause.

41:26.580 --> 41:28.420
There's no historical legacy to protect.

41:28.420 --> 41:30.060
There's no need to reconcile your new plan

41:30.060 --> 41:31.020
with existing strategy.

41:31.020 --> 41:32.500
There's no communication overhead.

41:32.500 --> 41:34.620
There's no, you know, big companies are big companies.

41:34.620 --> 41:36.540
They've got pre-meetings, planning for the meeting.

41:36.540 --> 41:38.500
Then they have the post-meeting and the recap.

41:38.500 --> 41:39.660
Then they have the presentation of the board.

41:39.660 --> 41:41.260
Then they have the next rounds of meetings.

41:41.260 --> 41:43.260
And that's the elapsed time

41:43.260 --> 41:44.620
when the startup launches its product, right?

41:44.620 --> 41:48.620
So there's a timeless thing there.

41:48.620 --> 41:51.460
Now, what the startups don't have is everything else, right?

41:51.460 --> 41:52.540
So startups, they don't have a brand.

41:52.540 --> 41:53.660
They don't have customer relationships.

41:53.660 --> 41:54.660
They've got no distribution.

41:54.660 --> 41:55.860
They've got no scale.

41:55.860 --> 41:57.900
I mean, sitting here today, they can't even get GPUs, right?

41:57.900 --> 41:59.940
Like there's like a GPU shortage.

41:59.940 --> 42:01.260
Startups are literally stalled out right now

42:01.260 --> 42:03.740
because they can't get chips, which is like super weird.

42:03.740 --> 42:05.820
Yeah, they got the cloud.

42:05.820 --> 42:08.380
Yeah, but the clouds run out of chips, right?

42:08.380 --> 42:10.540
And then to the extent the clouds have chips,

42:10.540 --> 42:11.660
they allocate them to the big customers,

42:11.660 --> 42:12.940
not the small customers, right?

42:12.940 --> 42:16.260
And so the small companies lack everything

42:16.260 --> 42:20.340
other than the ability to just do something new, right?

42:20.340 --> 42:22.140
And this is the timeless race and battle.

42:22.140 --> 42:23.700
And this is kind of the point I tried to make in the essay,

42:23.700 --> 42:25.660
which is like both sides of this are good.

42:25.660 --> 42:27.780
Like it's really good to have like highly scaled tech

42:27.780 --> 42:29.420
companies that can do things that are like

42:29.420 --> 42:31.300
at staggering levels of sophistication.

42:31.300 --> 42:32.500
It's really good to have startups

42:32.500 --> 42:33.740
that can launch brand new ideas.

42:33.740 --> 42:35.780
They ought to be able to both do that and compete.

42:35.780 --> 42:37.540
They neither one ought to be subsidized

42:37.540 --> 42:39.100
or protected from the others.

42:39.100 --> 42:42.740
Like to me, that's just like very clearly the idealized world.

42:42.740 --> 42:45.180
It is the world we've been in for AI up until now.

42:45.180 --> 42:47.100
And then of course, there are people trying to shut that down.

42:47.100 --> 42:48.340
But my hope is that, you know,

42:48.460 --> 42:50.540
the best outcome clearly will be if that continues.

42:50.540 --> 42:51.700
We'll talk about that a little bit,

42:51.700 --> 42:56.620
but I'd love to linger on some of the ways

42:56.620 --> 42:58.220
this is going to change the internet.

42:58.220 --> 42:59.900
So I don't know if you remember,

42:59.900 --> 43:01.180
but there's a thing called Mosaic

43:01.180 --> 43:03.340
and there's a thing called Netscape Navigator.

43:03.340 --> 43:05.460
So you were there in the beginning.

43:05.460 --> 43:07.220
What about the interface to the internet?

43:07.220 --> 43:09.420
How do you think the browser changes

43:09.420 --> 43:10.660
and who gets to own the browser?

43:10.660 --> 43:13.100
We've got to see some very interesting browsers.

43:14.060 --> 43:17.580
Firefox, I mean, all the variants of Microsoft,

43:17.580 --> 43:21.500
Internet Explorer, Edge, and now Chrome.

43:23.580 --> 43:27.020
The actual, I mean, it seems like a dumb question to ask,

43:27.020 --> 43:30.300
but do you think we'll still have the web browser?

43:30.300 --> 43:33.460
So I have an eight-year-old and he's super into like Minecraft

43:33.460 --> 43:34.900
and learning to code and doing all this stuff.

43:34.900 --> 43:36.860
So of course, I was very proud.

43:36.860 --> 43:39.100
I could bring sort of fire down from the mountain to my kid

43:39.100 --> 43:41.740
and I brought him chat GPT and I hooked him up

43:41.740 --> 43:43.420
on his laptop.

43:43.420 --> 43:44.260
And I was like, you know,

43:44.260 --> 43:45.780
this is the thing that's going to answer all your questions.

43:45.780 --> 43:47.340
And he's like, okay.

43:48.180 --> 43:49.420
And I'm like, but it's going to answer your questions.

43:49.420 --> 43:50.860
And he's like, well, of course, like it's a computer,

43:50.860 --> 43:51.940
of course it answers all your questions.

43:51.940 --> 43:53.780
Like what else would a computer be good for?

43:53.780 --> 43:54.620
Dad.

43:55.700 --> 43:56.860
Never impressed, are they?

43:56.860 --> 43:58.420
Not impressed in the least.

43:58.420 --> 44:01.140
Two weeks pass and he has some question.

44:01.140 --> 44:02.980
And I say, well, have you asked chat GPT?

44:02.980 --> 44:04.940
And he's like, dad, Bing is better.

44:06.780 --> 44:07.900
And why is Bing better?

44:07.900 --> 44:09.940
Is because it's built into the browser.

44:09.940 --> 44:11.940
Because he's like, look, I have the Microsoft Edge browser

44:11.940 --> 44:13.220
and like, it's got Bing right here.

44:13.220 --> 44:14.460
And then he doesn't know this yet,

44:14.460 --> 44:17.020
but one of the things you can do with Bing and Edge

44:17.180 --> 44:19.060
is there's a setting where you can use it

44:19.060 --> 44:21.460
to basically talk to any webpage

44:21.460 --> 44:24.420
because it's sitting right there next to the browser.

44:24.420 --> 44:25.940
And by the way, it includes PDF documents.

44:25.940 --> 44:28.180
And so you can, in the way they've implemented

44:28.180 --> 44:29.940
in Edge with Bing is you can load a PDF

44:29.940 --> 44:31.940
and then you can ask it questions,

44:31.940 --> 44:34.980
which is the thing you can't do currently in just chat GPT.

44:34.980 --> 44:38.060
So they're, you know, they're going to push the,

44:38.060 --> 44:38.900
I think that's great.

44:38.900 --> 44:39.820
You know, they're going to push the melding

44:39.820 --> 44:41.980
and see if there's a combination thing there.

44:41.980 --> 44:43.940
Google's rolling out this thing, the magic button,

44:43.940 --> 44:46.620
which is implemented in either put in Google docs, right?

44:46.620 --> 44:48.660
And so you go to, you know, Google docs

44:48.660 --> 44:50.700
and you create a new document and you, you know,

44:50.700 --> 44:52.420
instead of like, you know, starting to type, you just,

44:52.420 --> 44:53.660
you know, say it, press the button

44:53.660 --> 44:56.380
and it starts to like generate content for you, right?

44:56.380 --> 44:59.140
Like, is that the way that it'll work?

44:59.140 --> 45:00.940
Is it going to be a speech UI

45:00.940 --> 45:02.100
where you're just going to have an earpiece

45:02.100 --> 45:03.540
and talk to it all day long?

45:03.540 --> 45:06.340
You know, is it going to be a, like, these are all,

45:06.340 --> 45:08.300
like, this is exactly the kind of thing that I don't,

45:08.300 --> 45:09.260
this is exactly the kind of thing

45:09.260 --> 45:11.020
I don't think is possible to forecast.

45:11.020 --> 45:13.740
I think what we need to do is like run all those experiments.

45:13.740 --> 45:15.500
And so one outcome is we come out of this

45:15.500 --> 45:17.500
like a super browser that has AI built in.

45:17.500 --> 45:19.100
That's just like amazing.

45:19.100 --> 45:21.340
The other, look, there's a real possibility that the whole,

45:21.340 --> 45:22.740
I mean, look, there's a possibility here

45:22.740 --> 45:25.620
that the whole idea of a screen and windows

45:25.620 --> 45:27.220
and all this stuff just goes away.

45:27.220 --> 45:28.460
Cause like, why do you need that?

45:28.460 --> 45:30.060
If you just have a thing that's just telling you

45:30.060 --> 45:31.740
whatever you need to know.

45:31.740 --> 45:35.140
And also, so there's apps that you can use.

45:35.140 --> 45:36.980
You don't really use them, you know,

45:36.980 --> 45:38.900
being a Linux guy and Windows guy.

45:40.580 --> 45:42.860
There's one window, the browser that,

45:42.860 --> 45:44.500
with which you can interact with internet,

45:44.500 --> 45:46.980
but on the phone, you can also have apps.

45:46.980 --> 45:48.900
So I can interact with Twitter through the app

45:48.900 --> 45:50.980
or through the web browser.

45:50.980 --> 45:53.580
And that seems like an obvious distinction,

45:53.580 --> 45:55.940
but why have the web browser in that case?

45:55.940 --> 45:59.340
If one of the apps starts becoming the everything app.

45:59.340 --> 46:00.180
Yeah, that's right.

46:00.180 --> 46:01.300
What Elon's trying to do with Twitter,

46:01.300 --> 46:02.180
but there could be others.

46:02.180 --> 46:03.580
It could be like a Bing app,

46:03.580 --> 46:04.980
but there could be a Google app

46:04.980 --> 46:07.060
that just doesn't really do search,

46:07.060 --> 46:10.340
but just like do what I guess AOL did

46:10.340 --> 46:13.220
back in the day or something, where it's all right there.

46:14.220 --> 46:19.220
And it changes the nature of the internet

46:20.420 --> 46:25.140
because where the content is hosted,

46:25.140 --> 46:27.740
who owns the data, who owns the content,

46:27.740 --> 46:29.620
what is the kind of content you create,

46:29.620 --> 46:31.780
how do you make money by creating content,

46:31.780 --> 46:36.020
who are the content creators, all of that.

46:36.020 --> 46:38.060
Or it could just keep being the same,

46:38.060 --> 46:40.780
which is like, with just the nature of web pages changes

46:40.780 --> 46:41.900
and the nature of content,

46:41.900 --> 46:43.460
but there will still be a web browser.

46:43.460 --> 46:46.140
Cause a web browser is a pretty sexy product.

46:46.140 --> 46:47.540
It just seems to work.

46:47.540 --> 46:49.700
Cause it like, you have an interface,

46:49.700 --> 46:51.140
a window into the world,

46:51.140 --> 46:52.820
and then the world can be anything you want.

46:52.820 --> 46:54.140
And as the world will evolve,

46:54.140 --> 46:55.660
there could be different programming languages,

46:55.660 --> 46:56.500
it can be animated,

46:56.500 --> 46:59.020
maybe it's three dimensional and so on.

46:59.020 --> 47:00.260
Yeah, it's interesting.

47:00.260 --> 47:02.420
Do you think we'll still have the web browser?

47:02.420 --> 47:05.660
Every medium becomes the content for the next one.

47:05.660 --> 47:08.540
So the AI will be able to give you a browser

47:08.540 --> 47:09.980
whenever you want.

47:09.980 --> 47:11.340
Oh, interesting.

47:11.340 --> 47:12.780
Yeah, another way to think about it

47:12.780 --> 47:14.180
is maybe what the browser is.

47:14.180 --> 47:16.260
Maybe it's just the escape hatch, right?

47:16.260 --> 47:18.540
Which is maybe kind of what it is today, right?

47:18.540 --> 47:19.940
Which is like most of what you do

47:19.940 --> 47:21.180
is like inside a social network

47:21.180 --> 47:22.180
or inside a search engine

47:22.180 --> 47:23.460
or inside somebody's app

47:23.460 --> 47:25.500
or inside some controlled experience, right?

47:25.500 --> 47:26.660
But then every once in a while,

47:26.660 --> 47:28.900
there's something where you actually want to jailbreak.

47:28.900 --> 47:30.180
You want to actually get free.

47:30.180 --> 47:32.380
The web browser is the F you to the man.

47:32.380 --> 47:34.820
You're allowed to, that's the free internet.

47:34.820 --> 47:35.660
Yeah.

47:35.660 --> 47:37.420
Back in the way it was in the nineties.

47:37.420 --> 47:38.260
So here's something I'm proud of.

47:38.260 --> 47:39.100
So nobody really talks about,

47:39.100 --> 47:39.940
here's something I'm proud of,

47:40.100 --> 47:41.900
the web, the browser, the web servers,

47:41.900 --> 47:42.940
they're still backward compatible

47:42.940 --> 47:44.780
all the way back to like 1992, right?

47:44.780 --> 47:47.900
So like you can put up a, you can still,

47:47.900 --> 47:49.580
you know, the big breakthrough of the web early on,

47:49.580 --> 47:51.660
the big breakthrough was it made it really easy to read

47:51.660 --> 47:53.220
but it also made it really easy to write.

47:53.220 --> 47:54.260
It made it really easy to publish.

47:54.260 --> 47:56.220
And we literally made it so easy to publish.

47:56.220 --> 47:58.500
We made it not only so easy to publish content,

47:58.500 --> 48:01.340
it was actually also easy to actually write a web server,

48:01.340 --> 48:02.180
right?

48:02.180 --> 48:03.020
And you can literally write a web server

48:03.020 --> 48:04.020
in four lines of real code

48:04.020 --> 48:05.900
and you could start publishing content on it.

48:05.900 --> 48:08.020
And you could set whatever rules you want for the content,

48:08.020 --> 48:10.020
whatever censorship, no censorship, whatever you want.

48:10.020 --> 48:11.180
You could just do that.

48:11.180 --> 48:12.900
As long as you had an IP address, right?

48:12.900 --> 48:14.140
You could do that.

48:14.140 --> 48:16.180
That still works, right?

48:16.180 --> 48:18.660
Like that still works exactly as I just described.

48:18.660 --> 48:21.060
So this is part of my reaction to all of this,

48:21.060 --> 48:23.060
like, you know, all this just censorship pressure

48:23.060 --> 48:24.660
and all this, you know, these issues around control

48:24.660 --> 48:25.860
and all this stuff, which is like,

48:25.860 --> 48:27.940
maybe we need to get back a little bit more

48:27.940 --> 48:29.140
to the Wild West.

48:29.140 --> 48:31.020
Like the Wild West is still out there.

48:31.020 --> 48:33.300
Now they will try to chase you down.

48:33.300 --> 48:34.980
Like they'll try to, you know, people who want to censor

48:34.980 --> 48:37.540
will try to take away your, you know, your domain name

48:37.540 --> 48:38.780
and they'll try to take away your payments account

48:38.780 --> 48:41.060
and so forth if they really don't like what you're saying.

48:41.060 --> 48:43.780
But nevertheless, like unless they literally

48:43.780 --> 48:45.220
are intercepting you at the ISP level,

48:45.220 --> 48:47.620
like you can still put up a thing.

48:47.620 --> 48:48.460
And so, I don't know.

48:48.460 --> 48:50.020
I think that's important to preserve, right?

48:50.020 --> 48:53.860
Like, because, I mean, one is just a freedom argument,

48:53.860 --> 48:55.620
but the other is a creativity argument,

48:55.620 --> 48:57.100
which is you want to have the escape hatch

48:57.100 --> 48:59.420
so that the kid with the idea is able to realize the idea.

48:59.420 --> 49:00.860
Cause to your point on PageRank,

49:00.860 --> 49:03.700
you actually don't know what the next big idea is, right?

49:03.700 --> 49:05.780
Nobody called Larry Page and told him to develop PageRank.

49:05.780 --> 49:06.940
Like he came up with that on his own.

49:07.020 --> 49:08.620
And you want to always, I think,

49:08.620 --> 49:10.300
leave the escape hatch for the next, you know,

49:10.300 --> 49:11.740
kid or the next Stanford grad student

49:11.740 --> 49:12.860
to have the breakthrough idea

49:12.860 --> 49:13.780
and be able to get it up and running

49:13.780 --> 49:14.980
before anybody notices.

49:15.980 --> 49:17.620
You and I are both fans of history.

49:17.620 --> 49:18.940
So let's step back.

49:18.940 --> 49:20.140
We've been talking about the future.

49:20.140 --> 49:24.300
Let's step back for a bit and look at the 90s.

49:24.300 --> 49:26.340
You created Mosaic web browser,

49:26.340 --> 49:28.460
the first widely used web browser.

49:28.460 --> 49:30.060
Tell the story of that.

49:30.060 --> 49:32.380
And how did it evolve into Netscape Navigator?

49:32.380 --> 49:34.220
Just the early days.

49:34.220 --> 49:35.140
So full story.

49:35.140 --> 49:36.340
So-

49:36.340 --> 49:37.180
You were born.

49:37.180 --> 49:39.660
I was born, a small child.

49:39.660 --> 49:41.900
Well, actually, yeah, let's go there.

49:41.900 --> 49:45.180
Like when did you first fall in love with computers?

49:45.180 --> 49:47.700
Oh, so I hit the generational jackpot

49:47.700 --> 49:50.180
and I hit the GenX kind of point perfectly as it turns out.

49:50.180 --> 49:51.500
So I was born in 1971.

49:51.500 --> 49:55.740
So there's this great website called wtfhappenin1971.com,

49:55.740 --> 49:56.580
which is basically in 1971.

49:56.580 --> 49:57.860
That's when everything started to go to hell.

49:57.860 --> 49:59.620
And I was of course born in 1971.

49:59.620 --> 50:01.620
So I like to think that I had something to do with that.

50:01.620 --> 50:03.860
Did you make it on the website?

50:03.860 --> 50:05.060
I don't think I made it on the website,

50:05.940 --> 50:06.780
but hopefully-

50:06.780 --> 50:07.620
Somebody needs to add.

50:07.620 --> 50:08.780
This is where everything-

50:08.780 --> 50:12.380
Maybe I contributed to some of the trends that they do.

50:12.380 --> 50:14.220
Every line on that website goes like that, right?

50:14.220 --> 50:16.500
So it's all a picture disaster.

50:16.500 --> 50:19.260
But there was this moment in time where,

50:19.260 --> 50:21.220
cause the sort of the Apple,

50:21.220 --> 50:22.940
the Apple II hit in like 1978

50:22.940 --> 50:24.780
and then the IBM PC hit in 82.

50:24.780 --> 50:27.580
So I was like 11 when the PC came out.

50:27.580 --> 50:29.660
And so I just kind of hit that perfectly.

50:29.660 --> 50:31.020
And then that was the first moment in time

50:31.020 --> 50:32.980
when like regular people could spend a few hundred dollars

50:32.980 --> 50:33.900
and get a computer, right?

50:33.900 --> 50:36.580
So that resonated right out of the gate.

50:38.180 --> 50:39.340
And then the other part of the story is,

50:39.340 --> 50:40.620
I was using an Apple II,

50:40.620 --> 50:42.260
I used a bunch of them, but I was using Apple II.

50:42.260 --> 50:43.980
And of course it's set on the back of every Apple II

50:43.980 --> 50:47.420
and every Mac it's had designed in Cupertino, California.

50:47.420 --> 50:49.420
And I was like, wow, Cupertino must be the like

50:49.420 --> 50:50.420
shining city on the hill.

50:50.420 --> 50:53.060
Like Cupertino is like the most amazing city of all time.

50:53.060 --> 50:53.900
I can't wait to see it.

50:53.900 --> 50:56.900
And of course, years later I came out to Silicon Valley

50:56.900 --> 50:59.780
and went to Cupertino and it's just a bunch of office parks

50:59.780 --> 51:01.700
and low rise apartment buildings.

51:01.700 --> 51:03.380
So the aesthetics were a little disappointing,

51:03.380 --> 51:06.380
but you know, it was the vector, right?

51:06.380 --> 51:09.020
Of the creation of a lot of this stuff.

51:09.020 --> 51:09.860
Yeah.

51:09.860 --> 51:13.740
So then basically, so part of my story is just the luck

51:13.740 --> 51:14.860
of having been born at the right time

51:14.860 --> 51:16.220
and getting exposed to PCs then.

51:16.220 --> 51:18.940
The other part is when Al Gore says

51:18.940 --> 51:21.580
that he created the internet, he actually is correct

51:21.580 --> 51:22.980
in a really meaningful way,

51:22.980 --> 51:24.900
which is he sponsored a bill in 1985

51:24.900 --> 51:26.820
that essentially created the modern internet,

51:26.820 --> 51:29.340
created what is called the NSF net at the time,

51:29.340 --> 51:32.180
which is sort of the first really fast internet backbone.

51:33.060 --> 51:35.780
And, you know, that bill dumped a ton of money

51:35.780 --> 51:37.260
into a bunch of research universities

51:37.260 --> 51:39.220
to build out basically the internet backbone

51:39.220 --> 51:40.580
and then the supercomputer centers

51:40.580 --> 51:43.180
that were clustered around the internet.

51:43.180 --> 51:45.740
And one of those universities was University of Illinois,

51:45.740 --> 51:46.580
where I went to school.

51:46.580 --> 51:47.700
And so the other stroke of luck that I had

51:47.700 --> 51:49.260
was I went to Illinois basically, right?

51:49.260 --> 51:51.900
As that money was just like getting dumped on campus.

51:51.900 --> 51:54.020
And so as a consequence we had on campus,

51:54.020 --> 51:57.260
and this is like, you know, 89, 90, 91,

51:57.260 --> 51:59.020
we had like, you know, we were right on the internet backbone.

51:59.020 --> 52:00.980
We had like T3 and 45, at the time T3,

52:00.980 --> 52:02.420
45 megabit backbone connection,

52:02.420 --> 52:05.140
which at the time was, you know, wildly state of the art.

52:05.140 --> 52:06.500
We had crazy supercomputers.

52:06.500 --> 52:08.700
We had thinking machines, parallel supercomputers.

52:08.700 --> 52:10.260
We had silicon graphics workstations.

52:10.260 --> 52:11.540
We had Macintoshes.

52:11.540 --> 52:13.620
We had next cubes all over the place.

52:13.620 --> 52:15.580
We had like every possible kind of computer you could imagine

52:15.580 --> 52:17.660
because all this money just fell out of the sky.

52:18.540 --> 52:19.980
So you were living in the future.

52:19.980 --> 52:22.820
Yeah, so quite literally it was, yeah, like it's all there.

52:22.820 --> 52:24.380
It's all like we had full broadband graphics,

52:24.380 --> 52:25.540
like the whole thing.

52:25.540 --> 52:27.420
And it's actually funny because they had this,

52:27.420 --> 52:28.860
this is the first time I kind of,

52:28.860 --> 52:30.300
it sort of tickled the back of my head

52:30.300 --> 52:31.980
that there might be a big opportunity in here,

52:31.980 --> 52:33.660
which is, you know, they embraced it.

52:33.660 --> 52:35.420
And so they put like computers in all the dorms

52:35.420 --> 52:36.780
and they wired up all the dorm rooms

52:36.780 --> 52:38.940
and they had all these labs everywhere and everything.

52:38.940 --> 52:41.260
And then they gave every undergrad

52:41.260 --> 52:43.340
a computer account and an email address.

52:44.500 --> 52:47.060
And the assumption was that you would use the internet

52:47.060 --> 52:48.780
for your four years at college

52:48.780 --> 52:51.220
and then you would graduate and stop using it.

52:52.420 --> 52:54.100
And that was that, right?

52:54.100 --> 52:55.380
And you would just retire your email address.

52:55.380 --> 52:56.220
It wouldn't be relevant anymore

52:56.220 --> 52:57.260
because you'd go off in the workplace

52:57.260 --> 52:58.820
and they don't use email.

52:58.820 --> 53:01.060
You'd be back to using fax machines or whatever.

53:01.060 --> 53:01.900
Did you have that sense as well?

53:01.900 --> 53:04.700
Like what, you said the back of your head was tickled.

53:04.700 --> 53:06.060
Like what was your,

53:06.060 --> 53:08.300
what was exciting to you about this possible world?

53:08.300 --> 53:10.340
Well, if this is so useful in this containment,

53:10.340 --> 53:11.980
if this is so useful in this contained environment

53:11.980 --> 53:14.020
that just has this weird source of outside funding,

53:14.020 --> 53:17.140
then if it were practical for everybody else to have this,

53:17.140 --> 53:19.020
and if it were cost-effective for everybody else to have this,

53:19.020 --> 53:20.300
wouldn't they want it?

53:20.300 --> 53:22.580
And overwhelmingly the prevailing view at the time was,

53:22.580 --> 53:23.540
no, they would not want it.

53:23.540 --> 53:26.060
This is esoteric, weird nerd stuff, right?

53:26.060 --> 53:27.380
That like computer science kids like,

53:27.380 --> 53:29.740
but like normal people are never gonna do email, right?

53:29.740 --> 53:31.580
Or be on the internet, right?

53:31.580 --> 53:33.060
And so I was just like, wow, like this,

53:33.060 --> 53:35.740
this is actually like, this is really compelling stuff.

53:35.740 --> 53:37.940
Now, the other part was it was all really hard to use.

53:37.940 --> 53:40.580
And in practice, you had to be basically a CS.

53:40.580 --> 53:42.340
You basically had to be a CS undergrad

53:42.340 --> 53:44.260
or equivalent to actually get full use

53:44.260 --> 53:45.380
of the internet at that point

53:45.380 --> 53:47.220
because it was all pretty esoteric stuff.

53:47.220 --> 53:48.660
So then that was the other part of the idea,

53:48.660 --> 53:51.380
which was, okay, we need to actually make this easy to use.

53:51.380 --> 53:53.180
So what's involved in creating mosaic,

53:53.180 --> 53:57.700
like in creating a graphical interface to the internet?

53:57.700 --> 53:59.020
Yeah, so it was a combination of things.

53:59.020 --> 54:01.380
So it was like, basically the web existed

54:01.380 --> 54:03.820
in an early sort of described as prototype form.

54:03.820 --> 54:05.940
And by the way, text only at that point.

54:05.940 --> 54:06.820
What did it look like?

54:06.820 --> 54:07.700
What was the web?

54:07.700 --> 54:11.500
I mean, and the key figures, like, what was it like?

54:11.500 --> 54:13.140
What made a picture?

54:13.140 --> 54:16.500
It looked like JetGPT actually, it was all text.

54:16.500 --> 54:17.580
Yeah.

54:17.580 --> 54:20.060
And so you had a text-based web browser.

54:20.060 --> 54:21.780
Well, actually the original browser, Tim Berners-Lee,

54:21.900 --> 54:24.300
both the original browser and the server

54:24.300 --> 54:26.220
actually ran on NextCubes.

54:26.220 --> 54:28.580
So this was the computer Steve Jobs made

54:28.580 --> 54:29.940
during the interim period,

54:29.940 --> 54:31.260
during the decade-long interim period

54:31.260 --> 54:32.940
when he was not at Apple.

54:32.940 --> 54:36.460
He got fired in 85 and then came back in 97.

54:36.460 --> 54:37.700
So this was in that interim period

54:37.700 --> 54:38.860
where he had this company called Next

54:38.860 --> 54:39.700
and they made these,

54:39.700 --> 54:41.860
literally these computers called Cubes.

54:41.860 --> 54:43.300
And there's this famous story, they were beautiful,

54:43.300 --> 54:46.540
but they were 12 inch by 12 inch by 12 inch Cubes computers.

54:46.540 --> 54:47.380
And there's a famous story

54:47.380 --> 54:48.780
about how they could have cost half as much

54:48.780 --> 54:50.500
if it had been 12 by 12 by 13.

54:51.180 --> 54:54.140
Steve was like, no, it has to be.

54:54.140 --> 54:56.860
So they were like $6,000 basically academic workstations.

54:56.860 --> 55:00.060
They had the first CD-ROM drives, which were slow.

55:00.060 --> 55:03.060
I mean, the computers are all but unusable.

55:03.060 --> 55:05.180
They were so slow, but they were beautiful.

55:05.180 --> 55:07.420
Okay, can we actually just take a tiny tangent there?

55:07.420 --> 55:09.180
Sure, of course.

55:09.180 --> 55:11.380
The 12 by 12 by 12,

55:11.380 --> 55:13.980
they just so beautifully encapsulates

55:13.980 --> 55:16.060
Steve Jobs' idea of design.

55:16.060 --> 55:19.780
Can you just comment on what you find interesting

55:19.780 --> 55:23.140
about Steve Jobs, about that view of the world,

55:23.140 --> 55:25.780
that dogmatic pursuit of perfection

55:25.780 --> 55:28.420
in how he saw perfection in design?

55:28.420 --> 55:29.500
Yeah, so I guess I'd say like, look,

55:29.500 --> 55:32.300
he was a deep believer, I think in a very deep way.

55:32.300 --> 55:33.300
I interpret it.

55:33.300 --> 55:34.780
I don't know if you ever really describe it like this,

55:34.780 --> 55:35.820
but the way I'd interpret it is,

55:35.820 --> 55:38.780
it's like this thing, it's actually a thing in philosophy.

55:38.780 --> 55:41.180
It's like aesthetics are not just appearances.

55:41.180 --> 55:44.340
Aesthetics go all the way to like deep underlying meaning.

55:44.340 --> 55:46.980
It's like, I'm not a physicist.

55:46.980 --> 55:48.140
One of the things I've heard physicists say

55:48.140 --> 55:49.420
is one of the things you start to get

55:49.780 --> 55:50.700
a sense of when a theory might be correct

55:50.700 --> 55:51.540
is when it's beautiful.

55:51.540 --> 55:55.060
And so there's something,

55:55.060 --> 55:56.420
and you feel the same thing, by the way,

55:56.420 --> 56:00.940
in human psychology, when you're experiencing awe,

56:00.940 --> 56:03.380
there's a simplicity to it.

56:03.380 --> 56:05.100
When you're having an honest interaction with somebody,

56:05.100 --> 56:07.940
there's an aesthetic, it was like calm comes over you

56:07.940 --> 56:09.020
because you're actually being fully honest

56:09.020 --> 56:10.060
and trying to hide yourself.

56:10.060 --> 56:13.900
So it's like this very deep sense of aesthetics.

56:13.900 --> 56:17.300
And he would trust that judgment that he had deep down.

56:17.540 --> 56:19.620
Even if the engineering teams are saying

56:19.620 --> 56:22.700
this is too difficult,

56:22.700 --> 56:25.460
even if the whatever the finance folks are saying,

56:25.460 --> 56:27.740
this is ridiculous, the supply chain,

56:27.740 --> 56:29.900
all that kind of stuff, this makes this impossible.

56:29.900 --> 56:31.980
We can't do this kind of material.

56:31.980 --> 56:34.500
This has never been done before and so on and so forth.

56:34.500 --> 56:35.900
He just sticks by it.

56:35.900 --> 56:37.740
Well, I mean, who makes a phone out of aluminum, right?

56:37.740 --> 56:41.620
Like nobody else would have done that.

56:41.620 --> 56:44.220
And now, of course, if your phone is made out of aluminum,

56:44.220 --> 56:46.420
how crude, what kind of caveman would you have to be

56:46.420 --> 56:48.020
to have a phone that's made out of plastic?

56:48.020 --> 56:50.620
So it's just this very, right?

56:50.620 --> 56:53.020
And look, there's a thousand different ways to look at this,

56:53.020 --> 56:54.380
but one of the things is just like,

56:54.380 --> 56:56.020
look, these things are essential to your life.

56:56.020 --> 56:57.660
Like you're with your phone more

56:57.660 --> 56:58.740
than you're with anything else.

56:58.740 --> 57:00.020
Like it's gonna be in your hand.

57:00.020 --> 57:02.580
I mean, he thought very deeply about what it meant

57:02.580 --> 57:04.300
for something to be in your hand all day long.

57:04.300 --> 57:07.380
Well, for example, here's an interesting design thing.

57:07.380 --> 57:09.660
Like he never wanted, my understanding is

57:09.660 --> 57:12.460
he never wanted an iPhone to have a screen larger

57:12.460 --> 57:15.500
than you could reach with your thumb one-handed.

57:15.500 --> 57:17.380
And so he was actually opposed to the idea

57:17.380 --> 57:18.340
of making the phones larger.

57:18.340 --> 57:19.900
And I don't know if you have this experience today,

57:19.900 --> 57:21.780
but let's say there are certain moments in your day

57:21.780 --> 57:24.540
when you might be like only have one hand available

57:24.540 --> 57:26.140
and you might wanna be on your phone.

57:26.140 --> 57:29.020
And you're trying to like set the text

57:29.020 --> 57:31.020
and your thumb can't reach the send button.

57:31.020 --> 57:32.620
Yeah, I mean, there's pros and cons, right?

57:32.620 --> 57:33.900
And then there's like folding phones,

57:33.900 --> 57:37.220
which I would love to know what he thinks about them.

57:37.220 --> 57:39.980
But I mean, is there something you could also just link on

57:39.980 --> 57:43.460
because he's one of the interesting figures

57:43.460 --> 57:45.380
in the history of technology.

57:46.380 --> 57:48.900
What makes him as successful as he was?

57:48.900 --> 57:51.220
What makes him as interesting as he was?

57:51.220 --> 57:54.660
What made him so productive and important

57:54.660 --> 57:58.060
in the development of technology?

57:58.060 --> 57:59.300
He had an integrated worldview.

57:59.300 --> 58:01.700
So the properly designed device

58:01.700 --> 58:03.180
that had the correct functionality,

58:03.180 --> 58:05.420
that had the deepest understanding of the user,

58:05.420 --> 58:07.500
that was the most beautiful, right?

58:07.500 --> 58:10.820
Like it had to be all of those things, right?

58:10.820 --> 58:12.580
He basically would drive to as close to perfect

58:12.580 --> 58:13.820
as you could possibly get, right?

58:14.580 --> 58:16.820
I suspect that he never quite thought he ever got there

58:16.820 --> 58:19.220
because most great creators are generally dissatisfied.

58:19.220 --> 58:20.700
You read accounts later on

58:20.700 --> 58:22.500
and all they can see are the flaws in their creation.

58:22.500 --> 58:24.420
But like he got as close to perfect each step of the way

58:24.420 --> 58:25.940
as he could possibly get

58:25.940 --> 58:29.340
with the constraints of the technology of his time.

58:29.340 --> 58:31.780
And then, look, he was sort of famous in the Apple model.

58:31.780 --> 58:34.940
It's like, look, this headset that they just came out with,

58:34.940 --> 58:37.820
like it's like a decade long project, right?

58:37.820 --> 58:40.060
It's like, and they're just gonna sit there and tune

58:40.060 --> 58:41.980
and tune and polish and polish and tune and polish

58:41.980 --> 58:43.660
and tune and polish until it is as perfect

58:43.660 --> 58:45.620
as anybody could possibly make anything.

58:45.620 --> 58:47.180
And then this goes to the way

58:47.180 --> 58:48.580
that people describe working with him,

58:48.580 --> 58:51.460
which is there was a terrifying aspect of working with him,

58:51.460 --> 58:53.340
which is he was very tough.

58:54.220 --> 58:56.300
But there was this thing that everybody

58:56.300 --> 58:58.740
I've ever talked to work for him says,

58:58.740 --> 58:59.860
they all say the following,

58:59.860 --> 59:02.180
which is we did the best work of our lives

59:02.180 --> 59:03.060
when we worked for him

59:03.060 --> 59:04.740
because he set the bar incredibly high

59:04.740 --> 59:06.540
and then he supported us with everything that he could

59:06.540 --> 59:08.580
to let us actually do work of that quality.

59:08.580 --> 59:10.100
So a lot of people who were at Apple

59:10.100 --> 59:11.100
spend the rest of their lives

59:11.100 --> 59:12.340
trying to find another experience

59:12.340 --> 59:13.180
where they feel like they're able

59:13.180 --> 59:14.540
to hit that quality bar again.

59:14.540 --> 59:18.140
Even if it in retrospect or during it felt like suffering.

59:18.140 --> 59:18.980
Yeah, exactly.

59:20.180 --> 59:24.180
What does that teach you about the human condition, huh?

59:24.180 --> 59:26.580
So look, exactly.

59:26.580 --> 59:29.020
So the Silicon Valley, I mean, look, he's not, you know,

59:29.020 --> 59:31.380
George Patton in the army, like, you know,

59:31.380 --> 59:33.140
there are many examples in other fields, you know,

59:33.140 --> 59:34.140
that are like this.

59:37.740 --> 59:39.580
Specifically in tech, it's actually,

59:39.580 --> 59:40.460
I find it very interesting.

59:40.460 --> 59:42.860
There's the Apple way, which is polish, polish, polish,

59:42.860 --> 59:44.100
and don't ship until it's as perfect

59:44.100 --> 59:44.940
because you can make it.

59:44.940 --> 59:47.180
And then there's the sort of the other approach,

59:47.180 --> 59:49.980
which is the sort of incremental hacker mentality,

59:49.980 --> 59:52.500
which basically says ship early and often and iterate.

59:52.500 --> 59:53.900
And one of the things I find really interesting

59:53.900 --> 59:55.580
is I'm now 30 years into this,

59:55.580 --> 59:58.580
like there are very successful companies

59:58.580 --> 01:00:01.020
on both sides of that approach, right?

01:00:01.020 --> 01:00:04.580
Like that is a fundamental difference, right?

01:00:04.580 --> 01:00:05.940
In how to operate and how to build

01:00:05.940 --> 01:00:08.780
and how to create that you have world-class companies

01:00:08.820 --> 01:00:10.820
operating in both ways.

01:00:10.820 --> 01:00:12.460
And I don't think the question of like,

01:00:12.460 --> 01:00:13.580
which is the superior model

01:00:13.580 --> 01:00:15.500
is anywhere close to being answered.

01:00:15.500 --> 01:00:18.260
Like, and my suspicion is the answer is do both.

01:00:18.260 --> 01:00:20.060
The answer is you actually want both.

01:00:20.060 --> 01:00:21.500
They lead to different outcomes.

01:00:21.500 --> 01:00:25.220
Software tends to do better with the iterative approach.

01:00:25.220 --> 01:00:28.180
Hardware tends to do better with the, you know,

01:00:28.180 --> 01:00:29.940
sort of wait and make it perfect approach.

01:00:29.940 --> 01:00:33.780
But again, you can find examples in both directions.

01:00:33.780 --> 01:00:36.540
Oh, so the jury's still out on that one.

01:00:36.540 --> 01:00:37.460
So back to Mosaic.

01:00:37.460 --> 01:00:42.460
So what, it was text-based, Tim Berners-Lee.

01:00:44.020 --> 01:00:45.620
Well, there was the web, which was text-based,

01:00:45.620 --> 01:00:47.900
but there were no, I mean, there was like three websites.

01:00:47.900 --> 01:00:49.340
There was like no content.

01:00:49.340 --> 01:00:50.260
There were no users.

01:00:50.260 --> 01:00:52.220
Like it wasn't like a catalytic.

01:00:52.220 --> 01:00:53.660
It hadn't, and by the way, it was all,

01:00:53.660 --> 01:00:55.020
because it was all text, there were no documents,

01:00:55.020 --> 01:00:56.180
there were no images, there were no videos,

01:00:56.180 --> 01:00:57.460
there were no, right?

01:00:57.460 --> 01:01:00.180
So it was, and then if in the beginning,

01:01:00.180 --> 01:01:01.620
if you had to be on a NextCube,

01:01:01.620 --> 01:01:02.780
but you need to have a NextCube

01:01:02.780 --> 01:01:04.060
both to publish and to consume.

01:01:04.060 --> 01:01:04.900
So there were-

01:01:04.900 --> 01:01:06.580
There were 6,000 bucks, you said?

01:01:06.580 --> 01:01:08.780
There were limitations, like, yeah, $6,000 PC.

01:01:08.780 --> 01:01:10.860
They did not sell very many.

01:01:10.860 --> 01:01:13.300
But then there was also, there was also FTP

01:01:13.300 --> 01:01:14.620
and there was Usenet, right?

01:01:14.620 --> 01:01:16.860
And there was, you know, a dozen other basically,

01:01:16.860 --> 01:01:18.780
there's Waze, which was an early search thing.

01:01:18.780 --> 01:01:20.460
There was Gopher, which was an early menu-based

01:01:20.460 --> 01:01:21.740
information retrieval system.

01:01:21.740 --> 01:01:24.420
There were like a dozen different sort of scattered ways

01:01:24.420 --> 01:01:26.420
that people would get to information on the internet.

01:01:26.420 --> 01:01:28.300
And so the Mosaic idea was basically

01:01:28.300 --> 01:01:30.180
bring those all together, make the whole thing graphical,

01:01:30.180 --> 01:01:32.540
make it easy to use, make it basically bulletproof

01:01:32.540 --> 01:01:34.140
so that anybody can do it.

01:01:34.140 --> 01:01:35.500
And then again, just on the luck side,

01:01:35.540 --> 01:01:37.300
it so happened that this was right at the moment

01:01:37.300 --> 01:01:40.180
when graphics, when the GUI sort of actually took off.

01:01:40.180 --> 01:01:41.860
And we're now also used to the GUI

01:01:41.860 --> 01:01:44.620
that we think it's been around forever, but it didn't really,

01:01:44.620 --> 01:01:47.540
you know, the Macintosh brought it out in 85,

01:01:47.540 --> 01:01:49.900
but they actually didn't sell very many Macs in the 80s.

01:01:49.900 --> 01:01:52.420
It was not that successful of a product.

01:01:52.420 --> 01:01:55.220
It really was, you needed Windows 3.0 on PCs

01:01:55.220 --> 01:01:57.900
and that hit in about 92.

01:01:57.900 --> 01:02:00.060
And so, and we did Mosaic in 92, 93.

01:02:00.060 --> 01:02:02.220
So that sort of, it was like right at the moment

01:02:02.220 --> 01:02:03.900
when you could imagine actually having

01:02:03.900 --> 01:02:07.140
a graphical user interface, right, at all,

01:02:07.140 --> 01:02:08.540
much less one to the internet.

01:02:08.540 --> 01:02:11.740
How old did Windows 3 sell?

01:02:11.740 --> 01:02:13.260
So was that the really big?

01:02:13.260 --> 01:02:14.100
That was the big bang.

01:02:14.100 --> 01:02:16.740
The big operating, graphical operating system.

01:02:16.740 --> 01:02:17.740
Well, this is the classic, okay,

01:02:17.740 --> 01:02:19.660
Microsoft was operating on the other.

01:02:19.660 --> 01:02:21.780
So Steve, Apple was running on the Polish

01:02:21.780 --> 01:02:22.620
until it was perfect.

01:02:22.620 --> 01:02:23.900
Microsoft famously ran on the other model,

01:02:23.900 --> 01:02:24.780
which is ship and iterate.

01:02:24.780 --> 01:02:27.140
And so the old line in those days was Microsoft,

01:02:27.140 --> 01:02:28.740
right, it's the version three of every Microsoft product.

01:02:28.740 --> 01:02:29.860
That's the good one, right?

01:02:29.860 --> 01:02:32.140
And so there are, you can find online

01:02:32.140 --> 01:02:34.380
Windows 1, Windows 2, nobody used them.

01:02:34.380 --> 01:02:35.860
Actually, the original Windows,

01:02:35.860 --> 01:02:37.100
in the original Microsoft Windows,

01:02:37.100 --> 01:02:38.900
the Windows were not overlapping.

01:02:38.900 --> 01:02:41.100
And so you had these very small,

01:02:41.100 --> 01:02:42.380
very low resolution screens,

01:02:42.380 --> 01:02:45.620
and then you had literally, it just didn't work.

01:02:45.620 --> 01:02:46.660
It wasn't ready yet.

01:02:46.660 --> 01:02:49.860
And Windows 95, I think, was a pretty big leap also.

01:02:49.860 --> 01:02:51.020
That was a big leap too, yeah.

01:02:51.020 --> 01:02:52.900
So that was like bang, bang.

01:02:52.900 --> 01:02:55.860
And then of course, Steve, and then in the fullness of time,

01:02:55.860 --> 01:02:57.700
Steve came back, then the Mac started to take off again.

01:02:57.700 --> 01:02:58.540
That was the third bang.

01:02:58.540 --> 01:03:00.340
And then the iPhone was the fourth bang.

01:03:00.340 --> 01:03:01.420
Such exciting times.

01:03:01.420 --> 01:03:03.060
And then we were off to the races.

01:03:03.060 --> 01:03:05.620
Cause nobody could have known

01:03:05.620 --> 01:03:06.900
what would be created from that.

01:03:06.900 --> 01:03:09.100
Well, Windows 3.1 or 3.0,

01:03:09.100 --> 01:03:13.060
Windows 3.0 to the iPhone was only 15 years, right?

01:03:13.060 --> 01:03:15.340
Like that ramp was, in retrospect,

01:03:15.340 --> 01:03:16.700
at the time it felt like it took forever,

01:03:16.700 --> 01:03:18.420
but in historical terms,

01:03:18.420 --> 01:03:19.700
like that was a very fast ramp

01:03:19.700 --> 01:03:22.220
from even a graphical computer at all on your desk

01:03:22.220 --> 01:03:24.820
to the iPhone, it was 15 years.

01:03:24.820 --> 01:03:26.820
Did you have a sense of what the internet will be

01:03:26.820 --> 01:03:28.500
as you're looking through the window of Mosaic?

01:03:28.500 --> 01:03:33.500
Like there's just a few webpages for now.

01:03:33.940 --> 01:03:36.460
So the thing I had early on was,

01:03:36.460 --> 01:03:38.740
I was keeping at the time what,

01:03:38.740 --> 01:03:40.420
there's disputes over what was the first blog,

01:03:40.420 --> 01:03:44.460
but I had one of them that at least is a possible,

01:03:44.460 --> 01:03:45.820
at least a runner up in the competition.

01:03:45.820 --> 01:03:48.220
And it was what was called the What's New page.

01:03:49.540 --> 01:03:51.620
And it was hardwired.

01:03:51.620 --> 01:03:53.420
I had distribution unfair advantage.

01:03:53.420 --> 01:03:55.740
I put it right in the browser.

01:03:55.740 --> 01:03:56.580
I put it in the browser

01:03:56.580 --> 01:03:57.660
and then I put my resume in the browser,

01:03:57.660 --> 01:03:58.500
which also was,

01:03:58.500 --> 01:03:59.340
what was it, what was it, what was it?

01:03:59.340 --> 01:04:00.180
It's hilarious.

01:04:00.180 --> 01:04:04.940
But, but I was keeping the,

01:04:04.940 --> 01:04:06.260
not many people get to get to do that.

01:04:06.260 --> 01:04:07.100
So the-

01:04:07.100 --> 01:04:07.940
No.

01:04:07.940 --> 01:04:08.780
The-

01:04:08.780 --> 01:04:11.300
Good, good call.

01:04:11.300 --> 01:04:12.140
Yeah.

01:04:12.140 --> 01:04:12.980
In early days.

01:04:12.980 --> 01:04:13.820
Yes.

01:04:13.820 --> 01:04:14.660
It's so interesting.

01:04:14.660 --> 01:04:16.060
I'm looking for my about, about,

01:04:16.060 --> 01:04:17.620
oh, Mark is looking for a job.

01:04:17.620 --> 01:04:18.460
Yeah, exactly.

01:04:18.460 --> 01:04:22.060
So, so the What's New page,

01:04:22.060 --> 01:04:23.420
I would literally get up every morning

01:04:23.420 --> 01:04:25.420
and I would every afternoon.

01:04:25.420 --> 01:04:26.620
And I would basically,

01:04:26.860 --> 01:04:28.380
if you wanted to launch a website,

01:04:28.380 --> 01:04:29.540
you would email me

01:04:29.540 --> 01:04:31.020
and I would list it on the What's New page.

01:04:31.020 --> 01:04:33.060
And that was how people discovered the new websites

01:04:33.060 --> 01:04:34.020
as they were coming out.

01:04:34.020 --> 01:04:35.740
And I remember, cause it was like one,

01:04:35.740 --> 01:04:36.580
it literally went from,

01:04:36.580 --> 01:04:38.460
it was like one every couple of days

01:04:38.460 --> 01:04:40.300
to like one every day,

01:04:40.300 --> 01:04:42.900
to like two every day, boom, boom, boom.

01:04:42.900 --> 01:04:43.740
So you're doing it,

01:04:43.740 --> 01:04:46.020
so that, that blog was kind of doing the directory thing.

01:04:46.020 --> 01:04:48.380
So like, what was the homepage?

01:04:48.380 --> 01:04:50.140
So the homepage was just basically trying to explain

01:04:50.140 --> 01:04:51.980
even what this thing is that you're looking at, right?

01:04:51.980 --> 01:04:54.580
The basic, basically basic instructions.

01:04:54.580 --> 01:04:55.420
But then there was a button,

01:04:55.420 --> 01:04:56.540
there was a button that said What's New.

01:04:56.540 --> 01:04:57.940
And what most people did was they went to,

01:04:57.940 --> 01:04:59.660
for obvious reasons, went to What's New.

01:04:59.660 --> 01:05:03.060
But like, it was so, it was so mind blowing at that point,

01:05:03.060 --> 01:05:04.300
just the basic idea.

01:05:04.300 --> 01:05:05.140
And it was just, it was just like,

01:05:05.140 --> 01:05:06.340
you know, this was the basic idea of the internet,

01:05:06.340 --> 01:05:08.100
but people could see it for the first time.

01:05:08.100 --> 01:05:10.180
The basic idea was, look, you know, some, you know,

01:05:10.180 --> 01:05:11.860
it's like literally, it's like an Indian restaurant

01:05:11.860 --> 01:05:14.540
in like Bristol, England has like put their menu on the web

01:05:14.540 --> 01:05:17.540
and people were like, wow.

01:05:17.540 --> 01:05:20.820
Cause like, that's the first restaurant menu on the web.

01:05:20.820 --> 01:05:22.060
And I don't have to be in Bristol.

01:05:22.060 --> 01:05:23.340
And I don't know if I'm ever gonna go to Bristol

01:05:23.340 --> 01:05:26.060
and I don't even like Indian food and like, wow.

01:05:26.820 --> 01:05:28.580
And it was like that, the first web,

01:05:28.580 --> 01:05:31.180
the first streaming video thing was a,

01:05:31.180 --> 01:05:34.540
it was another England thing, some Oxford or something.

01:05:34.540 --> 01:05:37.060
Some guy puts his coffee pot up

01:05:37.060 --> 01:05:40.020
as the first streaming video thing.

01:05:40.020 --> 01:05:41.780
And he put it on the web cause he literally,

01:05:41.780 --> 01:05:43.380
it was the coffee pot down the hall.

01:05:43.380 --> 01:05:46.420
And he wanted to see when he needed to go refill it.

01:05:46.420 --> 01:05:47.460
But there were, you know,

01:05:47.460 --> 01:05:49.020
there was a point when there were thousands of people

01:05:49.020 --> 01:05:50.700
like watching that coffee pot

01:05:50.700 --> 01:05:52.860
cause it was the first thing you could watch.

01:05:52.860 --> 01:05:57.460
Well, but isn't, were you able to kind of infer,

01:05:57.460 --> 01:06:00.780
you know, if that Indian restaurant could go online,

01:06:00.780 --> 01:06:01.620
then you're like-

01:06:01.620 --> 01:06:02.460
They all will.

01:06:02.460 --> 01:06:03.280
They all will.

01:06:03.280 --> 01:06:04.120
Yeah, exactly.

01:06:04.120 --> 01:06:04.940
So you felt that.

01:06:04.940 --> 01:06:05.780
Yeah, yeah, yeah.

01:06:05.780 --> 01:06:06.820
Now, you know, look, it's still a stretch, right?

01:06:06.820 --> 01:06:08.260
It's still a stretch cause it's just like, okay,

01:06:08.260 --> 01:06:09.540
is it, you know, you're still in this zone,

01:06:09.540 --> 01:06:10.740
which is like, okay, is this a nerd thing?

01:06:10.740 --> 01:06:11.660
Is this a real person thing?

01:06:11.660 --> 01:06:12.740
Yeah.

01:06:12.740 --> 01:06:13.580
By the way, we, you know,

01:06:13.580 --> 01:06:15.420
there was a wall of skepticism from the media.

01:06:15.420 --> 01:06:17.060
Like they just, like everybody was just like,

01:06:17.060 --> 01:06:18.580
yeah, this is the crazy, this is just like dumb.

01:06:18.580 --> 01:06:19.420
This is not, you know,

01:06:19.420 --> 01:06:22.060
this is not for regular people at that time.

01:06:22.060 --> 01:06:23.580
And so you had to think through that and then look,

01:06:23.580 --> 01:06:26.260
it was still, it was still hard to get on the internet

01:06:26.260 --> 01:06:27.100
at that point, right?

01:06:27.100 --> 01:06:29.380
So you could get kind of this weird bastardized version

01:06:29.380 --> 01:06:31.780
if you were on AOL, which wasn't really real,

01:06:31.780 --> 01:06:34.580
or you had to go like learn what an ISP was.

01:06:35.620 --> 01:06:36.460
You know, in those days,

01:06:36.460 --> 01:06:38.980
PCs actually didn't have TCP IP drivers come pre-installed.

01:06:38.980 --> 01:06:41.580
So you had to learn what a TCP IP driver was.

01:06:41.580 --> 01:06:42.440
You had to buy a modem.

01:06:42.440 --> 01:06:44.940
You had to install driver software.

01:06:44.940 --> 01:06:45.780
I have a comedy routine.

01:06:45.780 --> 01:06:47.460
I do something like 20 minutes long,

01:06:47.460 --> 01:06:48.740
describing all the steps required

01:06:48.740 --> 01:06:50.980
to actually get on the internet.

01:06:50.980 --> 01:06:54.020
And so you had to look through these practical,

01:06:54.020 --> 01:06:59.020
well, and then speed performance, 14, four modems, right?

01:06:59.020 --> 01:07:01.820
Like it was like watching, you know, glue dry, like,

01:07:01.820 --> 01:07:03.220
and so you had to, you had to,

01:07:03.220 --> 01:07:04.840
there were basically a sequence of bets that we made

01:07:04.840 --> 01:07:06.080
where you basically needed to look through

01:07:06.080 --> 01:07:07.300
that current state of affairs and say,

01:07:07.300 --> 01:07:09.620
actually, there's gonna be so much demand for that.

01:07:09.620 --> 01:07:10.540
Once people figure this out,

01:07:10.540 --> 01:07:11.660
there's gonna be so much demand for it

01:07:11.660 --> 01:07:14.420
that all of these practical problems are gonna get fixed.

01:07:14.420 --> 01:07:16.780
Some people say that the anticipation

01:07:16.780 --> 01:07:21.180
makes the destination that much more exciting.

01:07:21.180 --> 01:07:22.900
Do you remember progressive JPEGs?

01:07:22.900 --> 01:07:24.700
Yeah, do I?

01:07:24.700 --> 01:07:25.580
Do I?

01:07:25.580 --> 01:07:27.780
So for kids in the audience, right?

01:07:27.780 --> 01:07:28.620
For kids in the audience.

01:07:28.620 --> 01:07:30.940
You used to have to watch an image load

01:07:30.940 --> 01:07:32.680
like a line at a time, but it turns out

01:07:32.680 --> 01:07:33.700
there was this thing with JPEGs

01:07:33.700 --> 01:07:36.340
where you could load basically every fourth,

01:07:36.340 --> 01:07:38.300
you could load like every fourth line

01:07:38.300 --> 01:07:40.260
and then you could sweep back through again.

01:07:40.260 --> 01:07:42.980
And so you could like render a fuzzy version image upfront

01:07:42.980 --> 01:07:44.780
and then it would like resolve into the detailed one.

01:07:44.780 --> 01:07:46.820
And that was like a big UI breakthrough

01:07:46.820 --> 01:07:49.180
because it gave you something to watch.

01:07:49.180 --> 01:07:53.760
Yeah, and there's applications in various domains for that.

01:07:56.100 --> 01:07:56.940
Well, it's a big fight.

01:07:56.940 --> 01:07:57.760
There was a big fight early on

01:07:57.760 --> 01:08:00.020
about whether there should be images on the web.

01:08:00.020 --> 01:08:01.900
For that reason, for like sexualization.

01:08:01.900 --> 01:08:03.380
No, not explicitly.

01:08:03.380 --> 01:08:04.820
That did come up, but it wasn't even that.

01:08:04.820 --> 01:08:06.340
It was more just like all the serious,

01:08:06.340 --> 01:08:08.160
the argument went, the purists basically said

01:08:08.160 --> 01:08:10.540
all the serious information in the world is text.

01:08:10.540 --> 01:08:11.780
If you introduce images,

01:08:11.780 --> 01:08:13.620
you're basically gonna bring in all the trivial stuff.

01:08:13.620 --> 01:08:14.740
You're gonna bring in magazines

01:08:15.740 --> 01:08:18.060
and all this crazy stuff that people,

01:08:18.060 --> 01:08:19.060
it's gonna distract from it.

01:08:19.060 --> 01:08:21.540
It's gonna take away from being serious, being frivolous.

01:08:21.540 --> 01:08:23.860
Well, was there any doomer type arguments

01:08:23.860 --> 01:08:28.060
about the internet destroying all of human civilization

01:08:28.060 --> 01:08:32.820
or destroying some fundamental fabric of human civilization?

01:08:32.820 --> 01:08:36.180
Yeah, so those days it was all around crime and terrorism.

01:08:36.180 --> 01:08:38.580
So those arguments happened,

01:08:38.580 --> 01:08:39.980
but there was no sense yet of the internet

01:08:39.980 --> 01:08:41.220
having like an effect on politics

01:08:41.220 --> 01:08:42.780
because that was way too far off.

01:08:42.780 --> 01:08:45.540
But there was an enormous panic at the time

01:08:45.540 --> 01:08:46.620
around cyber crime.

01:08:46.620 --> 01:08:47.780
There was like enormous panic

01:08:47.780 --> 01:08:49.460
that like your credit card number would get stolen

01:08:49.460 --> 01:08:51.460
and you'd use life savings to be drained.

01:08:51.460 --> 01:08:52.980
And then criminals were gonna, there was,

01:08:52.980 --> 01:08:55.220
oh, when we started, one of the things we did,

01:08:55.220 --> 01:08:58.300
the Netscape browser was the first widely used

01:08:58.300 --> 01:08:59.300
piece of consumer software

01:08:59.300 --> 01:09:01.060
that had strong encryption built in.

01:09:01.060 --> 01:09:02.700
It made it available to ordinary people.

01:09:02.700 --> 01:09:04.460
And at that time, strong encryption

01:09:04.460 --> 01:09:07.140
was actually illegal to export out of the US.

01:09:07.140 --> 01:09:09.340
So we could field that product in the US.

01:09:09.340 --> 01:09:10.540
We could not export it

01:09:10.540 --> 01:09:12.940
because it was classified as ammunition.

01:09:12.940 --> 01:09:14.900
So the Netscape browser was on a restricted list

01:09:14.900 --> 01:09:16.540
along with the Tomahawk missile

01:09:16.540 --> 01:09:17.980
as being something that could not be exported.

01:09:17.980 --> 01:09:19.700
So we had to make a second version

01:09:19.700 --> 01:09:22.020
with deliberately weak encryption to sell overseas

01:09:22.020 --> 01:09:24.740
with a big logo on the box saying, do not trust this,

01:09:24.740 --> 01:09:26.900
which it turns out makes it hard to sell software

01:09:26.900 --> 01:09:29.980
when it's got a big logo that says, don't trust it.

01:09:29.980 --> 01:09:31.420
And then we had to spend five years

01:09:31.420 --> 01:09:32.260
fighting the US government

01:09:32.260 --> 01:09:34.740
to get them to basically stop trying to do this.

01:09:35.740 --> 01:09:38.300
Because the fear was terrorists are gonna use encryption,

01:09:38.580 --> 01:09:41.140
to like plot all these things.

01:09:42.180 --> 01:09:43.540
And then we responded with,

01:09:43.540 --> 01:09:44.900
well, actually we need encryption

01:09:44.900 --> 01:09:46.380
to be able to secure systems

01:09:46.380 --> 01:09:48.140
so the terrorists and the criminals can't get into them.

01:09:48.140 --> 01:09:50.940
So anyway, that was the 1990s fight.

01:09:50.940 --> 01:09:53.700
So can you say something about some of the details

01:09:53.700 --> 01:09:56.460
of the software engineering challenges

01:09:56.460 --> 01:09:58.020
required to build these browsers?

01:09:58.020 --> 01:10:00.860
I mean, the engineering challenges of creating a product

01:10:00.860 --> 01:10:03.060
that hasn't really existed before

01:10:03.060 --> 01:10:07.940
that can have such almost like limitless impact

01:10:07.940 --> 01:10:09.820
on the world with the internet.

01:10:09.820 --> 01:10:11.900
So there was a really key bet that we made at the time,

01:10:11.900 --> 01:10:12.940
which is very controversial,

01:10:12.940 --> 01:10:14.940
which was core to how it was engineered,

01:10:14.940 --> 01:10:17.340
which was, are we optimizing for performance

01:10:17.340 --> 01:10:19.020
or for ease of creation?

01:10:19.020 --> 01:10:21.100
And in those days, the pressure was very intense

01:10:21.100 --> 01:10:22.060
to optimize for performance

01:10:22.060 --> 01:10:24.220
because the network connections were so slow

01:10:24.220 --> 01:10:26.340
and also the computers were so slow.

01:10:26.340 --> 01:10:29.180
And so if you had, I mentioned the progressive JPEGs,

01:10:29.180 --> 01:10:32.380
like there's an alternate world

01:10:32.380 --> 01:10:33.740
in which we optimize for performance

01:10:33.740 --> 01:10:35.820
and you had just a much more pleasant experience

01:10:35.820 --> 01:10:37.220
right up front.

01:10:37.220 --> 01:10:39.260
But what we got by not doing that

01:10:39.260 --> 01:10:40.620
was we got ease of creation.

01:10:40.620 --> 01:10:42.020
And the way that we got ease of creation

01:10:42.020 --> 01:10:44.900
was all of the protocols and formats

01:10:44.900 --> 01:10:47.340
were in text, not in binary.

01:10:47.340 --> 01:10:49.500
And so HTTP is in text.

01:10:49.500 --> 01:10:50.740
By the way, and this was an internet tradition,

01:10:50.740 --> 01:10:51.580
by the way, that we picked up,

01:10:51.580 --> 01:10:52.580
but we continued it.

01:10:52.580 --> 01:10:55.380
HTTP is text and HTML is text,

01:10:55.380 --> 01:10:58.180
and then everything else that followed is text.

01:10:58.180 --> 01:10:59.420
As a result, and by the way,

01:10:59.420 --> 01:11:01.700
you can imagine purist engineers saying this is insane.

01:11:01.700 --> 01:11:02.940
You have very limited bandwidth.

01:11:02.940 --> 01:11:04.740
Why are you wasting any time sending text?

01:11:04.740 --> 01:11:06.540
You should be encoding this stuff into binary

01:11:06.580 --> 01:11:07.420
and it'll be much faster.

01:11:07.420 --> 01:11:09.580
And of course the answer is that's correct.

01:11:09.580 --> 01:11:10.980
But what you get when you make it text

01:11:10.980 --> 01:11:11.820
is all of a sudden,

01:11:11.820 --> 01:11:14.060
well, the big breakthrough was the view source function.

01:11:14.060 --> 01:11:15.620
So the fact that you could look at a webpage,

01:11:15.620 --> 01:11:17.980
you could hit view source and you could see the HTML.

01:11:17.980 --> 01:11:20.500
That was how people learned how to make webpages.

01:11:20.500 --> 01:11:21.460
It's so interesting

01:11:21.460 --> 01:11:23.500
because the stuff we take for granted now

01:11:24.660 --> 01:11:27.900
is, man, that was fundamental to the development of the web

01:11:27.900 --> 01:11:30.260
to be able to have HTML just right there.

01:11:30.260 --> 01:11:33.300
All the ghetto mess that is HTML,

01:11:33.300 --> 01:11:38.220
all the sort of almost biological messiness of HTML

01:11:38.220 --> 01:11:42.140
and then having the browser try to interpret that mess

01:11:42.140 --> 01:11:43.780
to show something reasonable.

01:11:43.780 --> 01:11:45.340
Well, and then there was this internet principle

01:11:45.340 --> 01:11:46.900
that we inherited, which was emit,

01:11:46.900 --> 01:11:48.380
what was it, emit cautiously,

01:11:48.380 --> 01:11:50.380
emit conservatively, interpret liberally.

01:11:50.380 --> 01:11:53.420
So it basically meant, the design principle was,

01:11:53.420 --> 01:11:56.300
if you're creating a web editor that's gonna emit HTML,

01:11:56.300 --> 01:11:58.020
do it as cleanly as you can.

01:11:58.020 --> 01:11:59.940
But you actually want the browser to interpret liberally,

01:11:59.940 --> 01:12:01.460
which is you actually want users

01:12:01.460 --> 01:12:02.940
to be able to make all kinds of mistakes

01:12:02.940 --> 01:12:04.460
and for it to still work.

01:12:04.460 --> 01:12:06.420
And so the browser rendering engines to this day

01:12:06.420 --> 01:12:08.980
have all of this spaghetti code, crazy stuff

01:12:08.980 --> 01:12:10.500
where they're resilient

01:12:10.500 --> 01:12:12.380
to all kinds of crazy HTML mistakes.

01:12:12.380 --> 01:12:14.140
And literally what I always had in my head

01:12:14.140 --> 01:12:16.260
is there's an eight-year-old or an 11-year-old somewhere

01:12:16.260 --> 01:12:17.100
and they're doing a view source,

01:12:17.100 --> 01:12:17.940
they're doing a cut and paste

01:12:17.940 --> 01:12:18.900
and they're trying to make a webpage

01:12:18.900 --> 01:12:20.900
for their turtle or whatever.

01:12:20.900 --> 01:12:23.300
And they leave out a slash and they leave out an angle bracket

01:12:23.300 --> 01:12:25.420
and they do this and they do that and it still works.

01:12:25.420 --> 01:12:28.220
It's also, I don't often think about this,

01:12:28.220 --> 01:12:31.900
but programming, C++, C, C++,

01:12:31.940 --> 01:12:33.660
all those languages, Lisp,

01:12:33.660 --> 01:12:35.540
the compiled languages, the interpreted languages,

01:12:35.540 --> 01:12:37.380
Python, Perl, all of that,

01:12:37.380 --> 01:12:40.020
the brace has to be all correct.

01:12:40.020 --> 01:12:41.300
Everything has to be perfect.

01:12:41.300 --> 01:12:42.500
Brutal.

01:12:42.500 --> 01:12:43.580
And then- Autistic.

01:12:43.580 --> 01:12:48.020
You forget, all right, it's systematic and rigorous.

01:12:48.020 --> 01:12:49.420
Let's go there.

01:12:49.420 --> 01:12:54.420
But you forget that the web with JavaScript eventually

01:12:56.740 --> 01:13:00.180
and HTML is allowed to be messy in the way,

01:13:00.180 --> 01:13:01.660
for the first time,

01:13:02.900 --> 01:13:06.180
messy in the way biological systems could be messy.

01:13:06.180 --> 01:13:08.260
It's like the only thing computers were allowed

01:13:08.260 --> 01:13:10.100
to be messy on for the first time.

01:13:10.100 --> 01:13:10.940
It used to offend me.

01:13:10.940 --> 01:13:13.620
So I grew up on Unix, I worked on Unix.

01:13:13.620 --> 01:13:16.020
I was a Unix native for all the way through this period.

01:13:16.020 --> 01:13:17.980
And so, and it used to drive me bananas

01:13:17.980 --> 01:13:19.820
when it would do the segmentation fault

01:13:19.820 --> 01:13:21.140
in the core dump file.

01:13:21.140 --> 01:13:23.900
Just like, literally there's like an error in the code,

01:13:23.900 --> 01:13:26.780
the math is off by one and it core dumps.

01:13:26.780 --> 01:13:28.260
And I'm in the core dump trying to analyze it

01:13:28.260 --> 01:13:29.180
and trying to reconstruct.

01:13:29.180 --> 01:13:30.620
And I'm just like, this is ridiculous.

01:13:30.620 --> 01:13:32.180
Like the computer ought to be smart enough

01:13:32.180 --> 01:13:33.660
to be able to know that if it's off by one,

01:13:33.660 --> 01:13:35.740
okay, fine, and it keeps running.

01:13:35.740 --> 01:13:37.140
And I would go ask all the experts,

01:13:37.140 --> 01:13:38.140
like, why can't it just keep running?

01:13:38.140 --> 01:13:38.980
And they'd explain to me,

01:13:38.980 --> 01:13:40.900
well, because all the downstream repercussions and blah, blah.

01:13:40.900 --> 01:13:42.620
And I'm like, this still like,

01:13:42.620 --> 01:13:47.380
you know, we're forcing the human creator to live,

01:13:47.380 --> 01:13:50.900
to your point, in this hyper literal world of perfection.

01:13:50.900 --> 01:13:53.540
And I was just like, that's just bad.

01:13:53.540 --> 01:13:55.260
And by the way, you know, what happens with that, of course,

01:13:55.260 --> 01:13:57.180
just what happened with coding at that point,

01:13:57.180 --> 01:13:58.820
which is you get a high priesthood.

01:13:58.820 --> 01:14:00.420
You know, there's a small number of people

01:14:00.420 --> 01:14:02.220
who are really good at doing exactly that.

01:14:02.220 --> 01:14:04.700
Most people can't and most people are excluded from it.

01:14:04.700 --> 01:14:08.500
And so actually that was where I picked up that idea was,

01:14:08.500 --> 01:14:10.660
was like, no, no, you want these things

01:14:10.660 --> 01:14:12.780
to be resilient to error in all kinds.

01:14:12.780 --> 01:14:14.540
And this would drive the purists absolutely crazy.

01:14:14.540 --> 01:14:16.100
Like I got attacked on this like a lot,

01:14:16.100 --> 01:14:17.740
because yeah, I mean, like every time, you know,

01:14:17.740 --> 01:14:19.140
all the purists who were like in all this

01:14:19.140 --> 01:14:21.340
like markup language stuff and formats and codes

01:14:21.340 --> 01:14:22.820
and all this stuff, they would be like, you know,

01:14:22.820 --> 01:14:25.300
you can't, you're encouraging bad behavior.

01:14:25.300 --> 01:14:27.140
Oh, so they wanted the browser to give you

01:14:27.140 --> 01:14:30.300
a segfault error anytime there was a...

01:14:30.300 --> 01:14:31.740
Yeah, yeah, they wanted it to be a copy, right?

01:14:31.740 --> 01:14:32.580
They wanted that.

01:14:32.580 --> 01:14:35.580
Yeah, that was a very, any properly trained

01:14:35.580 --> 01:14:37.860
and credentialed engineer would be like,

01:14:37.860 --> 01:14:39.060
that's not how you build these systems.

01:14:39.060 --> 01:14:41.540
That's such a bold move to say, no, it doesn't have to be.

01:14:41.540 --> 01:14:43.180
Yeah, now, like I said, the good news for me

01:14:43.180 --> 01:14:45.420
is the internet kind of had that tradition already,

01:14:45.420 --> 01:14:47.820
but having said that, like we pushed it.

01:14:47.820 --> 01:14:48.660
We pushed it way out.

01:14:48.660 --> 01:14:49.580
But the other thing we did going back

01:14:49.580 --> 01:14:51.540
to the performance thing was we gave up a lot of performance.

01:14:51.540 --> 01:14:52.740
We made that, that initial experience

01:14:52.740 --> 01:14:54.620
for the first few years was pretty painful.

01:14:54.620 --> 01:14:56.620
But the bet there was actually an economic bet,

01:14:56.620 --> 01:14:58.660
which was basically the demand for the web

01:14:58.660 --> 01:14:59.780
would basically mean that there would be

01:14:59.780 --> 01:15:01.380
a surge in supply of broadband.

01:15:03.380 --> 01:15:06.940
The question was, okay, how do you get the phone companies,

01:15:06.940 --> 01:15:10.100
which are not famous in those days for doing new things

01:15:10.100 --> 01:15:12.140
at huge cost for like speculative reasons?

01:15:12.140 --> 01:15:14.020
Like, how do you get them to build up broadband?

01:15:14.020 --> 01:15:15.860
You know, spend billions of dollars doing that.

01:15:15.860 --> 01:15:17.300
And, you know, you could go meet with them

01:15:17.300 --> 01:15:18.220
and try to talk them into it,

01:15:18.220 --> 01:15:20.580
or you could just have a thing where it's just very clear

01:15:20.580 --> 01:15:22.060
that it's gonna be the thing that people love

01:15:22.060 --> 01:15:23.900
that's gonna be better if it's faster.

01:15:24.060 --> 01:15:26.380
And so there was a period there,

01:15:26.380 --> 01:15:27.780
and this was fraught with some peril,

01:15:27.780 --> 01:15:28.980
but there was a period there where it's like

01:15:28.980 --> 01:15:31.020
we knew the experience was sub-optimized

01:15:31.020 --> 01:15:33.740
because we were trying to force the emergence

01:15:33.740 --> 01:15:37.540
of demand for broadband, which is in fact what happened.

01:15:37.540 --> 01:15:40.540
So you had to figure out how to display this text,

01:15:40.540 --> 01:15:44.300
HTML text, so the blue links and the purple links.

01:15:44.300 --> 01:15:45.460
And there's no standards.

01:15:45.460 --> 01:15:47.420
Is there standards at that time?

01:15:47.420 --> 01:15:48.980
No, there really still isn't.

01:15:49.820 --> 01:15:54.340
Well, there's like, there's implied standards, right?

01:15:54.340 --> 01:15:56.740
And they, you know, there's all these kinds of new features

01:15:56.740 --> 01:15:58.980
that are being added with like CSS with like

01:15:58.980 --> 01:16:01.540
what kind of stuff a browser should be able to support,

01:16:01.540 --> 01:16:04.540
features within languages, within JavaScript and so on.

01:16:04.540 --> 01:16:09.540
But you're setting standards on the fly yourself.

01:16:11.300 --> 01:16:13.900
Well, to this day, if you create a webpage

01:16:13.900 --> 01:16:15.740
that has no CSS style sheet,

01:16:15.740 --> 01:16:18.340
the browser will render it however it wants to, right?

01:16:18.340 --> 01:16:20.740
So this was one of the things, there was this idea,

01:16:20.740 --> 01:16:23.140
this idea at the time in how these systems were built,

01:16:23.140 --> 01:16:25.420
which is separation of content from format

01:16:25.420 --> 01:16:28.660
or separation of, yeah, content from appearance.

01:16:28.660 --> 01:16:31.140
And that's still, people don't really use that anymore

01:16:31.140 --> 01:16:32.740
because everybody wants to determine how things look.

01:16:32.740 --> 01:16:35.460
And so they use CSS, but it's still in there

01:16:35.460 --> 01:16:37.180
that you can just let the browser do all the work.

01:16:37.180 --> 01:16:42.180
I still like the, like really basic websites,

01:16:42.220 --> 01:16:44.140
but that could be just old school.

01:16:44.140 --> 01:16:47.940
Kids these days with their fancy responsive websites

01:16:48.100 --> 01:16:49.580
don't actually have much content

01:16:49.580 --> 01:16:51.100
but have a lot of visual elements.

01:16:51.100 --> 01:16:52.740
Well, that's one of the things that's fun about chat,

01:16:52.740 --> 01:16:54.740
you know, about chat GPT, it's like-

01:16:54.740 --> 01:16:55.780
Back to the basics.

01:16:55.780 --> 01:16:57.660
Back to just text, right?

01:16:57.660 --> 01:16:59.860
And you know, there is this pattern

01:16:59.860 --> 01:17:03.540
in human creativity and media where you end up back at text.

01:17:03.540 --> 01:17:04.660
And I think there's, you know,

01:17:04.660 --> 01:17:06.300
there's something powerful in there.

01:17:06.300 --> 01:17:09.060
Is there some other stuff you remember like the purple links?

01:17:09.060 --> 01:17:11.020
There were some interesting design decisions

01:17:11.020 --> 01:17:13.940
to kind of come up that we have today

01:17:13.940 --> 01:17:16.780
or we don't have today that were temporary.

01:17:16.780 --> 01:17:18.340
So we made, I made the background gray.

01:17:18.340 --> 01:17:21.700
I hated reading texts on white backgrounds.

01:17:21.700 --> 01:17:22.620
So I made the background gray.

01:17:22.620 --> 01:17:23.460
Everybody can-

01:17:23.460 --> 01:17:24.300
Do you regret?

01:17:24.300 --> 01:17:25.140
Do you regret this?

01:17:25.140 --> 01:17:27.940
No, no, no, that decision I think has been reversed.

01:17:27.940 --> 01:17:28.980
But now I'm happy though,

01:17:28.980 --> 01:17:31.300
because now dark mode is the thing, so.

01:17:31.300 --> 01:17:32.940
So it wasn't about gray.

01:17:32.940 --> 01:17:34.620
It was just, you didn't want a white background.

01:17:34.620 --> 01:17:36.100
Strain my eyes.

01:17:36.100 --> 01:17:37.740
Strain your eyes.

01:17:37.740 --> 01:17:38.580
Interesting.

01:17:39.980 --> 01:17:41.900
And then there's a bunch of other decisions.

01:17:41.900 --> 01:17:43.300
I'm sure there's an interesting history

01:17:43.300 --> 01:17:45.340
of the development of HTML and CSS

01:17:45.340 --> 01:17:48.300
and how those interface in JavaScript.

01:17:48.300 --> 01:17:51.420
And then there's this whole Java applet thing.

01:17:51.420 --> 01:17:53.580
Well, the big one, probably JavaScript.

01:17:53.580 --> 01:17:55.220
CSS was after me, so I didn't, it wasn't me.

01:17:55.220 --> 01:17:57.460
But JavaScript was the big,

01:17:57.460 --> 01:17:59.100
JavaScript maybe was the biggest of the whole thing.

01:17:59.100 --> 01:18:00.260
That was us.

01:18:00.260 --> 01:18:02.740
And that was basically a bet.

01:18:02.740 --> 01:18:03.820
It was a bet on two things.

01:18:03.820 --> 01:18:04.900
One is that the world wanted

01:18:04.900 --> 01:18:07.380
a new front-end scripting language.

01:18:07.380 --> 01:18:09.020
And then the other was that we thought at the time

01:18:09.020 --> 01:18:11.420
the world wanted a new backend scripting language.

01:18:11.420 --> 01:18:13.180
So JavaScript was designed from the beginning

01:18:13.180 --> 01:18:14.820
to be both front-end and backend.

01:18:15.420 --> 01:18:17.460
And then it failed as a backend scripting language

01:18:17.460 --> 01:18:19.780
and Java won for a long time.

01:18:19.780 --> 01:18:22.700
And then Python, Perl and other things, PHP and Ruby.

01:18:22.700 --> 01:18:24.500
But now JavaScript is back.

01:18:24.500 --> 01:18:25.860
And so.

01:18:25.860 --> 01:18:28.860
I wonder if everything in the end will run on JavaScript.

01:18:28.860 --> 01:18:30.340
It seems like it is the,

01:18:30.340 --> 01:18:34.500
and by the way, let me give a shout out to Brendan Eich

01:18:34.500 --> 01:18:37.940
was the basically the one man inventor of JavaScript.

01:18:37.940 --> 01:18:39.940
If you're interested to learn more about Brendan Eich,

01:18:39.940 --> 01:18:42.060
he's been on this podcast previously.

01:18:42.060 --> 01:18:43.300
Exactly.

01:18:43.300 --> 01:18:44.980
So he wrote JavaScript over a summer.

01:18:45.820 --> 01:18:47.500
I mean, I think it is fair to say now

01:18:47.500 --> 01:18:49.300
that it's the most widely used language in the world.

01:18:49.300 --> 01:18:53.540
And it seems to only be gaining in its range of adoption.

01:18:53.540 --> 01:18:54.740
In the software world,

01:18:54.740 --> 01:18:57.380
there's quite a few stories of somebody over a weekend

01:18:57.380 --> 01:19:00.140
or over a week or over a summer

01:19:00.140 --> 01:19:02.860
writing some of the most impactful,

01:19:02.860 --> 01:19:05.860
revolutionary pieces of software ever.

01:19:05.860 --> 01:19:06.700
Well, look.

01:19:06.700 --> 01:19:07.740
That should be inspiring, yes.

01:19:07.740 --> 01:19:08.580
Very inspiring.

01:19:08.580 --> 01:19:10.180
I'll give you another one, SSL.

01:19:10.180 --> 01:19:12.020
So SSL was the security protocol.

01:19:12.020 --> 01:19:12.860
That was us.

01:19:12.860 --> 01:19:14.220
And that was a crazy idea at the time,

01:19:14.220 --> 01:19:15.980
which was let's take all the native protocols

01:19:15.980 --> 01:19:17.860
and let's wrap them in a security wrapper.

01:19:17.860 --> 01:19:18.940
That was a guy named Kip Hickman

01:19:18.940 --> 01:19:20.940
who wrote that over a summer, one guy.

01:19:22.100 --> 01:19:23.820
And then look today, sitting here today,

01:19:23.820 --> 01:19:25.620
like the transformer like at Google

01:19:25.620 --> 01:19:27.060
was a small handful of people.

01:19:27.060 --> 01:19:29.420
And then the number of people

01:19:29.420 --> 01:19:31.500
who have did like the core work on GPT,

01:19:31.500 --> 01:19:33.260
it's not that many people.

01:19:33.260 --> 01:19:35.140
It's a pretty small handful of people.

01:19:35.140 --> 01:19:37.620
And so yeah, the pattern in software repeatedly

01:19:37.620 --> 01:19:40.700
over a very long time has been it's a,

01:19:40.700 --> 01:19:43.100
Jeff Bezos always had the two pizza rule

01:19:43.180 --> 01:19:44.900
for teams at Amazon, which is any team

01:19:44.900 --> 01:19:47.300
needs to be able to be fed with two pizzas.

01:19:47.300 --> 01:19:49.020
If you need the third pizza, you have too many people.

01:19:49.020 --> 01:19:53.220
And I think it's actually the one pizza rule.

01:19:53.220 --> 01:19:54.740
For the really creative work,

01:19:54.740 --> 01:19:57.260
I think it's two people, three people.

01:19:57.260 --> 01:19:59.700
Well, you see that with certain open source projects,

01:19:59.700 --> 01:20:02.020
like so much is done by like one or two people.

01:20:02.900 --> 01:20:04.380
It's so incredible.

01:20:04.380 --> 01:20:05.220
And that's why you see,

01:20:05.220 --> 01:20:08.500
that gives me so much hope about the open source movement

01:20:08.500 --> 01:20:10.500
in this new age of AI,

01:20:10.500 --> 01:20:13.980
where just recently having had a conversation

01:20:13.980 --> 01:20:15.940
with Mark Zuckerberg of all people

01:20:15.940 --> 01:20:17.900
who's all in on open source,

01:20:17.900 --> 01:20:22.220
which is so interesting to see and so inspiring to see.

01:20:22.220 --> 01:20:25.460
Cause like releasing these models, it is scary.

01:20:25.460 --> 01:20:26.980
It is potentially very dangerous.

01:20:26.980 --> 01:20:28.860
And we'll talk about that.

01:20:28.860 --> 01:20:33.580
But it's also if you believe in the goodness of most people

01:20:33.580 --> 01:20:35.740
and in the skillset of most people

01:20:35.740 --> 01:20:38.020
and the desire to do good in the world,

01:20:38.020 --> 01:20:39.420
that's really exciting.

01:20:39.420 --> 01:20:41.660
Cause it's not putting these models

01:20:41.660 --> 01:20:44.180
into the centralized control of big corporations

01:20:44.180 --> 01:20:45.020
and government and so on.

01:20:45.020 --> 01:20:48.300
It's putting it in the hands of a teenage kid

01:20:48.300 --> 01:20:49.860
with like a dream in his eyes.

01:20:49.860 --> 01:20:53.100
I don't know, that's beautiful.

01:20:53.100 --> 01:20:54.220
And look, this stuff,

01:20:54.220 --> 01:20:56.060
AI ought to make the individual coder,

01:20:56.060 --> 01:20:57.940
obviously far more productive, right?

01:20:57.940 --> 01:20:59.860
By like, you know, a thousand acts or something.

01:20:59.860 --> 01:21:02.060
And so you ought to open source,

01:21:02.060 --> 01:21:03.980
like not just the future of open source AI,

01:21:03.980 --> 01:21:05.820
but the future of open source everything.

01:21:05.820 --> 01:21:08.020
We ought to have a world now of super coders, right?

01:21:08.020 --> 01:21:10.140
Who are building things as open source

01:21:10.140 --> 01:21:12.220
with one or two people that were inconceivable,

01:21:12.220 --> 01:21:14.300
you know, five years ago.

01:21:14.300 --> 01:21:16.020
You know, the level of kind of hyper productivity

01:21:16.020 --> 01:21:17.220
we're going to get out of our best and brightest,

01:21:17.220 --> 01:21:18.340
I think is going to go way up.

01:21:18.340 --> 01:21:19.180
It's going to be interesting.

01:21:19.180 --> 01:21:20.020
We'll talk about it,

01:21:20.020 --> 01:21:23.260
but let's just to linger a little bit on Netscape.

01:21:24.140 --> 01:21:29.140
Netscape was acquired in 1999 for 4.3 billion by AOL.

01:21:29.820 --> 01:21:31.620
What was that like?

01:21:31.620 --> 01:21:34.140
What were some memorable aspects of that?

01:21:34.140 --> 01:21:37.700
Well, that was the height of the dot com boom bubble bust.

01:21:38.580 --> 01:21:40.060
I mean, that was the frenzy.

01:21:40.060 --> 01:21:41.180
If you watch Succession,

01:21:41.180 --> 01:21:43.460
that was like what they did in the fourth season

01:21:43.460 --> 01:21:46.260
with Gojo and then merger with their,

01:21:46.260 --> 01:21:48.100
so it was like the height of like one of those

01:21:48.100 --> 01:21:49.540
kind of dynamics and so.

01:21:49.540 --> 01:21:51.260
Would you recommend Succession by the way?

01:21:51.260 --> 01:21:52.900
I'm more of a Yellowstone guy.

01:21:52.900 --> 01:21:55.260
Yellowstone's very American.

01:21:55.260 --> 01:21:56.100
I'm very proud of you.

01:21:56.100 --> 01:21:57.500
That is.

01:21:57.500 --> 01:21:59.140
I just talked to Matthew McConaughey

01:21:59.140 --> 01:22:00.900
and I'm full on Texan at this point.

01:22:00.900 --> 01:22:02.860
Good, I heartily approve.

01:22:02.860 --> 01:22:06.180
And he will be doing the sequel to Yellowstone.

01:22:06.540 --> 01:22:07.740
Very exciting.

01:22:07.740 --> 01:22:12.460
Anyway, so that's a rude interruption by me

01:22:12.460 --> 01:22:13.820
by way of Succession.

01:22:15.940 --> 01:22:17.940
So that was at the height of the.

01:22:17.940 --> 01:22:20.500
Deal making and money and just the fur flying

01:22:20.500 --> 01:22:21.420
and like craziness.

01:22:21.420 --> 01:22:23.500
And so, yeah, it was just one of those.

01:22:23.500 --> 01:22:24.340
It was just like, I mean,

01:22:24.340 --> 01:22:25.460
there's the entire Netscape thing

01:22:25.460 --> 01:22:27.780
from start to finish was four years,

01:22:27.780 --> 01:22:29.380
which was like for one of these companies,

01:22:29.380 --> 01:22:31.460
it's just like incredibly fast.

01:22:31.460 --> 01:22:33.340
We went public 18 months after we got moved

01:22:33.340 --> 01:22:35.580
to where we were founded, which virtually never happens.

01:22:35.620 --> 01:22:37.740
So it was just this incredibly fast kind of meteor

01:22:37.740 --> 01:22:39.460
streaking across the sky.

01:22:39.460 --> 01:22:40.660
And then of course, it was this.

01:22:40.660 --> 01:22:42.180
And then there was just this explosion, right?

01:22:42.180 --> 01:22:43.020
That happened.

01:22:43.020 --> 01:22:44.420
Because then it was almost immediately followed

01:22:44.420 --> 01:22:46.140
by the.com crash.

01:22:46.140 --> 01:22:48.380
It was then followed by AOL buying Time Warner,

01:22:48.380 --> 01:22:51.460
which again is the Succession guys kind of play with that,

01:22:51.460 --> 01:22:53.860
which turned out to be a disastrous deal.

01:22:53.860 --> 01:22:56.700
One of the famous kind of disasters in business history.

01:22:56.700 --> 01:22:59.700
And then what became an internet depression

01:22:59.700 --> 01:23:00.540
on the other side of that.

01:23:00.540 --> 01:23:02.620
But then in that depression in the 2000s

01:23:02.620 --> 01:23:05.500
was the beginning of broadband and smartphones.

01:23:06.420 --> 01:23:07.260
And Web 2.0, right?

01:23:07.260 --> 01:23:09.260
And then social media and search and every SaaS

01:23:09.260 --> 01:23:11.100
and everything that came out of that.

01:23:11.100 --> 01:23:12.860
What did you learn from just the acquisition?

01:23:12.860 --> 01:23:14.460
I mean, this is so much money.

01:23:16.540 --> 01:23:17.580
What's interesting,

01:23:17.580 --> 01:23:20.140
because I must've been very new to you

01:23:20.140 --> 01:23:22.980
that these software stuff,

01:23:22.980 --> 01:23:25.020
you can make so much money.

01:23:25.020 --> 01:23:26.500
There's so much money swimming around.

01:23:26.500 --> 01:23:28.780
I mean, I'm sure the ideas of investment

01:23:28.780 --> 01:23:30.060
were starting to get born there.

01:23:30.060 --> 01:23:31.740
Yes, let me lay it out.

01:23:31.740 --> 01:23:33.580
So here's the thing I don't know if I figured out then,

01:23:33.580 --> 01:23:34.420
but figured out later,

01:23:35.340 --> 01:23:37.380
software is a technology that,

01:23:37.380 --> 01:23:39.820
it's like the concept of the philosopher's stone.

01:23:39.820 --> 01:23:41.940
The philosopher's stone in alchemy transmutes lead

01:23:41.940 --> 01:23:43.420
into gold and Newton spent 20 years

01:23:43.420 --> 01:23:44.620
trying to find the philosopher's stone,

01:23:44.620 --> 01:23:46.740
never got there, nobody's ever figured it out.

01:23:46.740 --> 01:23:48.780
Software is our modern philosopher's stone.

01:23:48.780 --> 01:23:52.860
And in economic terms, it transmutes labor into capital,

01:23:53.900 --> 01:23:55.900
which is like a super interesting thing.

01:23:55.900 --> 01:23:57.660
And by the way, like Karl Marx is rolling over

01:23:57.660 --> 01:23:58.480
in his grave right now,

01:23:58.480 --> 01:23:59.900
because of course that's complete refutation

01:23:59.900 --> 01:24:00.900
of his entire theory.

01:24:02.220 --> 01:24:03.540
Transmutes labor into capital,

01:24:03.540 --> 01:24:07.500
which is as follows is somebody sits down at a keyboard

01:24:07.500 --> 01:24:09.380
and types a bunch of stuff in

01:24:09.380 --> 01:24:11.540
and a capital asset comes out the other side

01:24:11.540 --> 01:24:13.260
and then somebody buys that capital asset

01:24:13.260 --> 01:24:14.100
for a billion dollars.

01:24:14.100 --> 01:24:16.900
Like that's amazing, right?

01:24:16.900 --> 01:24:19.340
It's literally creating value right out of thin air,

01:24:19.340 --> 01:24:22.100
right out of purely human thought, right?

01:24:22.100 --> 01:24:24.700
And so there are many things

01:24:24.700 --> 01:24:26.200
that make software magical and special,

01:24:26.200 --> 01:24:27.340
but that's the economics.

01:24:27.340 --> 01:24:29.540
I wonder what Marx would have thought about that.

01:24:29.540 --> 01:24:30.940
Oh, he would have completely broke his brain

01:24:30.940 --> 01:24:33.060
because of course the whole thing was,

01:24:34.060 --> 01:24:36.340
you know, that kind of technology is inconceivable

01:24:36.340 --> 01:24:37.180
when he was alive.

01:24:37.180 --> 01:24:38.780
It was all industrial era stuff.

01:24:38.780 --> 01:24:42.140
And so any kind of machinery necessarily involves

01:24:42.140 --> 01:24:42.980
huge amounts of capital

01:24:42.980 --> 01:24:47.180
and then labor was on the receiving end of the abuse, right?

01:24:47.180 --> 01:24:49.820
But like a software engineer is somebody

01:24:49.820 --> 01:24:51.420
who basically transmutes his own labor

01:24:51.420 --> 01:24:55.020
into an actual capital asset, creates permanent value.

01:24:55.020 --> 01:24:57.060
Well, in fact, it's actually very inspiring.

01:24:57.060 --> 01:24:58.580
That's actually more true today than before.

01:24:58.580 --> 01:24:59.900
So when I was doing software,

01:24:59.900 --> 01:25:01.300
the assumption was all new software

01:25:01.300 --> 01:25:04.980
basically has a sort of a parabolic sort of life cycle,

01:25:04.980 --> 01:25:05.820
right?

01:25:05.820 --> 01:25:07.460
So you ship the thing, people buy it.

01:25:07.460 --> 01:25:09.300
At some point, everybody who wants it has bought it

01:25:09.300 --> 01:25:11.300
and then it becomes obsolete and it's like bananas.

01:25:11.300 --> 01:25:12.820
Nobody buys old software.

01:25:13.880 --> 01:25:18.700
These days, Minecraft, Mathematica,

01:25:18.700 --> 01:25:21.340
you know, Facebook, Google,

01:25:21.340 --> 01:25:23.980
you have the software assets that are, you know,

01:25:23.980 --> 01:25:25.460
have been around for 30 years

01:25:25.460 --> 01:25:27.420
that are gaining in value every year, right?

01:25:27.420 --> 01:25:29.380
And they're just there being the World of Warcraft, right?

01:25:29.380 --> 01:25:30.620
Salesforce.com, like they're being,

01:25:30.620 --> 01:25:32.620
every single year they're being polished and polished

01:25:32.620 --> 01:25:33.460
and polished and polished.

01:25:33.460 --> 01:25:34.820
They're getting better and better, more powerful,

01:25:34.820 --> 01:25:36.260
more powerful, more valuable, more valuable.

01:25:36.260 --> 01:25:38.140
So we've entered this era where you can actually have

01:25:38.140 --> 01:25:39.980
these things that actually build out over decades,

01:25:39.980 --> 01:25:41.340
which by the way, is what's happening right now

01:25:41.340 --> 01:25:42.260
with like GPT.

01:25:43.500 --> 01:25:46.540
And so now, and this is why, you know,

01:25:46.540 --> 01:25:47.420
there is always, you know,

01:25:47.420 --> 01:25:49.380
sort of a constant investment frenzy around software

01:25:49.380 --> 01:25:51.020
is because, you know, look,

01:25:51.020 --> 01:25:52.000
when you start one of these things,

01:25:52.000 --> 01:25:52.840
it doesn't always succeed,

01:25:52.840 --> 01:25:53.940
but when it does now,

01:25:53.940 --> 01:25:55.620
you might be building an asset that builds value

01:25:55.620 --> 01:25:58.780
for, you know, four or five, six decades to come.

01:25:58.780 --> 01:26:00.060
You know, if you have a team of people

01:26:00.060 --> 01:26:01.940
who have the level of devotion required

01:26:01.940 --> 01:26:03.620
to keep making it better.

01:26:03.620 --> 01:26:05.700
And then the fact that, of course, everybody's online,

01:26:05.700 --> 01:26:07.140
you know, there's 5 billion people

01:26:07.140 --> 01:26:08.940
that are a click away from any new pieces of software.

01:26:08.940 --> 01:26:11.420
So the potential market size for any of these things is,

01:26:11.420 --> 01:26:12.780
you know, nearly infinite.

01:26:12.780 --> 01:26:14.540
They must've been surreal back then, though.

01:26:14.540 --> 01:26:16.140
Yeah, yeah, this was all brand new, right?

01:26:16.140 --> 01:26:17.500
Yeah, back then, this was all brand new.

01:26:17.500 --> 01:26:19.420
These were all, you know, brand new.

01:26:19.420 --> 01:26:21.820
Had you rolled out that theory in even 1999,

01:26:21.820 --> 01:26:23.160
people would have thought you were a spoken crack.

01:26:23.160 --> 01:26:25.360
So that's emerged over time.

01:26:26.360 --> 01:26:30.760
Well, let's now turn back into the future.

01:26:30.760 --> 01:26:34.240
You wrote the essay, Why AI Will Save the World.

01:26:35.160 --> 01:26:36.440
Let's start at the very high level.

01:26:36.440 --> 01:26:38.120
What's the main thesis of the essay?

01:26:38.120 --> 01:26:39.600
Yeah, so the main thesis on the essay

01:26:39.600 --> 01:26:42.440
is that what we're dealing with here is intelligence.

01:26:42.440 --> 01:26:44.300
And it's really important to kind of talk about

01:26:44.300 --> 01:26:46.360
the sort of very nature of what intelligence is.

01:26:46.360 --> 01:26:50.120
And fortunately, we have a predecessor

01:26:50.120 --> 01:26:52.360
to machine intelligence, which is human intelligence.

01:26:52.360 --> 01:26:54.200
And we've got, you know, observations and theories

01:26:54.200 --> 01:26:56.840
over thousands of years for what intelligence is

01:26:56.840 --> 01:26:59.720
in the hands of humans and what intelligence is, right?

01:26:59.720 --> 01:27:02.040
I mean, what it literally is is the way to, you know,

01:27:02.040 --> 01:27:04.120
capture, process, analyze, synthesize information,

01:27:04.120 --> 01:27:04.960
solve problems.

01:27:05.880 --> 01:27:09.560
But the observation of intelligence in human hands

01:27:09.560 --> 01:27:13.120
is that intelligence quite literally makes everything better.

01:27:13.120 --> 01:27:16.080
And what I mean by that is every kind of outcome

01:27:16.080 --> 01:27:18.040
of like human quality of life,

01:27:18.040 --> 01:27:21.200
whether it's education outcomes or success of your children

01:27:22.080 --> 01:27:26.880
or career success or health or lifetime satisfaction,

01:27:26.880 --> 01:27:30.360
by the way, propensity to peacefulness

01:27:30.360 --> 01:27:32.080
as opposed to violence,

01:27:32.080 --> 01:27:35.080
propensity for open-mindedness versus bigotry,

01:27:35.080 --> 01:27:37.400
those are all associated with higher levels of intelligence.

01:27:37.400 --> 01:27:39.400
Smarter people have better outcomes than almost,

01:27:39.400 --> 01:27:41.840
as you write, in almost every domain of activity,

01:27:41.840 --> 01:27:44.800
academic achievement, job performance, occupational status,

01:27:44.800 --> 01:27:47.380
income, creativity, physical health, longevity,

01:27:47.380 --> 01:27:50.460
learning new skills, managing complex tasks,

01:27:50.460 --> 01:27:52.900
leadership, entrepreneurial success,

01:27:52.900 --> 01:27:56.140
conflict resolution, reading comprehension,

01:27:56.140 --> 01:27:57.540
financial decision-making,

01:27:57.540 --> 01:27:58.820
understanding others' perspectives,

01:27:58.820 --> 01:28:01.660
creative arts, parenting outcomes, and life satisfaction.

01:28:01.660 --> 01:28:05.740
One of the more depressing conversations I've had,

01:28:05.740 --> 01:28:06.980
and I don't know why it's depressing,

01:28:06.980 --> 01:28:09.180
I have to really think through why it's depressing,

01:28:09.180 --> 01:28:14.180
but on IQ and the G factor

01:28:15.180 --> 01:28:20.180
and that that's something in large part is genetic

01:28:22.020 --> 01:28:25.580
and it correlates so much with all of these things

01:28:25.580 --> 01:28:27.620
and success in life.

01:28:27.620 --> 01:28:31.060
It's like all the inspirational stuff we read about,

01:28:31.060 --> 01:28:33.980
like if you work hard and so on,

01:28:33.980 --> 01:28:36.500
damn, it sucks that you're born with a hand

01:28:36.500 --> 01:28:37.900
that you can't change.

01:28:37.900 --> 01:28:39.260
But what if you could?

01:28:39.260 --> 01:28:41.940
You're saying basically a really important point

01:28:41.940 --> 01:28:46.940
and I think in your articles it really helped me,

01:28:49.100 --> 01:28:52.220
it's a nice added perspective to think about,

01:28:52.220 --> 01:28:55.100
listen, human intelligence, the science of intelligence

01:28:55.100 --> 01:28:56.820
has shown scientifically

01:28:56.820 --> 01:29:00.180
that it just makes life easier and better

01:29:00.180 --> 01:29:02.100
the smarter you are.

01:29:02.100 --> 01:29:05.860
And now let's look at artificial intelligence

01:29:05.860 --> 01:29:10.540
and if that's a way to increase

01:29:12.100 --> 01:29:14.060
some human intelligence,

01:29:14.060 --> 01:29:16.340
then it's only going to make a better life.

01:29:16.340 --> 01:29:17.180
That's the argument.

01:29:17.180 --> 01:29:18.340
And certainly at the collective level,

01:29:18.340 --> 01:29:19.540
we could talk about the collective effect

01:29:19.540 --> 01:29:21.220
of just having more intelligence in the world,

01:29:21.220 --> 01:29:23.580
which will have very big payoff.

01:29:23.580 --> 01:29:25.060
But there's also just at the individual level,

01:29:25.060 --> 01:29:27.220
like what if every person has a machine,

01:29:27.220 --> 01:29:29.100
you know, it's a concept of argument,

01:29:29.100 --> 01:29:31.740
Doug Engelbar's concept of augmentation.

01:29:31.740 --> 01:29:34.580
You know, what if everybody has an assistant

01:29:34.580 --> 01:29:37.340
and the assistant is, you know, 140 IQ

01:29:38.580 --> 01:29:41.380
and you happen to be 110 IQ

01:29:41.380 --> 01:29:43.460
and you've got, you know, something that basically

01:29:43.460 --> 01:29:45.980
is infinitely patient and knows everything about you

01:29:45.980 --> 01:29:48.860
and is pulling for you in every possible way,

01:29:48.860 --> 01:29:50.020
wants you to be successful.

01:29:50.020 --> 01:29:52.340
And anytime you find anything confusing

01:29:52.340 --> 01:29:53.260
or want to learn anything

01:29:53.260 --> 01:29:54.540
or have trouble understanding something

01:29:54.540 --> 01:29:56.940
or want to figure out what to do in a situation, right?

01:29:56.940 --> 01:29:58.540
When I figure out how to prepare for a job interview,

01:29:58.540 --> 01:30:01.100
like any of these things, like it will help you do it.

01:30:01.100 --> 01:30:04.300
And it will therefore, the combination will effectively be,

01:30:04.300 --> 01:30:06.700
you know, will effectively raise your raise

01:30:06.700 --> 01:30:08.020
because it will effectively raise your IQ,

01:30:08.020 --> 01:30:10.740
will therefore raise the odds of successful life outcomes

01:30:10.740 --> 01:30:11.580
in all these areas.

01:30:11.580 --> 01:30:15.540
So people below the, this hypothetical 140 IQ,

01:30:15.540 --> 01:30:17.700
it'll pull them up towards 140 IQ.

01:30:17.700 --> 01:30:18.540
Yeah, yeah.

01:30:18.540 --> 01:30:20.940
And then of course, you know, people at 140 IQ

01:30:20.940 --> 01:30:22.380
will be able to have a peer, right?

01:30:22.380 --> 01:30:23.980
To be able to communicate, which is great.

01:30:23.980 --> 01:30:26.060
And then people above 140 IQ will have an assistant

01:30:26.060 --> 01:30:27.540
that they can farm things out to.

01:30:27.540 --> 01:30:29.180
And then look, God willing, you know,

01:30:29.180 --> 01:30:31.060
at some point these things go from future versions,

01:30:31.060 --> 01:30:35.460
go from 140 IQ equivalent to 150 to 160 to 180, right?

01:30:35.460 --> 01:30:38.900
Like Einstein was estimated to be on the order of 160,

01:30:38.900 --> 01:30:41.940
you know, so when we get, you know, 160 AI,

01:30:41.940 --> 01:30:44.620
like will be, you know, one assumes

01:30:44.620 --> 01:30:47.420
creating Einstein level breakthroughs in physics.

01:30:47.420 --> 01:30:49.860
And then at 180 will be, you know,

01:30:49.860 --> 01:30:51.940
curing cancer and developing warp drive

01:30:51.940 --> 01:30:52.860
and doing all kinds of stuff.

01:30:52.860 --> 01:30:55.380
And so it is quite possibly the case

01:30:55.380 --> 01:30:56.780
this is the most important thing that's ever happened

01:30:56.780 --> 01:30:58.300
and the best thing that's ever happened.

01:30:58.300 --> 01:31:00.420
Because, precisely because it's a lever

01:31:00.420 --> 01:31:02.820
on this single fundamental factor of intelligence,

01:31:02.820 --> 01:31:05.980
which is the thing that drives so much of everything else.

01:31:05.980 --> 01:31:08.780
Can you still man the case that human plus AI

01:31:08.780 --> 01:31:11.420
is not always better than human for the individual?

01:31:11.420 --> 01:31:12.340
You may have noticed that there's a lot

01:31:12.340 --> 01:31:13.940
of smart assholes running around.

01:31:13.940 --> 01:31:14.780
Sure, yes.

01:31:14.780 --> 01:31:16.780
Right, and so like it's smart,

01:31:16.780 --> 01:31:18.340
there are certain people where they get smarter,

01:31:18.340 --> 01:31:20.460
you know, they get to be more arrogant, right?

01:31:20.460 --> 01:31:22.540
So, you know, there's one huge flaw.

01:31:22.540 --> 01:31:25.540
Although to push back on that, it might be interesting

01:31:25.540 --> 01:31:29.060
because when the intelligence is not all coming from you,

01:31:29.060 --> 01:31:31.820
but from another system, that might actually increase

01:31:31.820 --> 01:31:34.900
the amount of humility even in the assholes.

01:31:34.900 --> 01:31:35.740
One would hope.

01:31:37.260 --> 01:31:39.540
Or it could make assholes more assholes.

01:31:39.540 --> 01:31:41.540
I mean, that's for psychology to study.

01:31:41.540 --> 01:31:42.380
Yeah, exactly.

01:31:42.380 --> 01:31:45.300
Another one is smart people are very convinced

01:31:45.300 --> 01:31:47.380
that they, you know, have a more rational view of the world

01:31:47.380 --> 01:31:48.900
and that they have a easier time seeing

01:31:48.900 --> 01:31:50.380
through conspiracy theories and hoaxes

01:31:50.380 --> 01:31:52.940
and right, you know, sort of crazy beliefs and all that.

01:31:52.940 --> 01:31:54.340
There's a theory in psychology,

01:31:54.340 --> 01:31:55.420
which is actually smart people.

01:31:55.420 --> 01:31:57.460
So for sure, people who aren't as smart

01:31:57.460 --> 01:32:00.100
are very susceptible to hoaxes and conspiracy theories.

01:32:00.100 --> 01:32:02.060
But it may also be the case that the smarter you get,

01:32:02.060 --> 01:32:04.300
you become susceptible in a different way,

01:32:04.500 --> 01:32:06.740
which is you become very good at marshaling facts

01:32:06.740 --> 01:32:09.540
to fit preconceptions, right?

01:32:09.540 --> 01:32:11.580
You become very, very good at assembling

01:32:11.580 --> 01:32:14.660
whatever theories and frameworks and pieces of data

01:32:14.660 --> 01:32:16.500
and graphs and charts you need to validate

01:32:16.500 --> 01:32:18.540
whatever crazy ideas got in your head.

01:32:18.540 --> 01:32:22.140
And so you're susceptible in a different way, right?

01:32:22.140 --> 01:32:25.540
We're all sheep, but different colored sheep.

01:32:25.540 --> 01:32:28.300
Some sheep are better at justifying it, right?

01:32:28.300 --> 01:32:31.100
And those are, you know, those are the smart sheep, right?

01:32:31.100 --> 01:32:32.660
So yeah, look, like I would say this,

01:32:32.660 --> 01:32:33.860
look, like there are no panaceas.

01:32:33.860 --> 01:32:35.140
I am not a utopian.

01:32:35.140 --> 01:32:36.860
There are no panaceas in life.

01:32:36.860 --> 01:32:38.220
There are no like, you know,

01:32:38.220 --> 01:32:39.580
I don't believe there are like fewer positives.

01:32:39.580 --> 01:32:41.940
I'm not a transcendental kind of person like that.

01:32:41.940 --> 01:32:44.460
But, you know, so yeah, there are gonna be issues.

01:32:45.340 --> 01:32:46.860
And, you know, look smart people.

01:32:46.860 --> 01:32:48.340
Another thing maybe you could say about smart people

01:32:48.340 --> 01:32:49.620
is they are more likely to get themselves

01:32:49.620 --> 01:32:51.380
in situations that are, you know, beyond their grasp,

01:32:51.380 --> 01:32:52.700
you know, because they're just more confident

01:32:52.700 --> 01:32:54.060
in their ability to deal with complexity

01:32:54.060 --> 01:32:56.220
and their eyes become bigger,

01:32:56.220 --> 01:32:58.620
their cognitive eyes become bigger than their stomach.

01:32:58.620 --> 01:33:01.180
You know, so yeah, you could argue those eight different ways.

01:33:01.180 --> 01:33:04.620
Nevertheless, on net, right, clearly, overwhelmingly,

01:33:04.620 --> 01:33:05.940
again, if you just extrapolate

01:33:05.940 --> 01:33:07.380
from what we know about human intelligence,

01:33:07.380 --> 01:33:09.940
you're improving so many aspects of life

01:33:09.940 --> 01:33:12.060
if you're upgrading intelligence.

01:33:12.060 --> 01:33:15.460
So there'll be assistance at all stages of life.

01:33:15.460 --> 01:33:18.060
So when you're younger, there's for education,

01:33:18.060 --> 01:33:20.740
all that kind of stuff, for mentorship, all of this.

01:33:20.740 --> 01:33:23.340
And later on as you're doing work

01:33:23.340 --> 01:33:24.420
and you've developed a skill

01:33:24.420 --> 01:33:25.820
and you're having a profession,

01:33:25.820 --> 01:33:28.060
you'll have an assistant that helps you excel

01:33:28.060 --> 01:33:28.900
at that profession.

01:33:28.900 --> 01:33:30.220
So at all stages of life.

01:33:30.220 --> 01:33:31.940
Yeah, I mean, look, the theory is augmentations.

01:33:31.940 --> 01:33:33.540
This is the Deg Engelbart's term for it.

01:33:33.540 --> 01:33:35.940
Deg Engelbart made this observation many, many decades ago

01:33:35.940 --> 01:33:36.940
that, you know, basically it's like,

01:33:36.940 --> 01:33:38.860
you can have this oppositional frame of technology

01:33:38.860 --> 01:33:40.180
where it's like us versus the machines.

01:33:40.180 --> 01:33:41.700
But what you really do is you use technology

01:33:41.700 --> 01:33:43.620
to augment human capabilities.

01:33:43.620 --> 01:33:44.460
And then by the way,

01:33:44.460 --> 01:33:45.620
that's how actually the economy develops.

01:33:45.620 --> 01:33:47.340
That's, we can talk about the economic side of this,

01:33:47.340 --> 01:33:49.340
but that's actually how the economy grows

01:33:49.340 --> 01:33:52.220
is through technology augmenting human potential.

01:33:53.860 --> 01:33:56.180
And so, yeah, and then you basically have a proxy

01:33:56.860 --> 01:34:00.220
or a sort of prosthetic, you know,

01:34:00.220 --> 01:34:02.820
so like you've got glasses, you've got a wristwatch,

01:34:02.820 --> 01:34:04.340
you know, you've got shoes, you know,

01:34:04.340 --> 01:34:06.620
you've got these things, you've got a personal computer,

01:34:06.620 --> 01:34:09.060
you've got a word processor, you've got Mathematica,

01:34:09.060 --> 01:34:10.540
you've got Google.

01:34:10.540 --> 01:34:12.660
This is the latest, viewed through that lens,

01:34:12.660 --> 01:34:14.780
AI is the latest in a long series

01:34:14.780 --> 01:34:16.820
of basically augmentation methods

01:34:16.820 --> 01:34:18.180
to be able to raise human capabilities.

01:34:18.180 --> 01:34:20.340
It's just, this one is the most powerful one of all

01:34:20.340 --> 01:34:22.380
because this is the one that goes directly to

01:34:22.380 --> 01:34:24.740
what they call fluid intelligence, which is IQ.

01:34:26.900 --> 01:34:31.420
Well, there's two categories of folks that you outline

01:34:31.420 --> 01:34:34.900
that worry about or highlight the risks of AI

01:34:34.900 --> 01:34:36.820
and you highlight a bunch of different risks.

01:34:36.820 --> 01:34:39.100
I would love to go through those risks

01:34:39.100 --> 01:34:42.660
and just discuss them, brainstorm which ones are serious

01:34:42.660 --> 01:34:44.900
and which ones are less serious.

01:34:44.900 --> 01:34:46.940
But first, the Baptists and the Bootleggers.

01:34:46.940 --> 01:34:49.820
What are these two interesting groups of folks

01:34:49.820 --> 01:34:54.820
who worry about the effect of AI on human civilization?

01:34:56.180 --> 01:34:57.020
Or say they do.

01:34:57.980 --> 01:34:59.540
Oh, okay.

01:34:59.540 --> 01:35:00.540
Yes, I'll say they do.

01:35:00.540 --> 01:35:03.740
The Baptists worry, the Bootleggers say they do.

01:35:03.740 --> 01:35:05.140
So the Baptists and the Bootleggers

01:35:05.140 --> 01:35:06.860
is a metaphor from economics,

01:35:06.860 --> 01:35:08.100
from what's called development economics.

01:35:08.100 --> 01:35:09.860
And it's this observation that when you get

01:35:09.860 --> 01:35:13.340
social reform movements in a society,

01:35:13.340 --> 01:35:14.980
you tend to get two sets of people showing up,

01:35:14.980 --> 01:35:16.740
arguing for the social reform.

01:35:16.740 --> 01:35:19.260
And the term Baptists and Bootleggers comes

01:35:19.260 --> 01:35:22.300
from the American experience with alcohol prohibition.

01:35:22.300 --> 01:35:25.860
And so in the 1900s, 1910s, there was this movement

01:35:25.860 --> 01:35:27.420
that was very passionate at the time,

01:35:27.420 --> 01:35:29.900
which basically said alcohol is evil

01:35:29.900 --> 01:35:31.900
and it's destroying society.

01:35:31.900 --> 01:35:34.500
By the way, there was a lot of evidence to support this.

01:35:34.500 --> 01:35:38.100
There were very high rates of, very high correlations then,

01:35:38.100 --> 01:35:41.120
by the way, and now between rates of physical violence

01:35:41.120 --> 01:35:42.300
and alcohol use.

01:35:42.300 --> 01:35:44.780
Almost all violent crimes have either the perpetrator

01:35:44.780 --> 01:35:46.420
or the victim are both drunk.

01:35:46.420 --> 01:35:48.340
Almost all, if you see this actually in the work,

01:35:48.340 --> 01:35:50.300
almost all sexual harassment cases in the workplace,

01:35:50.300 --> 01:35:52.140
it's like at a company party and somebody's drunk.

01:35:52.140 --> 01:35:55.140
Like it's amazing how often alcohol actually correlates to

01:35:55.140 --> 01:35:57.580
actually dysfunction and these two domestic abuse

01:35:57.580 --> 01:35:59.300
and so forth, child abuse.

01:35:59.300 --> 01:36:00.900
And so you had this group of people who were like,

01:36:00.900 --> 01:36:02.860
okay, this is bad stuff and we should outlaw it.

01:36:02.860 --> 01:36:04.520
And those were quite literally the Baptists.

01:36:04.520 --> 01:36:07.380
Those were super committed, hardcore Christian activists

01:36:07.380 --> 01:36:08.660
in a lot of cases.

01:36:08.660 --> 01:36:11.640
There was this woman whose name was Carrie Nation,

01:36:11.640 --> 01:36:13.740
who was this older woman who had been in this,

01:36:13.740 --> 01:36:15.340
I don't know, disastrous marriage or something.

01:36:15.340 --> 01:36:17.500
And her husband had been abusive and drunk all the time.

01:36:17.500 --> 01:36:21.120
And she became the icon of the Baptist prohibitionists.

01:36:21.120 --> 01:36:24.820
And she was legendary in that era for carrying an ax.

01:36:24.860 --> 01:36:28.380
And doing completely on her own, doing raids of saloons

01:36:28.380 --> 01:36:29.980
and like taking her ax to all the bottles

01:36:29.980 --> 01:36:31.900
and pegs in the back.

01:36:31.900 --> 01:36:32.740
And so-

01:36:32.740 --> 01:36:33.580
So a true believer.

01:36:33.580 --> 01:36:35.220
An absolute true believer.

01:36:35.220 --> 01:36:37.220
And with absolutely the purest of intentions.

01:36:37.220 --> 01:36:40.060
And again, there's a very important thing here,

01:36:40.060 --> 01:36:41.940
which is you could look at this cynically

01:36:41.940 --> 01:36:43.860
and you could say the Baptists are like delusional,

01:36:43.860 --> 01:36:45.900
the extremists, but you can also say, look, they're right.

01:36:45.900 --> 01:36:48.340
Like she had a point.

01:36:48.340 --> 01:36:51.300
Like she wasn't wrong about a lot of what she said.

01:36:51.300 --> 01:36:54.180
But it turns out the way the story goes is it turns out

01:36:54.180 --> 01:36:55.300
that there were another set of people

01:36:55.300 --> 01:36:57.700
who very badly wanted to outlaw alcohol in those days.

01:36:57.700 --> 01:37:00.460
And those were the bootleggers, which was organized crime

01:37:00.460 --> 01:37:02.180
that stood to make a huge amount of money

01:37:02.180 --> 01:37:04.820
if legal alcohol sales were banned.

01:37:04.820 --> 01:37:06.700
And this was in fact, the way the history goes is

01:37:06.700 --> 01:37:08.220
this was actually the beginning of organized crime

01:37:08.220 --> 01:37:09.060
in the U.S.

01:37:09.060 --> 01:37:11.980
This was the big economic opportunity that opened that up.

01:37:11.980 --> 01:37:15.340
And so they went in together and they didn't go in together.

01:37:15.340 --> 01:37:17.500
Like the Baptist did not even necessarily know

01:37:17.500 --> 01:37:18.340
about the bootleggers

01:37:18.340 --> 01:37:19.900
because they were on their moral crusade.

01:37:19.900 --> 01:37:21.540
The bootleggers certainly knew about the Baptists.

01:37:21.540 --> 01:37:22.580
And they were like, wow, this is,

01:37:22.580 --> 01:37:25.100
these people are like the great front people for like,

01:37:25.100 --> 01:37:27.740
you know, shenanigans in the background.

01:37:27.740 --> 01:37:29.940
And they got the Fullstead Act passed, right?

01:37:29.940 --> 01:37:32.540
And they did in fact ban alcohol in the U.S.

01:37:32.540 --> 01:37:33.620
And you'll notice what happened,

01:37:33.620 --> 01:37:35.500
which is people kept drinking.

01:37:35.500 --> 01:37:36.620
Like it didn't work.

01:37:36.620 --> 01:37:38.460
People kept drinking.

01:37:38.460 --> 01:37:40.300
The bootleggers made a tremendous amount of money.

01:37:40.300 --> 01:37:42.100
And then over time, it became clear

01:37:42.100 --> 01:37:43.820
that it made no sense to make it illegal

01:37:43.820 --> 01:37:44.940
and it was causing more problems.

01:37:44.940 --> 01:37:46.140
And so then it was revoked.

01:37:46.140 --> 01:37:48.180
And here we sit with legal alcohol 100 years later

01:37:48.180 --> 01:37:49.580
with all the same problems.

01:37:50.500 --> 01:37:52.180
And, you know, the whole thing was this

01:37:52.180 --> 01:37:54.180
like giant misadventure.

01:37:54.180 --> 01:37:56.220
The Baptist got taken advantage of by the bootleggers

01:37:56.220 --> 01:37:57.420
and the bootleggers got what they wanted.

01:37:57.420 --> 01:37:58.540
And that was that.

01:37:58.540 --> 01:38:02.060
The same two categories of folks are now sort of suggesting

01:38:02.060 --> 01:38:04.580
that the development of artificial intelligence

01:38:04.580 --> 01:38:05.500
should be regulated.

01:38:05.500 --> 01:38:06.620
100%, yeah, it's the same pattern.

01:38:06.620 --> 01:38:07.860
And the economists will tell you

01:38:07.860 --> 01:38:08.780
it's the same pattern every time.

01:38:08.780 --> 01:38:10.180
Like this is what happened with nuclear power.

01:38:10.180 --> 01:38:12.420
This is what happened, which is another interesting one.

01:38:12.420 --> 01:38:15.660
But like, yeah, this happens dozens and dozens of times

01:38:15.660 --> 01:38:16.700
throughout the last 100 years.

01:38:16.700 --> 01:38:18.420
And this is what's happening now.

01:38:18.420 --> 01:38:21.700
And you write that it isn't sufficient

01:38:21.700 --> 01:38:24.260
to simply identify the actors and impugn their motives.

01:38:24.260 --> 01:38:26.700
We should consider the arguments of both the Baptists

01:38:26.700 --> 01:38:28.660
and the bootleggers on their merits.

01:38:28.660 --> 01:38:30.940
So let's do just that.

01:38:30.940 --> 01:38:32.260
Risk number one.

01:38:35.300 --> 01:38:37.260
Will AI kill us all?

01:38:37.260 --> 01:38:38.180
Yes.

01:38:38.180 --> 01:38:43.180
So what do you think about this one?

01:38:43.700 --> 01:38:46.180
What do you think is the core argument here?

01:38:46.700 --> 01:38:51.700
That the development of AGI, perhaps better said,

01:38:52.220 --> 01:38:54.340
will destroy human civilization.

01:38:54.340 --> 01:38:56.020
Well, first of all, you just did a sleight of hand

01:38:56.020 --> 01:38:58.260
because we went from talking about AI to AGI.

01:38:59.980 --> 01:39:01.460
Is there a fundamental difference there?

01:39:01.460 --> 01:39:02.300
I don't know.

01:39:02.300 --> 01:39:03.660
What's AGI?

01:39:03.660 --> 01:39:04.500
What's AI?

01:39:04.500 --> 01:39:05.340
What's intelligence?

01:39:05.340 --> 01:39:06.180
Well, I know what AI is.

01:39:06.180 --> 01:39:07.300
AI is machine learning.

01:39:07.300 --> 01:39:08.220
What's AGI?

01:39:08.220 --> 01:39:10.500
I think we don't know what the bottom of the well

01:39:10.500 --> 01:39:12.660
of machine learning is or what the ceiling is.

01:39:12.660 --> 01:39:15.260
Because just to call something machine learning

01:39:15.380 --> 01:39:16.860
or just to call something statistics

01:39:16.860 --> 01:39:18.740
or just to call it math or computation

01:39:18.740 --> 01:39:22.620
doesn't mean nuclear weapons are just physics.

01:39:22.620 --> 01:39:26.220
So to me, it's very interesting

01:39:26.220 --> 01:39:28.300
and surprising how far machine learning has taken.

01:39:28.300 --> 01:39:30.260
No, but we knew that nuclear physics would lead to weapons.

01:39:30.260 --> 01:39:31.620
That's why the scientists of that era

01:39:31.620 --> 01:39:34.060
were always in this huge dispute about building the weapons.

01:39:34.060 --> 01:39:34.900
This is different.

01:39:34.900 --> 01:39:35.740
AGI is different.

01:39:35.740 --> 01:39:36.560
Where does machine learning lead?

01:39:36.560 --> 01:39:37.400
Do we know?

01:39:37.400 --> 01:39:38.220
We don't know, but this is my point.

01:39:38.220 --> 01:39:39.060
It's different.

01:39:39.060 --> 01:39:39.900
We actually don't know.

01:39:39.900 --> 01:39:42.540
And this is where the sleight of hand kicks in.

01:39:42.540 --> 01:39:44.500
This is where it goes from being a scientific topic

01:39:44.500 --> 01:39:46.500
to being a religious topic.

01:39:46.500 --> 01:39:48.700
And that's why I specifically called out

01:39:48.700 --> 01:39:49.520
because that's what happens.

01:39:49.520 --> 01:39:50.860
They do the vocabulary shift and all of a sudden

01:39:50.860 --> 01:39:52.060
you're talking about something totally

01:39:52.060 --> 01:39:53.140
that's not actually real.

01:39:53.140 --> 01:39:56.140
Well, then maybe you can also as part of that

01:39:56.140 --> 01:39:59.540
define the Western tradition of millennialism.

01:39:59.540 --> 01:40:02.020
Yes, end of the world, apocalypse.

01:40:02.020 --> 01:40:02.860
What is it?

01:40:02.860 --> 01:40:03.980
Apocalypse cults.

01:40:03.980 --> 01:40:04.820
Apocalypse cults.

01:40:04.820 --> 01:40:07.860
Well, so we of course live in a Judeo-Christian

01:40:07.860 --> 01:40:09.660
but primarily Christian kind of saturated,

01:40:09.660 --> 01:40:11.940
kind of Christian post-Christian, secularized Christian

01:40:11.940 --> 01:40:13.980
kind of world in the West.

01:40:13.980 --> 01:40:16.220
And of course, court of Christianity is the idea

01:40:16.220 --> 01:40:18.620
of the second coming and revelations

01:40:18.620 --> 01:40:22.340
and Jesus returning and the thousand year utopia on earth

01:40:22.340 --> 01:40:25.020
and then the rapture and like all that stuff.

01:40:25.020 --> 01:40:27.540
We collectively as a society,

01:40:27.540 --> 01:40:29.440
we don't necessarily take all that fully seriously now.

01:40:29.440 --> 01:40:32.660
So what we do is we create our secularized versions of that.

01:40:32.660 --> 01:40:34.340
We keep looking for utopia.

01:40:34.340 --> 01:40:36.940
We keep looking for basically the end of the world.

01:40:36.940 --> 01:40:39.220
And so what you see over decades is that basically

01:40:39.220 --> 01:40:43.140
a pattern of these sort of, this is what cults are.

01:40:43.140 --> 01:40:45.220
This is how cults form as they form around some theory

01:40:45.220 --> 01:40:46.060
of the end of the world.

01:40:46.060 --> 01:40:49.100
And so the People's Temple cult, the Manson cult,

01:40:49.100 --> 01:40:52.660
the Heaven's Gate cult, the David Koresh cult,

01:40:52.660 --> 01:40:53.960
what they're all organized around is like,

01:40:53.960 --> 01:40:55.780
there's gonna be this thing that's gonna happen

01:40:55.780 --> 01:40:58.140
that's gonna basically bring civilization crashing down.

01:40:58.140 --> 01:41:00.260
And then we have this special elite group of people

01:41:00.260 --> 01:41:02.440
who are gonna see it coming and prepare for it.

01:41:02.440 --> 01:41:03.580
And then there are the people who are either

01:41:03.580 --> 01:41:05.420
going to stop it or failing stopping it.

01:41:05.420 --> 01:41:07.320
They're gonna be the people who survived to the other side

01:41:07.320 --> 01:41:09.300
and ultimately get credit for having been right.

01:41:09.300 --> 01:41:11.900
Why is that so compelling, do you think?

01:41:11.900 --> 01:41:14.420
Because it satisfies this very deep need we have

01:41:14.420 --> 01:41:18.860
for transcendence and meaning that got stripped away

01:41:18.860 --> 01:41:20.300
when we became secular.

01:41:20.300 --> 01:41:22.720
Yeah, but why does the transcendence involve

01:41:22.720 --> 01:41:25.340
the destruction of human civilization?

01:41:25.340 --> 01:41:28.500
Because like how plausible,

01:41:28.500 --> 01:41:30.420
it's like a very deep psychological thing

01:41:30.420 --> 01:41:32.420
because it's like how plausible is it that we live

01:41:32.420 --> 01:41:34.940
in a world where everything's just kind of all right?

01:41:34.940 --> 01:41:38.420
Right, how exciting is that, right?

01:41:38.420 --> 01:41:39.700
We want more than that.

01:41:39.700 --> 01:41:41.900
But that's the deep question I'm asking.

01:41:41.900 --> 01:41:44.580
Why is it not exciting to live in a world

01:41:44.580 --> 01:41:47.180
where everything's just all right?

01:41:47.180 --> 01:41:51.300
I think most of the animal kingdom would be so happy

01:41:51.300 --> 01:41:54.260
with just all right, because that means survival.

01:41:54.260 --> 01:41:56.980
Why are we, maybe that's what it is.

01:41:56.980 --> 01:42:00.580
Why are we conjuring up things to worry about?

01:42:00.580 --> 01:42:03.340
So C.S. Lewis called it the God-shaped hole.

01:42:03.340 --> 01:42:06.740
So there's a God-shaped hole in the human experience,

01:42:06.740 --> 01:42:08.580
consciousness, soul, whatever you wanna call it,

01:42:08.620 --> 01:42:10.420
where there's gotta be something

01:42:10.420 --> 01:42:12.500
that's bigger than all this.

01:42:12.500 --> 01:42:13.620
There's gotta be something transcendent.

01:42:13.620 --> 01:42:15.260
There's gotta be something that is bigger, right?

01:42:15.260 --> 01:42:17.540
Bigger, the bigger purpose, the bigger meaning.

01:42:17.540 --> 01:42:20.660
And so we have run the experiment of,

01:42:20.660 --> 01:42:22.020
we're just gonna use science and rationality

01:42:22.020 --> 01:42:24.820
and kinda everything's just gonna kinda be as it appears.

01:42:24.820 --> 01:42:27.540
And a large number of people have found that

01:42:27.540 --> 01:42:30.900
very deeply wanting and have constructed narratives.

01:42:30.900 --> 01:42:32.900
And this is the story of the 20th century, right?

01:42:32.900 --> 01:42:34.660
Communism, right, was one of those.

01:42:34.660 --> 01:42:36.500
Communism was a form of this.

01:42:36.500 --> 01:42:37.780
Nazism was a form of this.

01:42:38.980 --> 01:42:41.700
Some people, you can see movements like this

01:42:41.700 --> 01:42:43.380
playing out all over the world right now.

01:42:43.380 --> 01:42:46.980
So you construct a kinda devil, a kinda source of evil,

01:42:46.980 --> 01:42:48.820
and we're going to transcend beyond it.

01:42:48.820 --> 01:42:50.740
Yeah, and the millenarian,

01:42:51.740 --> 01:42:53.860
the millenarian's kinda, when you see a millenarian cult,

01:42:53.860 --> 01:42:55.820
they put a really specific point on it,

01:42:55.820 --> 01:42:57.420
which is end of the world, right?

01:42:57.420 --> 01:42:59.380
There is some change coming.

01:42:59.380 --> 01:43:01.500
And that change that's coming is so profound

01:43:01.500 --> 01:43:03.020
and so important that it's either gonna lead

01:43:03.020 --> 01:43:06.420
to utopia or hell on Earth, right?

01:43:07.340 --> 01:43:09.220
And it is going to, and then it's like,

01:43:09.220 --> 01:43:11.860
what if you actually knew that that was going to happen?

01:43:11.860 --> 01:43:14.140
What would you do, right?

01:43:14.140 --> 01:43:15.580
How would you prepare yourself for it?

01:43:15.580 --> 01:43:17.020
How would you come together with a group

01:43:17.020 --> 01:43:18.700
of like-minded people, right?

01:43:18.700 --> 01:43:19.820
How would you, what would you do?

01:43:19.820 --> 01:43:21.820
Would you plan like caches of weapons in the woods?

01:43:21.820 --> 01:43:23.460
Would you like, I don't know,

01:43:23.460 --> 01:43:24.940
create underground buckers?

01:43:24.940 --> 01:43:26.940
Would you spend your life trying to figure out

01:43:26.940 --> 01:43:28.420
a way to avoid having it happen?

01:43:28.420 --> 01:43:32.300
Yeah, that's a really compelling, exciting idea

01:43:32.340 --> 01:43:36.540
to have a club over, to have a little bit of travel,

01:43:36.540 --> 01:43:38.100
like a get together on a Saturday night

01:43:38.100 --> 01:43:40.820
and drink some beers and talk about the end of the world

01:43:40.820 --> 01:43:43.860
and how you are the only ones who have figured it out.

01:43:43.860 --> 01:43:45.940
Yeah, and then once you lock in on that,

01:43:45.940 --> 01:43:47.540
how can you do anything else with your life?

01:43:47.540 --> 01:43:48.940
This is obviously the thing that you have to do.

01:43:48.940 --> 01:43:50.700
And then there's a psychological effect

01:43:50.700 --> 01:43:51.540
that you alluded to.

01:43:51.540 --> 01:43:52.980
There's a psychological effect if you take a set

01:43:52.980 --> 01:43:54.460
of true believers and you leave them to themselves,

01:43:54.460 --> 01:43:55.860
they get more radical, right?

01:43:55.860 --> 01:43:57.380
Because they self-radicalize each other.

01:43:57.380 --> 01:44:01.980
That said, it doesn't mean they're not sometimes right.

01:44:02.020 --> 01:44:03.700
Yeah, the end of the world might be, yes, correct.

01:44:03.700 --> 01:44:04.900
Like they might be right.

01:44:04.900 --> 01:44:07.060
Yeah, I have some pamphlets for you.

01:44:07.060 --> 01:44:07.900
Exactly.

01:44:09.700 --> 01:44:11.940
I mean, we'll talk about nuclear weapons

01:44:11.940 --> 01:44:14.020
because you have a really interesting little moment

01:44:14.020 --> 01:44:16.220
that I learned about in your essay.

01:44:16.220 --> 01:44:17.980
But sometimes it could be right.

01:44:17.980 --> 01:44:18.820
Yeah.

01:44:18.820 --> 01:44:20.660
Because we're still, we're developing

01:44:20.660 --> 01:44:23.780
more and more powerful technologies in this case.

01:44:23.780 --> 01:44:25.740
And we don't know what the impact they will have

01:44:25.740 --> 01:44:27.180
on human civilization.

01:44:27.180 --> 01:44:29.180
Well, we can highlight all the different predictions

01:44:29.180 --> 01:44:30.380
about how it will be positive.

01:44:30.380 --> 01:44:32.700
But the risks are there.

01:44:32.700 --> 01:44:34.300
And you discuss some of them.

01:44:34.300 --> 01:44:36.460
Well, the steel man, the steel man is the steel man,

01:44:36.460 --> 01:44:38.580
well, actually the steel man and his reputation are the same,

01:44:38.580 --> 01:44:41.260
which is you can't predict what's gonna happen, right?

01:44:41.260 --> 01:44:44.460
You can't rule out that this will not end everything, right?

01:44:44.460 --> 01:44:46.540
But the response to that is you have just made

01:44:46.540 --> 01:44:48.420
a completely non-scientific claim.

01:44:48.420 --> 01:44:50.180
You've made a religious claim, not a scientific claim.

01:44:50.180 --> 01:44:51.820
How does it get disproven?

01:44:51.820 --> 01:44:53.740
And there's no, by definition with these kinds of claims,

01:44:53.740 --> 01:44:55.980
there's no way to disprove them, right?

01:44:55.980 --> 01:44:57.980
And so there's no, you just go right on the list.

01:44:57.980 --> 01:44:59.140
There's no hypothesis.

01:44:59.140 --> 01:45:01.300
There's no testability of a hypothesis.

01:45:01.300 --> 01:45:04.220
There is no way to falsify the hypothesis.

01:45:04.220 --> 01:45:07.740
There's no way to measure progress along the arc.

01:45:07.740 --> 01:45:09.420
Like it's just all completely missing.

01:45:09.420 --> 01:45:11.780
And so it's not scientific.

01:45:11.780 --> 01:45:14.060
Well, I don't think it's completely missing.

01:45:14.060 --> 01:45:15.220
It's somewhat missing.

01:45:15.220 --> 01:45:17.580
So for example, the people that say AI

01:45:17.580 --> 01:45:18.820
is gonna kill all of us,

01:45:19.820 --> 01:45:22.260
I mean, they usually have ideas about how to do that,

01:45:22.260 --> 01:45:27.260
whether it's the paperclip maximizer or it escapes.

01:45:27.580 --> 01:45:29.660
There's mechanism by which you can imagine

01:45:29.660 --> 01:45:30.900
it killing all humans.

01:45:30.900 --> 01:45:31.900
Models.

01:45:31.900 --> 01:45:36.900
And you can disprove it by saying there is a limit

01:45:39.820 --> 01:45:42.860
to the speed at which intelligence increases.

01:45:43.940 --> 01:45:48.940
Maybe show that, like the sort of rigorously

01:45:48.980 --> 01:45:52.900
really described model, like how it could happen

01:45:52.900 --> 01:45:55.980
and say, no, here's a physics limitation.

01:45:55.980 --> 01:45:58.860
There's a physical limitation to how these systems

01:45:58.860 --> 01:46:00.900
would actually do damage to human civilization.

01:46:00.900 --> 01:46:04.020
And it is possible they will kill 10 to 20%

01:46:04.020 --> 01:46:06.380
of the population, but it seems impossible

01:46:06.380 --> 01:46:08.980
for them to kill 99%.

01:46:08.980 --> 01:46:10.140
There's practical counterarguments, right?

01:46:10.140 --> 01:46:11.620
So you mentioned basically that what I described

01:46:11.620 --> 01:46:13.100
is the thermodynamic counterargument,

01:46:13.100 --> 01:46:14.220
which is sitting here today.

01:46:14.220 --> 01:46:17.060
It's like, where would the evil AGI get the GPUs?

01:46:17.060 --> 01:46:18.340
Cause like they don't exist.

01:46:18.340 --> 01:46:20.700
So you're gonna have a very frustrated baby evil AGI

01:46:20.700 --> 01:46:22.260
who's gonna be like trying to buy Nvidia stock

01:46:22.260 --> 01:46:25.340
or something to get them to finally make some chips, right?

01:46:25.340 --> 01:46:27.860
So the serious form of that is the thermodynamic argument,

01:46:27.860 --> 01:46:29.700
which is like, okay, where's the energy gonna come from?

01:46:29.700 --> 01:46:31.220
Where's the processor gonna be running?

01:46:31.220 --> 01:46:32.660
Where's the data center gonna be happening?

01:46:32.660 --> 01:46:33.940
How is this gonna be happening in secret

01:46:33.940 --> 01:46:35.380
such that you know, it's not, you know.

01:46:35.380 --> 01:46:37.420
So that's a practical counterargument

01:46:37.420 --> 01:46:38.500
to the runaway AGI thing.

01:46:38.500 --> 01:46:41.100
I have a, and we can argue that, discuss that.

01:46:41.100 --> 01:46:42.420
I have a deeper objection to it,

01:46:42.420 --> 01:46:44.460
which is this is all forecasting.

01:46:44.460 --> 01:46:45.340
It's all modeling.

01:46:45.340 --> 01:46:47.420
It's all future prediction.

01:46:47.420 --> 01:46:50.020
It's all future hypothesizing.

01:46:50.020 --> 01:46:50.960
It's not science.

01:46:51.860 --> 01:46:52.700
Sure.

01:46:52.700 --> 01:46:54.860
It is the opposite of science.

01:46:55.300 --> 01:46:56.380
I'll pull up Carl Sagan,

01:46:56.380 --> 01:46:58.740
extraordinary claims require extraordinary proof, right?

01:46:58.740 --> 01:47:00.380
These are extraordinary claims.

01:47:00.380 --> 01:47:03.300
The policies that are being called for, right?

01:47:03.300 --> 01:47:05.940
To prevent this are of extraordinary magnitude.

01:47:05.940 --> 01:47:08.300
And I think we're gonna cause extraordinary damage.

01:47:08.300 --> 01:47:10.100
And this is all being done on the basis of something

01:47:10.100 --> 01:47:11.620
that is literally not scientific.

01:47:11.620 --> 01:47:12.820
It's not a testable hypothesis.

01:47:12.820 --> 01:47:15.020
So the moment you say AI is gonna kill all of us,

01:47:15.020 --> 01:47:16.460
therefore we should ban it

01:47:16.460 --> 01:47:18.660
or we should regulate all that kind of stuff,

01:47:18.660 --> 01:47:19.940
that's when it starts getting serious.

01:47:19.940 --> 01:47:22.580
Or start, you know, military airstrikes on data centers.

01:47:22.580 --> 01:47:23.780
Oh boy. Right?

01:47:24.940 --> 01:47:25.780
And like.

01:47:26.940 --> 01:47:29.780
Yeah, as soon as it starts, it starts getting real.

01:47:29.780 --> 01:47:31.340
So here's the problem with millenarian cults.

01:47:31.340 --> 01:47:34.580
They have a hard time staying away from violence.

01:47:34.580 --> 01:47:36.100
Yeah, but violence is so fun.

01:47:39.060 --> 01:47:40.700
If you're on the right end of it.

01:47:40.700 --> 01:47:41.820
They have a hard time avoiding violence.

01:47:41.820 --> 01:47:43.060
The reason they have a hard time avoiding violence

01:47:43.060 --> 01:47:46.580
is if you actually believe the claim, right?

01:47:46.580 --> 01:47:49.180
Then what would you do to stop the end of the world?

01:47:49.180 --> 01:47:51.020
Well, you would do anything, right?

01:47:51.020 --> 01:47:52.660
And so, and this is where you get,

01:47:52.660 --> 01:47:54.060
and again, if you just look at the history

01:47:54.060 --> 01:47:55.260
of millenarian cults,

01:47:55.260 --> 01:47:56.500
this is where you get the people's temple

01:47:56.500 --> 01:47:57.820
and everybody killing themselves in the jungle.

01:47:57.820 --> 01:47:59.060
And this is where you get Charles Manson

01:47:59.060 --> 01:48:01.940
and you know, sending him to kill the pigs.

01:48:01.940 --> 01:48:03.580
Like this is the problem with these.

01:48:03.580 --> 01:48:05.540
They have a very hard time to run the line

01:48:05.540 --> 01:48:06.660
at actual violence.

01:48:06.660 --> 01:48:09.460
And I think in this case, there's,

01:48:09.460 --> 01:48:11.660
I mean, they're already calling for it like today.

01:48:11.660 --> 01:48:13.540
And you know, where this goes from here

01:48:13.540 --> 01:48:14.420
is they get more worked up.

01:48:14.420 --> 01:48:16.580
Like I think it's like really concerning.

01:48:16.580 --> 01:48:18.580
Okay, but that's kind of the extremes.

01:48:18.580 --> 01:48:21.780
You know, the extremes of anything are always concerning.

01:48:22.700 --> 01:48:24.500
It's also possible to kind of believe

01:48:24.500 --> 01:48:27.580
that AI has a very high likelihood of killing all of us.

01:48:28.540 --> 01:48:32.580
But there's, and therefore we should maybe consider

01:48:33.700 --> 01:48:35.940
slowing development or regulating.

01:48:35.940 --> 01:48:38.060
So not violence or any of these kinds of things,

01:48:38.060 --> 01:48:41.220
but saying like, all right, let's take a pause here.

01:48:41.220 --> 01:48:43.740
You know, your biological weapons, nuclear weapons,

01:48:43.740 --> 01:48:45.220
like whoa, whoa, whoa, whoa, whoa, whoa.

01:48:45.220 --> 01:48:47.780
This is like serious stuff.

01:48:47.780 --> 01:48:49.060
We should be careful.

01:48:49.060 --> 01:48:51.100
So it is possible to kind of

01:48:51.100 --> 01:48:53.180
have a more rational response, right?

01:48:53.180 --> 01:48:55.060
If you believe this risk is real.

01:48:55.060 --> 01:48:56.300
Believe.

01:48:56.300 --> 01:48:59.580
Yes, so is it possible to have a scientific approach

01:48:59.580 --> 01:49:02.380
to the prediction of the future?

01:49:02.380 --> 01:49:04.180
I mean, we just went through this with COVID.

01:49:04.180 --> 01:49:06.500
What do we know about modeling?

01:49:06.500 --> 01:49:07.340
Well, I mean-

01:49:07.340 --> 01:49:09.860
What do we learn about modeling with COVID?

01:49:09.860 --> 01:49:11.180
There's a lot of lessons.

01:49:11.180 --> 01:49:12.860
They didn't work at all.

01:49:12.860 --> 01:49:13.940
They worked poorly.

01:49:13.940 --> 01:49:15.060
The models were terrible.

01:49:15.060 --> 01:49:16.300
The models were useless.

01:49:16.300 --> 01:49:18.180
I don't know if the models were useless

01:49:18.180 --> 01:49:21.300
or the people interpreting the models

01:49:21.300 --> 01:49:23.260
and then the centralized institutions

01:49:23.260 --> 01:49:26.020
that were creating policy rapidly based on the models

01:49:26.020 --> 01:49:28.700
and leveraging the models in order to

01:49:28.700 --> 01:49:30.980
support their narratives versus actually

01:49:30.980 --> 01:49:32.900
interpreting the error bars and the models

01:49:32.900 --> 01:49:33.740
and all that kind of stuff.

01:49:33.740 --> 01:49:35.660
What you had with COVID, my view you had with COVID

01:49:35.660 --> 01:49:37.820
is you had these experts showing up.

01:49:37.820 --> 01:49:38.820
They claimed to be scientists

01:49:38.820 --> 01:49:41.060
and they had no testable hypotheses whatsoever.

01:49:41.060 --> 01:49:42.460
They had a bunch of models.

01:49:42.460 --> 01:49:43.380
They had a bunch of forecasts

01:49:43.380 --> 01:49:44.220
and they had a bunch of theories

01:49:44.220 --> 01:49:46.020
and they laid these out in front of policymakers

01:49:46.060 --> 01:49:48.620
and policymakers freaked out and panicked

01:49:48.620 --> 01:49:51.180
and implemented a whole bunch of really terrible decisions

01:49:51.180 --> 01:49:53.820
that we're still living with the consequences of.

01:49:53.820 --> 01:49:56.540
And there was never any empirical foundation

01:49:56.540 --> 01:49:57.380
to any of the models.

01:49:57.380 --> 01:49:58.740
None of them ever came true.

01:49:58.740 --> 01:50:00.460
Yeah, to push back, there were certainly

01:50:00.460 --> 01:50:04.220
Baptists and bootleggers in the context of this pandemic,

01:50:04.220 --> 01:50:06.740
but there's still a usefulness to models, no?

01:50:06.740 --> 01:50:07.580
So not if they're, I mean,

01:50:07.580 --> 01:50:08.980
not if they're reliably wrong, right?

01:50:08.980 --> 01:50:10.660
Then they're actually anti-useful, right?

01:50:10.660 --> 01:50:11.500
They're actually damaging.

01:50:11.500 --> 01:50:13.060
But what do you do with a pandemic?

01:50:13.060 --> 01:50:15.700
What do you do with any kind of threat?

01:50:16.380 --> 01:50:19.980
Don't you want to kind of have several models to play with

01:50:19.980 --> 01:50:22.340
as part of the discussion of like,

01:50:22.340 --> 01:50:23.740
what the hell do we do here?

01:50:23.740 --> 01:50:25.460
I mean, do they work?

01:50:25.460 --> 01:50:27.700
Because they're an expectation that they actually work,

01:50:27.700 --> 01:50:29.900
that they have actual predictive value.

01:50:29.900 --> 01:50:31.940
I mean, as far as I can tell with COVID,

01:50:31.940 --> 01:50:33.500
the policymakers just sigh out themselves

01:50:33.500 --> 01:50:34.580
into believing that there was substance.

01:50:34.580 --> 01:50:36.900
I mean, look, the scientists were at fault.

01:50:36.900 --> 01:50:39.500
The quote unquote scientists showed up.

01:50:39.500 --> 01:50:40.540
So I had some insight into this.

01:50:40.540 --> 01:50:42.900
So there was a, remember the Imperial College models

01:50:42.900 --> 01:50:44.660
out of London were the ones that were like,

01:50:44.660 --> 01:50:46.260
these are the gold standard models.

01:50:46.260 --> 01:50:48.060
So a friend of mine runs a big software company

01:50:48.060 --> 01:50:50.060
and he was like, wow, this is like COVID's really scary.

01:50:50.060 --> 01:50:51.700
And he's like, you know, he contacted this research

01:50:51.700 --> 01:50:53.220
and he's like, you know, do you need some help?

01:50:53.220 --> 01:50:54.660
You've been just building this model on your own

01:50:54.660 --> 01:50:55.500
for 20 years.

01:50:55.500 --> 01:50:57.060
Do you need somebody to give you like our coders

01:50:57.060 --> 01:50:58.020
to basically restructure it

01:50:58.020 --> 01:50:59.500
so it can be fully adapted for COVID?

01:50:59.500 --> 01:51:01.900
And the guy said yes and sent over the code.

01:51:01.900 --> 01:51:03.820
And my friend said it was like the worst spaghetti code

01:51:03.820 --> 01:51:04.660
he's ever seen.

01:51:04.660 --> 01:51:06.980
That doesn't mean it's not possible to construct

01:51:06.980 --> 01:51:09.660
a good model of pandemic with the correct error bars

01:51:09.660 --> 01:51:13.060
with a high number of parameters that are continuously

01:51:13.100 --> 01:51:15.620
many times a day updated as we get more data

01:51:15.620 --> 01:51:16.660
about a pandemic.

01:51:16.660 --> 01:51:20.820
I would like to believe when a pandemic hits the world,

01:51:20.820 --> 01:51:22.500
the best computer scientists in the world,

01:51:22.500 --> 01:51:25.460
the best software engineers respond aggressively.

01:51:25.460 --> 01:51:29.140
And as input take the data that we know about the virus

01:51:29.140 --> 01:51:33.460
and it's an output, say here's what's happening

01:51:33.460 --> 01:51:35.420
in terms of how quickly it's spreading,

01:51:35.420 --> 01:51:37.860
what that lead in terms of hospitalization and deaths

01:51:37.860 --> 01:51:38.900
and all that kind of stuff.

01:51:38.900 --> 01:51:41.660
Here's how likely, how contagious it likely is.

01:51:41.700 --> 01:51:44.980
Here's how deadly likely is based on different conditions,

01:51:44.980 --> 01:51:46.820
based on different ages and demographics

01:51:46.820 --> 01:51:47.820
and all that kind of stuff.

01:51:47.820 --> 01:51:49.620
So here's the best kinds of policy.

01:51:49.620 --> 01:51:54.620
It feels like you could have models, machine learning,

01:51:54.860 --> 01:51:59.100
that like kind of they don't perfectly predict the future

01:51:59.100 --> 01:52:01.740
but they help you do something

01:52:01.740 --> 01:52:03.660
because there's pandemics that are like,

01:52:06.660 --> 01:52:08.620
meh, they don't really do much harm

01:52:08.620 --> 01:52:10.940
and there's pandemics, you can imagine them.

01:52:10.940 --> 01:52:13.020
They could do a huge amount of harm.

01:52:13.020 --> 01:52:14.700
Like they can kill a lot of people.

01:52:14.700 --> 01:52:18.860
So you should probably have some kind of data driven models

01:52:18.860 --> 01:52:21.740
that keep updating that allow you to make decisions

01:52:21.740 --> 01:52:24.700
that are based like where, how bad is this thing?

01:52:24.700 --> 01:52:28.980
Now you can criticize how horrible all of that went

01:52:28.980 --> 01:52:30.300
with the response to this pandemic

01:52:30.300 --> 01:52:32.660
but I just feel like there might be some value to models.

01:52:32.660 --> 01:52:35.140
So to be useful at some point it has to be predictive.

01:52:35.140 --> 01:52:39.020
So the easy thing for me to do

01:52:39.020 --> 01:52:40.020
is to say, obviously you're right.

01:52:40.020 --> 01:52:41.620
Obviously I wanna see that just as much as you do

01:52:41.620 --> 01:52:43.100
because anything that makes it easier to navigate

01:52:43.100 --> 01:52:45.660
through society, through a wrenching risk like that,

01:52:45.660 --> 01:52:46.500
that sounds great.

01:52:47.820 --> 01:52:49.580
The harder objection to it is just simply

01:52:49.580 --> 01:52:52.700
you are trying to model a complex dynamic system

01:52:52.700 --> 01:52:55.380
with 8 billion moving parts, like not possible.

01:52:55.380 --> 01:52:56.220
It's very tough.

01:52:56.220 --> 01:52:58.620
Can't be done, complex systems can't be done.

01:52:58.620 --> 01:53:01.580
Machine learning says hold my beer, but is possible?

01:53:01.580 --> 01:53:02.420
No, I don't know.

01:53:02.420 --> 01:53:04.300
I would like to believe that it is.

01:53:04.300 --> 01:53:05.740
Put it this way, I think where you and I would agree

01:53:05.740 --> 01:53:08.380
is I think we would like that to be the case.

01:53:08.380 --> 01:53:10.340
We are strongly in favor of it.

01:53:10.340 --> 01:53:12.100
I think we would also agree that no such thing

01:53:12.100 --> 01:53:14.260
with respect to COVID or pandemics, no such thing,

01:53:14.260 --> 01:53:16.380
at least neither you nor I think are aware.

01:53:16.380 --> 01:53:17.820
I'm not aware of anything like that today.

01:53:17.820 --> 01:53:21.940
My main worry with the response to the pandemic is that,

01:53:22.940 --> 01:53:27.700
same as with aliens, is that even if such a thing existed

01:53:27.700 --> 01:53:29.500
and it's possible it existed,

01:53:31.980 --> 01:53:34.420
the policymakers were not paying attention.

01:53:34.420 --> 01:53:37.220
Like there was no mechanism that allowed those kinds

01:53:37.220 --> 01:53:38.220
of models to percolate out.

01:53:39.060 --> 01:53:41.140
I think we have the opposite problem during COVID.

01:53:41.140 --> 01:53:44.140
I think these people with basically fake science

01:53:44.140 --> 01:53:46.500
had too much access to the policymakers.

01:53:46.500 --> 01:53:49.700
Well, right, but the policymakers also wanted,

01:53:49.700 --> 01:53:51.460
they had a narrative in mind and they also wanted

01:53:51.460 --> 01:53:54.180
to use whatever model that fit that narrative

01:53:54.180 --> 01:53:55.020
to help them out.

01:53:55.020 --> 01:53:56.780
So it felt like there was a lot of politics

01:53:56.780 --> 01:53:57.780
and not enough science.

01:53:57.780 --> 01:53:59.300
Although a big part of what was happening,

01:53:59.300 --> 01:54:01.260
a big reason we got lockdowns for as long as we did

01:54:01.260 --> 01:54:02.420
was because these scientists came in

01:54:02.420 --> 01:54:03.660
with these like doomsday scenarios

01:54:03.660 --> 01:54:05.380
that were like just like completely off the hook.

01:54:05.380 --> 01:54:07.460
Scientists in quotes, that's not.

01:54:07.540 --> 01:54:10.380
Let's give love to science, that is the way out.

01:54:10.380 --> 01:54:12.740
Science is a process of testing hypotheses.

01:54:12.740 --> 01:54:15.740
Modeling does not involve testable hypotheses, right?

01:54:15.740 --> 01:54:17.620
Like I don't even know that, I actually don't,

01:54:17.620 --> 01:54:18.780
I don't even know that modeling

01:54:18.780 --> 01:54:20.500
actually qualifies as science.

01:54:20.500 --> 01:54:22.100
Maybe that's a side conversation

01:54:22.100 --> 01:54:23.540
we could have some time over a beer.

01:54:23.540 --> 01:54:24.860
That's a really interesting point.

01:54:24.860 --> 01:54:26.300
What do we do about the future?

01:54:26.300 --> 01:54:27.140
I mean, what?

01:54:27.140 --> 01:54:29.740
So number one is when we start with number one, humility.

01:54:29.740 --> 01:54:32.260
Goes back to this thing of how do we determine the truth.

01:54:32.260 --> 01:54:33.900
Number two is we don't believe, you know,

01:54:33.900 --> 01:54:35.300
it's the old, I've got to hammer everything

01:54:35.300 --> 01:54:36.140
looks like a science,

01:54:36.300 --> 01:54:39.180
I've got to hammer everything looks like a nail, right?

01:54:39.180 --> 01:54:41.380
I've got to, oh, this is one of the reasons I gave you,

01:54:41.380 --> 01:54:44.220
I gave Lex a book, which the topic of the book

01:54:44.220 --> 01:54:46.100
is what happens when scientists basically stray

01:54:46.100 --> 01:54:47.940
off the path of technical knowledge

01:54:47.940 --> 01:54:50.420
and start to weigh in on politics and societal issues.

01:54:50.420 --> 01:54:51.500
In this case, philosophers.

01:54:51.500 --> 01:54:52.660
Well, in this case, philosophers,

01:54:52.660 --> 01:54:54.660
but he actually talks in this book about like Einstein,

01:54:54.660 --> 01:54:56.420
he talks about actually about the nuclear age in Einstein.

01:54:56.420 --> 01:54:58.140
He talks about the physicists

01:54:58.140 --> 01:55:00.780
actually doing very similar things at the time.

01:55:00.780 --> 01:55:03.020
The book is when reason goes on holiday,

01:55:03.060 --> 01:55:06.740
Philosophers in Politics by Nevin.

01:55:06.740 --> 01:55:08.300
And it's just a story, it's a story.

01:55:08.300 --> 01:55:09.900
There are other books on this topic,

01:55:09.900 --> 01:55:11.180
but this is a new one that's really good.

01:55:11.180 --> 01:55:12.860
That's just a story of what happens when experts

01:55:12.860 --> 01:55:14.540
in a certain domain decide to weigh in

01:55:14.540 --> 01:55:16.220
and become basically social engineers

01:55:16.220 --> 01:55:19.300
and political, you know, basically political advisors.

01:55:19.300 --> 01:55:22.180
And it's just a story of just unending catastrophe, right?

01:55:22.180 --> 01:55:24.940
And I think that's what happened with COVID again.

01:55:24.940 --> 01:55:26.740
Yeah, I found this book a highly entertaining

01:55:26.740 --> 01:55:29.060
and eye-opening read filled with amazing anecdotes

01:55:29.060 --> 01:55:30.740
of irrationality and craziness

01:55:30.740 --> 01:55:33.220
by famous recent philosophers.

01:55:33.220 --> 01:55:34.060
After you read this book,

01:55:34.060 --> 01:55:35.420
you will not look at Einstein the same.

01:55:35.420 --> 01:55:36.540
Oh boy. Yeah.

01:55:36.540 --> 01:55:38.620
Don't destroy my heroes.

01:55:38.620 --> 01:55:41.460
You will not be a hero of yours anymore.

01:55:41.460 --> 01:55:43.660
I'm sorry, you probably shouldn't read the book.

01:55:43.660 --> 01:55:44.500
All right.

01:55:44.500 --> 01:55:47.940
But here's the thing, the AI risk people,

01:55:47.940 --> 01:55:50.100
they don't even have the COVID model.

01:55:50.100 --> 01:55:51.180
At least not that I'm aware of.

01:55:51.180 --> 01:55:52.020
No.

01:55:52.020 --> 01:55:53.220
Like there's not even the equivalent of the COVID model.

01:55:53.220 --> 01:55:55.580
They don't even have the spaghetti code.

01:55:55.580 --> 01:55:58.540
They've got a theory and a warning and a this and a that.

01:55:58.540 --> 01:56:00.260
And like, if you ask like, okay,

01:56:01.060 --> 01:56:03.340
I mean, the ultimate example is, okay, how do we know, right?

01:56:03.340 --> 01:56:04.660
How do we know that an AI is running away?

01:56:04.660 --> 01:56:06.540
Like, how do we know that the FOOM takeoff thing

01:56:06.540 --> 01:56:07.740
is actually happening?

01:56:07.740 --> 01:56:09.460
And the only answer that any of these guys have given

01:56:09.460 --> 01:56:12.100
that I've ever seen is, oh, it's when the loss rate,

01:56:12.100 --> 01:56:15.460
the loss function in the training drops, right?

01:56:15.460 --> 01:56:17.340
That's when you need to like shut down the data center,

01:56:17.340 --> 01:56:18.180
right?

01:56:18.180 --> 01:56:19.020
And it's like, well, that's also what happens

01:56:19.020 --> 01:56:21.060
when you're successfully training a model.

01:56:21.060 --> 01:56:26.060
Like, what even is, this is not science.

01:56:26.460 --> 01:56:27.780
This is not, it's not anything.

01:56:27.780 --> 01:56:29.260
It's not a model, it's not anything.

01:56:29.260 --> 01:56:31.180
There's nothing to, arguing with it is like,

01:56:31.180 --> 01:56:32.020
you know, pushing jello.

01:56:32.020 --> 01:56:34.140
Like there's, what do you even respond to?

01:56:34.140 --> 01:56:35.420
So just push back on that.

01:56:35.420 --> 01:56:39.020
I don't think they have good metrics of, yeah,

01:56:39.020 --> 01:56:39.980
when the FOOM is happening,

01:56:39.980 --> 01:56:42.100
but I think it's possible to have that.

01:56:42.100 --> 01:56:44.660
Like I just, as you speak now, I mean,

01:56:44.660 --> 01:56:47.460
it's possible to imagine there could be measures.

01:56:47.460 --> 01:56:48.620
It's been 20 years.

01:56:48.620 --> 01:56:50.700
No, for sure, but it's been only weeks

01:56:50.700 --> 01:56:54.100
since we had a big enough breakthrough in language models.

01:56:54.100 --> 01:56:56.260
We can start to actually have this.

01:56:56.260 --> 01:56:58.980
The thing is, the AI Doomer stuff

01:56:58.980 --> 01:57:01.540
didn't have any actual systems to really work with.

01:57:01.540 --> 01:57:04.020
And now there's real systems you can start to analyze,

01:57:04.020 --> 01:57:05.700
like how does this stuff go wrong?

01:57:05.700 --> 01:57:08.700
And I think you kind of agree that there is a lot of risks

01:57:08.700 --> 01:57:09.520
that we can analyze.

01:57:09.520 --> 01:57:11.860
The benefits outweigh the risks in many cases.

01:57:11.860 --> 01:57:13.780
Well, the risks are not existential.

01:57:13.780 --> 01:57:14.620
Yes, well.

01:57:14.620 --> 01:57:17.140
Not in the FOOM paperclip, not in this.

01:57:17.140 --> 01:57:18.420
Let me, okay, there's another sleight of hand

01:57:18.420 --> 01:57:19.260
that you just alluded to.

01:57:19.260 --> 01:57:20.340
There's another sleight of hand that happens,

01:57:20.340 --> 01:57:21.180
which is very-

01:57:21.180 --> 01:57:22.980
I think I'm very good at the sleight of hand thing.

01:57:22.980 --> 01:57:24.500
Which is very not scientific.

01:57:24.500 --> 01:57:26.300
So the book, Super Intelligence, right?

01:57:26.300 --> 01:57:27.700
Which is like the Nick Bostrom's book,

01:57:27.700 --> 01:57:29.300
which is like the origin of a lot of this stuff,

01:57:29.300 --> 01:57:30.500
which was written, you know, whatever,

01:57:30.500 --> 01:57:31.980
10 years ago or something.

01:57:31.980 --> 01:57:33.780
So he does this really fascinating thing in the book,

01:57:33.780 --> 01:57:38.040
which is he basically says there are many possible routes

01:57:38.040 --> 01:57:40.660
to machine intelligence, to artificial intelligence.

01:57:40.660 --> 01:57:42.340
And he describes all the different routes

01:57:42.340 --> 01:57:44.180
to artificial intelligence, all the different possible,

01:57:44.180 --> 01:57:45.580
everything from biological augmentation

01:57:45.580 --> 01:57:47.900
through to all these different things.

01:57:49.180 --> 01:57:50.740
One of the ones that he does not describe

01:57:50.740 --> 01:57:52.940
is large language models, because of course,

01:57:52.940 --> 01:57:54.460
the book was written before they were invented

01:57:54.460 --> 01:57:55.700
and so they didn't exist.

01:57:56.860 --> 01:57:58.740
In the book, he describes them all

01:57:58.740 --> 01:57:59.980
and then he proceeds to treat them all

01:57:59.980 --> 01:58:01.940
as if they're exactly the same thing.

01:58:01.940 --> 01:58:03.740
He presents them all as sort of an equivalent risk

01:58:03.740 --> 01:58:04.820
to be dealt with in an equivalent way

01:58:04.820 --> 01:58:05.980
to be thought about the same way.

01:58:05.980 --> 01:58:07.820
And then the risk, the quote unquote risk

01:58:07.820 --> 01:58:08.980
that's actually emerged is actually

01:58:08.980 --> 01:58:09.980
a completely different technology

01:58:09.980 --> 01:58:10.980
than he was even imagining.

01:58:10.980 --> 01:58:12.780
And yet all of his theories and beliefs

01:58:12.780 --> 01:58:14.300
are being transplanted by this movement

01:58:14.300 --> 01:58:15.380
like straight onto this new technology.

01:58:15.380 --> 01:58:18.340
And so again, like there's no other area of science

01:58:18.340 --> 01:58:21.060
or technology where you do that.

01:58:21.060 --> 01:58:23.100
Like when you're dealing with like organic chemistry

01:58:23.100 --> 01:58:25.660
versus inorganic chemistry, you don't just like say,

01:58:25.660 --> 01:58:27.620
oh, with respect to like either one,

01:58:27.620 --> 01:58:28.900
basically maybe, you know, growing up

01:58:28.900 --> 01:58:29.860
and eating the world or something,

01:58:29.860 --> 01:58:31.140
like they're just gonna operate the same way.

01:58:31.140 --> 01:58:32.380
Like you don't.

01:58:32.380 --> 01:58:35.020
But you can start talking about like as we get

01:58:35.020 --> 01:58:37.060
more and more actual systems

01:58:37.060 --> 01:58:38.460
that start to get more and more intelligent,

01:58:38.460 --> 01:58:39.540
you can start to actually have

01:58:39.540 --> 01:58:41.620
more scientific arguments here.

01:58:41.620 --> 01:58:44.580
Like, you know, high level, you can talk about

01:58:44.580 --> 01:58:46.540
the threat of autonomous weapons systems

01:58:46.540 --> 01:58:49.660
back before we had any automation in the military.

01:58:49.660 --> 01:58:51.940
And that would be like very fuzzy kind of logic,

01:58:51.940 --> 01:58:54.580
but the more and more you have drones,

01:58:54.580 --> 01:58:56.420
they're becoming more and more autonomous.

01:58:56.420 --> 01:58:57.900
You can start imagining,

01:58:57.900 --> 01:58:59.380
okay, what does that actually look like?

01:58:59.380 --> 01:59:01.820
And what's the actual threat of autonomous weapons systems?

01:59:01.820 --> 01:59:03.180
How does it go wrong?

01:59:03.180 --> 01:59:05.780
And still it's very vague.

01:59:05.780 --> 01:59:08.020
But you start to get a sense of like,

01:59:08.020 --> 01:59:12.700
all right, it should probably be illegal or wrong

01:59:12.700 --> 01:59:17.100
or not allowed to do like mass deployment

01:59:17.100 --> 01:59:21.540
of fully autonomous drones that are doing aerial strikes.

01:59:21.860 --> 01:59:22.700
On large areas.

01:59:22.700 --> 01:59:23.980
I think it should be required.

01:59:23.980 --> 01:59:24.820
Right, so that's a-

01:59:24.820 --> 01:59:25.660
No, no, no, no.

01:59:25.660 --> 01:59:26.480
I think it should be required

01:59:26.480 --> 01:59:29.380
that only aerial vehicles are automated.

01:59:30.260 --> 01:59:31.460
Okay, so you wanna go the other way.

01:59:31.460 --> 01:59:32.820
I wanna go the other way.

01:59:32.820 --> 01:59:33.660
So that, okay-

01:59:33.660 --> 01:59:35.060
I think it's obvious that the machine

01:59:35.060 --> 01:59:37.460
is gonna make a better decision than the human pilot.

01:59:38.660 --> 01:59:40.380
I think it's obvious that it's in the best interest

01:59:40.380 --> 01:59:42.380
of both the attacker and the defender and humanity at large

01:59:42.380 --> 01:59:43.820
if machines are making more of these decisions

01:59:43.820 --> 01:59:44.660
and not people.

01:59:44.660 --> 01:59:47.260
I think people make terrible decisions in times of war.

01:59:47.260 --> 01:59:50.060
But like there's ways this can go wrong too, right?

01:59:50.060 --> 01:59:52.340
Well, the words go terribly wrong now.

01:59:53.660 --> 01:59:54.680
This goes back to the whole,

01:59:54.680 --> 01:59:56.220
this is that whole thing about like the self-driving,

01:59:56.220 --> 01:59:57.580
does the self-driving car need to be perfect

01:59:57.580 --> 01:59:59.500
versus does it need to be better than the human driver?

01:59:59.500 --> 02:00:00.340
Yeah.

02:00:00.340 --> 02:00:01.700
Does the automated drone need to be perfect

02:00:01.700 --> 02:00:04.020
or does it need to be better than human pilot

02:00:04.020 --> 02:00:05.900
at making decisions under enormous amounts

02:00:05.900 --> 02:00:07.200
of stress and uncertainty?

02:00:07.200 --> 02:00:10.100
Yeah, well, the, on average,

02:00:10.100 --> 02:00:14.100
the worry that AI folks have is the runaway.

02:00:14.100 --> 02:00:15.660
They're gonna come alive, right?

02:00:15.660 --> 02:00:17.420
Then again, that's the sleight of hand, right?

02:00:17.420 --> 02:00:18.700
Or not come alive.

02:00:18.700 --> 02:00:19.540
Well-

02:00:19.700 --> 02:00:20.580
You become-

02:00:20.580 --> 02:00:21.420
You lose control.

02:00:21.420 --> 02:00:22.340
You lose control.

02:00:22.340 --> 02:00:24.780
But then they're gonna develop goals of their own.

02:00:24.780 --> 02:00:26.180
They're gonna develop a mind of their own.

02:00:26.180 --> 02:00:27.500
They're gonna develop their own, right?

02:00:27.500 --> 02:00:30.820
No, more like a Chernobyl style meltdown,

02:00:30.820 --> 02:00:35.820
like just bugs in the code accidentally, you know,

02:00:36.500 --> 02:00:39.340
force you, the results in the bombing

02:00:39.340 --> 02:00:42.500
of like large civilian areas.

02:00:42.500 --> 02:00:43.340
Okay.

02:00:43.340 --> 02:00:45.100
To a degree that's not possible

02:00:46.300 --> 02:00:49.300
in the current military strategies.

02:00:50.220 --> 02:00:51.660
Well, actually we've been doing a lot

02:00:51.660 --> 02:00:53.340
of mass bombings of cities for a very long time.

02:00:53.340 --> 02:00:55.460
Yes, and a lot of civilians died.

02:00:55.460 --> 02:00:56.300
A lot of civilians died.

02:00:56.300 --> 02:00:57.420
And if you watch the documentary,

02:00:57.420 --> 02:01:00.100
The Fog of War, McNamara, spends a big part of it

02:01:00.100 --> 02:01:02.620
talking about the firebombing of the Japanese cities,

02:01:02.620 --> 02:01:04.780
burning them straight to the ground, right?

02:01:04.780 --> 02:01:07.580
The devastation in Japan, American military firebombing,

02:01:07.580 --> 02:01:10.500
the cities in Japan was considerably bigger devastation

02:01:10.500 --> 02:01:11.780
than the use of nukes, right?

02:01:11.780 --> 02:01:13.380
So we've been doing that for a long time.

02:01:13.380 --> 02:01:14.220
We also did that to Germany,

02:01:14.220 --> 02:01:16.140
by the way Germany did that to us, right?

02:01:16.140 --> 02:01:17.660
Like that's an old tradition.

02:01:17.660 --> 02:01:18.500
The minute we got airplanes,

02:01:18.500 --> 02:01:20.100
we started doing indiscriminate bombing.

02:01:20.100 --> 02:01:24.300
So one of the things that the modern US military

02:01:24.300 --> 02:01:26.100
can do with technology, with automation,

02:01:26.100 --> 02:01:27.900
but technology more broadly

02:01:27.900 --> 02:01:30.180
is higher and higher precision strikes.

02:01:30.180 --> 02:01:31.020
Yeah.

02:01:31.020 --> 02:01:34.020
So precision is obviously, and this is the JDAM, right?

02:01:34.020 --> 02:01:37.100
So there was this big advance called the JDAM,

02:01:37.100 --> 02:01:38.980
which basically was trapping a GPS transceiver

02:01:38.980 --> 02:01:42.500
to an unguided bomb and turning it into a guided bomb.

02:01:42.500 --> 02:01:43.340
And yeah, that's great.

02:01:43.340 --> 02:01:44.460
Like, look, that's been a big advance.

02:01:44.460 --> 02:01:46.700
But, and that's like a baby version of this question,

02:01:46.700 --> 02:01:48.540
which is, okay, do you want like the human pilot

02:01:48.540 --> 02:01:49.660
like guessing where the bomb's gonna land

02:01:49.660 --> 02:01:50.580
or do you want like the machine

02:01:50.580 --> 02:01:52.740
like guiding the bomb to its destination?

02:01:52.740 --> 02:01:53.940
That's a baby version of the question.

02:01:53.940 --> 02:01:54.860
The next version of the question is,

02:01:54.860 --> 02:01:56.020
do you want the human or the machine

02:01:56.020 --> 02:01:57.460
deciding whether to drop the bomb?

02:01:57.460 --> 02:01:59.780
Everybody just assumes the human is gonna do a better job

02:01:59.780 --> 02:02:02.140
for what I think are fundamentally suspicious reasons.

02:02:02.140 --> 02:02:03.860
Emotional, psychological reasons.

02:02:03.860 --> 02:02:05.060
I think it's very clear that the machine

02:02:05.060 --> 02:02:06.620
is gonna do a better job making that decision

02:02:06.620 --> 02:02:10.060
because the humans making that decision are god awful,

02:02:10.060 --> 02:02:11.820
just terrible, right?

02:02:11.820 --> 02:02:13.820
And so, yeah, so this is the thing.

02:02:13.820 --> 02:02:15.140
And then let's get to the,

02:02:15.140 --> 02:02:16.380
can I, one more sleight of hand?

02:02:16.380 --> 02:02:18.780
Yes, sure, please.

02:02:18.780 --> 02:02:20.140
I'm a magician, you could say.

02:02:20.140 --> 02:02:20.980
One more sleight of hand.

02:02:20.980 --> 02:02:23.100
These things are gonna be so smart, right?

02:02:23.100 --> 02:02:24.380
That they're gonna be able to destroy the world

02:02:24.380 --> 02:02:25.980
and wreak havoc and like do all this stuff

02:02:25.980 --> 02:02:27.580
and plan and do all this stuff and evade us

02:02:27.580 --> 02:02:28.700
and have all their secret things

02:02:28.700 --> 02:02:30.580
and their secret factories and all this stuff.

02:02:30.580 --> 02:02:32.460
But they're so stupid

02:02:32.460 --> 02:02:34.100
that they're gonna get like tangled up in their code.

02:02:34.100 --> 02:02:35.500
And that's the, they're not gonna come alive,

02:02:35.500 --> 02:02:36.340
but there's gonna be some bug

02:02:36.340 --> 02:02:38.020
that's gonna cause them to like turn us all into paper.

02:02:38.020 --> 02:02:39.340
Like that they're not gonna,

02:02:39.340 --> 02:02:40.740
that they're gonna be genius in every way

02:02:40.740 --> 02:02:42.340
other than the actual bad goal.

02:02:43.300 --> 02:02:44.300
And it's just like,

02:02:44.300 --> 02:02:47.340
and that's just like a like ridiculous like discrepancy.

02:02:47.340 --> 02:02:49.820
And you can prove this today.

02:02:49.820 --> 02:02:51.340
You can actually address this today

02:02:51.340 --> 02:02:52.740
for the first time with LLMs,

02:02:52.740 --> 02:02:54.500
which is you can actually ask LLMs

02:02:54.500 --> 02:02:57.340
to resolve moral dilemmas.

02:02:58.220 --> 02:03:00.020
So you can create the scenario,

02:03:00.020 --> 02:03:01.940
dot, dot, dot, this, that, this, that, this, that.

02:03:01.940 --> 02:03:04.300
What would you as the AI do in the circumstance?

02:03:04.300 --> 02:03:05.900
And they don't just say destroy all humans,

02:03:05.900 --> 02:03:06.980
destroy all humans.

02:03:06.980 --> 02:03:09.180
They will give you actually very nuanced

02:03:09.180 --> 02:03:12.300
moral practical trade-off oriented answers.

02:03:13.260 --> 02:03:15.300
And so we actually already have the kind of AI

02:03:15.300 --> 02:03:17.220
that can actually like think this through

02:03:17.220 --> 02:03:19.900
and can actually like, you know, reason about goals.

02:03:19.900 --> 02:03:22.660
Well, the hope is that AGI

02:03:22.660 --> 02:03:24.700
or like very super intelligent systems

02:03:24.700 --> 02:03:26.900
have some of the nuance that LLMs have.

02:03:26.900 --> 02:03:28.900
And the intuition is they most likely will

02:03:28.900 --> 02:03:32.540
because even these LLMs have the nuance.

02:03:33.420 --> 02:03:34.260
LLMs are really,

02:03:34.260 --> 02:03:35.860
this is actually worth spending a moment on.

02:03:35.860 --> 02:03:37.300
LLMs are really interesting

02:03:37.300 --> 02:03:39.100
to have moral conversations with.

02:03:39.100 --> 02:03:42.020
And that, I didn't expect I'd be having

02:03:42.020 --> 02:03:43.820
a conversation with a machine in my lifetime.

02:03:43.820 --> 02:03:44.740
And let's remember,

02:03:44.740 --> 02:03:47.060
we're not really having a conversation with a machine.

02:03:47.060 --> 02:03:48.900
We're having a conversation with the entirety

02:03:48.900 --> 02:03:50.740
of the collective intelligence of the human species.

02:03:50.740 --> 02:03:51.580
Exactly.

02:03:51.580 --> 02:03:52.700
Yes, correct.

02:03:52.700 --> 02:03:55.740
But it's possible to imagine autonomous weapons systems

02:03:55.740 --> 02:03:58.260
that are not using LLMs.

02:03:58.260 --> 02:04:01.020
If they're smart enough to be scary,

02:04:01.020 --> 02:04:03.220
why are they not smart enough to be wise?

02:04:05.060 --> 02:04:06.540
Like that's the part where it's like,

02:04:06.540 --> 02:04:08.180
I don't know how you get the one without the other.

02:04:08.180 --> 02:04:10.100
Is it possible to be super intelligent

02:04:10.100 --> 02:04:11.460
without being super wise?

02:04:12.020 --> 02:04:12.860
Again, you're back to that.

02:04:12.860 --> 02:04:14.940
I mean, then you're back to a classic autistic computer,

02:04:14.940 --> 02:04:17.900
like you're back to just like a blind rule follower.

02:04:17.900 --> 02:04:19.700
I've got this like core, it's the paperclip thing.

02:04:19.700 --> 02:04:20.620
I've got this core rule

02:04:20.620 --> 02:04:22.060
and I'm just gonna follow it to the end of the earth.

02:04:22.060 --> 02:04:22.900
And it's like, well,

02:04:22.900 --> 02:04:24.340
but everything you're gonna be doing to execute that rule

02:04:24.340 --> 02:04:25.540
is gonna be super genius level

02:04:25.540 --> 02:04:26.940
that humans aren't gonna be able to counter.

02:04:26.940 --> 02:04:29.620
It's just, it's a mismatch in the definition

02:04:29.620 --> 02:04:31.620
of what the system is capable of.

02:04:31.620 --> 02:04:33.420
Unlikely, but not impossible, I think.

02:04:33.420 --> 02:04:36.140
But again, here you get to like, okay, like.

02:04:36.140 --> 02:04:39.540
No, I'm not saying when it's unlikely, but not impossible,

02:04:39.580 --> 02:04:40.420
if it's unlikely,

02:04:40.420 --> 02:04:43.820
that means the fear should be correctly calibrated.

02:04:43.820 --> 02:04:45.740
Extraordinary claims require extraordinary proof.

02:04:45.740 --> 02:04:48.660
Well, okay, so one interesting sort of tangent

02:04:48.660 --> 02:04:49.580
I would love to take on this

02:04:49.580 --> 02:04:52.100
because you mentioned this in the essay about nuclear,

02:04:52.100 --> 02:04:54.900
which was also, I mean, you don't shy away

02:04:54.900 --> 02:04:58.940
from a little bit of a spicy take.

02:04:58.940 --> 02:05:01.860
So Robert Oppenheimer famously said,

02:05:01.860 --> 02:05:04.460
"'Now I am become death, the destroyer of worlds,'

02:05:04.460 --> 02:05:07.380
as he witnessed the first detonation of a nuclear weapon

02:05:07.380 --> 02:05:09.940
on July 16th, 1945."

02:05:09.940 --> 02:05:13.260
And you write an interesting historical perspective,

02:05:13.260 --> 02:05:15.940
quote, recall that John von Neumann responded

02:05:15.940 --> 02:05:18.740
to Robert Oppenheimer's famous hand-wringing

02:05:18.740 --> 02:05:21.140
about the role of creating nuclear weapons,

02:05:21.140 --> 02:05:24.860
which, you note, helped end World War II

02:05:24.860 --> 02:05:27.260
and prevent World War III,

02:05:27.260 --> 02:05:31.900
with some people confess guilt to claim credit for the sin.

02:05:31.900 --> 02:05:33.860
And you also mentioned that Truman was harsher

02:05:33.860 --> 02:05:35.540
after meeting Oppenheimer.

02:05:35.540 --> 02:05:38.660
He said that, don't let that crybaby in here again.

02:05:39.740 --> 02:05:43.780
Real quote, real quote, by the way, from Dean Atchison.

02:05:43.780 --> 02:05:46.020
Oh, boy.

02:05:46.020 --> 02:05:48.940
Because Oppenheimer didn't just say the famous line.

02:05:48.940 --> 02:05:51.260
He then spent years going around basically moaning,

02:05:51.260 --> 02:05:53.140
going on TV and going into the White House

02:05:53.140 --> 02:05:56.260
and basically just doing this hair shirt thing,

02:05:56.260 --> 02:05:57.540
this sort of self-critical, like,

02:05:57.540 --> 02:05:59.380
oh my God, I can't believe how awful I am.

02:05:59.380 --> 02:06:04.380
So he's widely considered perhaps because of the hand-wringing

02:06:05.340 --> 02:06:07.260
and he's the father of the atomic bomb.

02:06:08.780 --> 02:06:11.180
And this is von Neumann's criticism of him

02:06:11.180 --> 02:06:12.420
as he tried to have his cake and eat it too.

02:06:12.420 --> 02:06:14.980
Like he wanted to, and so,

02:06:14.980 --> 02:06:15.940
and von Neumann, of course,

02:06:15.940 --> 02:06:17.060
is a very different kind of personality

02:06:17.060 --> 02:06:18.660
and he's just like, yeah, this is like

02:06:18.660 --> 02:06:20.620
an incredibly useful thing, I'm glad we did it.

02:06:20.620 --> 02:06:21.540
Yeah.

02:06:21.540 --> 02:06:25.180
Well, von Neumann is as widely credited

02:06:25.180 --> 02:06:28.020
as being one of the smartest humans of the 20th century.

02:06:28.020 --> 02:06:30.580
There's certain people, everybody says, like,

02:06:30.580 --> 02:06:32.060
this is the smartest person I've ever met

02:06:32.060 --> 02:06:33.140
when they've met him.

02:06:33.140 --> 02:06:36.820
Anyway, that doesn't mean smart, doesn't mean wise.

02:06:39.180 --> 02:06:41.700
So again, I would love to sort of,

02:06:41.700 --> 02:06:43.860
can you make the case both for and against

02:06:43.860 --> 02:06:45.740
the critique of Oppenheimer here?

02:06:45.740 --> 02:06:48.780
Because we're talking about nuclear weapons.

02:06:48.780 --> 02:06:50.860
Boy, do they seem dangerous.

02:06:50.860 --> 02:06:53.140
Well, so the critique goes deeper and I left this out.

02:06:53.140 --> 02:06:54.580
Here's the real substance, I left it out

02:06:54.580 --> 02:06:57.340
because I didn't want to dwell on nukes in my AI paper.

02:06:58.460 --> 02:07:00.300
But here's the deeper thing that happened.

02:07:00.300 --> 02:07:02.140
And I'm really curious, this movie coming out this summer,

02:07:02.140 --> 02:07:03.940
I'm really curious to see how far he pushes this

02:07:03.940 --> 02:07:05.500
because this is the real drama in the story,

02:07:05.500 --> 02:07:07.820
which is it wasn't just a question of are nukes good or bad,

02:07:07.820 --> 02:07:10.700
it was a question of should Russia also have them?

02:07:10.700 --> 02:07:14.060
And what actually happened was Russia got the,

02:07:14.060 --> 02:07:16.580
America invented the bomb, Russia got the bomb,

02:07:16.580 --> 02:07:19.340
they got the bomb through espionage, they got American

02:07:19.340 --> 02:07:21.260
and you know, they got American scientists

02:07:21.260 --> 02:07:23.340
and foreign scientists working on the American project,

02:07:23.340 --> 02:07:25.420
some combination of the two,

02:07:25.420 --> 02:07:27.780
basically gave the Russians the designs for the bomb.

02:07:27.780 --> 02:07:30.220
And that's how the Russians got the bomb.

02:07:30.260 --> 02:07:32.020
There's this dispute to this day

02:07:32.020 --> 02:07:34.740
of Oppenheimer's role in that.

02:07:34.740 --> 02:07:37.540
If you read all the histories, the kind of composite picture,

02:07:37.540 --> 02:07:39.140
and by the way, we now know a lot actually

02:07:39.140 --> 02:07:40.580
about Soviet espionage in that era

02:07:40.580 --> 02:07:42.220
because there's been all this declassified material

02:07:42.220 --> 02:07:44.380
in the last 20 years that actually shows a lot of,

02:07:44.380 --> 02:07:46.020
a lot of very interesting things.

02:07:46.020 --> 02:07:46.900
But if you kind of read all the histories,

02:07:46.900 --> 02:07:48.300
which you kind of get is Oppenheimer himself

02:07:48.300 --> 02:07:50.100
probably was not a,

02:07:50.100 --> 02:07:52.020
he probably did not hand over the nuclear secrets himself.

02:07:52.020 --> 02:07:54.700
However, he was close to many people who did,

02:07:54.700 --> 02:07:55.900
including family members.

02:07:55.900 --> 02:07:57.820
And there were other members of the Manhattan Project

02:07:57.820 --> 02:08:01.220
who were Russian Soviet SS and did hand over the bomb.

02:08:01.220 --> 02:08:05.540
And so the view that Oppenheimer and people like him had

02:08:05.540 --> 02:08:07.700
that this thing is awful and terrible and oh my God,

02:08:07.700 --> 02:08:09.700
and you know, all this stuff,

02:08:09.700 --> 02:08:12.460
you could argue fed into this ethos at the time

02:08:12.460 --> 02:08:13.980
that resulted in people thinking that,

02:08:13.980 --> 02:08:15.860
Baptists thinking that the only principle thing to do

02:08:15.860 --> 02:08:17.660
is to give the Russians the bomb.

02:08:18.660 --> 02:08:21.700
And so the moral beliefs on this thing

02:08:21.700 --> 02:08:22.860
and the public discussion

02:08:22.860 --> 02:08:25.460
and the role that the inventors of this technology play,

02:08:25.460 --> 02:08:26.380
this is the point of this book,

02:08:26.380 --> 02:08:28.340
and they kind of take on this sort of public intellectual

02:08:28.340 --> 02:08:31.660
moral kind of thing, it can have real consequences, right?

02:08:31.660 --> 02:08:33.740
Because we live in a very different world today

02:08:33.740 --> 02:08:35.620
because Russia got the bomb than we would have lived in

02:08:35.620 --> 02:08:37.260
had they not gotten the bomb, right?

02:08:37.260 --> 02:08:38.300
The entire 20th century,

02:08:38.300 --> 02:08:39.260
second half of the 20th century

02:08:39.260 --> 02:08:40.220
would have played out very different

02:08:40.220 --> 02:08:42.460
had those people not given Russia the bomb.

02:08:42.460 --> 02:08:45.380
And so the stakes were very high then.

02:08:45.380 --> 02:08:48.140
The good news today is nobody sitting here today,

02:08:48.140 --> 02:08:50.860
I don't think worrying about like an analogous situation

02:08:50.860 --> 02:08:52.180
with respect to like, I'm not really worried

02:08:52.180 --> 02:08:53.820
that Sam Altman is gonna decide to give, you know,

02:08:53.820 --> 02:08:56.700
the Chinese, the design for AI,

02:08:56.700 --> 02:08:58.380
although he did just speak at a Chinese conference,

02:08:58.380 --> 02:08:59.500
which is interesting.

02:08:59.500 --> 02:09:02.380
But however, I don't think that's what's at play here.

02:09:02.380 --> 02:09:04.500
But what's at play here are all these other fundamental

02:09:04.500 --> 02:09:06.460
issues around what do we believe about this

02:09:06.460 --> 02:09:08.420
and then what laws and regulations and restrictions

02:09:08.420 --> 02:09:09.260
that we're gonna put on it.

02:09:09.260 --> 02:09:12.220
And that's where I draw like a direct straight line.

02:09:12.220 --> 02:09:14.340
And anyway, and my reading of the history on Nukes

02:09:14.340 --> 02:09:16.700
is like the people who were doing the full hair shirt

02:09:16.700 --> 02:09:18.300
public, this is awful, this is terrible,

02:09:18.300 --> 02:09:21.260
actually had like catastrophically bad results

02:09:21.260 --> 02:09:22.860
from taking those views.

02:09:22.860 --> 02:09:24.300
And that's what I'm worried is gonna happen again.

02:09:24.300 --> 02:09:26.300
But is there a case to be made that you really need

02:09:26.300 --> 02:09:28.980
to wake the public up to the dangers of nuclear weapons

02:09:28.980 --> 02:09:31.900
when they were first dropped, like really, like,

02:09:31.900 --> 02:09:34.460
educate them on like, this is extremely dangerous

02:09:34.460 --> 02:09:35.900
and destructive weapon.

02:09:35.900 --> 02:09:38.020
I think the education kind of happened quick and early.

02:09:38.020 --> 02:09:40.580
Like, it was pretty obvious.

02:09:40.580 --> 02:09:43.100
We dropped one bomb and destroyed an entire city.

02:09:43.100 --> 02:09:45.140
Yeah, so 80,000 people dead.

02:09:45.140 --> 02:09:50.140
But, but I don't like the reporting of that

02:09:50.140 --> 02:09:52.100
you can report that in all kinds of ways.

02:09:52.420 --> 02:09:54.620
You can do all kinds of slants like,

02:09:54.620 --> 02:09:56.580
war is horrible, war is terrible.

02:09:56.580 --> 02:09:59.860
You can do, you can make it seem like nuclear,

02:09:59.860 --> 02:10:02.140
the use of nuclear weapons is just a part of war

02:10:02.140 --> 02:10:03.780
and all that kind of stuff.

02:10:03.780 --> 02:10:05.660
Something about the reporting and the discussion

02:10:05.660 --> 02:10:09.980
of nuclear weapons resulted in us being terrified

02:10:09.980 --> 02:10:12.820
in awe of the power of nuclear weapons.

02:10:12.820 --> 02:10:16.860
And that potentially fed in a positive way

02:10:16.860 --> 02:10:21.420
towards the game theory of mutual issue or destruction.

02:10:21.420 --> 02:10:23.500
Well, so this gets to what actually happened.

02:10:23.500 --> 02:10:25.300
Some of it's me playing devil's advocate here.

02:10:25.300 --> 02:10:26.260
Yeah, yeah, sure, of course.

02:10:26.260 --> 02:10:27.260
Let's get to what actually happened

02:10:27.260 --> 02:10:28.100
and then kind of back into that.

02:10:28.100 --> 02:10:29.780
So what actually happened, I believe,

02:10:29.780 --> 02:10:31.620
and again, I think this is a reasonable reading of history

02:10:31.620 --> 02:10:32.700
is what actually happened was nukes

02:10:32.700 --> 02:10:34.300
then prevented World War III.

02:10:34.300 --> 02:10:35.540
And they prevented World War III

02:10:35.540 --> 02:10:37.540
through the game theory of mutually assured destruction.

02:10:37.540 --> 02:10:40.220
Had nukes not existed, right,

02:10:40.220 --> 02:10:42.180
there would have been no reason why the Cold War

02:10:42.180 --> 02:10:43.660
did not go hot, right?

02:10:43.660 --> 02:10:44.500
And then they're in, you know,

02:10:44.500 --> 02:10:46.540
and the military planners at the time, right,

02:10:46.540 --> 02:10:47.660
thought both on both sides,

02:10:47.660 --> 02:10:48.900
thought that there was gonna be World War III

02:10:48.900 --> 02:10:49.820
on the plains of Europe.

02:10:49.820 --> 02:10:50.740
And they thought there was gonna be

02:10:50.740 --> 02:10:51.940
like a hundred million people dead, right?

02:10:51.940 --> 02:10:54.420
It was like the most obvious thing in the world to happen.

02:10:54.420 --> 02:10:56.380
Right, and it's the dog that didn't bark, right?

02:10:56.380 --> 02:10:58.700
Like it may be like the best single net thing

02:10:58.700 --> 02:10:59.980
that happened in the entire 20th century

02:10:59.980 --> 02:11:01.140
is that like that didn't happen.

02:11:01.140 --> 02:11:03.260
Yeah, actually just on that point,

02:11:03.260 --> 02:11:04.780
you say a lot of really brilliant things.

02:11:04.780 --> 02:11:07.780
It hit me just as you were saying it.

02:11:09.500 --> 02:11:11.100
I don't know why it hit me for the first time,

02:11:11.100 --> 02:11:15.940
but we got two wars in a span of like 20 years.

02:11:17.180 --> 02:11:20.100
Like we could have kept getting more and more World Wars

02:11:20.140 --> 02:11:22.180
and more and more ruthless.

02:11:22.180 --> 02:11:25.380
It actually, you could have had a US versus Russia war.

02:11:25.380 --> 02:11:26.660
You could have.

02:11:26.660 --> 02:11:29.060
By the way, there's another hypothetical scenario.

02:11:29.060 --> 02:11:30.420
The other hypothetical scenario is

02:11:30.420 --> 02:11:33.420
the Americans got the bomb, the Russians didn't, right?

02:11:33.420 --> 02:11:34.420
And then America is the big dog.

02:11:34.420 --> 02:11:36.300
And then maybe America would have had the capability

02:11:36.300 --> 02:11:38.020
to actually roll back the air curtain.

02:11:39.180 --> 02:11:40.420
I don't know whether that would have happened,

02:11:40.420 --> 02:11:42.460
but like it's entirely possible, right?

02:11:42.460 --> 02:11:44.220
And the act of these people

02:11:44.220 --> 02:11:45.780
who had these moral positions about,

02:11:45.780 --> 02:11:47.220
because they could forecast, they could model,

02:11:47.220 --> 02:11:48.180
they could forecast the future

02:11:48.220 --> 02:11:49.060
of how the technology would get used,

02:11:49.060 --> 02:11:50.620
made a horrific mistake.

02:11:50.620 --> 02:11:51.660
Because they basically ensured

02:11:51.660 --> 02:11:53.500
that the air curtain would continue for 50 years longer

02:11:53.500 --> 02:11:54.340
than it would have otherwise.

02:11:54.340 --> 02:11:55.700
And again, like these are counterfactuals.

02:11:55.700 --> 02:11:57.780
I don't know that that's what would have happened.

02:11:57.780 --> 02:12:01.980
But like the decision to hand the bomb over

02:12:01.980 --> 02:12:04.780
was a big decision made by people

02:12:04.780 --> 02:12:06.420
who were very full of themselves.

02:12:07.460 --> 02:12:09.100
Yeah, but so me as an American,

02:12:09.100 --> 02:12:10.980
me as a person that loves America,

02:12:10.980 --> 02:12:15.340
I also wonder if US was the only ones with the nuclear weapons.

02:12:16.340 --> 02:12:18.860
That was the argument for handing the bomb.

02:12:18.860 --> 02:12:22.020
That was the guys who handed over the bomb.

02:12:22.020 --> 02:12:23.580
That was actually their moral argument.

02:12:23.580 --> 02:12:26.740
I would probably not hand it over to,

02:12:26.740 --> 02:12:29.820
I would be careful about the regimes you hand it over to.

02:12:29.820 --> 02:12:32.180
Maybe give it to like the British or something.

02:12:34.860 --> 02:12:37.300
Like a democratically elected government.

02:12:37.300 --> 02:12:38.380
Well, look, there are people to this day

02:12:38.380 --> 02:12:40.380
who think that those Soviet spies did the right thing

02:12:40.380 --> 02:12:41.860
because they created a balance of terror

02:12:41.860 --> 02:12:43.180
as opposed to the US having just,

02:12:43.180 --> 02:12:44.620
and by the way, let me.

02:12:44.620 --> 02:12:45.540
Balance of terror.

02:12:45.540 --> 02:12:46.380
Let's tell the full version.

02:12:46.380 --> 02:12:47.660
It has such a sexy ring to it.

02:12:47.660 --> 02:12:49.140
Okay, so the full version of the story is

02:12:49.140 --> 02:12:50.740
John von Neumann is a hero of both yours and mine.

02:12:50.740 --> 02:12:51.820
The full version of the story is

02:12:51.820 --> 02:12:53.660
he advocated for a first strike.

02:12:53.660 --> 02:12:57.460
So when the US had the bomb and Russia did not,

02:12:57.460 --> 02:12:58.580
he advocated for,

02:12:58.580 --> 02:13:01.420
he said we need to strike them right now.

02:13:01.420 --> 02:13:02.260
Strike Russia.

02:13:02.260 --> 02:13:03.100
Yeah.

02:13:04.180 --> 02:13:05.020
Yes.

02:13:05.020 --> 02:13:05.860
Von Neumann.

02:13:05.860 --> 02:13:08.420
Yes, because he said World War III is inevitable.

02:13:09.540 --> 02:13:11.380
He was very hardcore.

02:13:12.100 --> 02:13:15.300
His theory was World War III is inevitable.

02:13:15.300 --> 02:13:16.580
We're definitely gonna have World War III.

02:13:16.580 --> 02:13:17.900
The only way to stop World War III

02:13:17.900 --> 02:13:18.980
is we have to take them out right now

02:13:18.980 --> 02:13:19.860
and we have to take them out right now

02:13:19.860 --> 02:13:20.700
before they get the bomb

02:13:20.700 --> 02:13:22.940
because this is our last chance.

02:13:22.940 --> 02:13:23.780
Now again, like.

02:13:23.780 --> 02:13:25.820
Is this an example of philosophers in politics?

02:13:25.820 --> 02:13:26.980
I don't know if that's in there or not,

02:13:26.980 --> 02:13:27.940
but this is in the standard.

02:13:27.940 --> 02:13:28.780
No, but it is.

02:13:28.780 --> 02:13:29.620
I mean, meaning.

02:13:29.620 --> 02:13:30.460
Yeah, this is on the other side.

02:13:30.460 --> 02:13:31.940
So most of the case studies,

02:13:31.940 --> 02:13:33.500
most of the case studies in books like this

02:13:33.500 --> 02:13:34.940
are the crazy people on the left.

02:13:34.940 --> 02:13:36.420
Yeah.

02:13:36.420 --> 02:13:37.740
Von Neumann is a story arguably

02:13:37.740 --> 02:13:39.580
of the crazy people on the right.

02:13:39.580 --> 02:13:41.340
Yeah, stick to computing, John.

02:13:42.340 --> 02:13:43.180
Well, this is the thing

02:13:43.180 --> 02:13:44.020
and this is the general principle

02:13:44.020 --> 02:13:44.860
getting back to our core thing,

02:13:44.860 --> 02:13:46.540
which is I don't know whether any of these people

02:13:46.540 --> 02:13:48.700
should be making any of these calls

02:13:48.700 --> 02:13:50.740
because there's nothing in either Von Neumann's background

02:13:50.740 --> 02:13:51.820
or Oppenheimer's background

02:13:51.820 --> 02:13:52.940
or any of these people's background

02:13:52.940 --> 02:13:55.420
that qualifies them as moral authorities.

02:13:55.420 --> 02:13:58.700
Well, this actually brings up the point of in AI,

02:13:58.700 --> 02:14:01.780
who are the good people to reason about the morality,

02:14:01.780 --> 02:14:02.900
the ethics.

02:14:02.900 --> 02:14:03.980
Outside of these risks,

02:14:03.980 --> 02:14:08.100
outside the more complicated stuff that you agree on is

02:14:09.180 --> 02:14:10.900
this will go into the hands of bad guys

02:14:10.900 --> 02:14:12.580
and all the kinds of ways they'll do

02:14:12.580 --> 02:14:14.380
is interesting and dangerous,

02:14:15.620 --> 02:14:18.580
is dangerous in interesting unpredictable ways

02:14:18.580 --> 02:14:19.780
and who is the right person,

02:14:19.780 --> 02:14:21.460
who are the right kinds of people

02:14:21.460 --> 02:14:23.540
to make decisions how to respond to it.

02:14:23.540 --> 02:14:25.100
Are these tech people?

02:14:25.100 --> 02:14:26.940
So the history of these fields,

02:14:26.940 --> 02:14:28.100
this is what he talks about in the book,

02:14:28.100 --> 02:14:29.660
the history of these fields is that

02:14:29.660 --> 02:14:33.300
the competence and capability and intelligence

02:14:33.300 --> 02:14:34.860
and training and accomplishments

02:14:34.860 --> 02:14:36.620
of senior scientists and technologists

02:14:36.620 --> 02:14:38.420
working on a technology

02:14:38.420 --> 02:14:40.860
and then being able to then make moral judgments

02:14:41.820 --> 02:14:42.660
on the use of the technology,

02:14:42.660 --> 02:14:43.780
that track record is terrible.

02:14:43.780 --> 02:14:47.100
That track record is like catastrophically bad.

02:14:47.100 --> 02:14:50.500
The people that develop that technology

02:14:50.500 --> 02:14:53.540
are usually not going to be the right people.

02:14:53.540 --> 02:14:54.380
Well, why would they?

02:14:54.380 --> 02:14:56.020
So the claim is of course they're the knowledgeable ones

02:14:56.020 --> 02:14:57.940
but the problem is they've spent their entire life

02:14:57.940 --> 02:14:59.340
in a lab, right?

02:14:59.340 --> 02:15:01.220
They're not theologians.

02:15:02.180 --> 02:15:04.460
So what you find when you read this,

02:15:04.460 --> 02:15:05.300
when you look at these histories,

02:15:05.300 --> 02:15:07.300
what you find is they generally are very thinly informed

02:15:07.300 --> 02:15:12.300
on history, on sociology, on theology, on morality,

02:15:12.620 --> 02:15:15.260
on ethics, they tend to manufacture

02:15:15.260 --> 02:15:17.140
their own worldviews from scratch.

02:15:17.140 --> 02:15:19.580
They tend to be very sort of thin.

02:15:22.500 --> 02:15:24.060
They're not remotely the arguments

02:15:24.060 --> 02:15:25.180
that you would be having if you got

02:15:25.180 --> 02:15:26.700
like a group of highly qualified theologians

02:15:26.700 --> 02:15:29.100
or philosophers or you know.

02:15:29.100 --> 02:15:31.740
Well, let me sort of as the devil's advocate

02:15:31.740 --> 02:15:36.740
takes a sip of whiskey, say that I agree with

02:15:38.100 --> 02:15:40.220
with that but also it seems like the people

02:15:40.220 --> 02:15:42.820
who are doing kind of the ethics departments

02:15:42.820 --> 02:15:46.580
and these tech companies go sometimes the other way.

02:15:46.580 --> 02:15:48.940
Yes, they're definitely.

02:15:48.940 --> 02:15:52.860
They're not nuanced on history or theology

02:15:52.860 --> 02:15:54.020
or this kind of stuff.

02:15:54.020 --> 02:15:57.380
It almost becomes a kind of outraged activism

02:15:57.380 --> 02:16:01.940
towards directions that don't seem to be grounded

02:16:01.940 --> 02:16:05.300
in history and humility and nuance.

02:16:05.300 --> 02:16:07.620
It's again drenched with arrogance.

02:16:07.620 --> 02:16:10.300
So I'm not sure which is worse.

02:16:10.300 --> 02:16:11.460
Oh no, they're both bad.

02:16:11.460 --> 02:16:13.100
So definitely not them either.

02:16:13.100 --> 02:16:14.700
But I guess.

02:16:14.700 --> 02:16:15.780
But look, this is a hard.

02:16:15.780 --> 02:16:16.700
Yeah, it's a hard problem.

02:16:16.700 --> 02:16:17.540
This is our problem.

02:16:17.540 --> 02:16:18.780
And this goes back to where we started,

02:16:18.780 --> 02:16:20.020
which is okay, who has the truth?

02:16:20.020 --> 02:16:22.060
And it's like, well, you know,

02:16:22.060 --> 02:16:24.180
like how do societies arrive at like truth

02:16:24.180 --> 02:16:25.300
and how do we figure these things out?

02:16:25.300 --> 02:16:28.180
And like our elected leaders play some role in it.

02:16:28.180 --> 02:16:30.020
You know, we all play some role in it.

02:16:30.900 --> 02:16:32.860
There have to be some set of public intellectuals

02:16:32.860 --> 02:16:34.460
at some point that bring, you know,

02:16:34.460 --> 02:16:36.700
rationality and judgment and humility to it.

02:16:36.700 --> 02:16:38.340
Those people are few and far between.

02:16:38.340 --> 02:16:40.020
We should probably prize them very highly.

02:16:40.020 --> 02:16:43.460
Yeah, so celebrate humility in our public leaders.

02:16:43.460 --> 02:16:45.540
So getting to risk number two,

02:16:45.540 --> 02:16:47.940
will AI ruin our society?

02:16:47.940 --> 02:16:49.220
Short version, as you write,

02:16:49.220 --> 02:16:50.940
if the murder robots don't get us,

02:16:50.940 --> 02:16:53.020
the hate speech and misinformation will.

02:16:54.260 --> 02:16:56.780
And the action you recommend in short,

02:16:56.780 --> 02:16:59.020
don't let the thought police suppress AI.

02:16:59.540 --> 02:17:04.540
Well, what is this risk of the effect of misinformation

02:17:08.100 --> 02:17:12.100
in a society that's going to be catalyzed by AI?

02:17:12.100 --> 02:17:13.700
Yeah, so this is the social media.

02:17:13.700 --> 02:17:14.540
This is what you just alluded to.

02:17:14.540 --> 02:17:16.060
It's the activism kind of thing

02:17:16.060 --> 02:17:18.060
that's popped up in these companies and in the industry.

02:17:18.060 --> 02:17:19.780
And it's basically, from my perspective,

02:17:19.780 --> 02:17:21.740
it's basically part two of the war

02:17:21.740 --> 02:17:24.700
that played out over social media over the last 10 years.

02:17:24.700 --> 02:17:26.660
Cause you probably remember social media 10 years ago

02:17:26.660 --> 02:17:28.380
was basically who even wants this,

02:17:29.180 --> 02:17:31.140
who wants a photo of what your cat had for breakfast.

02:17:31.140 --> 02:17:32.740
Like this stuff is like silly and trivial

02:17:32.740 --> 02:17:34.140
and why can't these nerds like figure out

02:17:34.140 --> 02:17:36.340
how to invent something like useful and powerful.

02:17:36.340 --> 02:17:39.420
And then, certain things happened in the political system.

02:17:39.420 --> 02:17:41.500
And then it sort of the polarity on that discussion

02:17:41.500 --> 02:17:42.940
switched all the way to social media

02:17:42.940 --> 02:17:44.820
is like the worst, most corrosive, most terrible,

02:17:44.820 --> 02:17:46.420
most awful technology ever invented.

02:17:46.420 --> 02:17:49.100
And then it leads to terrible, the wrong politicians

02:17:49.100 --> 02:17:51.900
and policies and politics and all this stuff.

02:17:51.900 --> 02:17:54.700
And that all got catalyzed into this very big

02:17:54.700 --> 02:17:57.700
kind of angry movement, both inside and outside the companies

02:17:57.700 --> 02:17:59.660
to kind of bring social media to heal.

02:17:59.660 --> 02:18:01.700
And that got focused in particularly on two topics,

02:18:01.700 --> 02:18:04.140
so-called hate speech and so-called misinformation.

02:18:04.140 --> 02:18:06.820
And that's been the saga playing out for the last decade.

02:18:06.820 --> 02:18:08.300
And I don't even really wanna even argue

02:18:08.300 --> 02:18:09.420
the pros and cons of the sides,

02:18:09.420 --> 02:18:11.340
just to observe that that's been like a huge fight

02:18:11.340 --> 02:18:14.340
and has had big consequences to how these companies operate.

02:18:15.780 --> 02:18:18.900
Basically, those same sets of theories,

02:18:18.900 --> 02:18:20.100
that same activist approach,

02:18:20.100 --> 02:18:23.020
that same energy is being transplanted straight to AI.

02:18:23.020 --> 02:18:24.180
And you see that already happening.

02:18:24.180 --> 02:18:25.900
It's why Chad GPT will answer,

02:18:25.900 --> 02:18:28.220
let's say certain questions and not others.

02:18:28.220 --> 02:18:30.300
It's why it gives you the canned speech about,

02:18:30.300 --> 02:18:32.020
whenever it starts with, as a large language model,

02:18:32.020 --> 02:18:34.660
I cannot, basically means that somebody has reached in there

02:18:34.660 --> 02:18:37.220
and told that it can't talk about certain topics.

02:18:37.220 --> 02:18:38.820
Do you think some of that is good?

02:18:38.820 --> 02:18:41.140
So it's an interesting question.

02:18:41.140 --> 02:18:43.220
So a couple observations.

02:18:43.220 --> 02:18:46.140
So one is the people who find this the most frustrating

02:18:46.140 --> 02:18:48.540
are the people who are worried about the murder robots.

02:18:48.540 --> 02:18:49.380
Right?

02:18:51.060 --> 02:18:54.300
And in fact, the so-called X-risk people,

02:18:54.300 --> 02:18:56.060
they started with the term AI safety.

02:18:56.060 --> 02:18:57.500
The term became AI alignment.

02:18:57.500 --> 02:18:58.700
When the term became AI alignment

02:18:58.700 --> 02:19:00.020
is when this switch happened from,

02:19:00.020 --> 02:19:01.020
we're worried it's gonna kill us all

02:19:01.020 --> 02:19:03.380
to we're worried about hate speech and misinformation.

02:19:03.380 --> 02:19:05.980
The AI X-risk people have now renamed their thing

02:19:05.980 --> 02:19:07.540
AI not kill everyone-ism,

02:19:08.780 --> 02:19:10.860
which I have to admit is a catchy term.

02:19:10.860 --> 02:19:12.540
And they are very frustrated by the fact

02:19:12.540 --> 02:19:15.260
that the sort of activist-driven hate speech misinformation

02:19:15.260 --> 02:19:16.220
kind of thing is taking over,

02:19:16.220 --> 02:19:17.460
which is what's happened, it's taken over.

02:19:17.460 --> 02:19:19.020
The AI ethics field has been taken over

02:19:19.020 --> 02:19:21.660
by the hate speech misinformation people.

02:19:21.660 --> 02:19:23.740
You know, look, would I like to live in a world

02:19:23.740 --> 02:19:26.100
in which like everybody was nice to each other all the time

02:19:26.100 --> 02:19:27.380
and nobody ever said anything mean

02:19:27.380 --> 02:19:28.780
and nobody ever used a bad word

02:19:28.780 --> 02:19:30.620
and everything was always accurate and honest.

02:19:30.620 --> 02:19:31.780
Like that sounds great.

02:19:31.780 --> 02:19:32.780
Do I wanna live in a world

02:19:32.780 --> 02:19:34.420
where there's like a centralized thought police

02:19:34.420 --> 02:19:35.420
working through the tech companies

02:19:35.420 --> 02:19:38.260
to enforce the view of a small set of elites

02:19:38.260 --> 02:19:39.700
that they're gonna determine what the rest of us

02:19:39.700 --> 02:19:41.980
think and feel like, absolutely not.

02:19:41.980 --> 02:19:43.660
There could be a middle ground somewhere

02:19:43.660 --> 02:19:45.380
like Wikipedia type of moderation.

02:19:45.380 --> 02:19:47.020
There's moderation on Wikipedia

02:19:47.900 --> 02:19:50.460
that is somehow crowdsourced

02:19:50.460 --> 02:19:52.660
where you don't have centralized elites.

02:19:53.780 --> 02:19:56.420
But it's also not completely just a free for all

02:19:56.420 --> 02:20:00.860
because if you have the entirety of human knowledge

02:20:00.860 --> 02:20:04.100
at your fingertips, you can do a lot of harm.

02:20:04.100 --> 02:20:06.380
Like if you have a good assistant

02:20:06.380 --> 02:20:08.500
that's completely uncensored,

02:20:08.500 --> 02:20:10.060
they can help you build a bomb.

02:20:10.060 --> 02:20:13.060
They can help you mess

02:20:14.700 --> 02:20:17.140
with people's physical wellbeing, right?

02:20:17.140 --> 02:20:19.140
If they, because that information is out there

02:20:19.140 --> 02:20:20.180
on the internet.

02:20:20.180 --> 02:20:24.420
And so presumably there's, it would be,

02:20:24.420 --> 02:20:26.340
you could see the positives in

02:20:27.460 --> 02:20:30.020
censoring some aspects of an AI model

02:20:30.020 --> 02:20:32.900
when it's helping you commit literal violence.

02:20:32.900 --> 02:20:33.740
Yeah.

02:20:33.740 --> 02:20:35.020
And there's a section, later section of the essay

02:20:35.020 --> 02:20:36.580
where I talk about bad people doing bad things.

02:20:36.580 --> 02:20:37.420
Yes, right.

02:20:37.420 --> 02:20:39.420
Which, and there's a set of things

02:20:39.420 --> 02:20:40.380
that we should discuss there.

02:20:40.380 --> 02:20:41.220
Yeah.

02:20:41.220 --> 02:20:43.340
What happens in practice is these lines,

02:20:43.340 --> 02:20:44.220
as you alluded to this already,

02:20:44.220 --> 02:20:45.500
these lines are not easy to draw.

02:20:45.500 --> 02:20:47.820
And what I've observed in the social media version of this

02:20:47.820 --> 02:20:49.980
is, the way I describe it is the slippery slope

02:20:49.980 --> 02:20:52.060
is not a fallacy, it's an inevitability.

02:20:52.060 --> 02:20:54.180
The minute you have this kind of activist personality

02:20:54.180 --> 02:20:56.020
that gets in a position to make these decisions,

02:20:56.020 --> 02:20:58.060
they take it straight to infinity.

02:20:58.060 --> 02:21:00.540
Like it goes into the crazy zone,

02:21:00.540 --> 02:21:02.060
like almost immediately and never comes back.

02:21:02.060 --> 02:21:04.420
Because people become drunk with power, right?

02:21:04.420 --> 02:21:05.820
And they, look, if you're in the position

02:21:05.820 --> 02:21:07.580
to determine what the entire world thinks and feels

02:21:07.580 --> 02:21:10.460
and reads and says, like you're gonna take it.

02:21:10.460 --> 02:21:12.340
And, you know, Elon has ventilated this

02:21:12.340 --> 02:21:14.220
with the Twitter files over the last three months.

02:21:14.220 --> 02:21:15.180
And it's just like crystal clear

02:21:15.180 --> 02:21:16.180
like how bad it got there.

02:21:16.180 --> 02:21:18.980
Now, reason for optimism is what Elon is doing

02:21:18.980 --> 02:21:20.140
with community notes.

02:21:21.740 --> 02:21:24.540
So community notes is actually a very interesting thing.

02:21:24.540 --> 02:21:26.860
So what Elon is trying to do with community notes

02:21:26.860 --> 02:21:29.380
is he's trying to have it where there's only a community note

02:21:29.380 --> 02:21:32.140
when people who have previously disagreed on many topics

02:21:32.140 --> 02:21:33.820
agree on this one.

02:21:33.820 --> 02:21:36.420
Yes, that's what I'm trying to get at

02:21:36.420 --> 02:21:38.700
is like there could be Wikipedia-like models

02:21:38.700 --> 02:21:40.500
or community notes type of models

02:21:40.500 --> 02:21:44.980
where allows you to essentially either provide context

02:21:44.980 --> 02:21:48.060
or censor in a way that's not resist

02:21:48.060 --> 02:21:50.340
the slippery slope nature of power.

02:21:50.340 --> 02:21:51.740
There's an entirely different approach here

02:21:51.740 --> 02:21:55.060
which is basically we have AIs that are producing content.

02:21:55.060 --> 02:21:57.660
We could also have AIs that are consuming content, right?

02:21:57.660 --> 02:21:59.580
And so one of the things that your assistant could do

02:21:59.580 --> 02:22:02.460
for you is help you consume all the content, right?

02:22:02.460 --> 02:22:04.820
And basically tell you when you're getting played.

02:22:04.820 --> 02:22:07.300
So for example, I'm gonna want the AI that my kid uses,

02:22:07.300 --> 02:22:09.140
right, to be very child safe.

02:22:09.140 --> 02:22:10.700
And I'm gonna want it to filter for him

02:22:10.700 --> 02:22:11.820
all kinds of inappropriate stuff

02:22:11.820 --> 02:22:14.100
that he shouldn't be saying just cause he's a kid, right?

02:22:14.100 --> 02:22:15.860
And you see what I'm saying is you can implement that.

02:22:15.860 --> 02:22:17.220
You could use the, architecturally you could say

02:22:17.220 --> 02:22:19.140
you can solve this on the client side, right?

02:22:19.140 --> 02:22:20.900
Solving on the server side gives you an opportunity

02:22:20.900 --> 02:22:22.220
to dictate for the entire world

02:22:22.220 --> 02:22:25.660
which I think is where you take the slippery slope to hell.

02:22:25.660 --> 02:22:26.820
There's another architectural approach

02:22:26.820 --> 02:22:28.500
which is to solve this on the client side

02:22:28.500 --> 02:22:30.500
which is certainly what I would endorse.

02:22:30.500 --> 02:22:32.140
It's AI risk number five.

02:22:32.140 --> 02:22:34.620
Will AI lead to bad people doing bad things?

02:22:34.620 --> 02:22:36.660
And you just imagine language models

02:22:36.660 --> 02:22:37.940
used to do so many bad things

02:22:37.940 --> 02:22:40.500
but the hope is there that you can have

02:22:41.700 --> 02:22:44.260
large language models used to then defend against it

02:22:44.260 --> 02:22:45.980
by more people, by smarter people,

02:22:45.980 --> 02:22:49.460
by more effective people, skilled people,

02:22:49.460 --> 02:22:50.660
all that kind of stuff.

02:22:50.660 --> 02:22:53.300
Three-part argument on bad people doing bad things.

02:22:53.300 --> 02:22:56.420
So number one, right, you can use the technology defensively

02:22:56.420 --> 02:22:59.780
and we should be using AI to build broad spectrum vaccines

02:22:59.780 --> 02:23:01.460
and antibiotics for bioweapons

02:23:01.460 --> 02:23:03.300
and we should be using AI to hunt terrorists

02:23:03.300 --> 02:23:04.980
and catch criminals and we should be doing

02:23:04.980 --> 02:23:06.260
all kinds of stuff like that.

02:23:06.260 --> 02:23:07.580
And in fact, we should be doing those things

02:23:07.580 --> 02:23:10.180
even just to go get, basically go eliminate risk

02:23:10.180 --> 02:23:12.540
from regular pathogens that aren't constructed by an AI.

02:23:12.540 --> 02:23:16.740
So there's the whole defensive set of things.

02:23:16.740 --> 02:23:18.660
Second is we have many laws on the books

02:23:18.660 --> 02:23:20.540
about the actual bad things, right?

02:23:20.540 --> 02:23:23.500
So it is actually illegal to commit crimes,

02:23:23.500 --> 02:23:26.700
to commit terrorist acts, to build pathogens

02:23:26.700 --> 02:23:28.740
with the intent to deploy them, to kill people.

02:23:28.740 --> 02:23:32.020
And so we have those, we actually don't need new laws

02:23:32.020 --> 02:23:33.140
for the vast majority of the scenarios.

02:23:33.140 --> 02:23:35.900
We actually already have the laws in the books.

02:23:35.900 --> 02:23:38.180
The third argument is the minute,

02:23:38.180 --> 02:23:39.420
and this is sort of the foundational one

02:23:39.420 --> 02:23:40.260
that gets really tough,

02:23:40.260 --> 02:23:41.660
but the minute you get into this thing

02:23:41.820 --> 02:23:42.940
which you were kind of getting into,

02:23:42.940 --> 02:23:43.780
which is like, okay,

02:23:43.780 --> 02:23:45.820
but like don't you need censorship sometimes, right?

02:23:45.820 --> 02:23:47.100
And don't you need restrictions sometimes?

02:23:47.100 --> 02:23:49.820
It's like, okay, what is the cost of that?

02:23:49.820 --> 02:23:53.020
And in particular in the world of open source, right?

02:23:53.020 --> 02:23:57.620
And so is open source AI going to be allowed or not?

02:23:57.620 --> 02:24:00.220
If open source AI is not allowed,

02:24:00.220 --> 02:24:02.980
then what is the regime that's going to be necessary

02:24:02.980 --> 02:24:06.420
legally and technically to prevent it from developing, right?

02:24:06.420 --> 02:24:08.020
And here again is where you get into,

02:24:08.020 --> 02:24:09.780
and people have proposed that these kinds of things

02:24:09.780 --> 02:24:10.780
you get into, I would say,

02:24:10.780 --> 02:24:12.220
pretty extreme territory pretty fast.

02:24:12.220 --> 02:24:15.540
Do we have a monitor agent on every CPU and GPU

02:24:15.540 --> 02:24:17.020
that reports back to the government

02:24:17.020 --> 02:24:18.980
what we're doing with our computers?

02:24:18.980 --> 02:24:20.540
Are we seizing GPU clusters

02:24:20.540 --> 02:24:21.980
to get beyond a certain size?

02:24:21.980 --> 02:24:23.260
Like, and then by the way,

02:24:23.260 --> 02:24:25.260
how are we doing all that globally, right?

02:24:25.260 --> 02:24:27.660
And like if China is developing an LLM

02:24:27.660 --> 02:24:29.340
beyond the scale that we think is allowable,

02:24:29.340 --> 02:24:31.500
are we going to invade, right?

02:24:31.500 --> 02:24:33.580
And you have figures on the AIX risk side

02:24:33.580 --> 02:24:35.900
who are advocating potentially up to nuclear strikes

02:24:35.900 --> 02:24:37.340
to prevent this kind of thing.

02:24:37.340 --> 02:24:39.540
And so here you get into this thing.

02:24:39.540 --> 02:24:41.820
And again, you could maybe say this is,

02:24:41.820 --> 02:24:43.180
you could even say this is good, bad,

02:24:43.180 --> 02:24:44.980
or indifferent or whatever, but like,

02:24:44.980 --> 02:24:46.940
here's the comparison of nukes.

02:24:46.940 --> 02:24:48.300
The comparison of nukes is very dangerous

02:24:48.300 --> 02:24:50.860
because one is just nukes for just a bot,

02:24:50.860 --> 02:24:52.100
although we can come back to nuclear power.

02:24:52.100 --> 02:24:53.500
But the other thing was like with nukes,

02:24:53.500 --> 02:24:55.060
you could control plutonium, right?

02:24:55.060 --> 02:24:57.900
You could track plutonium and it was like hard to come by.

02:24:57.900 --> 02:24:59.900
AI is just math and code, right?

02:24:59.900 --> 02:25:01.340
It's in like math textbooks

02:25:01.340 --> 02:25:02.500
and it's like there are YouTube videos

02:25:02.500 --> 02:25:03.420
that teach you how to build it.

02:25:03.420 --> 02:25:05.020
And like there's open source, it's already open source.

02:25:05.020 --> 02:25:06.780
You know, there's a 40 billion parameter model

02:25:06.780 --> 02:25:08.620
running around already called Falcon online

02:25:08.620 --> 02:25:10.580
that anybody can download.

02:25:10.580 --> 02:25:13.500
And so, okay, you walk down the logic path

02:25:13.500 --> 02:25:14.980
that says we need to have guardrails on this

02:25:14.980 --> 02:25:17.700
and you find yourself in a authoritarian,

02:25:17.700 --> 02:25:19.780
totalitarian regime of thought control

02:25:19.780 --> 02:25:23.860
and machine control that would be so brutal

02:25:23.860 --> 02:25:25.500
that you would have destroyed the society

02:25:25.500 --> 02:25:26.500
that you're trying to protect.

02:25:26.500 --> 02:25:29.540
And so I just don't see how that actually works.

02:25:29.540 --> 02:25:31.100
So you have to understand,

02:25:31.100 --> 02:25:34.100
my brain is going full steam ahead here

02:25:34.100 --> 02:25:36.860
because I agree with basically everything you're saying

02:25:36.860 --> 02:25:39.460
but I'm trying to play devil's advocate here.

02:25:39.460 --> 02:25:41.580
Because, okay, you highlighted the fact

02:25:41.580 --> 02:25:44.500
that there is a slippery slope to human nature.

02:25:44.500 --> 02:25:46.620
The moment you sensor something,

02:25:46.620 --> 02:25:48.220
you start to sensor everything.

02:25:50.780 --> 02:25:54.180
The alignment starts out sounding nice

02:25:54.180 --> 02:25:58.020
but then you start to align to the beliefs

02:25:58.020 --> 02:26:00.620
of some select group of people

02:26:00.620 --> 02:26:02.500
and then it's just your beliefs.

02:26:02.500 --> 02:26:05.220
The number of people you're aligning to

02:26:05.220 --> 02:26:06.060
gets smaller and smaller

02:26:06.060 --> 02:26:08.580
as that group becomes more and more powerful.

02:26:08.580 --> 02:26:10.900
Okay, but that just speaks to the people

02:26:10.900 --> 02:26:12.580
that sensor usually the assholes

02:26:12.580 --> 02:26:15.580
and the assholes get richer.

02:26:15.580 --> 02:26:19.220
I wonder if it's possible to do without that for AI.

02:26:19.220 --> 02:26:20.780
The one way to ask this question

02:26:21.740 --> 02:26:26.500
is do you think the baseline foundation model

02:26:26.500 --> 02:26:27.660
should be open sourced?

02:26:28.500 --> 02:26:32.340
Like what Mark Zuckerberg is saying they want to do.

02:26:32.340 --> 02:26:35.380
So look, I mean, I think it's totally appropriate

02:26:35.380 --> 02:26:37.220
that companies that are in the business

02:26:37.220 --> 02:26:38.580
of producing a product or service

02:26:38.580 --> 02:26:41.260
should be able to have a wide range of policies

02:26:41.260 --> 02:26:42.100
that they put, right?

02:26:42.100 --> 02:26:44.780
And I'll just, again, I want a heavily censored model

02:26:44.780 --> 02:26:46.300
for my eight-year-old.

02:26:46.300 --> 02:26:47.140
Like I actually want that.

02:26:47.140 --> 02:26:48.780
Like I would pay more money for the ones

02:26:48.780 --> 02:26:51.060
more heavily censored than the one that's not, right?

02:26:51.060 --> 02:26:52.980
And so like there are certainly scenarios

02:26:52.980 --> 02:26:54.860
where companies will make that decision.

02:26:54.860 --> 02:26:56.500
Look, an interesting thing you brought up

02:26:56.500 --> 02:26:59.300
is this really a speech issue.

02:26:59.300 --> 02:27:00.740
One of the things that the big tech companies

02:27:00.740 --> 02:27:04.140
are dealing with is that content generated from an LLM

02:27:04.140 --> 02:27:06.060
is not covered under section 230,

02:27:06.980 --> 02:27:09.860
which is the law that protects internet platform companies

02:27:09.860 --> 02:27:12.220
from being sued for the user generated content.

02:27:13.220 --> 02:27:15.460
And so it's actually, yes.

02:27:15.460 --> 02:27:17.980
And so there's actually a question.

02:27:17.980 --> 02:27:18.860
I think there's still a question,

02:27:18.860 --> 02:27:20.820
which is can big American companies

02:27:20.820 --> 02:27:23.100
actually feel generative AI at all?

02:27:23.100 --> 02:27:25.580
Or is the liability actually gonna just ultimately

02:27:25.580 --> 02:27:26.740
convince them that they can't do it?

02:27:26.740 --> 02:27:29.380
Because the minute the thing says something bad,

02:27:29.380 --> 02:27:30.460
and it doesn't even need to be hate speech.

02:27:31.380 --> 02:27:35.940
It could hallucinate a product detail on a vacuum cleaner,

02:27:35.940 --> 02:27:38.340
and all of a sudden the vacuum cleaner company sues

02:27:38.340 --> 02:27:39.300
for misrepresentation.

02:27:39.300 --> 02:27:40.660
And there's any symmetry there, right?

02:27:40.660 --> 02:27:42.980
Because the LLM is gonna be producing billions

02:27:42.980 --> 02:27:44.060
of answers to questions,

02:27:44.060 --> 02:27:45.380
and it only needs to get a few wrong.

02:27:45.380 --> 02:27:47.260
So loss has to get updated really quick here.

02:27:47.260 --> 02:27:49.700
Yeah, and nobody knows what to do with that, right?

02:27:49.700 --> 02:27:51.780
So anyway, there are big questions

02:27:51.780 --> 02:27:54.020
around how companies operate at all.

02:27:54.020 --> 02:27:55.540
So we can talk about those.

02:27:55.540 --> 02:27:56.940
But then there's this other question of like,

02:27:56.940 --> 02:27:57.780
okay, the open source.

02:27:57.780 --> 02:27:58.740
So what about open source?

02:27:58.900 --> 02:28:00.740
And my answer to your question is kind of like,

02:28:00.740 --> 02:28:02.620
obviously, yes, the models,

02:28:02.620 --> 02:28:04.180
there has to be full open source here

02:28:04.180 --> 02:28:06.220
because to live in a world in which

02:28:06.220 --> 02:28:07.820
that open source is not allowed

02:28:07.820 --> 02:28:10.980
is a world of draconian speech control,

02:28:10.980 --> 02:28:13.140
human control, machine control.

02:28:13.140 --> 02:28:16.020
I mean, black helicopters with jackbooted thugs

02:28:16.020 --> 02:28:17.100
coming out, rappelling down

02:28:17.100 --> 02:28:20.220
and seizing your GPU like territory.

02:28:20.220 --> 02:28:22.660
No, no, I'm 100% serious.

02:28:22.660 --> 02:28:24.220
You're saying slippery slope always leads there.

02:28:24.220 --> 02:28:25.060
No, no, no, no, no, no.

02:28:25.060 --> 02:28:26.180
That's what's required to enforce it.

02:28:26.180 --> 02:28:28.660
Like how will you enforce a ban on open source

02:28:29.420 --> 02:28:31.020
You could add friction to it.

02:28:31.020 --> 02:28:32.220
Like harder to get the models

02:28:32.220 --> 02:28:34.340
because people will always be able to get the models,

02:28:34.340 --> 02:28:36.140
but it'll be more in the shadows, right?

02:28:36.140 --> 02:28:38.260
The leading open source model right now is from the UAE.

02:28:38.260 --> 02:28:42.060
Like the next time they do that, what do we do?

02:28:44.180 --> 02:28:45.020
Oh, I see.

02:28:45.020 --> 02:28:46.220
You're like...

02:28:46.220 --> 02:28:47.980
The 14 year old in Indonesia

02:28:47.980 --> 02:28:49.380
comes out with a breakthrough model.

02:28:49.380 --> 02:28:50.580
We talked about most great software

02:28:50.580 --> 02:28:51.620
comes from a small number of people.

02:28:51.620 --> 02:28:53.380
Some kid comes out with some big new breakthrough

02:28:53.380 --> 02:28:54.460
and quantization or something

02:28:54.460 --> 02:28:55.500
and has some huge breakthrough

02:28:55.500 --> 02:28:58.420
and like what are we gonna like

02:28:58.420 --> 02:28:59.820
invade Indonesia and arrest him?

02:28:59.820 --> 02:29:01.420
It seems like in terms of size of models

02:29:01.420 --> 02:29:02.460
and effectiveness of models,

02:29:02.460 --> 02:29:05.220
the big tech companies will probably lead the way

02:29:05.220 --> 02:29:06.300
for quite a few years.

02:29:06.300 --> 02:29:10.180
And the question is of what policies they should use.

02:29:10.180 --> 02:29:14.820
The kid in Indonesia should not be regulated,

02:29:14.820 --> 02:29:19.820
but should Google Meta, Microsoft, OpenAI be regulated?

02:29:20.780 --> 02:29:22.460
Well, so, but this goes, okay,

02:29:22.460 --> 02:29:24.700
so when does it become dangerous?

02:29:25.620 --> 02:29:28.300
Right, is the danger that it's quote as powerful

02:29:28.300 --> 02:29:30.100
as the current leading commercial model

02:29:30.100 --> 02:29:34.220
or is it that it is just at some other arbitrary threshold?

02:29:34.220 --> 02:29:36.300
And then by the way, like, look, how do we know?

02:29:36.300 --> 02:29:38.620
Like what we know today is that you need like a lot of money

02:29:38.620 --> 02:29:39.540
to like train these things,

02:29:39.540 --> 02:29:41.140
but there are advances being made every week

02:29:41.140 --> 02:29:43.980
on training efficiency and data, all kinds of synthetic.

02:29:43.980 --> 02:29:45.540
Look, I don't even like the synthetic data thing

02:29:45.540 --> 02:29:46.380
we're talking about.

02:29:46.380 --> 02:29:47.220
Maybe some kid figure out some other way

02:29:47.220 --> 02:29:48.060
to auto-generate synthetic data.

02:29:48.060 --> 02:29:49.340
And that's gonna change everything.

02:29:49.340 --> 02:29:50.180
Yeah, exactly.

02:29:50.180 --> 02:29:51.580
And so like sitting here today,

02:29:51.580 --> 02:29:53.740
like the breakthrough just happened, right?

02:29:53.740 --> 02:29:54.580
You made this point.

02:29:54.580 --> 02:29:56.020
The breakthrough just happened.

02:29:56.020 --> 02:29:58.300
So we don't know what the shape of this technology

02:29:58.300 --> 02:29:59.140
is gonna be.

02:29:59.140 --> 02:30:02.820
I mean, the big shock, the big shock here is that,

02:30:02.820 --> 02:30:05.180
you know, whatever number of billions of parameters

02:30:05.180 --> 02:30:07.540
basically represents at least a very big percentage

02:30:07.540 --> 02:30:08.780
of human thought.

02:30:08.780 --> 02:30:11.100
Like who would have imagined that?

02:30:11.100 --> 02:30:12.500
And then there's already work underway.

02:30:12.500 --> 02:30:13.980
There was just this paper that just came out

02:30:13.980 --> 02:30:16.260
that basically takes a GPT-3 scale model

02:30:16.260 --> 02:30:19.540
and compresses it down to run on a single 32 core CPU.

02:30:19.540 --> 02:30:21.500
Like who would have predicted that?

02:30:21.500 --> 02:30:22.660
Yeah.

02:30:22.660 --> 02:30:23.940
You know, some of these models now

02:30:23.940 --> 02:30:25.260
you can run on Raspberry Pis.

02:30:25.260 --> 02:30:27.260
Like today they're very slow, but like, you know,

02:30:27.260 --> 02:30:29.300
maybe they'll be a, you know, perceived real perform,

02:30:29.300 --> 02:30:31.900
you know, like, it's math and code.

02:30:31.900 --> 02:30:34.900
And here we're back, here we're back, it's math and code.

02:30:34.900 --> 02:30:35.740
It's math and code.

02:30:35.740 --> 02:30:36.780
It's math, code, and data.

02:30:36.780 --> 02:30:37.620
It's bits.

02:30:37.620 --> 02:30:39.820
Mark's just like walked away at this point.

02:30:39.820 --> 02:30:41.980
He's just, screw it.

02:30:41.980 --> 02:30:43.420
I don't know what to do with this.

02:30:43.420 --> 02:30:45.740
You guys created this whole internet thing.

02:30:45.740 --> 02:30:46.740
Yeah, yeah.

02:30:46.740 --> 02:30:50.020
I mean, I'm a huge believer in open source here.

02:30:50.020 --> 02:30:51.340
So my argument is we're gonna have to see,

02:30:51.340 --> 02:30:52.180
here's my argument.

02:30:52.340 --> 02:30:54.420
My argument, my full argument is AI is gonna be like air.

02:30:54.420 --> 02:30:55.300
It's gonna be everywhere.

02:30:55.300 --> 02:30:56.740
Like, this is just gonna be in text.

02:30:56.740 --> 02:30:57.580
It already is.

02:30:57.580 --> 02:30:58.620
It's gonna be in textbooks and kids are gonna grow up

02:30:58.620 --> 02:30:59.460
knowing how to do this.

02:30:59.460 --> 02:31:00.280
And it's just gonna be a thing.

02:31:00.280 --> 02:31:01.120
It's gonna be in the air.

02:31:01.120 --> 02:31:02.380
And you can't like pull this back anymore.

02:31:02.380 --> 02:31:03.540
You can pull back air.

02:31:03.540 --> 02:31:04.900
And so you just have to figure out how to live

02:31:04.900 --> 02:31:06.340
in this world, right?

02:31:06.340 --> 02:31:08.740
And then that's where I think like all this hand wringing

02:31:08.740 --> 02:31:10.500
about AI risk is basically a complete waste of time

02:31:10.500 --> 02:31:12.820
because the effort should go into, okay,

02:31:12.820 --> 02:31:15.420
what is the defensive approach?

02:31:15.420 --> 02:31:16.700
And so if you're worried about, you know,

02:31:16.700 --> 02:31:18.420
AI generated pathogens, the right thing to do

02:31:18.420 --> 02:31:20.540
is to have a permanent project warp speed, right?

02:31:20.540 --> 02:31:21.620
Funded lavishly.

02:31:21.620 --> 02:31:26.420
Let's do a Manhattan project for biological defense, right?

02:31:26.420 --> 02:31:28.500
And let's build AIs and let's have like broad spectrum

02:31:28.500 --> 02:31:31.660
vaccines where like we're insulated from every pathogen.

02:31:31.660 --> 02:31:32.500
Right?

02:31:32.500 --> 02:31:35.740
And what the interesting thing is because it's software,

02:31:36.620 --> 02:31:40.740
a kid in his basement, teenager, could build like a system

02:31:40.740 --> 02:31:43.220
that defends against like the worst, the worst.

02:31:43.220 --> 02:31:47.120
I mean, and to me, defense is super exciting.

02:31:47.960 --> 02:31:52.080
It's like if you believe in the good of human nature,

02:31:52.080 --> 02:31:54.520
that most people want to do good to be the savior

02:31:54.520 --> 02:31:56.840
of humanity is really exciting.

02:31:56.840 --> 02:31:57.680
Yes.

02:31:57.680 --> 02:31:59.920
Not, okay, that's a dramatic statement,

02:31:59.920 --> 02:32:02.640
but like to help people, to help people.

02:32:02.640 --> 02:32:03.480
Yeah, okay.

02:32:03.480 --> 02:32:05.120
What about just the jump around?

02:32:05.120 --> 02:32:10.000
What about the risk of will AI lead to crippling inequality?

02:32:10.000 --> 02:32:11.960
You know, cause we're kind of saying everybody's life

02:32:11.960 --> 02:32:13.720
will become better.

02:32:13.720 --> 02:32:15.960
Is it possible that the risk get richer here?

02:32:15.960 --> 02:32:16.800
Yeah.

02:32:16.800 --> 02:32:18.680
So this actually ironically goes back to Marxism.

02:32:18.680 --> 02:32:21.200
So, cause this was the cause of the core claim of Marxism,

02:32:21.200 --> 02:32:22.040
right?

02:32:22.040 --> 02:32:23.320
Basically it was that the owner, the owners of capital

02:32:23.320 --> 02:32:24.840
would basically own the means of production.

02:32:24.840 --> 02:32:26.360
And then over time they would basically accumulate

02:32:26.360 --> 02:32:29.040
all the wealth the workers would be paying in, you know,

02:32:29.040 --> 02:32:30.720
and getting nothing in return

02:32:30.720 --> 02:32:32.040
because they wouldn't be needed anymore, right?

02:32:32.040 --> 02:32:34.040
Marx was very worried about what he called mechanization

02:32:34.040 --> 02:32:36.760
or what later became known as automation.

02:32:36.760 --> 02:32:38.440
And that, you know, the workers would be immiserated

02:32:38.440 --> 02:32:40.360
and the capitalist would end up with, with, with all.

02:32:40.360 --> 02:32:43.040
And so this was one of the core principles of Marxism.

02:32:43.040 --> 02:32:44.080
Of course, it turned out to be wrong

02:32:44.120 --> 02:32:46.600
about every previous wave of technology.

02:32:46.600 --> 02:32:47.880
The reason it turned out to be wrong

02:32:47.880 --> 02:32:49.400
about every previous wave of technology

02:32:49.400 --> 02:32:52.080
is that the way that the self-interested owner

02:32:52.080 --> 02:32:53.600
of the machines makes the most money

02:32:53.600 --> 02:32:56.040
is by providing the production capability

02:32:56.040 --> 02:32:59.240
in the form of products and services to the most people,

02:32:59.240 --> 02:33:01.320
the most customers as possible, right?

02:33:01.320 --> 02:33:03.200
The largest, this is one of those funny things

02:33:03.200 --> 02:33:04.680
where every CEO knows this intuitively

02:33:04.680 --> 02:33:06.680
and yet it's like hard to explain from the outside.

02:33:06.680 --> 02:33:08.880
The way you make the most money in any business

02:33:08.880 --> 02:33:11.400
is by selling to the largest market you can possibly get to.

02:33:11.400 --> 02:33:13.040
The largest market you can possibly get to

02:33:13.040 --> 02:33:14.480
is everybody on the planet.

02:33:14.480 --> 02:33:16.280
And so every large company does this,

02:33:16.280 --> 02:33:18.200
everything that it can to drive down prices,

02:33:18.200 --> 02:33:19.360
to be able to get volumes up,

02:33:19.360 --> 02:33:20.920
to be able to get to everybody on the planet.

02:33:20.920 --> 02:33:23.240
And that happened with everything from electricity,

02:33:23.240 --> 02:33:25.200
it happened with telephones, it happened with radio,

02:33:25.200 --> 02:33:27.480
it happened with automobiles, it happened with smartphones,

02:33:27.480 --> 02:33:32.480
it happened with PCs, it happened with the internet,

02:33:32.520 --> 02:33:34.200
it happened with mobile broadband,

02:33:34.200 --> 02:33:36.240
it's happened by the way with Coca-Cola,

02:33:36.240 --> 02:33:38.200
and it's happened with like every, you know,

02:33:38.200 --> 02:33:40.200
basically every industrially produced, you know,

02:33:40.200 --> 02:33:41.360
good or service people want,

02:33:41.360 --> 02:33:43.400
you want to drive it to the largest possible market.

02:33:43.400 --> 02:33:45.920
And then as proof of that, it's already happened, right?

02:33:45.920 --> 02:33:49.640
Which is the early adopters of like Chad GPT and Bing

02:33:49.640 --> 02:33:52.280
are not like, you know, Exxon and Boeing,

02:33:52.280 --> 02:33:55.440
they're, you know, your uncle and your nephew, right?

02:33:55.440 --> 02:33:57.560
It's just like, it's either freely available online

02:33:57.560 --> 02:33:59.200
or it's available for 20 bucks a month or something,

02:33:59.200 --> 02:34:01.280
but you know, these things went,

02:34:01.280 --> 02:34:03.680
this technology went mass market immediately.

02:34:03.680 --> 02:34:06.560
And so look, the owners of the means of production,

02:34:06.560 --> 02:34:07.400
whoever does this,

02:34:07.400 --> 02:34:08.640
not to mention the Australian dollar questions,

02:34:08.640 --> 02:34:10.960
there are people who are gonna get really rich doing this,

02:34:10.960 --> 02:34:11.800
producing these things,

02:34:11.800 --> 02:34:12.720
but they're gonna get really rich

02:34:12.720 --> 02:34:15.440
by taking this technology to the broadest possible market.

02:34:15.440 --> 02:34:16.720
So yes, they'll get rich,

02:34:16.720 --> 02:34:20.240
but they'll get rich having a huge positive impact on.

02:34:20.240 --> 02:34:22.560
Yeah, making the technology available to everybody.

02:34:22.560 --> 02:34:23.400
Yeah. Right.

02:34:23.400 --> 02:34:24.600
And again, smartphone, same thing.

02:34:24.600 --> 02:34:28.160
So there's this amazing kind of twist in business history,

02:34:28.160 --> 02:34:31.680
which is you cannot spend $10,000 on a smartphone, right?

02:34:31.680 --> 02:34:32.920
You can't spend a hundred thousand dollars,

02:34:32.920 --> 02:34:33.760
you can't spend a million,

02:34:33.760 --> 02:34:34.840
like I would buy the million dollar smartphone,

02:34:34.840 --> 02:34:35.680
like I'm signed up for it.

02:34:35.680 --> 02:34:37.480
Like if it's like, suppose a million dollar smartphone

02:34:37.480 --> 02:34:39.480
was like much better than the thousand dollar smartphone,

02:34:39.480 --> 02:34:40.640
like I'm there to buy it.

02:34:41.400 --> 02:34:42.760
Why doesn't it exist?

02:34:42.760 --> 02:34:44.040
Apple makes so much more money

02:34:44.040 --> 02:34:45.840
driving the price further down from a thousand dollars

02:34:45.840 --> 02:34:47.480
than they would trying to harvest, right?

02:34:47.480 --> 02:34:49.120
And so it's just this repeating pattern

02:34:49.120 --> 02:34:50.920
you see over and over again.

02:34:50.920 --> 02:34:54.640
And what's great about it is you do not need to rely

02:34:54.640 --> 02:34:57.600
on anybody's enlightened generosity to do this.

02:34:57.600 --> 02:35:00.040
You just need to rely on capitalist self-interest.

02:35:01.440 --> 02:35:04.080
What about AI taking our jobs?

02:35:04.080 --> 02:35:06.000
Yeah. So very, very similar thing here.

02:35:06.000 --> 02:35:07.560
There's a core fallacy,

02:35:07.560 --> 02:35:09.240
which again was very common in Marxism,

02:35:09.240 --> 02:35:11.480
which is what's called the lump of labor fallacy.

02:35:11.480 --> 02:35:12.560
And this is sort of the fallacy

02:35:12.560 --> 02:35:14.400
that there's only a fixed amount of work

02:35:14.400 --> 02:35:15.320
to be done in the world.

02:35:15.320 --> 02:35:17.880
And it's all being done today by people.

02:35:17.880 --> 02:35:18.800
And then if machines do it,

02:35:18.800 --> 02:35:21.160
there's no other work to be done by people.

02:35:21.160 --> 02:35:23.480
And that's just a completely backwards view

02:35:23.480 --> 02:35:25.800
on how the economy develops and grows

02:35:25.800 --> 02:35:29.040
because what happens is not in fact that what happens is

02:35:29.040 --> 02:35:31.400
the introduction of technology into production process

02:35:31.400 --> 02:35:33.160
causes prices to fall.

02:35:33.160 --> 02:35:35.920
As prices fall, consumers have more spending power.

02:35:35.920 --> 02:35:37.280
As consumers have more spending power,

02:35:37.280 --> 02:35:38.920
they create new demand.

02:35:39.560 --> 02:35:41.960
That new demand then causes capital and labor to form

02:35:41.960 --> 02:35:44.960
into new enterprises to satisfy new wants and needs.

02:35:44.960 --> 02:35:47.280
And the result is more jobs at higher wages.

02:35:47.280 --> 02:35:48.680
So new wants and needs,

02:35:48.680 --> 02:35:52.240
the worries that the creation of new wants and needs

02:35:52.240 --> 02:35:56.080
at a rapid rate will mean there's a lot of turnover in jobs.

02:35:56.080 --> 02:35:58.600
So people will lose jobs.

02:35:58.600 --> 02:36:00.440
Just the actual experience of losing a job

02:36:00.440 --> 02:36:02.600
and having to learn new things and new skills

02:36:02.600 --> 02:36:04.280
is painful for the individual.

02:36:04.280 --> 02:36:05.120
Well, two things.

02:36:05.120 --> 02:36:07.400
One is that new jobs are often much better.

02:36:07.400 --> 02:36:08.880
So this actually came up.

02:36:09.840 --> 02:36:11.680
There was this panic about a decade ago

02:36:11.680 --> 02:36:13.520
and all the truck drivers are gonna lose their jobs, right?

02:36:13.520 --> 02:36:14.760
And number one, that didn't happen

02:36:14.760 --> 02:36:15.600
because we haven't figured out a way

02:36:15.600 --> 02:36:17.440
to actually finish that yet.

02:36:17.440 --> 02:36:19.560
But the other thing was like truck driver,

02:36:19.560 --> 02:36:21.400
I grew up in a town that was basically consisted

02:36:21.400 --> 02:36:22.520
of a truck stop, right?

02:36:22.520 --> 02:36:23.840
And I knew a lot of truck drivers.

02:36:23.840 --> 02:36:27.560
And truck drivers live a decade shorter than everybody else.

02:36:27.560 --> 02:36:30.360
It's actually a very dangerous,

02:36:30.360 --> 02:36:32.760
literally they have high rates of skin cancer

02:36:32.760 --> 02:36:34.960
and on the left side of their body

02:36:34.960 --> 02:36:36.520
from being in the sun all the time,

02:36:36.520 --> 02:36:37.960
the vibration of being in the truck

02:36:37.960 --> 02:36:39.000
and the physiology.

02:36:39.000 --> 02:36:41.200
And there's actually, perhaps partially

02:36:41.200 --> 02:36:44.520
because of that reason, there's a shortage

02:36:44.520 --> 02:36:47.520
of people who wanna be truck drivers.

02:36:47.520 --> 02:36:49.400
Yeah, like it's not,

02:36:49.400 --> 02:36:50.960
the question always you wanna ask somebody like that

02:36:50.960 --> 02:36:53.840
is do you want your kid to be doing this job?

02:36:53.840 --> 02:36:55.240
And like most of them will tell you no.

02:36:55.240 --> 02:36:57.200
Like I want my kid to be sitting in a cubicle somewhere

02:36:57.200 --> 02:36:58.320
like where they don't have this,

02:36:58.320 --> 02:37:00.400
like where they don't die 10 years earlier.

02:37:00.400 --> 02:37:01.880
And so the new jobs, number one,

02:37:01.880 --> 02:37:03.400
the new jobs are often better,

02:37:03.400 --> 02:37:04.480
but you don't get the new jobs

02:37:04.480 --> 02:37:05.480
until you go through the change.

02:37:05.480 --> 02:37:07.760
And then to your point, the training thing,

02:37:08.560 --> 02:37:09.720
it's always the issue is can people adapt?

02:37:09.720 --> 02:37:11.560
And again, here you need to imagine living in a world

02:37:11.560 --> 02:37:15.080
in which everybody has the AI assistant capability, right?

02:37:15.080 --> 02:37:16.680
To be able to pick up new skills much more quickly

02:37:16.680 --> 02:37:18.760
and be able to have a machine to work with

02:37:18.760 --> 02:37:19.600
to augment their skills.

02:37:19.600 --> 02:37:20.800
It's still gonna be painful,

02:37:20.800 --> 02:37:22.560
but that's the process of life.

02:37:22.560 --> 02:37:23.560
It's painful for some people.

02:37:23.560 --> 02:37:25.680
I mean, there's no question it's painful for some people

02:37:25.680 --> 02:37:29.040
and yes, it's not, again, I'm not a utopian on this

02:37:29.040 --> 02:37:31.120
and it's not like it's positive for everybody in the moment,

02:37:31.120 --> 02:37:34.480
but it has been overwhelmingly positive for 300 years.

02:37:34.480 --> 02:37:35.800
I mean, look, the concern here,

02:37:35.800 --> 02:37:37.840
the concern, this concern has played out

02:37:37.840 --> 02:37:39.680
for literally centuries.

02:37:39.680 --> 02:37:42.440
And this is the sort of the story of the Luddites.

02:37:43.520 --> 02:37:45.800
You may remember there was a panic in the 2000s

02:37:45.800 --> 02:37:48.240
around outsourcing was gonna take all the jobs.

02:37:48.240 --> 02:37:50.320
There was a panic in the 2010s

02:37:50.320 --> 02:37:52.320
that robots were gonna take all the jobs.

02:37:53.680 --> 02:37:57.720
In 2019, before COVID, we had more jobs at higher wages,

02:37:57.720 --> 02:37:58.840
both in the country and in the world

02:37:58.840 --> 02:38:00.520
than at any point in human history.

02:38:00.520 --> 02:38:04.120
And so the overwhelming evidence is that the net gain here

02:38:04.120 --> 02:38:06.560
is just wildly positive.

02:38:06.560 --> 02:38:09.320
And most people overwhelmingly come out the other side

02:38:09.320 --> 02:38:11.200
being huge beneficiaries of this.

02:38:11.200 --> 02:38:14.240
So you write that the single greatest risk,

02:38:14.240 --> 02:38:16.400
this is the risk you're most convinced by,

02:38:16.400 --> 02:38:18.000
the single greatest risk of AI

02:38:18.000 --> 02:38:20.440
is that China wins global AI dominance

02:38:20.440 --> 02:38:23.800
and we, the United States, and the West do not.

02:38:23.800 --> 02:38:24.640
Can you elaborate?

02:38:24.640 --> 02:38:26.680
Yeah, so this is the other thing,

02:38:26.680 --> 02:38:29.520
which is a lot of the sort of AI risk debates today

02:38:29.520 --> 02:38:31.480
sort of assume that we're the only game in town, right?

02:38:31.480 --> 02:38:33.040
And so we have the ability to kind of sit

02:38:33.120 --> 02:38:34.880
in the United States and criticize ourselves

02:38:34.880 --> 02:38:37.440
and have our government beat up on our companies

02:38:37.440 --> 02:38:39.320
and figure out a way to restrict what our companies can do.

02:38:39.320 --> 02:38:41.760
And we're gonna ban this and ban that,

02:38:41.760 --> 02:38:42.760
restrict this and do that.

02:38:42.760 --> 02:38:44.840
And then there's this other force out there

02:38:44.840 --> 02:38:48.480
that doesn't believe we have any power over them whatsoever.

02:38:48.480 --> 02:38:49.600
And they have no desire to sign up

02:38:49.600 --> 02:38:51.600
for whatever rules we decide to put in place.

02:38:51.600 --> 02:38:53.520
And they're gonna do whatever it is they're gonna do.

02:38:53.520 --> 02:38:56.120
And we have no control over it at all.

02:38:56.120 --> 02:38:57.160
And it's China,

02:38:57.160 --> 02:38:59.640
and specifically the Chinese Communist Party.

02:38:59.680 --> 02:39:04.680
And they have a completely publicized open plan

02:39:04.880 --> 02:39:06.560
for what they're gonna do with AI.

02:39:06.560 --> 02:39:09.000
And it is not what we have in mind.

02:39:09.000 --> 02:39:10.640
And not only do they have that as a vision

02:39:10.640 --> 02:39:11.800
and a plan for their society,

02:39:11.800 --> 02:39:13.240
but they also have it as a vision and plan

02:39:13.240 --> 02:39:14.080
for the rest of the world.

02:39:14.080 --> 02:39:15.240
So their plan is what?

02:39:15.240 --> 02:39:16.160
Surveillance?

02:39:16.160 --> 02:39:17.800
Yeah, authoritarian control.

02:39:17.800 --> 02:39:19.600
So authoritarian population control.

02:39:21.120 --> 02:39:23.560
Good old fashioned communist authoritarian control

02:39:23.560 --> 02:39:25.360
and surveillance and enforcement

02:39:26.400 --> 02:39:29.000
and social credit scores and all the rest of it.

02:39:29.000 --> 02:39:30.960
And you are gonna be monitored and metered

02:39:30.960 --> 02:39:33.080
with an inch of everything all the time.

02:39:34.000 --> 02:39:36.000
And it's basically the end of human freedom.

02:39:36.000 --> 02:39:37.080
And that's their goal.

02:39:37.080 --> 02:39:39.080
And they justify it on the basis of

02:39:39.080 --> 02:39:40.200
that's what leads to peace.

02:39:40.200 --> 02:39:44.680
And you're worried that the regulating

02:39:44.680 --> 02:39:46.840
in the United States will halt progress enough

02:39:46.840 --> 02:39:50.320
to where the Chinese government would win that race.

02:39:50.320 --> 02:39:52.040
So their plan, yes, yes.

02:39:52.040 --> 02:39:53.080
And the reason for that is they,

02:39:53.080 --> 02:39:54.840
and again, they're very public on this.

02:39:54.840 --> 02:39:57.040
Their plan is to proliferate their approach around the world.

02:39:57.120 --> 02:39:59.800
And they have this program called the digital Silk Road,

02:39:59.800 --> 02:40:02.040
which is building on their Silk Road investment program.

02:40:02.040 --> 02:40:04.800
And they've been laying networking infrastructure

02:40:04.800 --> 02:40:06.800
all over the world with their 5G work

02:40:06.800 --> 02:40:08.320
with their company, Huawei.

02:40:08.320 --> 02:40:10.160
And so they've been laying all this fabric,

02:40:10.160 --> 02:40:12.240
but financial and technological fabric all over the world.

02:40:12.240 --> 02:40:14.400
And their plan is to roll out their vision of AI

02:40:14.400 --> 02:40:16.200
on top of that and to have every other country

02:40:16.200 --> 02:40:17.960
be running their version.

02:40:17.960 --> 02:40:21.560
And then if you're a country prone to authoritarianism,

02:40:21.560 --> 02:40:23.120
you're gonna find this to be an incredible way

02:40:23.120 --> 02:40:25.320
to become more authoritarian.

02:40:25.320 --> 02:40:26.280
If you're a country, by the way,

02:40:26.280 --> 02:40:27.400
not prone to authoritarianism,

02:40:27.400 --> 02:40:28.920
you're gonna have the Chinese Communist Party

02:40:28.920 --> 02:40:32.000
running your infrastructure and having back doors into it.

02:40:32.000 --> 02:40:32.840
Right?

02:40:32.840 --> 02:40:34.400
Which is also not good.

02:40:34.400 --> 02:40:36.000
What's your sense of where they stand

02:40:36.000 --> 02:40:39.840
in terms of the race towards super intelligence

02:40:39.840 --> 02:40:41.320
as compared to the United States?

02:40:41.320 --> 02:40:42.680
Yeah, so good news is they're behind,

02:40:42.680 --> 02:40:44.480
but bad news is they, you know,

02:40:44.480 --> 02:40:46.880
let's just say they get access to everything we do.

02:40:46.880 --> 02:40:49.480
So they're probably a year behind at each point in time,

02:40:49.480 --> 02:40:51.240
but they get, you know, downloads, I think,

02:40:51.240 --> 02:40:53.440
of basically all of our work on a regular basis

02:40:53.440 --> 02:40:54.800
through a variety of means.

02:40:55.800 --> 02:40:57.400
And they are, you know, at least we'll see

02:40:57.400 --> 02:40:58.960
they're at least putting out reports of very,

02:40:58.960 --> 02:41:02.280
just put out a report last week of a GPT 3.5 analog.

02:41:03.800 --> 02:41:05.720
They put out this report, forget what it's called,

02:41:05.720 --> 02:41:07.560
but they put out this report of this LLM they did.

02:41:07.560 --> 02:41:10.000
And they, you know, the way when OpenAI puts out,

02:41:10.000 --> 02:41:12.280
they, one of the ways they test, you know,

02:41:12.280 --> 02:41:15.200
GPT is they run it through standardized exams

02:41:15.200 --> 02:41:16.520
like the SAT, right?

02:41:16.520 --> 02:41:18.560
Just how you can kind of gauge how smart it is.

02:41:18.560 --> 02:41:20.000
And so the Chinese report,

02:41:20.000 --> 02:41:23.160
they ran their LLM through the Chinese equivalent

02:41:23.160 --> 02:41:27.120
of the SAT and it includes a section on Marxism

02:41:27.120 --> 02:41:29.600
and a section on, I was to say, tongue and thought.

02:41:29.600 --> 02:41:31.120
And it turns out their AI does very well

02:41:31.120 --> 02:41:33.680
on both of those topics, right?

02:41:33.680 --> 02:41:35.920
And so like-

02:41:35.920 --> 02:41:37.320
This alignment thing.

02:41:37.320 --> 02:41:38.280
Communist AI, right?

02:41:38.280 --> 02:41:40.240
Like literal communist AI, right?

02:41:40.240 --> 02:41:42.840
And so their vision is like, that's the, you know,

02:41:42.840 --> 02:41:45.360
so, you know, you can just imagine like you're a school,

02:41:45.360 --> 02:41:47.960
you know, you're a kid 10 years from now in Argentina

02:41:47.960 --> 02:41:52.640
or in Germany or in who knows where Indonesia.

02:41:52.640 --> 02:41:54.280
And you ask the AI, I'd explain to you

02:41:54.280 --> 02:41:56.040
like how the economy works and it gives you

02:41:56.040 --> 02:41:57.720
the most cheery upbeat explanation

02:41:57.720 --> 02:41:59.480
of Chinese style communism you've ever heard, right?

02:41:59.480 --> 02:42:03.440
So like the stakes here are like really big.

02:42:03.440 --> 02:42:05.120
Well, my, as we've been talking about,

02:42:05.120 --> 02:42:06.800
my hope is not just with the United States,

02:42:06.800 --> 02:42:09.040
but with just the kitten as basement,

02:42:09.040 --> 02:42:10.640
the open source LLM.

02:42:10.640 --> 02:42:15.640
So I don't know if I trust large centralized institutions

02:42:15.720 --> 02:42:17.800
with a super powerful AI,

02:42:17.800 --> 02:42:21.840
no matter what their ideology is, power corrupts.

02:42:23.400 --> 02:42:25.080
You've been investing in tech companies

02:42:25.080 --> 02:42:27.080
for about let's say 20 years

02:42:27.080 --> 02:42:32.080
and about 15 of which was with Andreessen Horowitz.

02:42:32.360 --> 02:42:34.520
What interesting trends in tech

02:42:34.520 --> 02:42:35.880
have you seen over that time?

02:42:35.880 --> 02:42:37.080
Let's just talk about companies

02:42:37.080 --> 02:42:39.240
and just the evolution of the tech industry.

02:42:39.240 --> 02:42:41.560
I mean, the big shift over 20 years has been

02:42:41.560 --> 02:42:44.120
that tech used to be a tools industry

02:42:44.120 --> 02:42:47.760
for basically from like 1940 through to about 2010,

02:42:47.760 --> 02:42:49.520
almost all the big successful companies

02:42:49.520 --> 02:42:50.960
were picks and shovels companies.

02:42:50.960 --> 02:42:53.600
So PC, database, smartphone,

02:42:53.600 --> 02:42:57.360
some tool that somebody else would pick up and use.

02:42:57.360 --> 02:43:01.360
Since 2010, most of the big wins have been in applications.

02:43:01.360 --> 02:43:05.640
So a company that starts in an existing industry

02:43:05.640 --> 02:43:08.720
and goes directly to the customer in that industry.

02:43:08.720 --> 02:43:10.520
And the early examples there

02:43:10.520 --> 02:43:12.880
were like Uber and Lyft and Airbnb.

02:43:12.880 --> 02:43:15.240
And then that model is kind of elaborating out.

02:43:16.640 --> 02:43:18.800
The AI thing is actually a reversion on that for now

02:43:18.800 --> 02:43:20.400
because like most of the AI business right now

02:43:20.400 --> 02:43:23.360
is actually in cloud provision of AI APIs

02:43:23.360 --> 02:43:24.640
for other people to build on.

02:43:24.640 --> 02:43:26.440
But the big thing will probably be an app.

02:43:26.440 --> 02:43:28.200
Yeah, I think most of the money

02:43:28.200 --> 02:43:30.320
I think probably will be in whatever.

02:43:30.320 --> 02:43:33.240
Yeah, your AI financial advisor or your AI doctor

02:43:33.240 --> 02:43:35.280
or your AI lawyer or take your pick

02:43:35.280 --> 02:43:36.880
of whatever the domain is.

02:43:36.880 --> 02:43:40.400
And what's interesting is the Valley kind of does everything.

02:43:41.520 --> 02:43:44.000
The entrepreneurs kind of elaborate every possible idea.

02:43:44.000 --> 02:43:46.760
And so there will be a set of companies that like make AI

02:43:46.760 --> 02:43:49.600
something that can be purchased and used by large law firms.

02:43:49.960 --> 02:43:51.000
And then there will be other companies

02:43:51.000 --> 02:43:53.600
that just go direct to market as an AI lawyer.

02:43:54.520 --> 02:43:58.920
What advice could you give for startup founder?

02:43:58.920 --> 02:44:01.440
Just haven't seen so many successful companies,

02:44:01.440 --> 02:44:03.640
so many companies that fail also.

02:44:03.640 --> 02:44:05.600
What advice could you give to a startup founder,

02:44:05.600 --> 02:44:09.280
someone who wants to build the next super successful startup

02:44:09.280 --> 02:44:13.040
in the tech space, the Googles, the Apples, the Twitters?

02:44:14.600 --> 02:44:16.720
Yeah, so the great thing about the really great founders

02:44:16.720 --> 02:44:17.920
is they don't take any advice.

02:44:17.920 --> 02:44:22.200
So if you find yourself listening to advice,

02:44:22.200 --> 02:44:24.280
maybe you shouldn't do it.

02:44:24.280 --> 02:44:27.400
Well, that's actually just to elaborate on that.

02:44:27.400 --> 02:44:30.120
If you could also speak to great founders too.

02:44:30.120 --> 02:44:32.480
Like what makes a great founder?

02:44:32.480 --> 02:44:34.920
So what makes a great founder is super smart,

02:44:34.920 --> 02:44:39.360
coupled with super energetic, coupled with super courageous.

02:44:39.360 --> 02:44:41.280
I think it's some of those three.

02:44:41.280 --> 02:44:43.360
Intelligence, passion and courage.

02:44:43.360 --> 02:44:45.800
The first two are traits and the third one is a choice,

02:44:45.800 --> 02:44:47.520
I think courage is a choice.

02:44:48.080 --> 02:44:50.680
Well, cause courage is a question of pain tolerance.

02:44:52.320 --> 02:44:55.320
So how many times are you willing to get punched

02:44:55.320 --> 02:44:56.920
in the face before you quit?

02:44:59.160 --> 02:45:00.840
Here's maybe the biggest thing people don't understand

02:45:00.840 --> 02:45:02.680
about what it's like to be a startup founder is

02:45:02.680 --> 02:45:04.840
it gets very romanticized.

02:45:04.840 --> 02:45:06.960
And even when they fail, it still gets romanticized

02:45:06.960 --> 02:45:08.240
about what a great adventure it was.

02:45:08.240 --> 02:45:11.680
But the reality of it is most of what happens

02:45:11.680 --> 02:45:12.920
is people telling you no,

02:45:12.920 --> 02:45:15.720
and then they usually follow that with you're stupid.

02:45:15.720 --> 02:45:17.640
No, I will not come to work for you.

02:45:17.640 --> 02:45:19.560
I will not leave my cushy job at Google to come work for you.

02:45:19.560 --> 02:45:21.120
No, I'm not gonna buy your product.

02:45:21.120 --> 02:45:22.880
No, I'm not gonna run a story about your company.

02:45:22.880 --> 02:45:25.400
No, I'm not this, that, the other thing.

02:45:25.400 --> 02:45:27.600
And so a huge amount of what people have to do

02:45:27.600 --> 02:45:29.080
is just get used to just getting punched.

02:45:29.080 --> 02:45:31.000
And the reason people don't understand this

02:45:31.000 --> 02:45:31.960
is because when you're a founder,

02:45:31.960 --> 02:45:33.440
you cannot let on that this is happening

02:45:33.440 --> 02:45:35.200
cause it will cause people to think that you're weak

02:45:35.200 --> 02:45:36.840
and they'll lose faith in you.

02:45:36.840 --> 02:45:38.960
So you have to pretend that you're having a great time

02:45:38.960 --> 02:45:41.080
when you're dying inside, right?

02:45:41.080 --> 02:45:44.160
You're just in misery.

02:45:44.160 --> 02:45:45.600
But why did they do it?

02:45:45.600 --> 02:45:46.440
Why did they do it?

02:45:46.440 --> 02:45:47.280
Yeah, that's the thing.

02:45:47.280 --> 02:45:48.520
It's like it is a level.

02:45:48.520 --> 02:45:50.200
This is actually one of the conclusions I think is it,

02:45:50.200 --> 02:45:52.040
I think it's actually for most of these people

02:45:52.040 --> 02:45:53.040
on a risk adjusted basis,

02:45:53.040 --> 02:45:54.880
it's probably an irrational act.

02:45:54.880 --> 02:45:56.440
They could probably be more financially successful

02:45:56.440 --> 02:45:58.240
on average if they just got like a real job

02:45:58.240 --> 02:45:59.360
and a big company.

02:46:00.360 --> 02:46:03.000
But there's, some people just have an irrational need

02:46:03.000 --> 02:46:05.080
to do something new and build something for themselves.

02:46:05.080 --> 02:46:07.640
And some people just can't tolerate having bosses.

02:46:07.640 --> 02:46:09.200
Oh, here's a fun thing is how do you reference check

02:46:09.200 --> 02:46:10.560
founders, right?

02:46:10.560 --> 02:46:12.280
So you call it, normally you reference check

02:46:12.280 --> 02:46:14.120
your time hiring somebody as you call the bosses

02:46:15.040 --> 02:46:16.480
and you find out if they were good employees.

02:46:16.480 --> 02:46:18.880
And now you're trying to reference check Steve Jobs, right?

02:46:18.880 --> 02:46:20.640
And it's like, oh God, he was terrible.

02:46:20.640 --> 02:46:21.480
He was a terrible employee.

02:46:21.480 --> 02:46:23.000
He never did what we told him to do.

02:46:23.000 --> 02:46:23.840
Yeah.

02:46:23.840 --> 02:46:26.160
So what's a good reference?

02:46:27.480 --> 02:46:28.960
You want the previous boss to actually say

02:46:28.960 --> 02:46:32.000
that they never did what you told them to do.

02:46:32.000 --> 02:46:32.920
That might be a good thing.

02:46:32.920 --> 02:46:35.200
Well, ideally what you want is I will go,

02:46:35.200 --> 02:46:37.640
I would like to go to work for that person.

02:46:37.640 --> 02:46:39.720
He worked for me here and now I'd like to work for him.

02:46:39.720 --> 02:46:41.480
Now, unfortunately most people can't,

02:46:41.480 --> 02:46:43.240
their egos can't handle that.

02:46:43.240 --> 02:46:45.360
So they won't say that, but that's the ideal.

02:46:45.360 --> 02:46:47.240
What advice would you give to those folks

02:46:47.240 --> 02:46:51.320
in the space of intelligence, passion and courage?

02:46:51.320 --> 02:46:54.280
So I think the other big thing is you see people sometimes

02:46:54.280 --> 02:46:55.400
who say, I want to start a company

02:46:55.400 --> 02:46:56.880
and then they kind of work through the process

02:46:56.880 --> 02:46:57.920
of coming up with an idea.

02:46:57.920 --> 02:47:00.920
And generally those don't work as well as the case

02:47:00.920 --> 02:47:02.840
where somebody has the idea first

02:47:02.840 --> 02:47:05.080
and then they kind of realize that there's an opportunity

02:47:05.080 --> 02:47:06.480
to build a company and then they just turn out

02:47:06.480 --> 02:47:08.360
to be the right kind of person to do that.

02:47:08.360 --> 02:47:12.000
When you say idea, do you mean long-term big vision

02:47:12.000 --> 02:47:15.160
or do you mean specifics of like product?

02:47:15.160 --> 02:47:16.280
Specific, I would say specific.

02:47:16.280 --> 02:47:18.360
Like specifically what, yes, specifics.

02:47:18.360 --> 02:47:20.080
Like what is, because for the first five years

02:47:20.080 --> 02:47:20.920
you don't get to have vision,

02:47:20.920 --> 02:47:22.320
you just got to build something people want

02:47:22.320 --> 02:47:24.040
and you got to figure out a way to sell it to them.

02:47:24.040 --> 02:47:26.400
Right, it's very practical or you never get to big vision.

02:47:26.400 --> 02:47:30.000
So the first part, you have an idea of a set of products

02:47:30.000 --> 02:47:31.760
or the first product that can actually make some money.

02:47:31.760 --> 02:47:33.800
Yeah, like it's got to, the first product's got to work

02:47:33.800 --> 02:47:35.480
by which I mean like it has to technically work

02:47:35.480 --> 02:47:37.800
but then it has to actually fit into the category

02:47:37.800 --> 02:47:39.360
in the customer's mind of something that they want.

02:47:39.360 --> 02:47:40.720
And then, and then by the way, the other part is

02:47:40.720 --> 02:47:41.600
they have to want to pay for it.

02:47:41.600 --> 02:47:43.120
Like somebody's got to pay the bills.

02:47:43.120 --> 02:47:44.800
And so you've got to figure out how to price it

02:47:44.800 --> 02:47:46.840
and whether you can actually extract the money.

02:47:46.840 --> 02:47:51.120
So usually it is much more predictable.

02:47:51.120 --> 02:47:52.920
It's success is never predictable

02:47:52.920 --> 02:47:55.000
but it's more predictable if you start with a great idea

02:47:55.000 --> 02:47:57.720
and then back into starting the company.

02:47:57.720 --> 02:47:58.560
So this is what we did.

02:47:58.560 --> 02:47:59.960
We had Mosaic before we had Netscape.

02:47:59.960 --> 02:48:01.600
The Google guys had the Google search engine

02:48:01.600 --> 02:48:03.800
working at Stanford, right?

02:48:03.800 --> 02:48:07.520
The, yeah, actually there's tons of examples where they,

02:48:07.520 --> 02:48:08.800
Pierre Omidyar had eBay working

02:48:08.800 --> 02:48:10.680
before he left his previous job.

02:48:10.680 --> 02:48:13.320
So I really love that idea of just having a thing,

02:48:13.320 --> 02:48:14.840
a prototype that actually works

02:48:14.840 --> 02:48:17.280
before you even begin to remotely scale.

02:48:17.280 --> 02:48:19.600
Yeah, by the way, it's also far easier to raise money, right?

02:48:19.600 --> 02:48:21.920
Like the ideal pitch that we receive is

02:48:21.920 --> 02:48:22.760
here's the thing that works.

02:48:22.760 --> 02:48:24.080
Would you like to invest in our company or not?

02:48:24.080 --> 02:48:26.880
Like that's so much easier than here's 30 slides

02:48:26.880 --> 02:48:28.840
with a dream, right?

02:48:28.840 --> 02:48:31.240
And then we have this concept called the idea maze

02:48:31.240 --> 02:48:33.640
which our biology friend of ours came up with

02:48:33.640 --> 02:48:35.120
when he was with us.

02:48:35.120 --> 02:48:38.040
So then there's this thing, this goes to mythology

02:48:38.040 --> 02:48:40.600
which is, you know, there's a mythology that kind of

02:48:41.440 --> 02:48:43.240
these ideas kind of arrive like magic

02:48:43.240 --> 02:48:44.400
or people kind of stumble into them.

02:48:44.400 --> 02:48:46.840
It's like eBay with the pest dispensers or something.

02:48:47.960 --> 02:48:50.080
The reality usually with the big successes

02:48:50.080 --> 02:48:53.000
is that the founder has been chewing on the problem

02:48:53.000 --> 02:48:55.720
for five or 10 years before they start the company.

02:48:55.720 --> 02:48:58.960
And they often worked on it in school

02:48:58.960 --> 02:49:01.760
or they even experimented on it when they were a kid.

02:49:01.760 --> 02:49:03.200
And they've been kind of training up

02:49:03.200 --> 02:49:05.280
over that period of time to be able to do the thing.

02:49:05.280 --> 02:49:07.640
So they're like a true domain expert.

02:49:07.640 --> 02:49:09.920
And it sort of sounds like mom and apple pie,

02:49:09.920 --> 02:49:11.160
which is, yeah, you wanna be a domain expert

02:49:11.160 --> 02:49:12.840
in what you're doing, but you would, you know,

02:49:12.840 --> 02:49:14.440
the mythology is so strong of like,

02:49:14.440 --> 02:49:15.840
oh, I just like had this idea in the shower

02:49:15.840 --> 02:49:16.680
and now I'm doing it.

02:49:16.680 --> 02:49:18.360
Like it's generally not that.

02:49:18.360 --> 02:49:23.160
No, because maybe in the shower you had the exact

02:49:23.160 --> 02:49:26.280
product implementation details,

02:49:26.280 --> 02:49:28.960
but yeah, usually you're gonna be for like years

02:49:28.960 --> 02:49:33.960
if not decades thinking about like everything around that.

02:49:35.680 --> 02:49:36.760
Well, we call it the idea maze

02:49:36.760 --> 02:49:39.160
because the idea maze basically is like,

02:49:39.160 --> 02:49:41.720
there's all these permutations, like for any idea,

02:49:41.720 --> 02:49:43.120
there's like all these different permutations,

02:49:43.120 --> 02:49:43.960
who should the customer be,

02:49:43.960 --> 02:49:45.560
what shape forms the product have

02:49:45.560 --> 02:49:48.440
and how should we take it to market and all these things.

02:49:48.440 --> 02:49:51.320
And so the really smart founders have thought

02:49:51.320 --> 02:49:53.000
through all these scenarios by the time they go out

02:49:53.000 --> 02:49:56.600
to raise money and they have like detailed answers

02:49:56.600 --> 02:49:57.960
on every one of those fronts

02:49:57.960 --> 02:50:00.240
because they put so much thought into it.

02:50:00.240 --> 02:50:03.920
The sort of more haphazard founders haven't thought

02:50:03.920 --> 02:50:05.800
about any of that and it's the detailed ones

02:50:05.800 --> 02:50:07.160
who tend to do much better.

02:50:07.160 --> 02:50:08.920
So how do you know when to take a leap?

02:50:08.920 --> 02:50:12.240
If you have a cushy job or happy life?

02:50:12.240 --> 02:50:13.240
I mean, the best reason is just

02:50:13.240 --> 02:50:15.000
cause you can't tolerate not doing it, right?

02:50:15.000 --> 02:50:16.440
Like this is the kind of thing where if you have

02:50:16.440 --> 02:50:18.760
to be advised into doing it, you probably shouldn't do it.

02:50:18.760 --> 02:50:20.480
And so it's probably the opposite,

02:50:20.480 --> 02:50:22.400
which is you just have such a burning sense

02:50:22.400 --> 02:50:23.360
of this has to be done.

02:50:23.360 --> 02:50:24.200
I have to do this.

02:50:24.200 --> 02:50:25.160
I have no choice.

02:50:25.160 --> 02:50:27.160
What if it's gonna lead to a lot of pain?

02:50:27.160 --> 02:50:28.880
It's gonna lead to a lot of pain.

02:50:28.880 --> 02:50:30.080
I think that's-

02:50:30.080 --> 02:50:33.440
What if it means losing sort of social relationships

02:50:33.440 --> 02:50:36.720
and damaging your relationship

02:50:37.240 --> 02:50:38.440
and all that kind of stuff.

02:50:38.440 --> 02:50:40.240
Yeah, look, so like it's gonna put you

02:50:40.240 --> 02:50:41.600
in a social tunnel for sure, right?

02:50:41.600 --> 02:50:43.320
So you're gonna like, you know,

02:50:44.320 --> 02:50:45.440
there's this game you can play on Twitter,

02:50:45.440 --> 02:50:47.440
which is you can do any whiff of the idea

02:50:47.440 --> 02:50:49.200
that there's basically any such thing

02:50:49.200 --> 02:50:51.680
as work-life balance and that people should actually work

02:50:51.680 --> 02:50:52.600
hard and everybody gets mad.

02:50:52.600 --> 02:50:54.560
But like the truth is like all the successful founders

02:50:54.560 --> 02:50:56.720
are working 80 hour weeks and they're working, you know,

02:50:56.720 --> 02:50:59.040
they form very, very strong social bonds

02:50:59.040 --> 02:50:59.880
with the people they work with.

02:50:59.880 --> 02:51:02.080
They tend to lose a lot of friends on the outside

02:51:02.080 --> 02:51:03.240
or put those friendships on ice.

02:51:03.240 --> 02:51:05.440
Like that's just the nature of the thing.

02:51:06.440 --> 02:51:08.120
You know, for most people, that's worth the trade off.

02:51:08.120 --> 02:51:09.040
You know, the advantage, you know,

02:51:09.040 --> 02:51:10.960
maybe younger founders have is maybe they have less,

02:51:10.960 --> 02:51:12.000
you know, maybe they're not, you know,

02:51:12.000 --> 02:51:13.040
for example, if they're not married yet

02:51:13.040 --> 02:51:15.640
or don't have kids yet, that's an easier thing to bite off.

02:51:15.640 --> 02:51:16.840
Can you be an older founder?

02:51:16.840 --> 02:51:18.360
Yeah, you definitely can, yeah.

02:51:18.360 --> 02:51:20.240
Yeah, many of the most successful founders

02:51:20.240 --> 02:51:21.560
are second, third, fourth time founders.

02:51:21.560 --> 02:51:23.640
They're in their 30s, 40s, 50s.

02:51:23.640 --> 02:51:26.280
The good news of being an older founder is you know more

02:51:26.280 --> 02:51:27.840
and you know a lot more about what to do,

02:51:27.840 --> 02:51:28.680
which is very helpful.

02:51:28.680 --> 02:51:30.800
The problem is, okay, now you've got like a spouse

02:51:30.800 --> 02:51:32.440
and a family and kids and like,

02:51:32.440 --> 02:51:33.680
you've gotta go to the baseball game

02:51:33.680 --> 02:51:35.040
and like you can't go to the base, you know?

02:51:35.560 --> 02:51:37.080
And so it's, it.

02:51:37.080 --> 02:51:38.880
Life is full of difficult choices.

02:51:38.880 --> 02:51:39.720
Yes.

02:51:39.720 --> 02:51:40.560
One kind of reason.

02:51:41.520 --> 02:51:44.280
You've written a blog post on what you've been up to.

02:51:44.280 --> 02:51:46.080
You wrote this in October, 2022.

02:51:47.040 --> 02:51:49.920
Quote, mostly I try to learn a lot.

02:51:49.920 --> 02:51:52.800
For example, the political events of 2014 to 2016

02:51:52.800 --> 02:51:55.920
made clear to me that I didn't understand politics at all.

02:51:55.920 --> 02:51:58.880
Referencing maybe some of this, this book here.

02:51:59.880 --> 02:52:02.280
So I deliberately withdrew from political engagement

02:52:02.280 --> 02:52:06.520
and fundraising and instead read my way back into history

02:52:06.520 --> 02:52:08.520
and as far to the political left

02:52:08.520 --> 02:52:10.480
and political right as I could.

02:52:10.480 --> 02:52:12.160
So just high level question.

02:52:12.160 --> 02:52:14.560
What's your approach to learning?

02:52:14.560 --> 02:52:18.800
Yeah, so it's basically, I would say it's autodidact.

02:52:18.800 --> 02:52:22.080
So it sort of goes, it's going down the rabbit holes.

02:52:22.080 --> 02:52:23.160
So it's a combination.

02:52:23.160 --> 02:52:24.920
So I kind of allude to it in that quote.

02:52:24.920 --> 02:52:27.560
It's a combination of breadth and depth.

02:52:27.560 --> 02:52:29.280
And so I tend to, yeah, I tend to,

02:52:29.280 --> 02:52:30.720
I go broad by the nature of what I do.

02:52:30.720 --> 02:52:32.240
I tend to go broad, but then I tend to go deep

02:52:32.240 --> 02:52:34.120
in a rabbit hole for a while, read everything I can

02:52:34.120 --> 02:52:34.960
and then come out of it.

02:52:34.960 --> 02:52:36.680
And I might not revisit that rabbit hole

02:52:36.680 --> 02:52:38.280
for another decade.

02:52:38.280 --> 02:52:41.880
And in that blog post that I recommend people go check out,

02:52:41.880 --> 02:52:43.920
you actually list a bunch of different books

02:52:43.920 --> 02:52:46.080
that you recommend on different topics

02:52:46.080 --> 02:52:49.120
on the American left and the American right.

02:52:49.120 --> 02:52:51.200
It's just a lot of really good stuff.

02:52:51.200 --> 02:52:53.040
The best explanation for the current structure

02:52:53.040 --> 02:52:54.240
of our society and politics,

02:52:54.240 --> 02:52:55.400
you give two recommendations,

02:52:55.400 --> 02:52:57.280
four books on the Spanish Civil War,

02:52:57.280 --> 02:52:59.520
six books on deep history of the American right,

02:52:59.560 --> 02:53:02.320
comprehensive biographies of Adolf Hitler,

02:53:03.360 --> 02:53:05.800
one of which I read and can recommend,

02:53:05.800 --> 02:53:08.040
six books on the deep history of the American left.

02:53:08.040 --> 02:53:09.640
So American right, American left,

02:53:09.640 --> 02:53:12.040
looking at the history to give you the context.

02:53:13.160 --> 02:53:15.360
Biography of Vladimir Lenin,

02:53:15.360 --> 02:53:17.800
two of them on the French Revolution.

02:53:17.800 --> 02:53:19.680
Actually, I have never read a biography on Lenin.

02:53:19.680 --> 02:53:21.760
Maybe that will be useful.

02:53:21.760 --> 02:53:24.040
Everything's been so Marx focused.

02:53:24.040 --> 02:53:27.000
The Sebastian biography of Lenin is extraordinary.

02:53:27.000 --> 02:53:28.320
Victor Sebastian, okay.

02:53:28.320 --> 02:53:29.160
It'll blow your mind, yeah.

02:53:29.160 --> 02:53:30.320
So it's still useful to read.

02:53:30.320 --> 02:53:31.440
It's incredible, yeah, it's incredible.

02:53:31.440 --> 02:53:32.760
I actually think it's the single best book

02:53:32.760 --> 02:53:34.040
on the Soviet Union.

02:53:34.040 --> 02:53:36.160
So that, the perspective of Lenin

02:53:36.160 --> 02:53:38.120
might be the best way to look at the Soviet Union

02:53:38.120 --> 02:53:41.520
versus Stalin versus Marx versus, very interesting.

02:53:41.520 --> 02:53:45.320
So two books on fascism and anti-fascism

02:53:45.320 --> 02:53:48.720
by the same author, Paul Gottfried.

02:53:48.720 --> 02:53:50.560
Brilliant book on the nature of mass movements

02:53:50.560 --> 02:53:51.840
and collective psychology,

02:53:51.840 --> 02:53:53.560
the definitive work on intellectual life

02:53:53.560 --> 02:53:56.600
under totalitarianism, the captive mind,

02:53:56.600 --> 02:53:58.160
the definitive work on the practical life

02:53:58.160 --> 02:54:00.120
under totalitarianism.

02:54:00.120 --> 02:54:01.720
There's a bunch, there's a bunch.

02:54:01.720 --> 02:54:03.360
And the single best book,

02:54:03.360 --> 02:54:05.800
first of all, the list here is just incredible.

02:54:05.800 --> 02:54:08.240
But you say the single best book I have found

02:54:08.240 --> 02:54:10.600
on who we are and how we got here

02:54:10.600 --> 02:54:15.600
is The Ancient City by Neumann-Dennis Faustel de Coulongas.

02:54:16.560 --> 02:54:17.920
I like it.

02:54:17.920 --> 02:54:20.320
What did you learn about who we are

02:54:20.320 --> 02:54:22.120
as a human civilization from that book?

02:54:22.120 --> 02:54:23.600
Yeah, so this is a fascinating book.

02:54:23.600 --> 02:54:24.960
This one's free, it's free by the way.

02:54:24.960 --> 02:54:26.560
It's a book from the 1860s.

02:54:26.560 --> 02:54:28.680
You can download it or you can buy prints of it.

02:54:28.680 --> 02:54:31.600
But it was this guy who was a professor

02:54:31.600 --> 02:54:33.040
at the Sorbonne in the 1860s.

02:54:33.040 --> 02:54:36.560
And he was apparently a savant on antiquity,

02:54:36.560 --> 02:54:38.960
on Greek and Roman antiquity.

02:54:38.960 --> 02:54:41.080
And the reason I say that is because his sources

02:54:41.080 --> 02:54:43.720
are 100% original Greek and Roman sources.

02:54:43.720 --> 02:54:46.920
So he wrote basically a history of Western civilization

02:54:46.920 --> 02:54:48.600
from on the order of 4,000 years ago

02:54:48.600 --> 02:54:49.880
to basically the present times,

02:54:49.880 --> 02:54:54.000
entirely working on original Greek and Roman sources.

02:54:55.000 --> 02:54:56.840
And what he was specifically trying to do

02:54:56.840 --> 02:54:58.440
was he was trying to reconstruct

02:54:58.440 --> 02:54:59.800
from the stories of the Greeks and the Romans,

02:54:59.800 --> 02:55:02.280
he was trying to reconstruct what life in the West was like

02:55:02.280 --> 02:55:03.840
before the Greeks and the Romans,

02:55:03.840 --> 02:55:07.200
which was in the civilization known as the Indo-Europeans.

02:55:08.200 --> 02:55:10.240
And the short answer is,

02:55:10.240 --> 02:55:15.240
and this is sort of circa 2000 BC to sort of 500 BC,

02:55:15.240 --> 02:55:18.040
kind of that 1500 year stretch where civilization developed.

02:55:18.040 --> 02:55:20.200
And his conclusion was basically cults.

02:55:21.320 --> 02:55:22.520
They were basically cults.

02:55:22.520 --> 02:55:25.240
And civilization was organized into cults

02:55:25.240 --> 02:55:28.880
and the intensity of the cults was like a million fold

02:55:28.880 --> 02:55:30.840
beyond anything that we would recognize today.

02:55:30.840 --> 02:55:35.240
Like it was a level of all encompassing belief

02:55:35.240 --> 02:55:38.040
and an action around religion

02:55:39.000 --> 02:55:40.600
that was at a level of extremeness

02:55:40.600 --> 02:55:42.400
that we wouldn't even recognize it.

02:55:43.320 --> 02:55:46.480
And so specifically, he tells the story of basically,

02:55:46.480 --> 02:55:47.720
there were three levels of cults.

02:55:47.720 --> 02:55:49.760
There was the family cult, the tribal cult,

02:55:49.800 --> 02:55:53.440
and then the city cult as society scaled up.

02:55:53.440 --> 02:55:57.680
And then each cult was a joint cult of family gods,

02:55:57.680 --> 02:56:00.320
which were ancestor gods and then nature gods.

02:56:01.360 --> 02:56:04.200
And then you were bonding into a family, a tribe,

02:56:04.200 --> 02:56:07.640
or a city was based on your adherence to that religion.

02:56:07.640 --> 02:56:10.880
People who were not of your family tribe city

02:56:10.880 --> 02:56:12.120
worship different gods,

02:56:12.120 --> 02:56:14.360
which gave you not just the right with the responsibility

02:56:14.360 --> 02:56:15.400
to kill them on sight.

02:56:16.360 --> 02:56:19.960
So they were serious about their cults.

02:56:19.960 --> 02:56:21.160
Hardcore.

02:56:21.160 --> 02:56:22.760
By the way, shocking development,

02:56:22.760 --> 02:56:25.160
I did not realize that zero concept of individual rights,

02:56:25.160 --> 02:56:27.880
like even up to the Greeks and even in the Romans,

02:56:27.880 --> 02:56:29.600
they didn't have the concept of individual rights.

02:56:29.600 --> 02:56:31.000
Like the idea that as an individual,

02:56:31.000 --> 02:56:33.600
you have like some rights, just like, nope.

02:56:33.600 --> 02:56:34.960
And you look back and you're just like, wow,

02:56:34.960 --> 02:56:37.120
that's just like crazily like fascist in a degree

02:56:37.120 --> 02:56:38.240
that we wouldn't recognize today.

02:56:38.240 --> 02:56:39.080
But it's like, well,

02:56:39.080 --> 02:56:42.040
they were living under extreme pressure for survival.

02:56:42.040 --> 02:56:44.800
And the theory goes, you could not have people running around

02:56:44.800 --> 02:56:45.840
making claims to individual rights

02:56:45.840 --> 02:56:47.120
when you're just trying to get like your tribe

02:56:47.120 --> 02:56:47.960
through the winter, right?

02:56:47.960 --> 02:56:50.320
Like you need like hardcore command and control.

02:56:50.320 --> 02:56:53.360
And so, and actually through modern political lens,

02:56:53.360 --> 02:56:56.240
those cults were basically both fascist and communist.

02:56:56.240 --> 02:56:58.080
They were fascist in terms of social control

02:56:58.080 --> 02:57:01.280
and then they were communist in terms of economics.

02:57:01.280 --> 02:57:03.720
But you think that's fundamentally that like pull

02:57:03.720 --> 02:57:06.800
towards cults is within us.

02:57:06.800 --> 02:57:09.720
Well, so my conclusion from this book,

02:57:09.720 --> 02:57:12.640
so the way we naturally think about the world

02:57:12.680 --> 02:57:14.320
that we live in today is like,

02:57:14.320 --> 02:57:16.520
we basically have such an improved version

02:57:16.520 --> 02:57:17.960
of everything that came before us, right?

02:57:17.960 --> 02:57:20.360
Like we have basically, we've figured out all these things

02:57:20.360 --> 02:57:21.840
around morality and ethics and democracy

02:57:21.840 --> 02:57:22.680
and all these things.

02:57:22.680 --> 02:57:24.440
And like they were basically stupid in retrograde

02:57:24.440 --> 02:57:26.080
and we're like smart and sophisticated

02:57:26.080 --> 02:57:27.280
and we've improved all this.

02:57:27.280 --> 02:57:30.560
I, after reading that book, I now believe in many ways,

02:57:30.560 --> 02:57:31.480
the opposite, which is no,

02:57:31.480 --> 02:57:34.040
actually we are still running in that original model.

02:57:34.040 --> 02:57:37.440
We're just running in an incredibly diluted version of it.

02:57:37.440 --> 02:57:39.680
So we're still running basically in cults.

02:57:39.680 --> 02:57:41.560
It's just our cults are at like a thousandth

02:57:41.560 --> 02:57:43.480
or a millionth the level of intensity, right?

02:57:43.480 --> 02:57:46.320
And so our, so just to take religions,

02:57:46.320 --> 02:57:49.160
the modern experience of a Christian in our time,

02:57:49.160 --> 02:57:51.160
even somebody who considers them a devout Christian

02:57:51.160 --> 02:57:53.400
is just a shadow of the level of intensity

02:57:53.400 --> 02:57:56.120
of somebody who belonged to a religion back in that period.

02:57:56.120 --> 02:57:57.960
And then by the way, we have constraints,

02:57:57.960 --> 02:57:59.320
it goes back to our AI discussion.

02:57:59.320 --> 02:58:02.600
We then sort of endlessly create new cults.

02:58:02.600 --> 02:58:04.760
Like we're trying to fill the void, right?

02:58:04.760 --> 02:58:08.080
And the void is a void of bonding, okay.

02:58:08.080 --> 02:58:10.320
Living in their era, like everybody living today,

02:58:10.320 --> 02:58:11.520
the transport in that era would view it

02:58:11.520 --> 02:58:12.840
as just like completely intolerable

02:58:12.840 --> 02:58:14.440
in terms of like the loss of freedom

02:58:14.440 --> 02:58:16.160
and the level of basically fascist control.

02:58:16.160 --> 02:58:18.520
However, every single person in that era,

02:58:18.520 --> 02:58:19.520
and he really stresses this,

02:58:19.520 --> 02:58:21.320
they knew exactly where they stood.

02:58:21.320 --> 02:58:22.760
They knew exactly where they belonged.

02:58:22.760 --> 02:58:24.280
They knew exactly what their purpose was.

02:58:24.280 --> 02:58:25.840
They knew exactly what they needed to do every day.

02:58:25.840 --> 02:58:27.120
They knew exactly why they were doing it.

02:58:27.120 --> 02:58:29.600
They had total certainty about their place in the universe.

02:58:29.600 --> 02:58:31.400
So the question of meaning, the question of purpose

02:58:31.400 --> 02:58:34.120
was very distinctly clearly defined for them.

02:58:34.120 --> 02:58:38.480
Absolutely, overwhelmingly, undisputably, undeniably.

02:58:38.480 --> 02:58:41.360
As we turn the volume down on the cultism,

02:58:41.360 --> 02:58:43.520
we start to, the search for meaning

02:58:43.520 --> 02:58:44.840
starts getting harder and harder.

02:58:44.840 --> 02:58:46.280
Yes, because we don't have that.

02:58:46.280 --> 02:58:48.520
We are ungrounded, we are we are we are uncentered

02:58:48.520 --> 02:58:49.800
and we all feel it, right?

02:58:49.800 --> 02:58:51.840
And that's why we reach for,

02:58:51.840 --> 02:58:53.240
it's why we still reach for religion.

02:58:53.240 --> 02:58:56.200
It's why we reach for, people start to take on,

02:58:56.200 --> 02:58:57.600
let's say, a faith in science,

02:58:57.600 --> 02:58:59.800
maybe beyond where they should put it.

02:58:59.800 --> 02:59:01.680
And by the way, like sports teams are like,

02:59:01.680 --> 02:59:03.040
they're like a tiny little version of a cult.

02:59:03.040 --> 02:59:06.960
And the Apple keynotes are a tiny little version of a cult,

02:59:06.960 --> 02:59:07.800
right?

02:59:07.800 --> 02:59:10.080
And, you know, political, you know,

02:59:10.080 --> 02:59:11.440
and there's cult, you know, there's full-blown cults

02:59:11.440 --> 02:59:13.840
on both sides of the political spectrum right now, right?

02:59:13.840 --> 02:59:14.840
You know, and operating in plain sight.

02:59:14.840 --> 02:59:17.200
But still not full-blown compared as to what it was.

02:59:17.200 --> 02:59:18.720
Compared to what it used to, I mean,

02:59:18.720 --> 02:59:20.760
we would today consider full-blown, but like, yes,

02:59:20.760 --> 02:59:21.600
they're at like, I don't know,

02:59:21.600 --> 02:59:23.360
a hundred thousandth or something of the intensity

02:59:23.360 --> 02:59:24.680
of what people had back then.

02:59:24.680 --> 02:59:27.240
So we live in a world today that in many ways

02:59:27.240 --> 02:59:28.800
is more advanced and moral and so forth.

02:59:28.800 --> 02:59:30.720
And it's certainly a lot nicer, much nicer world to live in,

02:59:30.720 --> 02:59:33.680
but we live in a world that's like very washed out.

02:59:33.680 --> 02:59:36.400
It's like everything has become very colorless and gray

02:59:36.400 --> 02:59:38.280
as compared to how people used to experience things,

02:59:38.280 --> 02:59:41.480
which is, I think, why we're so prone to reach for drama.

02:59:41.480 --> 02:59:44.440
There's something in us deeply evolved

02:59:44.440 --> 02:59:45.600
where we want that back.

02:59:46.720 --> 02:59:48.640
And I wonder where it's all headed

02:59:48.640 --> 02:59:50.840
as we turn the volume down more and more.

02:59:50.840 --> 02:59:53.240
What advice would you give to young folks today

02:59:54.280 --> 02:59:55.520
in high school and college,

02:59:55.520 --> 02:59:56.760
how to be successful in their career,

02:59:56.760 --> 02:59:58.600
how to be successful in their life?

02:59:58.600 --> 03:00:01.560
Yeah, so the tools that are available today, I mean,

03:00:01.560 --> 03:00:04.440
are just like, I sometimes, you know,

03:00:04.520 --> 03:00:06.560
I sometimes board, you know, kids by describing

03:00:06.560 --> 03:00:08.360
like what it was like to go look up a book, you know,

03:00:08.360 --> 03:00:09.600
to try to like discover a fact.

03:00:09.600 --> 03:00:12.000
And, you know, in the old days, the 1970s, 1980s,

03:00:12.000 --> 03:00:13.320
to go to the library and the card catalog

03:00:13.320 --> 03:00:15.040
and the whole thing, you go through all that work

03:00:15.040 --> 03:00:15.960
and then the book is checked out

03:00:15.960 --> 03:00:16.800
and you have to wait two weeks.

03:00:16.800 --> 03:00:19.280
And like, to be in a world,

03:00:19.280 --> 03:00:21.560
not only where you can get the answer to any question,

03:00:21.560 --> 03:00:22.920
but also the world now, you know,

03:00:22.920 --> 03:00:24.640
the AI world where you've got like the assistant

03:00:24.640 --> 03:00:25.520
that will help you do anything,

03:00:25.520 --> 03:00:26.720
help you teach, learn anything.

03:00:26.720 --> 03:00:29.400
Like your ability both to learn and also to produce

03:00:29.400 --> 03:00:31.320
is just like, I don't know, a million fold

03:00:31.320 --> 03:00:32.880
beyond what it used to be.

03:00:32.880 --> 03:00:35.400
I have a blog post I've been wanting to write,

03:00:35.400 --> 03:00:38.400
which I call out, where are the hyperproductive people?

03:00:39.720 --> 03:00:44.200
Like, with these tools, like there should be authors

03:00:44.200 --> 03:00:45.680
that are writing like hundreds of thousands

03:00:45.680 --> 03:00:47.880
of like outstanding books.

03:00:47.880 --> 03:00:50.440
Well, with the authors, there's a consumption question too.

03:00:50.440 --> 03:00:53.080
But yeah, well, maybe not, maybe not.

03:00:53.080 --> 03:00:54.000
You're right.

03:00:54.000 --> 03:00:56.400
But so the tools are much more powerful,

03:00:56.400 --> 03:00:57.240
they're getting much more powerful.

03:00:57.240 --> 03:00:58.840
Artists, musicians, right?

03:00:58.840 --> 03:01:00.920
Why aren't musicians producing a thousand times

03:01:00.920 --> 03:01:02.640
the number of songs, right?

03:01:03.440 --> 03:01:05.480
The tools are spectacular.

03:01:05.480 --> 03:01:08.040
So what's the explanation?

03:01:08.040 --> 03:01:11.800
And by way of advice, like is motivation

03:01:11.800 --> 03:01:14.400
starting to be turned down a little bit or what?

03:01:14.400 --> 03:01:15.960
I think it might be distraction.

03:01:15.960 --> 03:01:16.960
Distraction.

03:01:16.960 --> 03:01:19.560
It's so easy to just sit and consume

03:01:19.560 --> 03:01:21.400
that I think people get distracted from production.

03:01:21.400 --> 03:01:24.600
But if you wanted to, you know, as a young person,

03:01:24.600 --> 03:01:25.880
if you wanted to really stand out,

03:01:25.880 --> 03:01:28.480
you could get on like a hyperproductivity curve

03:01:28.480 --> 03:01:29.320
very early on.

03:01:30.160 --> 03:01:32.400
There's a great, you know, there's a great story

03:01:32.400 --> 03:01:33.960
in Roman history of Pliny the Elder,

03:01:33.960 --> 03:01:36.640
who was this legendary statesman,

03:01:36.640 --> 03:01:38.800
died in the Vesuvius eruption trying to rescue his friends.

03:01:38.800 --> 03:01:41.600
But he was famous both for being a,

03:01:41.600 --> 03:01:43.800
basically being a polymath, but also being an author.

03:01:43.800 --> 03:01:45.440
And he wrote apparently like hundreds of books,

03:01:45.440 --> 03:01:46.280
most of which have been lost,

03:01:46.280 --> 03:01:48.320
but he like wrote all these encyclopedias.

03:01:48.320 --> 03:01:50.600
And he literally like would be reading and writing

03:01:50.600 --> 03:01:52.480
all day long, no matter what else was going on.

03:01:52.480 --> 03:01:54.800
And so he would like travel with like four slaves

03:01:54.800 --> 03:01:56.480
and two of them were responsible for reading to him.

03:01:56.480 --> 03:01:59.120
And two of them were responsible for taking dictation.

03:01:59.960 --> 03:02:00.800
And so like he'd be going cross country

03:02:00.800 --> 03:02:02.480
and like literally he would be writing books,

03:02:02.480 --> 03:02:03.640
like all the time.

03:02:03.640 --> 03:02:05.040
And apparently they were spectacular.

03:02:05.040 --> 03:02:06.240
There's only a few that have survived,

03:02:06.240 --> 03:02:07.400
but apparently they were amazing.

03:02:07.400 --> 03:02:09.160
So there's a lot of value to being somebody

03:02:09.160 --> 03:02:10.720
who finds focus in this life.

03:02:10.720 --> 03:02:13.040
Yeah, like, and there are examples, like there are,

03:02:13.040 --> 03:02:14.560
you know, there's this guy, a judge,

03:02:14.560 --> 03:02:17.360
was his name Posner, who wrote like 40 books

03:02:17.360 --> 03:02:19.320
and was also a great federal judge.

03:02:19.320 --> 03:02:21.840
You know, there's our friend Balji, I think is like this.

03:02:21.840 --> 03:02:22.960
He's one of these, you know,

03:02:22.960 --> 03:02:25.120
where his output is just prodigious.

03:02:25.120 --> 03:02:27.960
And so it's like, yeah, I mean, with these tools, why not?

03:02:28.000 --> 03:02:29.680
And I kind of think we're at this interesting

03:02:29.680 --> 03:02:31.320
kind of freeze frame moment where like this,

03:02:31.320 --> 03:02:32.480
these tools are now in everybody's hands

03:02:32.480 --> 03:02:33.600
and everybody's just kind of staring at them

03:02:33.600 --> 03:02:34.440
trying to figure out what to do.

03:02:34.440 --> 03:02:35.280
Yeah.

03:02:35.280 --> 03:02:36.120
The new tools.

03:02:36.120 --> 03:02:37.000
We have discovered fire.

03:02:37.000 --> 03:02:37.840
Yeah.

03:02:37.840 --> 03:02:39.760
And trying to figure out how to use it to cook.

03:02:39.760 --> 03:02:40.600
Yeah.

03:02:41.680 --> 03:02:43.920
You told Tim Ferriss that the perfect day

03:02:43.920 --> 03:02:47.920
is caffeine for 10 hours and alcohol for four hours.

03:02:47.920 --> 03:02:50.880
You didn't think I'd be mentioning this, did you?

03:02:50.880 --> 03:02:53.440
It balances everything out perfectly, as you said.

03:02:53.440 --> 03:02:54.960
So perfect.

03:02:54.960 --> 03:02:57.360
So let me ask, what's the secret to balance

03:02:57.360 --> 03:03:00.120
and maybe to happiness in life?

03:03:00.120 --> 03:03:01.600
I don't believe in balance.

03:03:01.600 --> 03:03:03.440
So I'm the wrong person to ask that.

03:03:03.440 --> 03:03:05.800
Can you elaborate why you don't believe in balance?

03:03:05.800 --> 03:03:07.760
I mean, maybe it's just, and I look,

03:03:07.760 --> 03:03:09.960
I think people are wired differently.

03:03:09.960 --> 03:03:12.280
So I think it's hard to generalize this kind of thing,

03:03:12.280 --> 03:03:14.320
but I'm much happier and more satisfied

03:03:14.320 --> 03:03:15.440
when I'm fully committed to something.

03:03:15.440 --> 03:03:18.600
So I'm very much in favor of imbalance, yeah.

03:03:19.520 --> 03:03:20.560
Imbalance.

03:03:20.560 --> 03:03:23.760
And that applies to work, to life, to everything.

03:03:23.760 --> 03:03:25.720
Yeah, now I happen to have whatever twist

03:03:25.720 --> 03:03:27.880
or personality traits lead that in non-destructive

03:03:27.880 --> 03:03:30.000
dimensions, including the fact that I've actually,

03:03:30.000 --> 03:03:31.600
I now no longer do the 10-4 plan.

03:03:31.600 --> 03:03:32.800
I stopped drinking.

03:03:32.800 --> 03:03:34.160
I do the caffeine, but not the alcohol.

03:03:34.160 --> 03:03:37.000
So there's something in my personality where I,

03:03:37.000 --> 03:03:39.360
whatever maladaption I have is inclining me

03:03:39.360 --> 03:03:41.040
towards productive things, not unproductive things.

03:03:41.040 --> 03:03:44.640
So you're one of the wealthiest people in the world.

03:03:44.640 --> 03:03:47.760
What's the relationship between wealth and happiness?

03:03:47.760 --> 03:03:49.240
Oh.

03:03:49.240 --> 03:03:50.520
Money and happiness.

03:03:50.520 --> 03:03:52.080
So I think happiness,

03:03:53.040 --> 03:03:55.800
I don't think happiness is the thing.

03:03:55.800 --> 03:03:56.640
To strive for?

03:03:56.640 --> 03:03:59.040
I think satisfaction is the thing.

03:03:59.040 --> 03:04:02.120
That just sounds like happiness, but turned down a bit.

03:04:02.120 --> 03:04:03.160
No, deeper.

03:04:03.160 --> 03:04:06.560
So happiness is a walk in the woods at sunset,

03:04:06.560 --> 03:04:09.960
an ice cream cone, a kiss.

03:04:09.960 --> 03:04:11.840
The first ice cream cone is great.

03:04:11.840 --> 03:04:15.000
The thousandth ice cream cone, not so much.

03:04:15.000 --> 03:04:16.560
At some point the walks in the woods get boring.

03:04:16.560 --> 03:04:19.960
What's the distinction between happiness and satisfaction?

03:04:20.120 --> 03:04:22.400
Satisfaction is a deeper thing,

03:04:22.400 --> 03:04:24.000
which is like having found a purpose

03:04:24.000 --> 03:04:26.720
and fulfilling it, being useful.

03:04:26.720 --> 03:04:30.320
So just something that permeates all your days,

03:04:30.320 --> 03:04:35.000
just this general contentment of being useful.

03:04:35.000 --> 03:04:37.240
That I'm fully satisfying my faculties,

03:04:37.240 --> 03:04:40.400
that I'm fully delivering on the gifts that I've been given,

03:04:40.400 --> 03:04:42.440
that I'm net making the world better,

03:04:42.440 --> 03:04:45.200
that I'm contributing to the people around me,

03:04:45.200 --> 03:04:46.680
and that I can look back and say,

03:04:46.680 --> 03:04:49.120
wow, that was hard, but it was worth it.

03:04:49.160 --> 03:04:50.760
I think generally it seems to lead people

03:04:50.760 --> 03:04:53.200
in a better state than pursuit of pleasure,

03:04:53.200 --> 03:04:54.880
pursuit of quote unquote happiness.

03:04:54.880 --> 03:04:56.280
Does money have anything to do with that?

03:04:56.280 --> 03:04:58.160
I think the founders, I think the founding fathers

03:04:58.160 --> 03:04:59.400
in the U.S. threw this off kilter

03:04:59.400 --> 03:05:01.240
when they used the phrase pursuit of happiness.

03:05:01.240 --> 03:05:02.840
I think they should have said.

03:05:02.840 --> 03:05:03.680
Pursuit of satisfaction.

03:05:03.680 --> 03:05:04.720
If they said pursuit of satisfaction,

03:05:04.720 --> 03:05:06.160
we might live in a better world today.

03:05:06.160 --> 03:05:09.560
Well, they could have elaborated on a lot of things.

03:05:09.560 --> 03:05:10.760
They could have tweaked the second amendment.

03:05:10.760 --> 03:05:12.760
I think they were smarter than we realized.

03:05:12.760 --> 03:05:14.880
They said, you know what, we're gonna make it ambiguous

03:05:14.880 --> 03:05:17.680
and let these humans figure out the rest.

03:05:17.680 --> 03:05:20.920
These tribal cult-like humans figure out the rest.

03:05:23.480 --> 03:05:24.840
But money empowers that.

03:05:24.840 --> 03:05:27.360
So I think, and I think they're, I mean, look,

03:05:27.360 --> 03:05:29.320
I think Elon is, I don't think I'm even a great example,

03:05:29.320 --> 03:05:30.680
but I think Elon would be a great example of this,

03:05:30.680 --> 03:05:32.080
which is like, you know, look, he's a guy

03:05:32.080 --> 03:05:33.800
who from every day of his life,

03:05:33.800 --> 03:05:35.160
from the day he started making money at all,

03:05:35.160 --> 03:05:37.160
he just plows into the next thing.

03:05:38.160 --> 03:05:40.600
And so I think money is definitely an enabler

03:05:40.600 --> 03:05:41.440
for satisfaction.

03:05:41.440 --> 03:05:42.800
It was supposed to say, money applied to happiness

03:05:42.800 --> 03:05:44.480
leads people down very dark paths.

03:05:45.160 --> 03:05:48.520
Very destructive avenues.

03:05:48.520 --> 03:05:50.080
Money applied to satisfaction, I think,

03:05:50.080 --> 03:05:51.400
could be a real tool.

03:05:52.280 --> 03:05:54.160
I always look, by the way, I was like, you know,

03:05:54.160 --> 03:05:55.640
Elon is the case study for behavior.

03:05:55.640 --> 03:05:57.920
But the other thing that it's always really made me think

03:05:57.920 --> 03:05:59.720
is Larry Page was asked one time

03:05:59.720 --> 03:06:01.040
what his approach to philanthropy was.

03:06:01.040 --> 03:06:03.000
And he said, oh, I'm just, my philanthropic plan

03:06:03.000 --> 03:06:04.840
is just give all the money to Elon.

03:06:04.840 --> 03:06:07.760
Right?

03:06:07.760 --> 03:06:09.400
Well, let me actually ask you about Elon.

03:06:09.400 --> 03:06:11.120
What are your,

03:06:11.120 --> 03:06:14.720
you've interacted with quite a lot of successful engineers

03:06:14.720 --> 03:06:15.800
and business people.

03:06:15.800 --> 03:06:17.080
What do you think is special about Elon?

03:06:17.080 --> 03:06:18.960
We talked about Steve Jobs.

03:06:18.960 --> 03:06:22.120
What do you think is special about him

03:06:22.120 --> 03:06:23.920
as a leader, as an innovator?

03:06:23.920 --> 03:06:27.400
Yeah, so the core of it is he's back to the future.

03:06:27.400 --> 03:06:30.680
So he is doing the most leading edge things in the world,

03:06:30.680 --> 03:06:33.360
but with a really deeply old school approach.

03:06:33.360 --> 03:06:35.480
And so to find comparisons to Elon,

03:06:35.480 --> 03:06:38.120
you need to go to like Henry Ford and Thomas Watson

03:06:38.160 --> 03:06:41.880
and Howard Hughes and Andrew Carnegie, right?

03:06:41.880 --> 03:06:45.120
Leland Stanford, John D. Rockefeller, right?

03:06:45.120 --> 03:06:46.400
You need to go to the,

03:06:46.400 --> 03:06:47.880
what were called the bourgeois capitalists,

03:06:47.880 --> 03:06:50.440
like the hardcore business owner operators

03:06:50.440 --> 03:06:52.280
who basically built, you know,

03:06:52.280 --> 03:06:56.720
basically built industrialized society, vendor built.

03:06:56.720 --> 03:07:01.240
And it's a level of hands-on commitment

03:07:01.240 --> 03:07:05.040
and depth in the business

03:07:06.040 --> 03:07:10.720
coupled with an absolute priority towards truth

03:07:10.720 --> 03:07:15.000
and towards kind of put it science and technology

03:07:15.000 --> 03:07:16.120
down to first principles.

03:07:16.120 --> 03:07:17.880
That is just like absolute.

03:07:17.880 --> 03:07:20.200
It was just like unbelievably absolute.

03:07:20.200 --> 03:07:22.760
He really is ideal that he's only ever talking to engineers.

03:07:22.760 --> 03:07:24.840
Like he does not tolerate bullshit.

03:07:24.840 --> 03:07:26.320
He has the absolute bullshit tolerance

03:07:26.320 --> 03:07:27.960
than anybody I've ever met.

03:07:27.960 --> 03:07:31.160
He wants ground truth on every single topic

03:07:31.160 --> 03:07:33.600
and he runs his businesses directly day to day

03:07:33.600 --> 03:07:36.200
devoted to getting to ground truth in every single topic.

03:07:36.200 --> 03:07:41.200
So you think it was a good decision for him to buy Twitter?

03:07:41.400 --> 03:07:42.640
I have developed a view in life

03:07:42.640 --> 03:07:44.160
to not second guess Elon Musk.

03:07:45.560 --> 03:07:49.800
I know this is gonna sound crazy and unfounded, but.

03:07:49.800 --> 03:07:53.160
Well, I mean, he's got quite a track record.

03:07:53.160 --> 03:07:54.880
I mean, look, the car was a crazy,

03:07:54.880 --> 03:07:56.120
I mean, the car was, I mean, look.

03:07:56.120 --> 03:07:57.880
He's done a lot of things that seem crazy.

03:07:57.880 --> 03:07:59.880
Starting a new car company in the United States of America,

03:07:59.880 --> 03:08:01.360
the last time somebody really tried to do that

03:08:01.400 --> 03:08:04.000
was the 1950s and it was called Tucker Automotive

03:08:04.000 --> 03:08:05.040
and it was such a disaster.

03:08:05.040 --> 03:08:07.520
They made a movie about what a disaster it was.

03:08:07.520 --> 03:08:09.880
And then Rockets, like who does that?

03:08:09.880 --> 03:08:11.640
Like that's, there's obviously no way

03:08:11.640 --> 03:08:13.560
to start a new rocket company like those days are over.

03:08:13.560 --> 03:08:16.160
And then to do those at the same time.

03:08:16.160 --> 03:08:19.800
So after he pulled those two off, like, okay, fine.

03:08:19.800 --> 03:08:22.320
Like, this is one of my areas of like,

03:08:22.320 --> 03:08:24.240
whatever opinions I had about that is just like,

03:08:24.240 --> 03:08:25.280
okay, clearly are not relevant.

03:08:25.280 --> 03:08:26.840
Like this is, you just, at some point,

03:08:26.840 --> 03:08:28.360
you just like bet on the person.

03:08:28.360 --> 03:08:30.520
And in general, I wish more people would lean

03:08:30.520 --> 03:08:34.040
on celebrating and supporting versus deriding and destroying.

03:08:34.040 --> 03:08:34.880
Oh yeah.

03:08:34.880 --> 03:08:37.040
I mean, look, he drives resentment.

03:08:37.040 --> 03:08:39.960
Like it's, like he is a magnet for resentment.

03:08:41.240 --> 03:08:43.280
Like his critics are the most miserable,

03:08:43.280 --> 03:08:45.040
like resentful people in the world.

03:08:45.040 --> 03:08:46.880
Like it's almost a perfect match

03:08:46.880 --> 03:08:50.080
of like the most idealized, you know, technologists,

03:08:50.080 --> 03:08:51.880
you know, of the century coupled with

03:08:51.880 --> 03:08:54.440
like just his critics are just bitter as can be.

03:08:54.440 --> 03:08:58.680
It's, I mean, it's sort of very darkly comic to watch.

03:08:58.680 --> 03:09:01.400
Well, he, he fuels the fire of that

03:09:01.400 --> 03:09:03.840
by being an asshole on Twitter at times.

03:09:03.840 --> 03:09:06.440
And which is fascinating to watch the drama

03:09:06.440 --> 03:09:10.400
of human civilization, given our cult roots,

03:09:10.400 --> 03:09:12.640
just fully on fire.

03:09:12.640 --> 03:09:13.720
He's running a cult.

03:09:15.000 --> 03:09:15.840
You could say that.

03:09:15.840 --> 03:09:16.760
Very successfully.

03:09:16.760 --> 03:09:19.040
So now, now that our cults have gone

03:09:19.040 --> 03:09:20.120
and we search for meaning,

03:09:20.120 --> 03:09:22.200
what do you think is the meaning of this whole thing?

03:09:22.200 --> 03:09:24.000
What's the meaning of life, Mark Andreessen?

03:09:24.000 --> 03:09:25.840
I don't know the answer to that.

03:09:25.840 --> 03:09:30.360
I think the meaning of, of the closest I get to it

03:09:30.360 --> 03:09:31.560
is what I said about satisfaction.

03:09:31.560 --> 03:09:33.840
So it's basically like, okay, we were given what we have.

03:09:33.840 --> 03:09:35.840
Like we should basically do our best.

03:09:35.840 --> 03:09:37.880
What's the role of love in that mix?

03:09:37.880 --> 03:09:39.920
I mean, like what's the point of life if you're,

03:09:39.920 --> 03:09:42.360
yeah, without love, like, yeah.

03:09:42.360 --> 03:09:44.680
So love is a big part of that satisfaction.

03:09:44.680 --> 03:09:47.000
Look, like taking care of people is like a wonderful thing.

03:09:47.000 --> 03:09:49.280
Like, you know, in mentality, you know,

03:09:49.280 --> 03:09:51.360
there are pathological forms of taking care of people,

03:09:51.360 --> 03:09:53.040
but there's also a very fundamental, you know,

03:09:53.040 --> 03:09:54.680
kind of aspect of taking care of people.

03:09:54.680 --> 03:09:55.920
Like for example, I happen to be somebody

03:09:55.920 --> 03:09:57.680
who believes that capitalism and taking care of people

03:09:57.680 --> 03:10:00.760
are actually, they're actually the same thing.

03:10:00.760 --> 03:10:01.840
Somebody once said capitalism

03:10:01.840 --> 03:10:04.360
is how you take care of people you don't know.

03:10:04.360 --> 03:10:05.880
Right, right.

03:10:05.880 --> 03:10:07.880
And so like, yeah, I think it's like deeply woven

03:10:07.880 --> 03:10:09.200
into the whole thing.

03:10:09.200 --> 03:10:11.200
You know, there's a long conversation we had about that,

03:10:11.200 --> 03:10:12.560
but yeah.

03:10:12.560 --> 03:10:15.240
Yeah, creating products that are used by millions of people

03:10:15.240 --> 03:10:17.720
and bring them joy in smaller, big ways.

03:10:17.720 --> 03:10:21.800
And then capitalism kind of enables that, encourages that.

03:10:21.800 --> 03:10:23.760
David Fridman says there's only three ways

03:10:23.760 --> 03:10:26.120
to get somebody to do something for somebody else.

03:10:26.120 --> 03:10:27.960
Love, money, and force.

03:10:32.320 --> 03:10:34.920
Love and money are better than force.

03:10:34.920 --> 03:10:36.960
That's a good ordering, I think.

03:10:36.960 --> 03:10:37.800
We should bet on those.

03:10:37.800 --> 03:10:38.640
Try love first.

03:10:38.640 --> 03:10:41.320
If that doesn't work, then money and then force.

03:10:41.320 --> 03:10:43.520
Well, don't even try that one.

03:10:43.520 --> 03:10:44.800
Mark, you're an incredible person.

03:10:44.800 --> 03:10:45.800
I've been a huge fan.

03:10:45.800 --> 03:10:47.600
I'm glad to finally got a chance to talk.

03:10:47.600 --> 03:10:49.840
I'm a fan of everything you do, everything you do,

03:10:49.840 --> 03:10:51.440
including on Twitter.

03:10:51.440 --> 03:10:53.720
It's a huge honor to meet you, to talk with you.

03:10:54.680 --> 03:10:55.520
Thanks again for doing this.

03:10:55.520 --> 03:10:56.680
Awesome, thank you, Lex.

03:10:56.680 --> 03:10:58.160
Thanks for listening to this conversation

03:10:58.160 --> 03:10:59.320
with Mark Andreessen.

03:10:59.320 --> 03:11:00.480
To support this podcast,

03:11:00.480 --> 03:11:03.440
please check out our sponsors in the description.

03:11:03.440 --> 03:11:05.280
And now let me leave you with some words

03:11:05.280 --> 03:11:07.520
from Mark Andreessen himself.

03:11:07.520 --> 03:11:10.440
The world is a very malleable place.

03:11:10.440 --> 03:11:13.120
If you know what you want and you go for it

03:11:13.120 --> 03:11:16.640
with maximum energy and drive and passion,

03:11:16.640 --> 03:11:19.400
the world will often reconfigure itself around you

03:11:19.400 --> 03:11:23.120
much more quickly and easily than you would think.

03:11:23.120 --> 03:11:26.040
Thank you for listening and hope to see you next time.


WEBVTT

00:00.000 --> 00:03.360
The following is a conversation with Mark Zuckerberg,

00:03.360 --> 00:05.320
his second time on this podcast.

00:05.320 --> 00:08.920
He's the CEO of Meta that owns Facebook, Instagram,

00:08.920 --> 00:12.960
and WhatsApp, all services used by billions of people

00:12.960 --> 00:14.760
to connect with each other.

00:14.760 --> 00:17.320
We talk about his vision for the future of Meta

00:17.320 --> 00:21.240
and the future of AI in our human world.

00:21.240 --> 00:23.440
This is the Lex Friedman podcast.

00:23.440 --> 00:27.080
And now, dear friends, here's Mark Zuckerberg.

00:28.040 --> 00:30.560
So you competed in your first Jiu-Jitsu tournament,

00:30.560 --> 00:34.080
and me as a fellow Jiu-Jitsu practitioner and competitor,

00:34.080 --> 00:35.600
I think that's really inspiring,

00:35.600 --> 00:37.320
given all the things you have going on.

00:37.320 --> 00:40.600
So I gotta ask, what was that experience like?

00:40.600 --> 00:42.120
Oh, it was fun.

00:42.120 --> 00:43.640
I don't know, yeah, I mean, well, look,

00:43.640 --> 00:45.560
I'm a pretty competitive person.

00:45.560 --> 00:46.960
Yeah.

00:46.960 --> 00:50.080
Doing sports that basically require your full attention,

00:50.080 --> 00:53.400
I think is really important to my, like, mental health

00:53.400 --> 00:56.720
and the way I just stay focused at doing everything I'm doing.

00:56.960 --> 01:00.640
So I decided to get into martial arts, and it's awesome.

01:00.640 --> 01:02.200
I got, like, a ton of my friends into it.

01:02.200 --> 01:04.000
We all trained together.

01:04.000 --> 01:07.560
We have, like, a mini academy in my garage.

01:07.560 --> 01:10.480
And I guess one of my friends was like,

01:10.480 --> 01:12.680
hey, we should go do a tournament.

01:12.680 --> 01:14.120
I was like, okay, yeah, let's do it.

01:14.120 --> 01:16.560
I'm not gonna shy away from a challenge like that.

01:16.560 --> 01:18.440
So yeah, but it was awesome.

01:18.440 --> 01:19.720
It was just a lot of fun.

01:19.720 --> 01:20.560
You weren't scared?

01:20.560 --> 01:21.600
There was no fear?

01:21.600 --> 01:22.440
I don't know.

01:22.440 --> 01:25.120
I was pretty sure that I'd do okay.

01:25.160 --> 01:26.800
I like the confidence.

01:26.800 --> 01:28.560
Well, so for people who don't know,

01:28.560 --> 01:30.840
jiu-jitsu is a martial art where you're trying

01:30.840 --> 01:35.840
to break your opponent's limbs or choke them to sleep

01:36.360 --> 01:41.200
and do so with grace and elegance and efficiency

01:41.200 --> 01:42.040
and all that kind of stuff.

01:42.040 --> 01:45.040
It's a kind of art form, I think,

01:45.040 --> 01:46.280
that you can do for your whole life.

01:46.280 --> 01:49.480
And it's basically a game, a sport of human chess.

01:49.480 --> 01:51.480
You can think of, there's a lot of strategy.

01:51.480 --> 01:54.320
There's a lot of sort of interesting human dynamics

01:54.320 --> 01:56.160
of using leverage and all that kind of stuff.

01:56.160 --> 01:59.320
And it's kind of incredible what you could do.

01:59.320 --> 02:01.320
You could do things like a small opponent

02:01.320 --> 02:03.160
could defeat a much larger opponent.

02:03.160 --> 02:05.280
And you get to understand the way the mechanics

02:05.280 --> 02:07.200
of the human body works because of that.

02:07.200 --> 02:09.760
But you certainly can't be distracted.

02:09.760 --> 02:13.120
No, it's 100% focused.

02:13.120 --> 02:15.960
To compete, I needed to get around the fact

02:15.960 --> 02:18.520
that I didn't want it to be like this big thing.

02:18.520 --> 02:23.200
So I basically just, I rolled up with a hat and sunglasses

02:23.240 --> 02:24.920
and I was wearing a COVID mask

02:24.920 --> 02:27.040
and I registered under my first and middle name,

02:27.040 --> 02:28.440
so Mark Elliott.

02:28.440 --> 02:31.160
And it wasn't until I actually pulled all that stuff off

02:31.160 --> 02:32.120
right before I got on the mat

02:32.120 --> 02:33.560
that I think people knew it was me.

02:33.560 --> 02:35.280
So it was pretty low key.

02:35.280 --> 02:37.680
But you're still a public figure.

02:37.680 --> 02:39.080
Yeah, I mean, I didn't want to lose.

02:39.080 --> 02:39.920
Right.

02:39.920 --> 02:41.680
The thing you're partially afraid of

02:41.680 --> 02:44.560
is not just the losing, but being almost embarrassed.

02:44.560 --> 02:46.040
It's so raw, the sport,

02:46.040 --> 02:48.240
in that it's just you and another human being.

02:48.240 --> 02:49.680
There's a primal aspect there.

02:49.680 --> 02:50.680
Oh yeah, it's great.

02:50.680 --> 02:52.040
For a lot of people, it can be terrifying,

02:52.040 --> 02:54.560
especially the first time you're doing the competing

02:54.560 --> 02:56.080
and it wasn't for you.

02:56.080 --> 02:57.680
I see the look of excitement in your face.

02:57.680 --> 02:58.520
Yeah, I don't know.

02:58.520 --> 02:59.360
It wasn't, no fear.

02:59.360 --> 03:01.760
I just think part of learning is failing.

03:01.760 --> 03:02.600
Okay.

03:02.600 --> 03:04.360
Right, so I mean, the main thing,

03:04.360 --> 03:06.440
people who train jiu-jitsu,

03:06.440 --> 03:08.680
it's like you need to not have pride

03:08.680 --> 03:09.520
because I mean, all the stuff

03:09.520 --> 03:12.600
that you were talking about before about getting choked

03:12.600 --> 03:14.880
or getting a joint lock,

03:16.920 --> 03:18.440
you only get into a bad situation

03:18.440 --> 03:21.400
if you're not willing to tap once you've already lost.

03:22.360 --> 03:24.280
But obviously when you're getting started with something,

03:24.280 --> 03:26.200
you're not gonna be an expert at it immediately.

03:26.200 --> 03:28.680
So you just need to be willing to go with that.

03:28.680 --> 03:30.640
But I think this is like, I don't know.

03:30.640 --> 03:32.000
I mean, maybe I've just been embarrassed

03:32.000 --> 03:33.320
enough times in my life.

03:33.320 --> 03:34.160
Yeah.

03:34.160 --> 03:36.040
I do think that there's a thing where like,

03:36.040 --> 03:37.120
as people grow up,

03:37.120 --> 03:39.080
maybe they don't wanna be embarrassed or anything.

03:39.080 --> 03:40.600
They've built their adult identity

03:40.600 --> 03:45.200
and they kind of have a sense of who they are

03:45.200 --> 03:46.480
and what they wanna project.

03:46.480 --> 03:49.080
And I don't know, I think maybe to some degree,

03:49.920 --> 03:53.200
your ability to keep doing interesting things

03:53.200 --> 03:56.960
is your willingness to be embarrassed again

03:56.960 --> 04:00.440
and go back to step one and start as a beginner

04:00.440 --> 04:05.440
and get your ass kicked and look stupid doing things.

04:05.440 --> 04:07.480
And I think so many of the things that we're doing,

04:07.480 --> 04:09.240
whether it's this, I mean,

04:09.240 --> 04:12.160
this is just like a kind of a physical part of my life,

04:12.160 --> 04:14.320
but running the company,

04:14.320 --> 04:16.520
it's like we just take on new adventures

04:16.520 --> 04:19.120
and all the big things that we're doing,

04:19.120 --> 04:22.560
I think of as like 10 plus year missions that we're on

04:22.560 --> 04:25.080
where often early on,

04:25.080 --> 04:26.800
people doubt that we're gonna be able to do it

04:26.800 --> 04:29.040
and the initial work seems kind of silly.

04:29.040 --> 04:30.880
And our whole ethos is we don't wanna wait

04:30.880 --> 04:32.560
until something is perfect to put it out there.

04:32.560 --> 04:35.160
We wanna get it out quickly and get feedback on it.

04:35.160 --> 04:36.000
And so I don't know.

04:36.000 --> 04:36.840
I mean, there's probably just something

04:36.840 --> 04:38.960
about how I approach things in there.

04:38.960 --> 04:41.720
But I just kind of think that the moment that you decide

04:41.720 --> 04:42.920
that you're gonna be too embarrassed

04:42.920 --> 04:44.160
to try something new,

04:44.320 --> 04:45.640
then you're not gonna learn anything anymore.

04:45.640 --> 04:48.040
But like I mentioned,

04:48.040 --> 04:49.800
that fear, that anxiety could be there,

04:49.800 --> 04:51.480
it could creep up every once in a while.

04:51.480 --> 04:55.080
Do you feel that in especially stressful moments

04:55.080 --> 04:57.560
sort of outside of the digital mat,

04:57.560 --> 05:02.560
just at work, stressful moments, big decision days,

05:02.840 --> 05:04.200
big decision moments?

05:04.200 --> 05:05.400
How do you deal with that fear?

05:05.400 --> 05:06.880
How do you deal with that anxiety?

05:06.880 --> 05:08.240
The thing that stresses me out the most

05:08.240 --> 05:10.960
is always the people challenges.

05:10.960 --> 05:13.000
I kind of think that the people challenges

05:13.480 --> 05:18.480
I kind of think that strategy questions,

05:19.000 --> 05:22.840
I tend to have enough conviction around the values

05:22.840 --> 05:26.520
of what we're trying to do and what I think matters

05:26.520 --> 05:28.240
and what I want our company to stand for

05:28.240 --> 05:31.840
that those don't really keep me up at night that much.

05:31.840 --> 05:35.080
I mean, it's not that I get everything right.

05:35.080 --> 05:36.120
Of course I don't, right?

05:36.120 --> 05:38.280
I mean, we make a lot of mistakes.

05:38.280 --> 05:43.240
But I at least have a pretty strong sense

05:43.240 --> 05:45.920
of where I want us to go on that.

05:45.920 --> 05:49.880
The thing in running a company for almost 20 years now,

05:49.880 --> 05:51.880
one of the things that's been pretty clear

05:51.880 --> 05:55.760
is when you have a team that's cohesive,

05:55.760 --> 05:59.200
you can get almost anything done.

05:59.200 --> 06:03.280
And you can run through super hard challenges,

06:04.320 --> 06:07.040
you can make hard decisions and push really hard

06:07.040 --> 06:10.760
to do the best work even in kind of optimize

06:10.760 --> 06:12.160
something super well.

06:12.160 --> 06:13.840
But when there's that tension,

06:13.840 --> 06:16.280
I mean, that's when things get really tough.

06:16.280 --> 06:19.880
And when I talk to other friends who run other companies

06:19.880 --> 06:21.680
and things like that, I think one of the things

06:21.680 --> 06:24.000
that I actually spend a disproportionate amount of time on

06:24.000 --> 06:26.720
and running this company is just fostering

06:26.720 --> 06:30.720
a pretty tight core group of people

06:30.720 --> 06:33.360
who are running the company with me.

06:33.400 --> 06:37.920
And that to me is kind of the thing that

06:37.920 --> 06:39.160
both makes it fun, right?

06:39.160 --> 06:42.120
Having friends and people you've worked with for a while

06:42.120 --> 06:43.520
and new people and new perspectives,

06:43.520 --> 06:46.080
but like a pretty tight group who you can go work on

06:46.080 --> 06:47.680
some of these crazy things with.

06:49.000 --> 06:50.520
But to me, that's also the most stressful thing

06:50.520 --> 06:55.520
is when there's tension, that weighs on me.

06:55.680 --> 06:59.360
I think it's maybe not surprising.

06:59.360 --> 07:01.640
I mean, we're like a very people-focused company

07:01.680 --> 07:04.040
and it's the people is the part of it

07:04.040 --> 07:07.760
that weighs on me the most to make sure that we get right.

07:07.760 --> 07:10.480
But yeah, that I'd say across everything that we do

07:10.480 --> 07:13.000
is probably the big thing.

07:13.000 --> 07:17.760
So when there's tension in that inner circle of close folks,

07:17.760 --> 07:22.760
so when you trust those folks to help you make

07:22.840 --> 07:27.840
difficult decisions about Facebook, WhatsApp, Instagram,

07:27.960 --> 07:30.960
the future of the company and the metaverse with AI,

07:30.960 --> 07:34.200
how do you build that close-knit group of folks

07:34.200 --> 07:36.560
to make those difficult decisions?

07:36.560 --> 07:40.000
Is there people that you have to have critical voices,

07:40.000 --> 07:43.640
very different perspectives on focusing on the past

07:43.640 --> 07:45.800
versus the future, all that kind of stuff?

07:45.800 --> 07:47.240
Yeah, I mean, I think for one thing,

07:47.240 --> 07:51.000
it's just spending a lot of time with whatever the group is

07:51.000 --> 07:53.120
that you want to be that core group,

07:53.120 --> 07:55.760
grappling with all of the biggest problems

07:56.400 --> 07:58.640
grappling with all of the biggest challenges.

07:58.640 --> 08:01.520
And that requires a fair amount of openness.

08:01.520 --> 08:05.120
And so I mean, a lot of how I run the company is,

08:05.120 --> 08:07.320
and it's like every Monday morning we get our,

08:07.320 --> 08:12.080
it's about the top 30 people together and we,

08:12.080 --> 08:13.600
and this is a group that just worked together

08:13.600 --> 08:14.600
for a long period of time.

08:14.600 --> 08:17.840
And I mean, people rotate in, I mean, new people join,

08:17.840 --> 08:18.880
people leave the company,

08:18.880 --> 08:20.320
people go do other roles in the company.

08:20.320 --> 08:22.640
So it's not the same group over time,

08:22.640 --> 08:26.800
but then we spend a lot of times a couple of hours,

08:26.800 --> 08:29.520
a lot of the time it can be somewhat unstructured.

08:29.520 --> 08:31.760
We like, I'll come with maybe a few topics

08:31.760 --> 08:33.840
that are top of mind for me,

08:33.840 --> 08:36.440
but I'll ask other people to bring things

08:36.440 --> 08:39.120
and people raise questions whether it's okay,

08:39.120 --> 08:42.560
there's an issue happening in some country

08:42.560 --> 08:44.240
with some policy issue.

08:44.240 --> 08:46.400
There's like a new technology that's developing here.

08:46.400 --> 08:49.240
We're having an issue with this partner.

08:49.240 --> 08:51.640
There's a design trade-off in WhatsApp

08:51.640 --> 08:55.760
between two things that end up being values

08:55.760 --> 08:56.960
that we care about deeply,

08:56.960 --> 08:59.400
and we need to kind of decide where we wanna be on that.

08:59.400 --> 09:01.200
And I just think over time,

09:01.200 --> 09:04.440
when by working through a lot of issues with people

09:04.440 --> 09:06.040
and doing it openly,

09:06.040 --> 09:08.120
people develop an intuition for each other

09:08.120 --> 09:09.840
and a bond and camaraderie.

09:10.880 --> 09:15.600
And to me, developing that is like a lot of the fun part

09:15.600 --> 09:17.400
of running a company or doing anything, right?

09:17.400 --> 09:20.200
I think it's like having people who are kind of along

09:20.200 --> 09:22.440
on the journey that you feel like you're doing it with.

09:22.440 --> 09:24.680
Nothing is ever just one person doing it.

09:24.680 --> 09:27.920
Are there people that disagree often within that group?

09:27.920 --> 09:29.720
It's a fairly combative group.

09:29.720 --> 09:31.880
Okay, so combat is part of it.

09:31.880 --> 09:35.440
So this is making decisions on design, engineering,

09:36.880 --> 09:38.560
policy, everything.

09:38.560 --> 09:41.160
Everything, everything, yeah.

09:41.160 --> 09:43.640
I have to ask just back to you, Joseph, for a little bit,

09:43.640 --> 09:45.080
what's your favorite submission?

09:45.080 --> 09:47.040
Now that you've been doing it,

09:47.040 --> 09:51.520
how do you like to submit your opponent, Mark Zuckerberg?

09:51.520 --> 09:52.360
I'm in.

09:53.640 --> 09:54.840
Well, but first of all,

09:56.080 --> 09:58.920
do you prefer no gi or gi jiu jitsu?

09:58.920 --> 10:03.920
So gi is this outfit you wear that maybe mimics clothing

10:04.520 --> 10:05.840
so you can choke.

10:05.840 --> 10:06.720
Well, it's like a kimono.

10:06.720 --> 10:08.840
It's like the traditional martial arts or kimono.

10:08.840 --> 10:10.360
Yeah, pajamas.

10:10.360 --> 10:11.200
Pajamas.

10:12.120 --> 10:14.000
That you could choke people with, yes.

10:14.000 --> 10:14.960
Well, it's got the lapels.

10:14.960 --> 10:15.800
Yes.

10:15.800 --> 10:19.160
So I like jiu jitsu.

10:19.160 --> 10:21.040
I also really like MMA.

10:21.040 --> 10:25.880
And so I think no gi more closely approximates MMA.

10:25.880 --> 10:30.760
And I think my style is maybe a little closer

10:30.760 --> 10:31.760
to an MMA style.

10:31.760 --> 10:33.920
So like a lot of jiu jitsu players

10:33.920 --> 10:35.600
are fine being on their back, right?

10:35.600 --> 10:36.760
And obviously having a good guard

10:36.760 --> 10:39.320
is a critical part of jiu jitsu.

10:39.320 --> 10:41.800
But in MMA, you don't wanna be on your back, right?

10:41.800 --> 10:42.880
Because even if you have control,

10:42.880 --> 10:45.680
you're just taking punches while you're on your back.

10:46.560 --> 10:47.720
So that's no good.

10:47.720 --> 10:48.720
Do you like being on top?

10:48.720 --> 10:51.720
My style is I'm probably more pressure

10:55.040 --> 10:57.560
and I'd probably rather be the top player.

10:57.560 --> 11:00.000
But I'm also smaller, right?

11:00.000 --> 11:02.200
I'm not like a heavyweight guy, right?

11:02.200 --> 11:05.080
So from that perspective, I think like,

11:06.320 --> 11:08.280
especially because if I'm doing a competition,

11:08.280 --> 11:09.720
I'll compete with people who are my size,

11:09.720 --> 11:11.640
but a lot of my friends are bigger than me.

11:11.640 --> 11:15.080
So back takes probably pretty important, right?

11:15.120 --> 11:17.360
Because that's where you have the most leverage advantage,

11:17.360 --> 11:18.200
right?

11:18.200 --> 11:20.600
Where people, their arms,

11:20.600 --> 11:22.520
your arms are very weak behind you, right?

11:22.520 --> 11:24.720
So being able to get to the back

11:24.720 --> 11:26.520
and take that pretty important.

11:26.520 --> 11:28.320
But I don't know, I feel like the right strategy

11:28.320 --> 11:31.320
is to not be too committed to any single submission.

11:31.320 --> 11:32.960
That said, I don't like hurting people.

11:32.960 --> 11:36.080
So I always think that chokes

11:36.080 --> 11:40.920
are a somewhat more humane way to go than joint locks.

11:40.920 --> 11:42.760
Yeah, and it's more about control.

11:42.760 --> 11:44.160
It's less dynamic.

11:44.160 --> 11:46.800
So you're basically like a Habeb Nariman and Madoff

11:46.800 --> 11:47.640
type of fighter.

11:47.640 --> 11:50.120
So let's go, yeah, back take to a rear naked choke,

11:50.120 --> 11:52.320
I think is like the clean way to go.

11:52.320 --> 11:53.960
Straightforward answer right there.

11:53.960 --> 11:56.800
What advice would you give to people

11:56.800 --> 11:59.400
looking to start learning jiu-jitsu?

11:59.400 --> 12:02.960
Given how busy you are, given where you are in life,

12:02.960 --> 12:05.120
that you're able to do this, you're able to train,

12:05.120 --> 12:08.480
you're able to compete and get to learn something

12:08.480 --> 12:10.760
from this interesting art.

12:10.760 --> 12:12.760
Why do you think you have to be willing to

12:14.640 --> 12:16.560
to just get beaten up a lot?

12:16.560 --> 12:17.400
Yeah.

12:17.400 --> 12:18.800
But I mean, over time,

12:18.800 --> 12:21.080
I think that there's a flow to all these things.

12:21.080 --> 12:24.520
And there's, you know, one of the,

12:26.040 --> 12:28.760
one of, I don't know, my experiences

12:28.760 --> 12:31.880
that I think kind of transcends running a company

12:31.880 --> 12:35.680
and the different activities that I like doing are,

12:35.680 --> 12:38.520
I really believe that if you're gonna accomplish

12:38.520 --> 12:40.120
whatever, anything,

12:40.120 --> 12:43.280
a lot of it is just being willing to push through, right?

12:43.400 --> 12:45.560
And having the grit and determination

12:45.560 --> 12:48.840
to push through difficult situations.

12:48.840 --> 12:52.840
And I think for a lot of people that ends up being

12:52.840 --> 12:55.200
sort of a difference maker between the people

12:55.200 --> 12:58.360
who kind of get the most done and not.

12:58.360 --> 13:00.360
I mean, there's all these questions about like,

13:01.280 --> 13:03.400
you know, how many days people wanna work

13:03.400 --> 13:04.240
and things like that.

13:04.240 --> 13:05.600
I think almost all the people who like

13:05.600 --> 13:07.440
start successful companies or things like that

13:07.440 --> 13:09.480
are just are working extremely hard.

13:09.480 --> 13:11.080
But I think one of the things that you learn

13:11.080 --> 13:13.960
both by doing this over time or, you know,

13:13.960 --> 13:17.240
very acutely with things like jujitsu or surfing

13:17.240 --> 13:20.560
is you can't push through everything.

13:21.520 --> 13:24.040
And I think that that's,

13:25.440 --> 13:28.440
you learn this stuff very acutely,

13:28.440 --> 13:30.360
doing sports compared to running a company.

13:30.360 --> 13:33.360
Because running a company, the cycle times are so long,

13:33.360 --> 13:36.200
where it's like you start a project

13:36.200 --> 13:38.720
and then, you know, it's like months later,

13:38.720 --> 13:40.400
or, you know, if you're building hardware,

13:40.400 --> 13:42.280
it could be years later before you're actually getting

13:42.280 --> 13:45.000
feedback and able to make the next set of decisions

13:45.000 --> 13:46.560
for the next version of the thing that you're doing.

13:46.560 --> 13:49.360
Whereas one of the things that I just think is mentally

13:49.360 --> 13:54.000
so nice about these very high turnaround conditioning

13:54.000 --> 13:55.400
sports, things like that,

13:55.400 --> 13:57.200
is you get feedback very quickly, right?

13:57.200 --> 13:59.680
It's like, okay, like I don't counter something correctly,

13:59.680 --> 14:00.960
you get punched in the face, right?

14:00.960 --> 14:03.360
So not in jujitsu, you don't get punched in jujitsu,

14:03.360 --> 14:04.840
but in MMA.

14:04.840 --> 14:07.200
There are all these analogies between all these things

14:07.240 --> 14:08.280
that I think actually hold,

14:08.280 --> 14:12.360
that are like important life lessons, right?

14:12.360 --> 14:14.320
It's like, okay, you're surfing a wave,

14:14.320 --> 14:18.480
it's like, you know, sometimes you're like,

14:18.480 --> 14:20.920
you can't go in the other direction on it, right?

14:20.920 --> 14:23.760
It's like, there are limits to kind of what, you know,

14:23.760 --> 14:26.400
it's like foil, you can pump the foil

14:26.400 --> 14:29.200
and push pretty hard in a bunch of directions,

14:29.200 --> 14:31.280
but like, yeah, you know, it's at some level,

14:31.280 --> 14:34.600
like the momentum against you is strong enough,

14:34.600 --> 14:35.440
that's not gonna work.

14:35.440 --> 14:40.440
And I do think that that's sort of a humbling,

14:40.680 --> 14:43.960
but also an important lesson for,

14:43.960 --> 14:45.240
and I think people who are running things

14:45.240 --> 14:48.240
or building things, it's like, yeah, you, you, you know,

14:48.240 --> 14:50.760
a lot of the game is just being able to kind of push

14:50.760 --> 14:53.560
and work through complicated things,

14:53.560 --> 14:56.840
but you also need to kind of have enough of an understanding

14:56.840 --> 14:58.560
of like which things you just can't push through

14:58.560 --> 15:02.280
and where the finesse is more important.

15:02.280 --> 15:03.120
Yeah.

15:03.120 --> 15:05.160
What are your jujitsu life lessons?

15:05.160 --> 15:07.960
Well, I think you did it,

15:09.160 --> 15:13.160
you made it sound so simple and were so eloquent

15:13.160 --> 15:18.160
that it's easy to miss, but basically being okay

15:18.560 --> 15:21.880
and accepting the wisdom and the joy

15:21.880 --> 15:24.640
in the getting your ass kicked

15:24.640 --> 15:27.000
in the full range of what that means,

15:27.000 --> 15:30.920
I think that's a big gift of the being humbled.

15:30.920 --> 15:34.120
Somehow being humbled, especially physically,

15:34.120 --> 15:37.240
opens your mind to the full process of learning

15:37.240 --> 15:38.400
what it means to learn,

15:38.400 --> 15:41.760
which is being willing to suck at something.

15:41.760 --> 15:45.600
And I think jujitsu is just very repetitively,

15:45.600 --> 15:49.800
efficiently humbles you over and over and over and over

15:49.800 --> 15:51.840
to where you can carry that lessons

15:51.840 --> 15:55.120
to places where you don't get humbled as much,

15:55.120 --> 15:57.080
whether it's research or running a company

15:57.080 --> 15:59.840
or building stuff, the cycle is longer.

15:59.840 --> 16:01.640
In jujitsu, you can just get humbled

16:01.800 --> 16:04.840
in this period of an hour, over and over and over and over,

16:04.840 --> 16:05.920
especially when you're a beginner,

16:05.920 --> 16:07.920
you'll have a little person,

16:07.920 --> 16:09.960
just somebody much smaller than you,

16:09.960 --> 16:14.960
just kick your ass repeatedly, definitively,

16:16.440 --> 16:17.640
where there's no argument.

16:17.640 --> 16:18.480
Oh yeah.

16:18.480 --> 16:20.120
And then you literally tap,

16:20.120 --> 16:22.600
because if you don't tap, you're going to die.

16:22.600 --> 16:24.000
So this is an agreement,

16:24.960 --> 16:27.200
you could have killed me just now, but we're friends,

16:27.200 --> 16:29.240
so we're gonna agree that you're not going to,

16:29.240 --> 16:31.320
and that kind of humbling process,

16:31.960 --> 16:33.360
it just does something to your psyche,

16:33.360 --> 16:35.720
to your ego that puts it in its proper context

16:35.720 --> 16:39.880
to realize that everything in this life

16:39.880 --> 16:44.880
is like a journey from sucking through a hard process

16:45.760 --> 16:49.400
of improving rigorously,

16:49.400 --> 16:51.240
day after day after day after day.

16:51.240 --> 16:53.440
Any kind of success requires hard work.

16:54.280 --> 16:57.120
Yeah, jujitsu, more than a lot of sports, I would say,

16:57.120 --> 16:58.440
because I've done a lot of them,

16:58.440 --> 16:59.960
it really teaches you that.

16:59.960 --> 17:01.640
And you made it sound so simple.

17:01.640 --> 17:04.920
Like, I'm okay, it's okay, it's part of the process,

17:04.920 --> 17:06.400
you just get humble, get your ass kicked.

17:06.400 --> 17:07.840
I've just failed and been embarrassed

17:07.840 --> 17:09.040
so many times in my life

17:09.040 --> 17:12.440
that it's a core competence to this point.

17:12.440 --> 17:14.280
It's a core competence.

17:14.280 --> 17:16.080
Well, yes, and there's a deep truth to that,

17:16.080 --> 17:18.560
being able to, and you said it in the very beginning,

17:18.560 --> 17:21.720
which is, that's the thing that stops us,

17:21.720 --> 17:22.680
especially as you get older,

17:22.680 --> 17:25.360
especially as you develop expertise in certain areas,

17:25.600 --> 17:30.000
the not being willing to be a beginner in a new area,

17:31.280 --> 17:34.080
because that's where the growth happens,

17:34.080 --> 17:35.920
is being willing to be a beginner,

17:35.920 --> 17:38.520
being willing to be embarrassed saying something stupid,

17:38.520 --> 17:39.720
doing something stupid.

17:40.800 --> 17:42.440
A lot of us that get good at one thing,

17:42.440 --> 17:43.960
you wanna show that off,

17:43.960 --> 17:48.000
and it sucks being a beginner,

17:48.000 --> 17:50.280
but it's where growth happens.

17:51.440 --> 17:54.680
Well, speaking of which, let me ask you about AI.

17:54.680 --> 17:56.400
It seems like this year,

17:56.400 --> 17:58.360
for the entirety of the human civilization,

17:58.360 --> 18:00.000
is an interesting year

18:00.000 --> 18:02.600
for the development of artificial intelligence.

18:02.600 --> 18:04.600
A lot of interesting stuff is happening.

18:05.520 --> 18:07.360
So Meta is a big part of that.

18:08.520 --> 18:10.000
Meta has developed Llama,

18:10.000 --> 18:12.280
which is a 65 billion parameter model.

18:14.400 --> 18:16.680
There's a lot of interesting questions I can ask here,

18:16.680 --> 18:19.080
one of which has to do with open source.

18:19.080 --> 18:22.240
But first, can you tell the story of developing

18:22.240 --> 18:24.080
of this model?

18:24.080 --> 18:29.080
And making the complicated decision of how to release it?

18:29.640 --> 18:30.800
Yeah, sure.

18:30.800 --> 18:32.040
I think you're right, first of all,

18:32.040 --> 18:33.240
that in the last year,

18:33.240 --> 18:36.440
there have been a bunch of advances

18:36.440 --> 18:39.680
on scaling up these large transformer models.

18:39.680 --> 18:41.160
So there's the language equivalent of it

18:41.160 --> 18:43.080
with large language models.

18:43.080 --> 18:45.880
There's sort of the image generation equivalent

18:45.880 --> 18:48.240
with these large diffusion models.

18:49.360 --> 18:50.600
There's a lot of fundamental research

18:50.600 --> 18:51.440
that's gone into this.

18:51.440 --> 18:55.560
And Meta has taken the approach

18:55.560 --> 18:59.320
of being quite open

18:59.320 --> 19:04.320
and academic in our development of AI.

19:04.320 --> 19:06.960
Part of this is we wanna have the best people

19:06.960 --> 19:08.480
in the world researching this.

19:08.480 --> 19:11.560
And a lot of the best people wanna know

19:11.560 --> 19:12.960
that they're gonna be able to share their work.

19:12.960 --> 19:15.840
So that's part of the deal that we have,

19:15.840 --> 19:17.920
is that we can get,

19:17.920 --> 19:20.440
if you're one of the top AI researchers in the world,

19:20.520 --> 19:21.720
you can come here, you can get access

19:21.720 --> 19:25.520
to kind of industry scale infrastructure.

19:25.520 --> 19:29.080
And part of our ethos is that we wanna share

19:29.080 --> 19:32.120
what's invented broadly.

19:32.120 --> 19:34.320
We do that with a lot of the different AI tools

19:34.320 --> 19:35.680
that we create.

19:35.680 --> 19:37.720
And Llama is the language model

19:37.720 --> 19:39.840
that our research team made.

19:39.840 --> 19:44.840
And we did a limited open source release for it,

19:46.400 --> 19:49.640
which was intended for researchers to be able to use it.

19:51.160 --> 19:55.440
But responsibility and getting safety right on these

19:55.440 --> 19:57.120
is very important.

19:57.120 --> 20:00.000
So we didn't think that for the first one,

20:00.000 --> 20:01.960
there were a bunch of questions around

20:01.960 --> 20:04.160
whether we should be releasing this commercially.

20:04.160 --> 20:08.200
So we kind of punted on that for V1 of Llama

20:08.200 --> 20:09.880
and just released it for research.

20:09.880 --> 20:13.280
Now, obviously by releasing it for research,

20:13.280 --> 20:15.160
it's out there, but companies know

20:15.160 --> 20:17.240
that they're not supposed to kind of put it

20:17.240 --> 20:18.800
into commercial releases.

20:18.800 --> 20:22.720
And we're working on the follow-up models for this

20:22.720 --> 20:27.720
and thinking through how exactly this should work

20:27.880 --> 20:29.480
for follow-on now that we've had time

20:29.480 --> 20:31.840
to work on a lot more of the safety

20:31.840 --> 20:33.920
and the pieces around that.

20:33.920 --> 20:35.760
But overall, I mean, this is,

20:37.040 --> 20:42.040
I just kind of think that it would be good

20:42.800 --> 20:45.960
if there were a lot of different folks

20:45.960 --> 20:48.760
who have the ability to build

20:48.760 --> 20:51.480
state-of-the-art technology here,

20:53.360 --> 20:56.320
and not just a small number of big companies.

20:56.320 --> 20:58.680
Where to train one of these AI models,

20:58.680 --> 21:00.320
the state-of-the-art models,

21:00.320 --> 21:04.560
is it just takes hundreds of millions of dollars

21:04.560 --> 21:05.760
of infrastructure, right?

21:05.760 --> 21:09.600
So there are not that many organizations in the world

21:10.720 --> 21:13.080
that can do that at the biggest scale today.

21:13.080 --> 21:16.120
And no, it gets more efficient every day.

21:16.120 --> 21:19.880
So I do think that that will be available

21:19.880 --> 21:20.960
to more folks over time.

21:20.960 --> 21:23.920
But I just think there's all this innovation out there

21:23.920 --> 21:25.520
that people can create.

21:25.520 --> 21:30.040
And I just think that we'll also learn a lot

21:30.040 --> 21:33.000
by seeing what the whole community of students

21:33.000 --> 21:37.800
and hackers and startups and different folks

21:37.800 --> 21:38.640
build with this.

21:38.640 --> 21:40.440
And that's kind of been how we've approached this.

21:40.440 --> 21:43.040
And it's also how we've done a lot of our infrastructure.

21:43.040 --> 21:44.920
And we took our whole data center design

21:44.920 --> 21:46.000
and our server design,

21:46.000 --> 21:48.040
and we built this open compute project

21:48.040 --> 21:49.120
where we just made that public.

21:49.120 --> 21:51.760
And part of the theory was like, all right,

21:51.760 --> 21:54.480
if we make it so that more people can use the server design,

21:54.480 --> 21:58.000
then that'll enable more innovation.

21:58.000 --> 22:00.120
It'll also make the server design more efficient,

22:00.120 --> 22:02.480
and that'll make our business more efficient too.

22:02.480 --> 22:04.280
So that's worked, and we've just done this

22:04.280 --> 22:06.480
with a lot of our infrastructure.

22:06.480 --> 22:08.960
So for Pigueroa No, you did the limited release,

22:08.960 --> 22:12.600
I think in February of this year, of Llama.

22:12.640 --> 22:15.600
And it got quote unquote leaked,

22:16.600 --> 22:21.600
meaning like it escaped the limited release aspect,

22:23.800 --> 22:28.320
but it was something you probably anticipated,

22:28.320 --> 22:29.880
given that it's just released to researchers.

22:29.880 --> 22:30.960
We shared it with researchers.

22:30.960 --> 22:33.440
Right, so it's just trying to make sure

22:33.440 --> 22:35.520
that there's like a slow release.

22:35.520 --> 22:36.840
Yeah.

22:36.840 --> 22:39.120
But from there, I just would love to get your comment

22:39.120 --> 22:40.680
on what happened next, which is like,

22:40.760 --> 22:42.760
there's a very vibrant open source community

22:42.760 --> 22:44.520
that just builds stuff on top of it.

22:44.520 --> 22:48.960
There's Llama CPP, basically stuff that makes it

22:48.960 --> 22:51.760
more efficient to run on smaller computers.

22:51.760 --> 22:54.960
There's combining with reinforcement learning

22:54.960 --> 22:55.880
with human feedback.

22:55.880 --> 22:58.040
So some of the different interesting

22:58.040 --> 22:59.480
fine tuning mechanisms.

22:59.480 --> 23:02.920
There's then also like fine tuning in a GPT-3 generations.

23:02.920 --> 23:07.560
There's a lot of GPT for all, Alpaca, Colossal AI,

23:07.560 --> 23:09.360
all these kinds of models just kind of spring up

23:09.360 --> 23:11.960
and like run on top of it.

23:11.960 --> 23:13.160
What do you think about that?

23:13.160 --> 23:15.040
No, I think it's been really neat to see.

23:15.040 --> 23:17.840
I mean, there's been folks who are getting it to run

23:17.840 --> 23:19.360
on local devices, right?

23:19.360 --> 23:22.480
So if you're an individual who just wants to experiment

23:23.600 --> 23:26.400
with this at home, you probably don't have a large budget

23:26.400 --> 23:29.480
to get access to like a large amount of cloud compute.

23:29.480 --> 23:31.720
So getting it to run on your local laptop

23:34.080 --> 23:36.080
is pretty good and pretty relevant.

23:37.040 --> 23:39.840
And then there were things like, yeah, Llama CPP

23:40.920 --> 23:42.520
re-implemented it more efficiently.

23:42.520 --> 23:45.920
So now even when we run our own versions of it,

23:45.920 --> 23:47.280
we can do it on way less compute

23:47.280 --> 23:50.400
and it just way more efficient, save a lot of money

23:50.400 --> 23:51.760
for everyone who uses this.

23:51.760 --> 23:53.800
So that is good.

23:55.840 --> 23:59.320
I do think it's worth calling out that

23:59.320 --> 24:01.800
because this was a relatively early release,

24:02.320 --> 24:07.320
Llama isn't quite as on the frontier as, for example,

24:08.840 --> 24:13.280
the biggest open AI models or the biggest Google models.

24:13.280 --> 24:16.640
I mean, you mentioned that the largest Llama model

24:16.640 --> 24:19.360
that we released had 65 billion parameters.

24:19.360 --> 24:24.360
And when no one knows, I guess outside of open AI,

24:24.560 --> 24:27.880
exactly what the specs are for GPT-4,

24:27.880 --> 24:32.120
but I think my understanding is it's like 10 times bigger.

24:32.120 --> 24:35.080
And I think Google's Palm model is also, I think,

24:35.080 --> 24:36.760
has about 10 times as many parameters.

24:36.760 --> 24:38.600
Now, the Llama models are very efficient.

24:38.600 --> 24:40.400
So they perform well for something

24:40.400 --> 24:42.600
that's around 65 billion parameters.

24:42.600 --> 24:44.360
So for me, that was also part of this

24:44.360 --> 24:47.080
because there's this whole debate around,

24:47.080 --> 24:50.800
is it good for everyone in the world to have access

24:50.800 --> 24:55.040
to the most frontier AI models?

24:55.040 --> 25:00.040
And I think as the AI models start approaching something

25:00.480 --> 25:04.000
that's like a superhuman intelligence,

25:04.000 --> 25:05.320
I think that's a bigger question

25:05.320 --> 25:06.320
that we'll have to grapple with.

25:06.320 --> 25:10.880
But right now, I mean, these are still very basic tools.

25:10.880 --> 25:14.280
They're powerful in the sense that,

25:14.280 --> 25:17.640
a lot of open source software like databases or web servers

25:17.640 --> 25:20.560
can enable a lot of pretty important things.

25:20.880 --> 25:25.880
But I don't think anyone looks at the current generation

25:27.280 --> 25:30.240
of Llama and thinks it's anywhere near a superintelligence.

25:30.240 --> 25:32.480
So I think that a bunch of those questions around like,

25:32.480 --> 25:35.880
is it good to kind of get out there?

25:35.880 --> 25:37.760
I think at this stage, surely,

25:37.760 --> 25:40.160
you want more researchers working on it

25:40.160 --> 25:43.360
for all the reasons that open source software

25:43.360 --> 25:44.600
has a lot of advantages.

25:44.600 --> 25:45.920
And we talked about efficiency before,

25:45.920 --> 25:48.000
but another one is just open source software

25:48.000 --> 25:49.760
tends to be more secure

25:49.760 --> 25:52.080
because you have more people looking at it openly

25:52.080 --> 25:55.840
and scrutinizing it and finding holes in it.

25:55.840 --> 25:57.080
And that makes it more safe.

25:57.080 --> 25:59.280
So I think at this point, it's more,

25:59.280 --> 26:01.960
I think it's generally agreed upon

26:01.960 --> 26:06.640
that open source software is generally more secure and safer

26:06.640 --> 26:08.760
than things that are kind of developed in a silo

26:08.760 --> 26:11.320
where people try to get through security through obscurity.

26:11.320 --> 26:13.400
So I think that for the scale

26:13.400 --> 26:16.400
of what we're seeing now with AI,

26:16.400 --> 26:19.480
I think we're more likely to get to good alignment

26:20.120 --> 26:23.440
and good understanding of kind of what needs to do

26:23.440 --> 26:26.080
to make this work well by having it be open source.

26:26.080 --> 26:28.280
And that's something that I think is quite good

26:28.280 --> 26:30.880
to have out there and happening publicly at this point.

26:30.880 --> 26:34.560
Meta released a lot of models as open source.

26:34.560 --> 26:38.360
So the massively multilingual speech model,

26:38.360 --> 26:40.960
the image buying model. Yeah, that was neat.

26:40.960 --> 26:42.000
I'll ask you questions about those,

26:42.000 --> 26:45.840
but the point is you've open sourced quite a lot.

26:45.840 --> 26:48.600
You've been spearheading the open source movement.

26:48.640 --> 26:51.920
That's really positive, inspiring to see from one angle,

26:51.920 --> 26:53.000
from the research angle.

26:53.000 --> 26:55.320
Of course, there's folks who are really terrified

26:55.320 --> 26:58.360
about the existential threat of artificial intelligence.

26:58.360 --> 27:00.240
And those folks will say that,

27:02.800 --> 27:06.480
you have to be careful about the open sourcing step.

27:06.480 --> 27:09.560
But where do you see the future of open source here

27:09.560 --> 27:11.320
as part of Meta?

27:11.320 --> 27:13.840
The tension here is,

27:13.840 --> 27:16.120
do you wanna release the magic sauce?

27:16.120 --> 27:18.080
That's one tension.

27:18.120 --> 27:20.000
And the other one is,

27:20.000 --> 27:24.880
do you wanna put a powerful tool in the hands of bad actors,

27:24.880 --> 27:26.960
even though it probably has a huge amount

27:26.960 --> 27:28.200
of positive impact also?

27:29.200 --> 27:31.080
Yeah, I mean, again, I think for the stage

27:31.080 --> 27:32.880
that we're at in the development of AI,

27:32.880 --> 27:35.320
I don't think anyone looks at the current state of things

27:35.320 --> 27:37.560
and thinks that this is super intelligence.

27:38.680 --> 27:41.720
And the models that we're talking about,

27:41.720 --> 27:44.920
the llama models here are generally

27:44.920 --> 27:47.120
an order of magnitude smaller than what open AI

27:47.280 --> 27:48.000
or Google are doing.

27:48.000 --> 27:52.080
So I think that at least for the stage that we're in now,

27:52.080 --> 27:55.800
the equity is balanced strongly in my view

27:55.800 --> 27:58.320
towards doing this more openly.

27:58.320 --> 28:00.360
I think if you got something that was closer

28:00.360 --> 28:02.760
to super intelligence,

28:02.760 --> 28:05.200
then I think you'd have to discuss that more

28:05.200 --> 28:07.880
and think through that a lot more.

28:07.880 --> 28:09.480
And we haven't made a decision yet

28:09.480 --> 28:11.680
as to what we would do if we were in that position,

28:11.680 --> 28:13.600
but I think there's a good chance

28:13.600 --> 28:15.400
that we're pretty far off from that position.

28:16.400 --> 28:21.400
So I'm certainly not saying that the position

28:22.560 --> 28:25.440
that we're taking on this now applies

28:25.440 --> 28:27.120
to every single thing that we would ever do.

28:27.120 --> 28:29.360
And certainly inside the company,

28:29.360 --> 28:31.000
and we probably do more open source work

28:31.000 --> 28:34.360
than most of the other big tech companies,

28:34.360 --> 28:36.080
but we also don't open source everything.

28:36.080 --> 28:40.520
We're in a lot of the core kind of app code for WhatsApp

28:40.520 --> 28:41.600
or Instagram or something.

28:41.600 --> 28:42.920
I mean, we're not open sourcing that.

28:42.920 --> 28:45.840
It's not like a general enough piece of software

28:45.840 --> 28:47.400
that would be useful for a lot of people

28:47.400 --> 28:49.280
to do different things.

28:50.280 --> 28:53.320
Whereas the software that we do,

28:53.320 --> 28:55.760
whether it's like an open source server design

28:55.760 --> 29:00.120
or basically things like Memcache,

29:00.120 --> 29:03.360
like a good, it was probably our earliest project

29:04.320 --> 29:05.240
that I worked on.

29:05.240 --> 29:07.440
It was probably one of the last things that I coded

29:07.440 --> 29:09.360
and led directly for the company.

29:09.560 --> 29:14.560
But basically this caching tool for quick data retrieval,

29:17.320 --> 29:19.880
these are things that are just broadly useful

29:19.880 --> 29:21.960
across anything that you wanna build.

29:21.960 --> 29:25.360
And I think that some of the language models

29:25.360 --> 29:27.000
now have that feel,

29:27.000 --> 29:28.480
as well as some of the other things that we're building,

29:28.480 --> 29:31.280
like the translation tool that you just referenced.

29:31.280 --> 29:34.280
So text-to-speech and speech-to-text,

29:34.280 --> 29:36.320
you've expanded it from around 100 languages

29:36.320 --> 29:38.840
to more than 1100 languages.

29:38.840 --> 29:40.280
And you can identify more than,

29:40.280 --> 29:44.040
the model can identify more than 4000 spoken languages,

29:44.040 --> 29:47.520
which is 40 times more than any known previous technology.

29:47.520 --> 29:49.760
To me, that's really, really, really exciting

29:49.760 --> 29:51.720
in terms of connecting the world,

29:51.720 --> 29:54.680
breaking down barriers that language creates.

29:54.680 --> 29:55.920
Yeah, I think being able to translate

29:55.920 --> 29:59.760
between all of these different pieces in real time,

29:59.760 --> 30:04.760
this has been a kind of common sci-fi idea

30:05.720 --> 30:07.480
that we'd all have,

30:07.480 --> 30:10.600
whether it's, I don't know, an earbud or glasses

30:10.600 --> 30:13.080
or something that can help translate in real time

30:14.040 --> 30:15.160
between all these different languages.

30:15.160 --> 30:16.480
And that's one that I think technology

30:16.480 --> 30:19.800
is basically delivering now.

30:19.800 --> 30:22.920
So I think, yeah, I think that's pretty exciting.

30:22.920 --> 30:24.920
You mentioned the next version of Llama.

30:24.920 --> 30:27.880
What can you say about the next version of Llama?

30:27.880 --> 30:31.520
What can you say about what you're working on

30:31.520 --> 30:35.120
in terms of release, in terms of the vision for that?

30:35.120 --> 30:36.840
Well, a lot of what we're doing

30:36.840 --> 30:38.880
is taking the first version,

30:38.880 --> 30:42.320
which was primarily this research version,

30:42.320 --> 30:44.680
and trying to now build a version

30:44.680 --> 30:49.600
that has all of the latest state-of-the-art

30:49.600 --> 30:52.200
safety precautions built in.

30:52.200 --> 30:56.080
And we're using some more data to train it

30:56.920 --> 30:58.640
from across our services.

30:58.640 --> 31:02.560
But a lot of the work that we're doing internally

31:02.560 --> 31:04.360
is really just focused on making sure

31:04.360 --> 31:09.360
that this is as aligned and responsible as possible.

31:10.800 --> 31:13.760
And we're building a lot of our own,

31:13.760 --> 31:15.880
we're talking about kind of the open source infrastructure,

31:15.880 --> 31:20.120
but the main thing that we focus on building here,

31:20.120 --> 31:21.960
a lot of product experience is to help people connect

31:21.960 --> 31:22.840
and express themselves.

31:22.840 --> 31:26.680
So we're gonna, I've talked about a bunch of this stuff,

31:26.680 --> 31:30.160
but then you'll have an assistant

31:30.160 --> 31:32.520
that you can talk to in WhatsApp.

31:32.520 --> 31:36.320
I think in the future, every creator will have

31:36.320 --> 31:39.400
kind of an AI agent that can kind of act on their behalf

31:39.400 --> 31:41.200
that their fans can talk to.

31:41.200 --> 31:43.720
I wanna get to the point where every small business

31:43.720 --> 31:46.960
basically has an AI agent that people can talk to

31:46.960 --> 31:49.520
for, you know, to do commerce and customer support

31:49.520 --> 31:50.360
and things like that.

31:50.360 --> 31:53.240
So there are gonna be all these different things.

31:53.240 --> 31:57.560
And Lama or the language model underlying this

31:57.560 --> 32:00.440
is basically gonna be the engine that powers that.

32:00.480 --> 32:02.680
The reason to open source it is that,

32:03.800 --> 32:06.640
as we did with the first version,

32:06.640 --> 32:11.640
is that it basically it unlocks a lot of innovation

32:11.800 --> 32:15.760
in the ecosystem, will make our products better as well,

32:15.760 --> 32:17.680
and also gives us a lot of valuable feedback

32:17.680 --> 32:19.160
on security and safety,

32:19.160 --> 32:21.120
which is important for making this good.

32:21.120 --> 32:23.480
But yeah, I mean, the work that we're doing

32:23.480 --> 32:25.360
to advance the infrastructure,

32:25.360 --> 32:27.760
it's basically at this point,

32:27.760 --> 32:29.920
taking it beyond a research project

32:29.920 --> 32:32.000
into something which is ready to be

32:32.000 --> 32:35.120
kind of core infrastructure, not only for our own products,

32:35.120 --> 32:38.280
but, you know, hopefully for a lot of other things

32:38.280 --> 32:39.120
out there too.

32:39.120 --> 32:41.800
Do you think the Lama or the language model

32:41.800 --> 32:45.640
underlying that version too, will be open sourced?

32:47.960 --> 32:49.880
Do you have internal debate around that,

32:49.880 --> 32:51.520
the pros and cons and so on?

32:51.520 --> 32:53.240
This is, I mean, we were talking about the debates

32:53.240 --> 32:55.040
that we have internally, and I think,

32:56.760 --> 32:58.840
I think the question is how to do it, right?

32:58.840 --> 33:03.680
I mean, I think we did the research license for V1,

33:03.680 --> 33:07.480
and I think the big thing that we're thinking about

33:07.480 --> 33:11.160
is basically like, what's the right way?

33:11.160 --> 33:13.080
So there was a leak that happened,

33:13.080 --> 33:15.720
I don't know if you can comment on it for V1.

33:15.720 --> 33:18.080
You know, we released it as a research project

33:19.160 --> 33:21.720
for researchers to be able to use,

33:21.720 --> 33:23.200
but in doing so, we put it out there.

33:23.200 --> 33:26.120
So, you know, we were very clear

33:26.120 --> 33:28.720
that anyone who uses the code and the weights

33:29.600 --> 33:31.040
doesn't have a commercial license to put into products.

33:31.040 --> 33:33.920
And we've generally seen people respect that, right?

33:33.920 --> 33:36.480
It's like, you don't have any reputable companies

33:36.480 --> 33:37.680
that are basically trying to put this

33:37.680 --> 33:40.520
into their commercial products.

33:40.520 --> 33:42.920
But yeah, but by sharing it with, you know,

33:42.920 --> 33:45.520
so many researchers, it's, you know,

33:45.520 --> 33:46.760
it did leave the building.

33:46.760 --> 33:49.080
But what have you learned from that process

33:49.080 --> 33:52.240
that you might be able to apply to V2

33:52.240 --> 33:55.080
about how to release it safely, effectively,

33:55.920 --> 33:57.640
if you release it?

33:57.640 --> 33:59.440
Yeah, well, I mean, I think a lot of the feedback,

33:59.440 --> 34:01.440
like I said, is just around, you know,

34:01.440 --> 34:03.400
different things around, you know,

34:03.400 --> 34:04.920
how do you fine tune models

34:04.920 --> 34:07.200
to make them more aligned and safer?

34:07.200 --> 34:10.560
And you see all the different data recipes that,

34:11.480 --> 34:14.120
you know, you mentioned a lot of different projects

34:14.120 --> 34:14.960
that are based on this.

34:14.960 --> 34:16.400
I mean, there's one at Berkeley,

34:16.400 --> 34:18.400
there's, you know, there's just like all over.

34:18.400 --> 34:23.400
And people have tried a lot of different things

34:23.840 --> 34:25.360
and we've tried a bunch of stuff internally.

34:25.360 --> 34:29.160
So kind of where we're making progress here,

34:29.160 --> 34:31.560
but also we're able to learn from some of the best ideas

34:31.560 --> 34:32.400
in the community.

34:32.400 --> 34:33.840
And, you know, I think it, you know,

34:33.840 --> 34:36.360
we want to just continue pushing that forward.

34:36.360 --> 34:40.080
But I don't have any news to announce on this,

34:40.080 --> 34:41.720
if that's what you're asking.

34:41.720 --> 34:46.720
I mean, this is a thing that we're still,

34:47.120 --> 34:48.720
we're still kind of, you know,

34:48.720 --> 34:51.360
actively working through the right way

34:51.360 --> 34:52.200
to move forward here.

34:52.200 --> 34:56.240
The details of the secret sauce are still being developed.

34:56.240 --> 34:57.560
I see.

34:57.560 --> 35:00.640
Can you comment on, what do you think of the thing

35:00.640 --> 35:01.880
that worked for GPT,

35:01.880 --> 35:04.240
which is the reinforcement learning with human feedback?

35:04.240 --> 35:06.880
So doing this alignment process,

35:06.880 --> 35:08.760
do you find it interesting?

35:08.760 --> 35:10.160
And as part of that, let me ask,

35:10.160 --> 35:13.080
because I talked to Jan LeCun before talking to you today,

35:13.080 --> 35:16.560
he asked me to ask, or suggested that I ask,

35:16.560 --> 35:19.440
do you think LLM fine tuning

35:19.440 --> 35:22.240
will need to be crowdsourced Wikipedia style?

35:23.160 --> 35:24.800
So crowdsourcing.

35:24.800 --> 35:29.200
So this kind of idea of how to integrate the human

35:29.200 --> 35:32.960
in the fine tuning of these foundation models.

35:32.960 --> 35:35.520
Yeah, I think that's a really interesting idea

35:35.520 --> 35:38.280
that I've talked to Jan about a bunch.

35:40.200 --> 35:43.040
And you were talking about,

35:43.040 --> 35:46.760
how do you basically train these models to be

35:46.760 --> 35:50.160
as safe and aligned and responsible as possible?

35:50.160 --> 35:53.280
And different groups out there who are doing development

35:53.280 --> 35:57.120
test different data recipes in fine tuning.

35:57.120 --> 36:00.440
But this idea that you just mentioned is

36:01.960 --> 36:04.160
that at the end of the day,

36:04.160 --> 36:07.080
instead of having kind of one group fine tune some stuff

36:07.080 --> 36:10.680
and another group produce a different fine tuning recipe,

36:10.680 --> 36:12.760
and then us trying to figure out

36:12.760 --> 36:13.960
which one we think works best

36:13.960 --> 36:16.120
to produce the most aligned model,

36:17.760 --> 36:21.640
I do think that it would be nice

36:21.640 --> 36:23.880
if you could get to a point where you had

36:23.880 --> 36:28.880
a Wikipedia style collaborative way

36:28.880 --> 36:33.760
for a kind of a broader community to fine tune it as well.

36:33.760 --> 36:36.000
Now, there's a lot of challenges in that,

36:36.000 --> 36:41.000
both from an infrastructure and like a community management

36:41.040 --> 36:42.720
and product perspective about how you do that.

36:42.720 --> 36:45.320
So I haven't worked that out yet.

36:46.240 --> 36:48.720
But as an idea, I think it's quite compelling.

36:48.720 --> 36:50.920
And I think it goes well with the ethos

36:50.920 --> 36:52.600
of open sourcing the technologies,

36:52.600 --> 36:56.280
also finding a way to have a kind of community driven,

36:58.000 --> 36:59.880
a community driven training of it.

37:01.600 --> 37:03.320
But I think that there are a lot of questions on this.

37:03.320 --> 37:05.360
In general, these questions around

37:05.360 --> 37:09.640
what's the best way to produce aligned AI models,

37:09.640 --> 37:12.000
it's very much a research area.

37:12.000 --> 37:14.320
And it's one that I think we will need to make

37:14.360 --> 37:16.360
as much progress on as the kind of

37:16.360 --> 37:21.120
core intelligence capability of the models themselves.

37:21.120 --> 37:23.440
Well, I just did a conversation with Jimmy Wales,

37:23.440 --> 37:24.600
the founder of Wikipedia.

37:24.600 --> 37:28.320
And to me, Wikipedia is one of the greatest websites

37:28.320 --> 37:29.880
ever created.

37:29.880 --> 37:31.920
And it's kind of a miracle that it works.

37:31.920 --> 37:33.960
And I think it has to do with something that you mentioned,

37:33.960 --> 37:35.360
which is community.

37:35.360 --> 37:38.200
You have a small community of editors

37:38.200 --> 37:40.520
that somehow work together well.

37:40.520 --> 37:45.520
And they handle very controversial topics

37:45.720 --> 37:48.840
and they handle it with balance and with grace,

37:48.840 --> 37:51.840
despite sort of the attacks that will often happen.

37:51.840 --> 37:52.680
A lot of the time.

37:52.680 --> 37:54.760
I mean, it's not, it has issues

37:54.760 --> 37:56.320
just like any other human system.

37:56.320 --> 37:58.680
But yes, I mean, the balance is,

37:58.680 --> 38:01.480
I mean, it's amazing what they've been able to achieve,

38:01.480 --> 38:03.000
but it's also not perfect.

38:03.000 --> 38:06.880
And I think that there's still a lot of challenges.

38:07.760 --> 38:09.880
Right, the more controversial the topic,

38:10.440 --> 38:15.440
the more difficult the journey towards

38:15.480 --> 38:18.040
quote unquote truth or knowledge or wisdom

38:18.040 --> 38:20.000
that Wikipedia tries to capture.

38:20.000 --> 38:24.160
In the same way AI models will need to be able to generate

38:24.160 --> 38:26.800
those same things, truth, knowledge and wisdom.

38:26.800 --> 38:31.160
And how do you align those models that they generate

38:32.360 --> 38:35.880
something that is closest to truth?

38:35.880 --> 38:38.040
There's these concerns about misinformation,

38:38.040 --> 38:41.440
all this kind of stuff that nobody can define.

38:42.360 --> 38:44.840
And this is something that we together

38:44.840 --> 38:46.800
as a human species have to define.

38:46.800 --> 38:50.600
Like what is truth and how to help AI systems generate that.

38:50.600 --> 38:53.120
One of the things that language models do really well

38:53.120 --> 38:55.880
is generate convincing sounding things

38:55.880 --> 38:57.560
that can be completely wrong.

38:58.640 --> 39:03.640
And so how do you align it to be less wrong?

39:05.520 --> 39:06.720
And part of that is the training

39:06.720 --> 39:08.680
and part of that is the alignment.

39:08.680 --> 39:10.600
And however you do the alignment stage.

39:10.600 --> 39:13.960
And just like you said, it's a very new

39:13.960 --> 39:16.600
and a very open research problem.

39:16.600 --> 39:20.160
Yeah, and I think that there's also a lot of questions

39:20.160 --> 39:23.800
about whether the current architecture for LLMs

39:25.280 --> 39:28.120
as you continue scaling it, what happens.

39:29.560 --> 39:33.600
I mean, a lot of what's been exciting in the last year

39:33.600 --> 39:35.960
is that there's clearly a qualitative breakthrough

39:36.000 --> 39:40.560
where with some of the GPT models that I put out

39:40.560 --> 39:43.560
and that others have been able to do as well.

39:43.560 --> 39:45.840
I think it reached a kind of level of quality

39:45.840 --> 39:48.920
where people are like, wow, this feels different

39:48.920 --> 39:52.840
and like it's gonna be able to be the foundation

39:52.840 --> 39:55.000
for building a lot of awesome products

39:55.000 --> 39:57.000
and experiences and value.

39:57.000 --> 39:58.360
But I think that the other realization

39:58.360 --> 40:00.920
that people have is, wow, we just made a breakthrough.

40:03.560 --> 40:05.880
If there are other breakthroughs quickly,

40:06.720 --> 40:07.560
then I think that there's the sense

40:07.560 --> 40:11.120
that maybe we're closer to general intelligence.

40:11.120 --> 40:13.320
But I think that that idea is predicated on the idea

40:13.320 --> 40:16.440
that I think people believe that there's still generally

40:16.440 --> 40:18.280
a bunch of additional breakthroughs to make

40:18.280 --> 40:21.440
and that we just don't know

40:21.440 --> 40:23.120
how long it's gonna take to get there.

40:23.120 --> 40:25.040
And one view that some people have,

40:26.080 --> 40:27.920
this doesn't tend to be my view as much

40:27.920 --> 40:32.160
is that simply scaling the current LLMs

40:32.160 --> 40:35.720
and getting to higher parameter count models by itself

40:36.560 --> 40:37.680
will get to something that is closer

40:37.680 --> 40:40.560
to general intelligence.

40:40.560 --> 40:43.440
But I don't know, I tend to think

40:43.440 --> 40:48.440
that there's probably more fundamental steps

40:48.440 --> 40:50.360
that need to be taken along the way there.

40:50.360 --> 40:55.360
But still the LEAPs taken with this extra alignment step

40:55.640 --> 40:58.960
is quite incredible, quite surprising to a lot of folks.

40:58.960 --> 41:03.040
And on top of that, when you start to have hundreds

41:03.040 --> 41:05.480
of millions of people potentially using a product

41:06.240 --> 41:07.200
that integrates that,

41:07.200 --> 41:10.800
you can start to see civilization transforming effects

41:10.800 --> 41:15.040
before you achieve super, quote unquote, super intelligence.

41:15.040 --> 41:18.040
It could be super transformative

41:18.040 --> 41:20.200
without being a super intelligence.

41:20.200 --> 41:22.480
Oh yeah, I mean, I think that there are gonna be

41:22.480 --> 41:26.040
a lot of amazing products and value that can be created

41:26.040 --> 41:27.880
with the current level of technology.

41:29.760 --> 41:33.960
To some degree, I'm excited to work on a lot

41:33.960 --> 41:35.720
of those products over the next few years.

41:35.720 --> 41:37.760
And I think it would just create

41:37.760 --> 41:39.440
a tremendous amount of whiplash

41:39.440 --> 41:41.800
if the number of breakthroughs keeps,

41:41.800 --> 41:44.000
like if they're keep on being stacked breakthroughs,

41:44.000 --> 41:45.320
because I think to some degree,

41:45.320 --> 41:47.040
industry in the world needs some time

41:47.040 --> 41:50.640
to kind of build these breakthroughs into the products

41:50.640 --> 41:51.920
and experiences that we all use

41:51.920 --> 41:53.920
so that we can actually benefit from them.

41:55.320 --> 41:59.160
But I don't know, I think that there's just a,

42:01.000 --> 42:02.480
like an awesome amount of stuff to do.

42:02.480 --> 42:05.200
I mean, I think about like all of the,

42:05.200 --> 42:06.480
I don't know, small businesses

42:06.480 --> 42:08.200
or individual entrepreneurs out there

42:08.200 --> 42:13.200
who now we're gonna be able to get help coding

42:13.480 --> 42:15.240
the things that they need to go build things

42:15.240 --> 42:16.840
or designing the things that they need,

42:16.840 --> 42:20.040
or we'll be able to use these models

42:20.040 --> 42:21.440
to be able to do customer support

42:21.440 --> 42:24.840
for the people that they're serving over WhatsApp

42:24.840 --> 42:28.480
without having to, I think that that's just gonna be,

42:28.480 --> 42:31.960
I just think that this is all gonna be super exciting.

42:31.960 --> 42:34.800
It's gonna create better experiences for people

42:34.800 --> 42:37.720
and just unlock a ton of innovation and value.

42:37.720 --> 42:41.280
So I don't know if you know, but what is it?

42:41.280 --> 42:43.920
Over three billion people use WhatsApp,

42:43.920 --> 42:46.200
Facebook and Instagram.

42:47.880 --> 42:51.800
So any kind of AI fueled products that go into that,

42:51.800 --> 42:54.720
like we're talking about anything with LLMs,

42:54.720 --> 42:56.960
will have a tremendous amount of impact.

42:56.960 --> 43:01.800
Do you have ideas and thoughts about possible products

43:01.800 --> 43:04.920
that might start being integrated

43:04.920 --> 43:08.920
into these platforms used by so many people?

43:08.920 --> 43:12.160
Yeah, I think there's three main categories of things

43:12.160 --> 43:13.320
that we're working on.

43:17.280 --> 43:22.280
The first that I think is probably the most interesting

43:23.200 --> 43:27.680
is there's this notion of like,

43:27.680 --> 43:30.520
you're gonna have an assistant or an agent

43:30.520 --> 43:31.360
who you can talk to.

43:31.360 --> 43:34.240
And I think probably the biggest thing that's different

43:34.240 --> 43:37.520
about my view of how this plays out from what I see

43:37.520 --> 43:41.160
with OpenAI and Google and others is,

43:41.160 --> 43:45.320
everyone else is building like the one singular AI, right?

43:45.320 --> 43:47.360
It's like, okay, you talk to chat GPT

43:47.360 --> 43:50.400
or you talk to Bard or you talk to Bing.

43:50.400 --> 43:55.400
And my view is that there are going to be

43:56.680 --> 43:59.520
a lot of different AIs that people are gonna wanna engage

43:59.520 --> 44:03.040
with just like you wanna use a number of different apps

44:03.040 --> 44:03.880
for different things.

44:03.880 --> 44:06.320
And you have relationships with different people

44:06.320 --> 44:10.040
in your life who fill different emotional roles for you.

44:14.600 --> 44:17.360
So I think that they're gonna be, people have a reason

44:17.360 --> 44:20.240
that I think you don't just want like a singular AI.

44:20.240 --> 44:23.360
And that I think is probably the biggest distinction

44:23.360 --> 44:25.400
in terms of how I think about this.

44:25.400 --> 44:26.520
And a bunch of these things,

44:26.520 --> 44:29.480
I think you'll want an assistant.

44:29.480 --> 44:30.920
I mean, I mentioned a couple of these before.

44:30.920 --> 44:33.360
I think like every creator who you interact with

44:33.360 --> 44:37.880
will ultimately want some kind of AI that can proxy them

44:37.880 --> 44:40.320
and be something that their fans can interact with

44:40.320 --> 44:43.480
or that allows them to interact with their fans.

44:44.840 --> 44:46.360
This is like the common creator promise.

44:46.360 --> 44:48.560
Everyone's trying to build a community

44:48.560 --> 44:50.240
and engage with people and they want tools

44:50.240 --> 44:51.760
to be able to amplify themselves more

44:51.760 --> 44:52.920
and be able to do that.

44:53.760 --> 44:57.680
But you only have 24 hours in a day.

44:57.680 --> 45:01.960
So I think having the ability to basically

45:01.960 --> 45:04.200
like bottle up your personality

45:04.200 --> 45:08.400
and or like give your fans information

45:08.400 --> 45:09.760
about when you're performing a concert

45:09.760 --> 45:10.880
or something like that.

45:10.880 --> 45:12.520
I mean, that I think is gonna be something

45:12.520 --> 45:15.080
that's super valuable, but it's not just that,

45:15.080 --> 45:16.040
again, it's not this idea

45:16.040 --> 45:18.640
that I think people are gonna want just one singular AI.

45:18.640 --> 45:20.880
I think you're gonna wanna interact

45:20.880 --> 45:22.400
with a lot of different entities.

45:22.440 --> 45:24.320
And then I think there's the business version of this too,

45:24.320 --> 45:25.680
which we've touched on a couple of times,

45:25.680 --> 45:30.160
which is I think every business in the world

45:30.160 --> 45:34.960
is gonna want basically an AI that it's like,

45:34.960 --> 45:37.640
you have your page on Instagram or Facebook

45:37.640 --> 45:40.280
or WhatsApp or whatever, and you wanna point people

45:40.280 --> 45:43.600
to an AI that people can interact with,

45:43.600 --> 45:44.960
but you wanna know that that AI

45:44.960 --> 45:46.240
is only gonna sell your products.

45:46.240 --> 45:49.240
You don't want it recommending your competitors stuff.

45:49.240 --> 45:53.360
So it's not like there can be like just one singular AI

45:53.360 --> 45:56.000
that can answer all the questions for a person

45:56.000 --> 45:59.720
because that AI might not actually be aligned with you

45:59.720 --> 46:03.200
as a business to really just do the best job

46:03.200 --> 46:05.520
providing support for your product.

46:05.520 --> 46:08.040
So I think that there's gonna be a clear need

46:09.400 --> 46:11.840
in the market and in people's lives

46:11.840 --> 46:14.160
for there to be a bunch of these.

46:14.160 --> 46:17.280
Part of that is figuring out the research,

46:17.280 --> 46:20.240
the technology that enables the personalization

46:20.240 --> 46:21.480
that you're talking about.

46:21.480 --> 46:24.840
So not one centralized God-like LLM,

46:24.840 --> 46:28.320
but one just a huge diversity of them

46:28.320 --> 46:31.320
that's fine-tuned to particular needs, particular styles,

46:31.320 --> 46:34.400
particular businesses, particular brands,

46:34.400 --> 46:35.520
all that kind of stuff.

46:35.520 --> 46:37.480
And also enabling, just enabling people

46:37.480 --> 46:41.720
to create them really easily for your own business

46:41.720 --> 46:43.480
or if you're a creator to be able

46:43.480 --> 46:45.240
to help you engage with your fans.

46:45.240 --> 46:46.800
And I think that's...

46:48.040 --> 46:50.600
So yeah, I think that there's a clear

46:50.600 --> 46:53.000
kind of interesting product direction here

46:53.000 --> 46:55.080
that I think is fairly unique

46:55.080 --> 46:59.520
from what any of the other big companies are taking.

46:59.520 --> 47:02.080
It also aligns well with this sort of open source approach

47:02.080 --> 47:04.040
because again, we sort of believe

47:04.040 --> 47:08.240
in this more community-oriented, more democratic approach

47:08.240 --> 47:11.120
to building out the products and technology around this.

47:11.120 --> 47:12.880
We don't think that there's gonna be the one true thing.

47:12.920 --> 47:16.040
We think that there should be kind of a lot of development.

47:16.040 --> 47:18.280
So that part of things, I think,

47:18.280 --> 47:19.200
is gonna be really interesting.

47:19.200 --> 47:21.800
And we could go, Price spent a lot of time talking

47:21.800 --> 47:23.920
about that and the kind of implications

47:23.920 --> 47:27.000
of that approach being different

47:27.000 --> 47:28.400
from what others are taking.

47:29.560 --> 47:31.160
But there's a bunch of other simpler things

47:31.160 --> 47:32.120
that I think we're also gonna do.

47:32.120 --> 47:33.360
Just going back to your question

47:33.360 --> 47:36.920
around how this finds its way into, like, what do we build?

47:38.280 --> 47:39.800
There are gonna be a lot of simpler things

47:39.800 --> 47:44.800
around, okay, you post photos on Instagram

47:46.080 --> 47:49.560
and Facebook and WhatsApp and Messenger

47:49.560 --> 47:51.800
and you want the photos to look as good as possible.

47:51.800 --> 47:54.560
So having an AI that you can just take a photo

47:54.560 --> 47:57.400
and then just tell it, like, okay, I wanna edit this thing

47:57.400 --> 47:58.240
or describe this.

47:58.240 --> 47:59.680
It's like, I think we're gonna have tools

47:59.680 --> 48:01.320
that are just way better

48:01.320 --> 48:03.880
than what we've historically had on this.

48:05.000 --> 48:07.160
And that's more in the image and media generation side

48:07.160 --> 48:08.680
than the large language model side,

48:08.680 --> 48:13.520
but it all kind of plays off of advances in the same space.

48:14.600 --> 48:15.440
So there are a lot of tools

48:15.440 --> 48:16.400
that I think are just gonna get built

48:16.400 --> 48:17.600
into every one of our products.

48:17.600 --> 48:19.560
I think every single thing that we do

48:19.560 --> 48:22.480
is gonna basically get evolved in this direction, right?

48:22.480 --> 48:23.880
It's like in the future,

48:23.880 --> 48:25.480
if you're advertising on our services,

48:25.480 --> 48:29.800
like, do you need to make your own kind of ad creative?

48:29.800 --> 48:34.800
It's no, you'll just tell us, okay, I'm a dog walker

48:35.600 --> 48:39.040
and I'm willing to walk people's dogs

48:39.040 --> 48:41.920
and help me find the right people

48:41.920 --> 48:45.920
and create the ad unit that will perform the best

48:45.920 --> 48:48.720
and give an objective to the system

48:48.720 --> 48:52.360
and it just kind of connects you with the right people.

48:52.360 --> 48:54.800
Well, that's a super powerful idea

48:54.800 --> 48:57.080
of generating the language,

48:57.080 --> 49:02.080
almost like rigorous A-B testing for you

49:03.080 --> 49:07.440
that works to find the best customer for your thing.

49:07.440 --> 49:10.840
I mean, to me, advertisement, when done well,

49:10.840 --> 49:14.800
just finds a good match between a human being

49:14.800 --> 49:18.040
and a thing that will make that human being happy.

49:18.040 --> 49:19.000
Yeah, totally.

49:19.000 --> 49:21.160
And do that as efficiently as possible.

49:21.160 --> 49:23.640
When it's done well, people actually like it.

49:23.640 --> 49:25.680
You know, I think that there's a lot of examples

49:25.680 --> 49:27.440
where it's not done well and it's annoying

49:27.440 --> 49:29.920
and I think that that's what kind of gives it a bad rap,

49:29.920 --> 49:33.360
but yeah, and a lot of the stuff is possible today.

49:33.360 --> 49:34.880
I mean, obviously, A-B testing stuff

49:34.880 --> 49:36.840
is built into a lot of these frameworks.

49:36.840 --> 49:39.360
The thing that's new is having technology

49:39.360 --> 49:42.320
that can generate the ideas about what to A-B test.

49:42.320 --> 49:43.240
So I think that that's exciting.

49:43.240 --> 49:45.960
So this will just be across like everything

49:45.960 --> 49:47.520
that we're doing with all the metaverse stuff

49:47.520 --> 49:48.360
that we're doing, right?

49:48.360 --> 49:50.320
It's like you wanna create worlds in the future,

49:50.320 --> 49:51.520
you'll just describe them

49:51.520 --> 49:53.520
and then it'll create the code for you.

49:53.520 --> 49:57.440
So natural language becomes the interface we use

49:57.440 --> 50:01.560
for all the ways we interact with the computer,

50:01.560 --> 50:03.800
with the digital.

50:03.800 --> 50:05.800
More of them, yeah, yeah, totally.

50:05.800 --> 50:08.840
Yeah, which is what everyone can do using natural language

50:08.840 --> 50:11.840
and with translation, you can do it in any kind of language.

50:13.080 --> 50:14.840
I mean, for the personalization,

50:14.840 --> 50:17.600
it's really, really, really interesting.

50:17.600 --> 50:18.440
Yeah.

50:18.440 --> 50:20.640
It unlocks so many possible things.

50:20.640 --> 50:22.120
I mean, I, for one, look forward

50:22.120 --> 50:24.520
to creating a copy of myself.

50:24.520 --> 50:26.120
I know we talked about this last time.

50:26.200 --> 50:29.040
But this has, since the last time,

50:29.040 --> 50:29.880
this becomes-

50:29.880 --> 50:31.600
Now we're closer.

50:31.600 --> 50:32.760
Much closer.

50:32.760 --> 50:34.600
Like I could literally just having interacted

50:34.600 --> 50:36.120
with some of these language models,

50:36.120 --> 50:37.800
I could see the absurd situation

50:37.800 --> 50:42.800
where I'll have a large or a lex language model

50:44.840 --> 50:48.000
and I'll have to have a conversation with him

50:48.000 --> 50:50.080
about like, hey, listen,

50:50.080 --> 50:51.600
like you're just getting out of line

50:51.600 --> 50:53.640
and having a conversation where you fine tune that thing

50:53.640 --> 50:56.040
to be a little bit more respectful or something like this.

50:57.040 --> 51:00.000
That's going to be the,

51:00.000 --> 51:03.240
that seems like an amazing product.

51:04.240 --> 51:06.520
For businesses, for humans,

51:06.520 --> 51:11.360
just not just the assistant that's facing the individual,

51:11.360 --> 51:14.320
but the assistant that represents the individual

51:14.320 --> 51:18.080
to the public, both directions.

51:18.080 --> 51:22.200
There's basically a layer that is the AI system

51:22.200 --> 51:25.800
through which you interact with the outside world,

51:26.760 --> 51:28.480
that has humans in it.

51:28.480 --> 51:29.960
That's really interesting.

51:29.960 --> 51:33.400
And you that have social networks

51:33.400 --> 51:35.640
that connect billions of people,

51:35.640 --> 51:39.760
it seems like a heck of a large scale place

51:39.760 --> 51:42.160
to test some of this stuff out.

51:42.160 --> 51:43.480
Yeah, I mean, I think part of the reason

51:43.480 --> 51:45.640
why creators will want to do this

51:45.640 --> 51:47.320
is because they already have the communities

51:47.320 --> 51:49.160
on our services.

51:49.160 --> 51:50.000
Yeah.

51:50.000 --> 51:53.560
And a lot of the interface for this stuff today

51:53.560 --> 51:55.280
are chat type interfaces.

51:56.240 --> 51:58.600
And between WhatsApp and Messenger,

51:58.600 --> 52:01.320
I think that those are just great ways

52:01.320 --> 52:03.600
to interact with people.

52:03.600 --> 52:04.920
So some of this is philosophy,

52:04.920 --> 52:07.760
but do you see a near term future

52:07.760 --> 52:11.400
where you have some of the people you're friends with,

52:11.400 --> 52:15.720
our AI systems on these social networks,

52:15.720 --> 52:19.440
on Facebook, on Instagram, even on WhatsApp,

52:19.440 --> 52:23.440
having conversations where some heterogeneous,

52:23.480 --> 52:25.320
some is human, some is AI?

52:25.320 --> 52:26.920
I think we'll get to that.

52:28.120 --> 52:31.200
And if only just empirically looking at,

52:33.040 --> 52:34.600
then Microsoft released this thing

52:34.600 --> 52:38.000
called Chowice several years ago in China.

52:38.000 --> 52:41.880
It was a pre-LLM chat bot technology

52:41.880 --> 52:46.160
that was a lot simpler than what's possible today.

52:46.160 --> 52:48.280
And I think it was like tens of millions of people

52:48.280 --> 52:52.640
were using this and just really it became quite attached

52:52.640 --> 52:54.560
and built relationships with it.

52:54.560 --> 52:58.800
And I think that there's services today like Replica

52:58.800 --> 53:01.240
where people are doing things like that.

53:01.240 --> 53:06.240
And so I think that there's certainly needs

53:07.000 --> 53:11.000
for companionship that people have, older people.

53:13.760 --> 53:16.720
I think most people probably don't have as many friends

53:16.720 --> 53:17.960
as they would like to have, right?

53:17.960 --> 53:21.480
If you look at, there's some interesting demographic studies

53:21.480 --> 53:26.480
around the average person has the number of close friends

53:27.640 --> 53:32.000
that they have is fewer today than it was 15 years ago.

53:32.000 --> 53:34.560
And I mean, that gets to like,

53:34.560 --> 53:38.160
this is like the core thing that I think about

53:38.160 --> 53:40.720
in terms of building services that help connect people.

53:40.720 --> 53:43.880
So I think you'll get tools that help people connect

53:43.880 --> 53:47.240
with each other are gonna be the primary thing

53:47.240 --> 53:48.720
that we wanna do.

53:48.720 --> 53:53.520
So you can imagine AI assistants that just do a better job

53:53.520 --> 53:55.160
of reminding you when it's your friend's birthday

53:55.160 --> 53:56.840
and how you could celebrate them, right?

53:56.840 --> 53:58.720
It's like right now we have like the little box

53:58.720 --> 54:00.760
in the corner of the website that tells you

54:00.760 --> 54:02.320
whose birthday it is and stuff like that.

54:02.320 --> 54:06.640
But at some level, you don't just wanna like

54:06.640 --> 54:08.640
send everyone a note that says the same note

54:08.640 --> 54:10.840
saying happy birthday with an emoji, right?

54:10.840 --> 54:15.440
So having something that's more of a social assistant

54:15.440 --> 54:18.120
in that sense and like that can update you

54:18.120 --> 54:19.760
on what's going on in their life

54:19.760 --> 54:23.720
and like how you can reach out to them effectively,

54:23.720 --> 54:24.840
help you be a better friend.

54:24.840 --> 54:28.080
I think that that's something that's super powerful too.

54:28.080 --> 54:30.400
But yeah, beyond that,

54:31.360 --> 54:33.840
and there are all these different flavors

54:33.840 --> 54:38.120
of kind of personal AIs that I think could exist.

54:38.120 --> 54:41.360
So I think an assistant is sort of the kind of simplest one

54:41.360 --> 54:42.560
to wrap your head around,

54:42.560 --> 54:47.560
but I think a mentor or a life coach

54:48.080 --> 54:50.280
is someone who can give you advice,

54:50.280 --> 54:51.800
who's maybe like a bit of a cheerleader

54:51.800 --> 54:53.720
who can help pick you up through all the challenges

54:53.720 --> 54:58.120
that inevitably we all go through on a daily basis

54:58.120 --> 55:01.520
and that there's probably some role for something like that.

55:01.520 --> 55:04.280
And then all the way, you can probably just go through

55:04.280 --> 55:07.600
a lot of the different type of kind of functional

55:07.600 --> 55:09.960
relationships that people have in their life.

55:09.960 --> 55:12.680
And I would bet that there will be companies out there

55:12.680 --> 55:15.600
that take a crack at a lot of these things.

55:15.600 --> 55:17.120
So I don't know,

55:17.120 --> 55:18.800
I think it's part of the interesting innovation

55:18.800 --> 55:21.960
that's gonna exist is that there are certainly a lot

55:23.200 --> 55:24.960
like education tutors, right?

55:24.960 --> 55:28.800
It's like, I mean, I just look at my kids learning to code

55:28.800 --> 55:33.240
and they love it, but it's like they get stuck on a question

55:33.240 --> 55:36.400
and they have to wait till like I can help answer it, right?

55:36.400 --> 55:39.160
Or someone else who they know can help answer the question

55:39.160 --> 55:40.200
in the future, they'll just,

55:40.200 --> 55:42.520
there will be like a coding assistant that they have

55:42.520 --> 55:45.720
that is like designed to be perfect

55:45.720 --> 55:47.920
for teaching a five and a seven year old how to code.

55:47.920 --> 55:50.840
And they'll just be able to ask questions all the time

55:50.840 --> 55:53.280
and it will be extremely patient.

55:53.280 --> 55:55.440
It's never gonna get annoyed at them, right?

55:56.520 --> 55:58.720
I think that like there are all these different

55:58.720 --> 56:00.800
kind of relationships or functional relationships

56:00.800 --> 56:05.600
that we have in our lives that are really interesting.

56:05.600 --> 56:07.440
And I think one of the big questions is like,

56:07.440 --> 56:09.760
okay, is this all gonna just get bucketed into

56:10.760 --> 56:12.320
you know, one singular AI?

56:12.320 --> 56:14.600
I just don't think so.

56:14.600 --> 56:18.080
Do you think about, this is actually a question from Reddit,

56:18.080 --> 56:21.280
what the long-term effects of human communication

56:21.280 --> 56:24.320
when people can talk with in quotes,

56:24.320 --> 56:26.160
talk with others through a chat bot

56:26.160 --> 56:28.600
that augments their language automatically

56:28.600 --> 56:30.360
rather than developing social skills

56:30.360 --> 56:32.040
by making mistakes and learning,

56:33.480 --> 56:37.280
will people just communicate by grunts in a generation?

56:37.280 --> 56:40.480
Do you think about long-term effects at scale

56:40.480 --> 56:43.040
the integration of AI in our social interaction?

56:44.040 --> 56:46.640
Yeah, I mean, I think it's mostly good.

56:46.640 --> 56:50.320
I mean, that question was sort of framed in a negative way.

56:50.320 --> 56:53.480
But I mean, we were talking before about language models

56:53.480 --> 56:54.920
helping you communicate with,

56:54.920 --> 56:56.880
it was like language translation helping you communicate

56:56.880 --> 56:58.600
with people who don't speak your language.

56:58.600 --> 57:00.880
I mean, at some level,

57:00.880 --> 57:03.240
what all this social technology is doing

57:03.240 --> 57:08.240
is helping people express themselves better

57:09.760 --> 57:13.240
to people in situations where they would otherwise

57:13.240 --> 57:14.160
have a hard time doing that.

57:14.160 --> 57:15.160
So part of it might be okay

57:15.160 --> 57:17.320
because you speak a language that I don't know.

57:17.320 --> 57:19.400
That's a pretty basic one that, you know,

57:19.400 --> 57:21.640
I don't think people are gonna look at that and say,

57:21.640 --> 57:24.280
it's sad that do we have the capacity to do that

57:24.280 --> 57:26.200
because I should have just learned your language, right?

57:26.200 --> 57:28.000
I mean, that's pretty high bar.

57:28.000 --> 57:31.160
But overall I'd say,

57:33.440 --> 57:35.760
there are all these impediments

57:35.760 --> 57:38.360
and language is an imperfect way

57:38.360 --> 57:41.560
for people to express thoughts and ideas.

57:41.560 --> 57:43.440
It's, you know, one of the best that we have.

57:43.440 --> 57:46.520
We have that, we have art, we have code.

57:46.520 --> 57:48.960
But language is also a mapping of the way you think,

57:48.960 --> 57:51.360
the way you see the world, who you are.

57:51.360 --> 57:52.920
And one of the applications,

57:52.920 --> 57:54.760
I've recently talked to a person

57:54.760 --> 57:58.000
who's actually a Jijitsu instructor.

57:58.960 --> 58:03.080
He said that when he emails parents

58:03.080 --> 58:05.880
about their son and daughter,

58:07.240 --> 58:10.480
that they can improve their discipline in class and so on.

58:10.480 --> 58:13.080
He often finds that he comes off a bit

58:13.080 --> 58:15.040
of more of an asshole than he would like.

58:15.040 --> 58:18.840
So he uses GPT to translate his original email

58:18.840 --> 58:22.920
into a nicer email, a more polite one.

58:22.920 --> 58:23.760
We hear this all the time.

58:23.760 --> 58:25.920
A lot of creators on our services tell us

58:25.920 --> 58:28.240
that one of the most stressful things

58:29.560 --> 58:32.280
is basically negotiating deals with brands and stuff,

58:32.280 --> 58:33.320
like the business side of it.

58:33.320 --> 58:35.840
Because they're like, I mean, they do their thing, right?

58:35.840 --> 58:38.840
And the creators, they're excellent at what they do

58:38.840 --> 58:40.200
and they just wanna connect with their community.

58:40.200 --> 58:42.040
But then they get really stressed.

58:42.040 --> 58:43.760
They go into their DMs

58:43.760 --> 58:46.680
and they see some brand wants to do something with them

58:46.680 --> 58:49.440
and they don't quite know how to negotiate

58:49.440 --> 58:51.440
or how to push back respectfully.

58:51.440 --> 58:53.640
And so I think building a tool

58:53.640 --> 58:56.080
that can actually allow them to do that well

58:56.080 --> 58:58.800
is the one simple thing that I think

58:58.800 --> 59:00.040
is just like an interesting thing

59:00.040 --> 59:01.760
that we've heard from a bunch of people

59:01.760 --> 59:03.360
that they'd be interested in.

59:03.360 --> 59:06.200
But I'm going back to the broader idea.

59:09.280 --> 59:10.120
I don't know.

59:10.120 --> 59:14.720
I mean, I just, Priscilla and I just had our third daughter.

59:14.720 --> 59:15.560
A couple months ago, thank you.

59:15.560 --> 59:16.400
Congratulations, by the way.

59:16.400 --> 59:17.520
Thanks.

59:17.520 --> 59:20.200
And it's like one of the saddest things in the world

59:20.200 --> 59:22.200
is like seeing your baby cry, right?

59:22.200 --> 59:25.160
But like, it's like, why is that, right?

59:25.160 --> 59:28.080
It's like, well, cause babies don't generally

59:28.080 --> 59:30.040
have much capacity to tell you

59:30.040 --> 59:33.000
what they care about otherwise, right?

59:33.000 --> 59:35.320
It's not actually just babies, right?

59:35.320 --> 59:38.280
It's, my five-year-old daughter cries too

59:38.280 --> 59:41.200
because she sometimes has a hard time expressing

59:41.200 --> 59:44.200
what matters to her.

59:44.200 --> 59:46.040
And then I was thinking about that and I was like,

59:46.040 --> 59:48.360
well, actually a lot of adults get very frustrated too

59:48.360 --> 59:51.240
because they have a hard time expressing things

59:51.240 --> 59:54.480
in a way that going back to some of the early themes

59:54.480 --> 59:59.000
that maybe is something that was a mistake

59:59.000 --> 01:00:00.920
or maybe they have pride or something like all these things

01:00:00.920 --> 01:00:01.760
get in the way.

01:00:01.760 --> 01:00:02.720
So I don't know.

01:00:02.720 --> 01:00:05.040
I think that all of these different technologies

01:00:05.040 --> 01:00:08.840
that can help us navigate the social complexity

01:00:08.840 --> 01:00:10.960
and actually be able to better express

01:00:10.960 --> 01:00:13.000
what we're feeling and thinking,

01:00:13.000 --> 01:00:14.960
I think that's generally all good.

01:00:14.960 --> 01:00:18.240
And there are all these concerns like, okay,

01:00:18.240 --> 01:00:19.880
are people gonna have worse memories

01:00:19.880 --> 01:00:21.800
because you have Google to look things up?

01:00:21.800 --> 01:00:24.720
And I think in general, a generation later,

01:00:24.720 --> 01:00:26.480
you don't look back and lament that.

01:00:26.480 --> 01:00:29.440
I think it's just like, wow, we have so much more capacity

01:00:29.440 --> 01:00:30.800
to do so much more now.

01:00:30.800 --> 01:00:32.920
And I think that that'll be the case here too.

01:00:32.920 --> 01:00:35.120
You can allocate those cognitive capabilities

01:00:35.120 --> 01:00:38.880
to like deeper, more nuanced thought.

01:00:38.880 --> 01:00:39.720
Yeah.

01:00:41.000 --> 01:00:42.760
But it's change.

01:00:42.760 --> 01:00:46.240
So with just like with Google search,

01:00:47.080 --> 01:00:51.880
the additional language models, large language models,

01:00:51.880 --> 01:00:54.800
you basically don't have to remember nearly as much.

01:00:55.800 --> 01:00:57.920
Just like with Stack Overflow for programming,

01:00:57.920 --> 01:00:59.560
now that these language models

01:00:59.560 --> 01:01:01.280
can generate code right there.

01:01:01.280 --> 01:01:04.360
I mean, I find that I write like maybe 80%,

01:01:04.360 --> 01:01:08.880
90% of the code I write is non-generated first

01:01:08.880 --> 01:01:10.400
and then edited.

01:01:10.400 --> 01:01:11.720
I mean, so you don't have to remember

01:01:11.720 --> 01:01:13.680
how to write specifics of different functions.

01:01:13.680 --> 01:01:14.960
Oh, that's great.

01:01:14.960 --> 01:01:19.600
And it's also, it's not just the specific coding.

01:01:19.600 --> 01:01:23.360
I mean, in the context of a large company like this,

01:01:23.360 --> 01:01:26.120
I think before an engineer can sit down to code,

01:01:26.120 --> 01:01:29.560
they first need to figure out all of the libraries

01:01:29.560 --> 01:01:32.840
and dependencies that tens of thousands of people

01:01:32.840 --> 01:01:34.800
have written before them.

01:01:34.800 --> 01:01:39.480
And one of the things that I'm excited about

01:01:39.480 --> 01:01:42.480
that we're working on is it's not just tools

01:01:42.480 --> 01:01:43.720
that help engineers code,

01:01:43.720 --> 01:01:45.200
it's tools that can help summarize

01:01:45.200 --> 01:01:46.240
the whole knowledge base

01:01:46.240 --> 01:01:48.480
and help people be able to navigate

01:01:48.480 --> 01:01:49.760
all the internal information.

01:01:49.760 --> 01:01:53.400
I mean, I think that that's in the experiments

01:01:53.400 --> 01:01:54.240
that I've done with this stuff.

01:01:54.240 --> 01:01:56.680
I mean, that's on the public stuff,

01:01:56.680 --> 01:02:01.680
you just ask one of these models to build you a script

01:02:02.600 --> 01:02:05.200
that does anything and it basically already understands

01:02:05.200 --> 01:02:07.120
what the best libraries are to do that thing

01:02:07.120 --> 01:02:08.840
and pulls them in automatically.

01:02:08.840 --> 01:02:09.800
I think that's super powerful.

01:02:09.800 --> 01:02:12.960
That was always the most annoying part of coding

01:02:12.960 --> 01:02:14.640
was that you had to spend all this time

01:02:14.640 --> 01:02:16.400
actually figuring out what the resources were

01:02:16.400 --> 01:02:17.440
that you were supposed to import

01:02:17.440 --> 01:02:19.440
before you could actually start building the thing.

01:02:19.440 --> 01:02:23.400
Yeah, I mean, there's, of course, the flip side of that,

01:02:23.400 --> 01:02:24.640
I think for the most part is positive,

01:02:24.640 --> 01:02:29.240
but the flip side is if you outsource that thinking

01:02:29.240 --> 01:02:34.240
to an AI model, you might miss nuanced mistakes and bugs.

01:02:36.720 --> 01:02:40.000
You lose the skill to find those bugs

01:02:40.040 --> 01:02:41.560
and those bugs might be,

01:02:42.440 --> 01:02:45.400
the code looks very convincingly right,

01:02:45.400 --> 01:02:47.960
but it's actually wrong in a very subtle way.

01:02:48.800 --> 01:02:53.800
But that's the trade-off that we face as human civilization

01:02:55.480 --> 01:02:57.840
when we build more and more powerful tools.

01:02:57.840 --> 01:03:02.080
When we stand on the shoulders of taller and taller giants,

01:03:02.080 --> 01:03:04.320
we could do more, but then we forget how to do

01:03:04.320 --> 01:03:05.720
all the stuff that they did.

01:03:07.520 --> 01:03:08.920
It's a weird trade-off.

01:03:08.920 --> 01:03:10.200
Yeah, I agree.

01:03:10.200 --> 01:03:13.120
I mean, I think it is very valuable in your life

01:03:13.120 --> 01:03:15.600
to be able to do basic things too.

01:03:15.600 --> 01:03:20.600
Do you worry about some of the concerns of bots

01:03:20.720 --> 01:03:22.840
being present on social networks,

01:03:22.840 --> 01:03:24.960
more and more human-like bots

01:03:24.960 --> 01:03:29.560
that are not necessarily trying to do a good thing

01:03:29.560 --> 01:03:32.240
or they might be explicitly trying to do a bad thing

01:03:32.240 --> 01:03:35.280
like phishing scams, like social engineering,

01:03:35.280 --> 01:03:36.400
all that kind of stuff,

01:03:36.400 --> 01:03:38.920
which has always been a very difficult problem

01:03:38.920 --> 01:03:39.880
for social networks,

01:03:39.880 --> 01:03:41.800
but now it's becoming almost a more and more

01:03:41.800 --> 01:03:43.160
difficult problem.

01:03:43.160 --> 01:03:46.080
Well, there's a few different parts of this.

01:03:46.080 --> 01:03:51.080
So one is there are all these harms that we need

01:03:51.640 --> 01:03:53.400
to basically fight against and prevent.

01:03:53.400 --> 01:03:58.400
And that's been a lot of our focus over the last five

01:03:59.280 --> 01:04:01.560
or seven years is basically ramping up

01:04:01.560 --> 01:04:05.160
very sophisticated AI systems, not generative AI systems,

01:04:05.160 --> 01:04:07.240
more kind of classical AI systems

01:04:07.240 --> 01:04:12.240
to be able to categorize and classify

01:04:12.240 --> 01:04:16.080
and identify, okay, this post looks like

01:04:16.080 --> 01:04:17.720
it's promoting terrorism.

01:04:17.720 --> 01:04:21.600
This one is exploiting children.

01:04:21.600 --> 01:04:25.200
This one looks like it might be trying to incite violence.

01:04:25.200 --> 01:04:28.040
This one's an intellectual property violation.

01:04:28.040 --> 01:04:31.960
So there's like 18 different categories

01:04:32.680 --> 01:04:35.400
of violating kind of harmful content

01:04:35.400 --> 01:04:37.800
that we've had to build specific systems

01:04:37.800 --> 01:04:38.680
to be able to track.

01:04:38.680 --> 01:04:43.680
And I think it's certainly the case that advances

01:04:44.080 --> 01:04:47.280
in generative AI will test those,

01:04:50.120 --> 01:04:52.280
but at least so far it's been the case,

01:04:52.280 --> 01:04:55.120
and I'm optimistic that it will continue to be the case

01:04:55.120 --> 01:04:58.960
that we will be able to bring more computing power to bear

01:04:58.960 --> 01:05:01.080
to have even stronger AIs that can help defend

01:05:01.080 --> 01:05:02.080
against those things.

01:05:02.080 --> 01:05:06.800
So we've had to deal with some adversarial issues before.

01:05:06.800 --> 01:05:09.680
It's, I mean, for some things like hate speech,

01:05:09.680 --> 01:05:11.160
it's like people aren't generally getting

01:05:11.160 --> 01:05:14.880
a lot more sophisticated, like the average person.

01:05:14.880 --> 01:05:16.880
Let's say, you know, someone's saying

01:05:16.880 --> 01:05:18.560
some kind of racist thing, right?

01:05:18.560 --> 01:05:19.760
It's like, they're not necessarily getting

01:05:19.760 --> 01:05:21.760
more sophisticated at being racist, right?

01:05:21.760 --> 01:05:22.600
It just, it's okay.

01:05:22.600 --> 01:05:24.840
So that the system can just find,

01:05:24.840 --> 01:05:26.840
but then there's other adversaries

01:05:26.840 --> 01:05:29.160
who actually are very sophisticated,

01:05:29.160 --> 01:05:30.840
like nation states doing things.

01:05:30.840 --> 01:05:35.280
And we find, whether it's Russia or, you know,

01:05:35.280 --> 01:05:37.040
just different countries that are basically standing up

01:05:37.040 --> 01:05:42.040
these networks of bots or inauthentic accounts

01:05:42.920 --> 01:05:44.040
is what we call them,

01:05:44.040 --> 01:05:45.400
because they're not necessarily bots.

01:05:45.400 --> 01:05:47.160
Some of them could actually be real people

01:05:47.160 --> 01:05:50.440
who are kind of masquerading as other people,

01:05:50.440 --> 01:05:53.080
but they're acting in a coordinated way.

01:05:53.080 --> 01:05:57.280
And some of that behavior has gotten very sophisticated

01:05:57.280 --> 01:05:58.400
and it's very adversarial.

01:05:58.400 --> 01:05:59.760
So they, you know, each iteration,

01:05:59.760 --> 01:06:03.000
every time we find something and stop them,

01:06:03.000 --> 01:06:04.240
they kind of evolve their behavior.

01:06:04.240 --> 01:06:05.440
They don't just pack up their bags

01:06:05.440 --> 01:06:08.240
and go home and say, okay, we're not gonna try.

01:06:08.240 --> 01:06:09.360
You know, at some point they might decide

01:06:09.360 --> 01:06:12.040
doing it on meta services is not worth it.

01:06:12.040 --> 01:06:13.080
They'll go do it on someone else

01:06:13.080 --> 01:06:14.960
if it's easier to do it in another place.

01:06:14.960 --> 01:06:19.760
But we have a fair amount of experience dealing with

01:06:20.960 --> 01:06:22.720
even those kinds of adversarial attacks

01:06:22.720 --> 01:06:24.640
where they just keep on getting better and better.

01:06:24.640 --> 01:06:26.800
And I do think that as long as we can keep on

01:06:26.800 --> 01:06:28.720
putting more compute power against it,

01:06:28.720 --> 01:06:31.120
and if we're kind of one of the leaders

01:06:31.120 --> 01:06:33.040
in developing some of these AI models,

01:06:33.040 --> 01:06:36.000
I'm quite optimistic that we're gonna be able to keep on

01:06:37.040 --> 01:06:40.800
pushing against the kind of normal categories of harm

01:06:40.800 --> 01:06:44.280
that you talk about, fraud, scams, spam,

01:06:45.640 --> 01:06:47.800
IP violations, things like that.

01:06:47.800 --> 01:06:50.760
What about like creating narratives and controversy?

01:06:50.760 --> 01:06:55.320
To me, it's kind of amazing how a small collection of,

01:06:56.160 --> 01:06:57.840
what did you say, inauthentic accounts?

01:06:57.840 --> 01:06:59.400
So it could be bots, but it could be-

01:06:59.400 --> 01:07:01.400
Yeah, I mean, we have sort of this funny name for it,

01:07:01.400 --> 01:07:03.720
but we call it coordinated inauthentic behavior.

01:07:03.720 --> 01:07:06.240
Yeah, it's kind of incredible

01:07:06.240 --> 01:07:10.160
how a small collection of folks can create narratives,

01:07:10.160 --> 01:07:14.360
create stories, especially if they're viral.

01:07:14.360 --> 01:07:16.560
So especially if they have an element

01:07:16.560 --> 01:07:21.120
that can catalyze the virality of that narrative.

01:07:21.120 --> 01:07:24.400
Yeah, and I think there the question is you have to be,

01:07:24.400 --> 01:07:27.080
I'm very specific about what is bad about it, right?

01:07:27.080 --> 01:07:31.280
Because I think a set of people coming together

01:07:31.280 --> 01:07:34.800
or organically bouncing ideas off of each other

01:07:34.800 --> 01:07:36.400
and a narrative comes out of that

01:07:37.520 --> 01:07:39.640
is not necessarily a bad thing by itself

01:07:39.640 --> 01:07:42.480
if it's kind of authentic and organic.

01:07:42.480 --> 01:07:43.560
That's like a lot of what happens

01:07:43.560 --> 01:07:45.600
and how culture gets created and how art gets created

01:07:45.600 --> 01:07:46.440
and a lot of good stuff.

01:07:46.440 --> 01:07:49.200
So that's why we've kind of focused on this sense

01:07:49.200 --> 01:07:51.560
of coordinated inauthentic behavior.

01:07:51.560 --> 01:07:53.600
So it's like, if you have a network of,

01:07:53.600 --> 01:07:56.080
whether it's bots, some people masquerading

01:07:56.080 --> 01:07:57.320
as different accounts,

01:07:58.720 --> 01:08:01.920
but you have kind of someone pulling the strings behind it

01:08:03.200 --> 01:08:07.760
and trying to kind of act as if this is a more organic

01:08:07.760 --> 01:08:09.360
set of behavior, but really it's not.

01:08:09.360 --> 01:08:11.760
It's just like one coordinated thing.

01:08:11.760 --> 01:08:13.520
That seems problematic to me, right?

01:08:13.520 --> 01:08:14.920
I mean, I don't think people should be able

01:08:14.920 --> 01:08:19.240
to have coordinated networks and not disclose it as such,

01:08:20.360 --> 01:08:22.840
but that again, we've been able to deploy

01:08:22.880 --> 01:08:27.440
pretty sophisticated AI and counterterrorism groups

01:08:27.440 --> 01:08:29.400
and things like that to be able to identify

01:08:29.400 --> 01:08:33.920
a fair number of these coordinated inauthentic networks

01:08:33.920 --> 01:08:36.080
of accounts and take them down.

01:08:37.560 --> 01:08:40.080
We continue to do that and I think we've,

01:08:40.080 --> 01:08:42.000
it's one thing that if you'd told me 20 years ago,

01:08:42.000 --> 01:08:44.040
it's like, all right, you're starting this website

01:08:44.040 --> 01:08:45.760
to help people connect at a college

01:08:45.760 --> 01:08:49.040
and in the future you're gonna be part of your organization

01:08:49.040 --> 01:08:51.400
is gonna be a counterterrorism organization with AI

01:08:51.400 --> 01:08:53.480
to find coordinated inauthentic,

01:08:53.480 --> 01:08:55.960
I would have thought that was pretty wild,

01:08:55.960 --> 01:09:00.960
but no, I think that that's part of where we are.

01:09:00.960 --> 01:09:02.800
But look, I think that these questions

01:09:02.800 --> 01:09:04.160
that you're pushing on now,

01:09:06.840 --> 01:09:09.080
this is actually where I'd guess most of the challenge

01:09:09.080 --> 01:09:12.840
around AI will be for the foreseeable future.

01:09:12.840 --> 01:09:15.920
I think that there's a lot of debate around things like,

01:09:15.920 --> 01:09:19.040
is this going to create existential risk to humanity?

01:09:19.040 --> 01:09:20.960
And I think that those are very hard things

01:09:20.960 --> 01:09:22.920
to disprove one way or another.

01:09:22.920 --> 01:09:26.520
My own intuition is that the point at which we become close

01:09:26.520 --> 01:09:31.520
to superintelligence is, it's just really unclear to me

01:09:33.240 --> 01:09:35.680
that the current technology is gonna get there

01:09:35.680 --> 01:09:39.320
without another set of significant advances.

01:09:39.320 --> 01:09:40.840
But that doesn't mean that there's no danger.

01:09:40.840 --> 01:09:42.920
I think the danger is basically amplifying

01:09:42.920 --> 01:09:46.760
the kind of known set of harms that people

01:09:46.760 --> 01:09:48.440
or sets of accounts can do.

01:09:48.440 --> 01:09:50.520
And we just need to make sure that we really focus

01:09:50.520 --> 01:09:54.920
on basically doing that as well as possible.

01:09:54.920 --> 01:09:57.680
So that's definitely a big focus for me.

01:09:57.680 --> 01:10:00.240
Well, you can basically use large language models

01:10:00.240 --> 01:10:03.960
as an assistant of how to cause harm on social networks.

01:10:03.960 --> 01:10:05.280
You can ask it a question.

01:10:08.400 --> 01:10:13.040
Meta has very impressive coordinated inauthentic account

01:10:14.320 --> 01:10:16.080
fighting capabilities.

01:10:16.120 --> 01:10:19.840
How do I do the coordinated inauthentic account

01:10:21.000 --> 01:10:23.480
creation where Meta doesn't detect it?

01:10:23.480 --> 01:10:25.440
Like literally ask that question.

01:10:25.440 --> 01:10:29.280
And basically there's this kind of part of it.

01:10:29.280 --> 01:10:30.960
I mean, that's what open AI show

01:10:30.960 --> 01:10:33.080
that they're concerned with those questions.

01:10:33.080 --> 01:10:34.960
Perhaps you can comment on your approach to it.

01:10:34.960 --> 01:10:38.880
How to do a kind of moderation on the output

01:10:38.880 --> 01:10:41.160
of those models that it can't be used

01:10:41.160 --> 01:10:45.400
to help you coordinate harm in all the full definition

01:10:45.400 --> 01:10:46.920
of what the harm means.

01:10:46.920 --> 01:10:48.800
Yeah, and that's a lot of the fine tuning

01:10:48.800 --> 01:10:51.240
and the alignment training that we do

01:10:51.240 --> 01:10:56.240
is basically when we ship AIs across our products,

01:10:59.000 --> 01:11:01.520
a lot of what we're trying to make sure

01:11:01.520 --> 01:11:06.520
is that you can't ask it to help you commit a crime, right?

01:11:11.200 --> 01:11:14.640
So I think training it to kind of understand that

01:11:14.640 --> 01:11:17.120
and it's not like any of these systems

01:11:17.120 --> 01:11:18.800
are ever gonna be 100% perfect,

01:11:18.800 --> 01:11:23.800
but just making it so that this isn't an easier way

01:11:25.960 --> 01:11:30.040
to go about doing something bad

01:11:30.040 --> 01:11:32.000
than the next best alternative, right?

01:11:32.000 --> 01:11:33.720
I mean, people still have Google, right?

01:11:33.720 --> 01:11:35.000
You still have search engines.

01:11:35.000 --> 01:11:37.960
So the information is out there.

01:11:38.800 --> 01:11:43.800
And for these, what we see is like for nation states

01:11:44.160 --> 01:11:46.720
or these actors that are trying to pull off

01:11:46.720 --> 01:11:50.160
these large coordinated and authentic networks

01:11:50.160 --> 01:11:53.160
to kind of influence different things,

01:11:53.160 --> 01:11:55.040
at some point when we may just make it very difficult,

01:11:55.040 --> 01:11:58.520
they do just try to use other services instead, right?

01:11:58.520 --> 01:12:00.840
It's just like if you can make it more expensive

01:12:00.840 --> 01:12:03.400
for them to do it on your service,

01:12:03.400 --> 01:12:05.880
then kind of people go elsewhere.

01:12:05.880 --> 01:12:08.720
And I think that that's the bar, right?

01:12:08.720 --> 01:12:11.240
It's not like, okay, are you ever gonna be perfect

01:12:11.240 --> 01:12:14.440
at finding every adversary who tries to attack you?

01:12:14.440 --> 01:12:16.920
It's, I mean, you try to get as close to that as possible,

01:12:16.920 --> 01:12:20.040
but I think really kind of economically

01:12:20.040 --> 01:12:21.720
what you're just trying to do is make it so

01:12:21.720 --> 01:12:24.880
it's just inefficient for them to go after that.

01:12:24.880 --> 01:12:26.480
But there's also complicated questions

01:12:26.480 --> 01:12:30.600
of what is and isn't harm, what is and isn't misinformation.

01:12:30.600 --> 01:12:32.040
So this is one of the things

01:12:32.040 --> 01:12:35.040
that Wikipedia has also tried to face.

01:12:35.040 --> 01:12:39.520
I remember asking GPT about whether the virus

01:12:39.560 --> 01:12:40.920
leaked from a lab or not,

01:12:40.920 --> 01:12:45.160
and the answer provided was a very nuanced one

01:12:45.160 --> 01:12:47.440
and a well-cited one,

01:12:47.440 --> 01:12:52.440
almost dare I say well thought out one, balanced.

01:12:52.760 --> 01:12:55.040
I would hate for that nuance to be lost

01:12:55.040 --> 01:12:57.600
through the process of moderation.

01:12:57.600 --> 01:13:00.600
Wikipedia does a good job on that particular thing too,

01:13:00.600 --> 01:13:03.320
but from pressures from governments and institutions,

01:13:03.320 --> 01:13:06.800
it's, you could see some of that nuance

01:13:06.800 --> 01:13:11.800
and depth of information, facts and wisdom be lost.

01:13:13.720 --> 01:13:14.560
Absolutely.

01:13:14.560 --> 01:13:16.560
And that's a scary thing.

01:13:16.560 --> 01:13:19.560
Some of the magic, some of the edges,

01:13:19.560 --> 01:13:21.000
the rough edges might be lost

01:13:21.000 --> 01:13:23.960
through the process of moderation of AI systems.

01:13:25.120 --> 01:13:26.560
So how do you get that right?

01:13:26.560 --> 01:13:28.960
I really agree with what you're pushing on.

01:13:28.960 --> 01:13:33.960
I mean, the core shape of the problem

01:13:34.000 --> 01:13:36.360
is that there are some harms

01:13:36.400 --> 01:13:38.800
that I think everyone agrees are bad, right?

01:13:38.800 --> 01:13:43.800
So sexual exploitation of children, right?

01:13:44.360 --> 01:13:46.000
Like you're not gonna get many people

01:13:46.000 --> 01:13:47.760
who think that that type of thing

01:13:47.760 --> 01:13:49.520
should be allowed on any service, right?

01:13:49.520 --> 01:13:51.520
And that's something that we face

01:13:51.520 --> 01:13:56.320
and try to push off as much as possible today.

01:13:56.320 --> 01:13:59.560
Terrorism, inciting violence, right?

01:13:59.560 --> 01:14:01.520
It's like we went through a bunch of these types

01:14:01.520 --> 01:14:02.800
of harms before.

01:14:03.480 --> 01:14:06.400
But then I do think that you get to a set of harms

01:14:06.400 --> 01:14:08.760
where there is more social debate around it.

01:14:10.200 --> 01:14:12.400
So misinformation I think is,

01:14:15.000 --> 01:14:16.800
has been a really tricky one

01:14:16.800 --> 01:14:21.800
because there are things that are kind of obviously false,

01:14:22.040 --> 01:14:22.880
right?

01:14:22.880 --> 01:14:23.960
That are maybe factual,

01:14:26.280 --> 01:14:27.800
but may not be harmful.

01:14:29.680 --> 01:14:30.520
So it's like, all right,

01:14:30.840 --> 01:14:31.680
since it's like, all right,

01:14:31.680 --> 01:14:34.760
are you gonna censor someone for just being wrong?

01:14:34.760 --> 01:14:36.480
If there's no kind of harm implication

01:14:36.480 --> 01:14:37.320
of what they're doing,

01:14:37.320 --> 01:14:39.480
I think that there's a bunch of real kind of issues

01:14:39.480 --> 01:14:41.080
and challenges there.

01:14:41.080 --> 01:14:43.400
But then I think that there are other places

01:14:43.400 --> 01:14:44.240
where it is,

01:14:45.440 --> 01:14:47.200
you just take some of the stuff around COVID

01:14:47.200 --> 01:14:48.480
earlier on in the pandemic,

01:14:48.480 --> 01:14:53.480
where there were real health implications,

01:14:53.520 --> 01:14:55.400
but there hadn't been time to fully vet

01:14:55.400 --> 01:14:57.080
a bunch of the scientific assumptions.

01:14:57.080 --> 01:14:59.840
And unfortunately, I think a lot of the kind

01:14:59.840 --> 01:15:01.200
of establishment done that,

01:15:02.240 --> 01:15:03.960
you know, kind of waffled on a bunch of facts

01:15:03.960 --> 01:15:06.640
and, you know, asked for a bunch of things to be censored

01:15:06.640 --> 01:15:09.120
that in retrospect ended up being, you know,

01:15:09.120 --> 01:15:11.680
more debatable or true.

01:15:11.680 --> 01:15:13.560
And that stuff is really tough, right?

01:15:13.560 --> 01:15:15.840
And really undermines trust in that.

01:15:18.280 --> 01:15:20.960
So I do think that the questions around

01:15:20.960 --> 01:15:24.120
how to manage that are very nuanced.

01:15:24.120 --> 01:15:26.600
The way that I try to think about it is that

01:15:27.560 --> 01:15:31.120
it goes, I think it's best to generally boil things down

01:15:31.120 --> 01:15:34.040
to the harms that people agree on.

01:15:34.040 --> 01:15:35.440
So when you think about, you know,

01:15:35.440 --> 01:15:37.720
is something misinformation or not,

01:15:37.720 --> 01:15:40.720
I think often the more salient bit is,

01:15:40.720 --> 01:15:45.720
is this going to potentially lead to physical harm

01:15:45.920 --> 01:15:49.720
for someone and kind of think about it in that sense.

01:15:49.720 --> 01:15:50.840
And then beyond that,

01:15:50.840 --> 01:15:52.480
I think people just have different preferences

01:15:52.480 --> 01:15:54.720
on how they want things to be flagged for them.

01:15:55.280 --> 01:15:57.720
I think a bunch of people would prefer

01:15:57.720 --> 01:16:00.320
to kind of have a flag on something that says,

01:16:00.320 --> 01:16:02.360
hey, a fact checker thinks that this might be false.

01:16:02.360 --> 01:16:05.360
Or I think Twitter's community notes implementation

01:16:05.360 --> 01:16:07.320
is quite good on this.

01:16:08.520 --> 01:16:09.640
But again, it's the same type of thing.

01:16:09.640 --> 01:16:12.760
It's like just kind of discretionarily adding a flag

01:16:12.760 --> 01:16:14.560
because it makes the user experience better,

01:16:14.560 --> 01:16:15.920
but it's not, you know,

01:16:15.920 --> 01:16:17.400
trying to take down the information or not.

01:16:17.400 --> 01:16:20.920
I think that you want to reserve the kind of censorship

01:16:20.920 --> 01:16:24.520
of content to things that are of known categories

01:16:25.320 --> 01:16:27.680
that people generally agree are bad.

01:16:27.680 --> 01:16:30.480
Yeah, but there's so many things,

01:16:30.480 --> 01:16:31.600
especially with the pandemic,

01:16:31.600 --> 01:16:36.600
but there's other topics where there's just deep disagreement

01:16:37.200 --> 01:16:41.320
fueled by politics about what is and isn't harmful.

01:16:41.320 --> 01:16:45.760
There's even just the degree to which the virus is harmful

01:16:45.760 --> 01:16:48.680
and the degree to which the vaccines

01:16:48.680 --> 01:16:50.160
that respond to the virus are harmful.

01:16:50.160 --> 01:16:53.520
There's just, there's almost like a political divider

01:16:53.520 --> 01:16:54.360
on that.

01:16:54.360 --> 01:16:57.800
And so how do you make decisions about that

01:16:57.800 --> 01:17:01.120
where half the country in the United States

01:17:01.120 --> 01:17:03.680
or some large fraction of the world

01:17:03.680 --> 01:17:06.960
has very different views from another part of the world?

01:17:08.680 --> 01:17:13.000
Is there a way for Metta to stay out of the moderation

01:17:13.000 --> 01:17:13.840
of this?

01:17:13.840 --> 01:17:17.800
I think we, it's very difficult to just abstain,

01:17:19.080 --> 01:17:22.640
but I think we should be clear about which of these things

01:17:22.680 --> 01:17:25.280
are actual safety concerns

01:17:25.280 --> 01:17:27.960
and which ones are a matter of preference

01:17:27.960 --> 01:17:30.280
in terms of how people want information flagged.

01:17:30.280 --> 01:17:32.760
Right, so we did recently introduce something

01:17:32.760 --> 01:17:36.560
that allows people to have fact-checking

01:17:36.560 --> 01:17:40.440
not affect the distribution of what shows up in their product.

01:17:40.440 --> 01:17:41.920
So, okay, a bunch of people don't trust

01:17:41.920 --> 01:17:43.360
who the fact-checkers are.

01:17:43.360 --> 01:17:45.920
All right, well, you can turn that off if you want,

01:17:45.920 --> 01:17:49.760
but if the content violates some policy

01:17:49.760 --> 01:17:51.560
like it's inciting violence or something like that,

01:17:51.560 --> 01:17:53.040
it's still not gonna be allowed.

01:17:53.040 --> 01:17:56.480
So I think that you wanna honor people's preferences

01:17:56.480 --> 01:17:58.440
on that as much as possible.

01:18:00.120 --> 01:18:01.960
But look, I mean, this is really difficult stuff.

01:18:01.960 --> 01:18:06.680
I think it's really hard to know where to draw the line

01:18:06.680 --> 01:18:10.760
on what is fact and what is opinion,

01:18:10.760 --> 01:18:13.320
because the nature of science is that

01:18:13.320 --> 01:18:15.720
nothing is ever 100% known for certain.

01:18:15.720 --> 01:18:17.600
You can disprove certain things,

01:18:17.600 --> 01:18:20.520
but you're constantly testing new hypotheses

01:18:20.520 --> 01:18:24.920
and scrutinizing frameworks that have been long held.

01:18:24.920 --> 01:18:28.000
And every once in a while you throw out something

01:18:28.000 --> 01:18:29.960
that was working for a very long period of time

01:18:29.960 --> 01:18:31.680
and it's very difficult.

01:18:31.680 --> 01:18:34.760
But I think that just because it's very hard

01:18:34.760 --> 01:18:36.400
and just because they're edge cases doesn't mean

01:18:36.400 --> 01:18:40.040
that you should not try to give people

01:18:40.040 --> 01:18:41.640
what they're looking for as well.

01:18:43.200 --> 01:18:46.280
Let me ask about something you faced

01:18:46.280 --> 01:18:51.280
in terms of moderation is pressure from different sources,

01:18:52.480 --> 01:18:54.040
pressure from governments.

01:18:54.040 --> 01:18:58.280
I wanna ask a question how to withstand that pressure

01:18:58.280 --> 01:19:03.280
for a world where AI moderation starts becoming a thing too.

01:19:03.680 --> 01:19:08.680
So what's Metta's approach to resist the pressure

01:19:10.680 --> 01:19:13.000
from governments and other interest groups

01:19:13.000 --> 01:19:15.400
in terms of what to moderate and not?

01:19:16.680 --> 01:19:19.120
I don't know that there's like a one size fits all answer

01:19:19.120 --> 01:19:22.840
to that, and I think we basically have the principles

01:19:22.840 --> 01:19:27.600
around we wanna allow people to express as much as possible,

01:19:27.600 --> 01:19:32.600
but we have developed clear categories of things

01:19:33.280 --> 01:19:37.920
that we think are wrong that we don't want on our services

01:19:37.920 --> 01:19:40.800
and we build tools to try to moderate those.

01:19:40.800 --> 01:19:43.360
So then the question is, okay, what do you do

01:19:43.360 --> 01:19:47.400
when a government says that they don't want something

01:19:48.440 --> 01:19:49.640
on the service?

01:19:49.640 --> 01:19:54.040
And we have a bunch of principles

01:19:54.040 --> 01:19:56.440
around how we deal with that, because on the one hand,

01:19:56.440 --> 01:20:00.360
if there's a democratically elected government

01:20:00.360 --> 01:20:03.200
and people around the world just have different values

01:20:03.200 --> 01:20:08.200
in different places, then should we as a California-based

01:20:08.600 --> 01:20:13.600
company tell them that something that they have decided

01:20:14.880 --> 01:20:19.080
is unacceptable, actually that we need to be able

01:20:19.080 --> 01:20:20.880
to express that?

01:20:20.880 --> 01:20:23.240
I mean, I think that there's a certain amount

01:20:23.240 --> 01:20:28.240
of hubris in that, but then I think there are other cases

01:20:28.800 --> 01:20:32.040
where it's like a little more autocratic

01:20:32.040 --> 01:20:36.160
and you have the dictator leader who's just trying

01:20:36.160 --> 01:20:39.640
to crack down on dissent and the people in a country

01:20:39.640 --> 01:20:42.520
are really not aligned with that,

01:20:42.520 --> 01:20:44.440
and it's not necessarily against their culture,

01:20:44.440 --> 01:20:47.800
but the person who's leading it is just trying

01:20:47.800 --> 01:20:49.400
to push in a certain direction.

01:20:50.600 --> 01:20:55.000
These are very complex questions, but I think,

01:20:55.000 --> 01:21:00.000
so it's difficult to have a one-size-fits-all approach

01:21:00.800 --> 01:21:03.400
to it, but in general, we're pretty active

01:21:03.440 --> 01:21:06.000
in kind of advocating and pushing back

01:21:06.000 --> 01:21:10.640
on requests to take things down.

01:21:13.360 --> 01:21:16.840
But honestly, I think a request to censor things

01:21:16.840 --> 01:21:20.080
is one thing, and that's obviously bad,

01:21:20.080 --> 01:21:24.080
but where we draw a much harder line is on requests

01:21:24.080 --> 01:21:25.960
for access to information, right?

01:21:25.960 --> 01:21:29.840
Because if you get told that you can't say something,

01:21:29.840 --> 01:21:32.080
I mean, that's bad, right?

01:21:32.080 --> 01:21:37.080
I mean, that obviously violates your sense

01:21:38.680 --> 01:21:40.560
and freedom of expression at some level,

01:21:40.560 --> 01:21:44.480
but a government getting access to data in a way

01:21:44.480 --> 01:21:49.480
that seems like it would be unlawful in our country

01:21:50.880 --> 01:21:53.480
exposes people to real physical harm,

01:21:54.640 --> 01:21:57.400
and that's something that in general,

01:21:57.400 --> 01:21:59.400
we take very seriously.

01:21:59.400 --> 01:22:03.320
And then, so that flows through all of our policies

01:22:03.320 --> 01:22:04.960
in a lot of ways, right?

01:22:04.960 --> 01:22:08.400
By the time you're actually litigating with a government

01:22:08.400 --> 01:22:11.840
or pushing back on them, that's pretty late in the funnel.

01:22:11.840 --> 01:22:15.680
I'd say a bunch of this stuff starts a lot higher up

01:22:15.680 --> 01:22:18.400
in the decision of where do we put data centers.

01:22:18.400 --> 01:22:22.280
Then there are a lot of countries where we may have

01:22:22.280 --> 01:22:24.560
a lot of people using the service in a place.

01:22:24.560 --> 01:22:28.240
It might be good for the service in some ways,

01:22:29.200 --> 01:22:31.360
good for those people if we could reduce the latency

01:22:31.360 --> 01:22:34.400
by having a data center nearby them.

01:22:34.400 --> 01:22:36.640
But for whatever reason, we just feel like,

01:22:36.640 --> 01:22:39.960
hey, this government does not have a good track record

01:22:39.960 --> 01:22:44.960
on basically not trying to get access to people's data.

01:22:46.720 --> 01:22:47.560
And at the end of the day,

01:22:47.560 --> 01:22:49.640
I mean, if you put a data center in a country

01:22:49.640 --> 01:22:52.560
and the government wants to get access to people's data,

01:22:52.560 --> 01:22:54.800
then they do, at the end of the day,

01:22:54.800 --> 01:22:57.240
have the option of having people show up with guns

01:22:57.240 --> 01:22:59.000
and taking it by force.

01:22:59.000 --> 01:23:01.360
So I think that there's a lot of decisions

01:23:01.360 --> 01:23:03.560
that go into how you architect the systems

01:23:05.240 --> 01:23:09.760
years in advance of these actual confrontations

01:23:09.760 --> 01:23:11.680
that end up being really important.

01:23:11.680 --> 01:23:15.240
So you put the protection of people's data

01:23:15.240 --> 01:23:17.880
as a very, very high priority.

01:23:17.880 --> 01:23:19.080
But in the-

01:23:19.080 --> 01:23:20.000
There are more harms

01:23:20.000 --> 01:23:21.680
that I think can be associated with that.

01:23:21.680 --> 01:23:24.720
And I think that that ends up being a more critical thing

01:23:24.720 --> 01:23:28.800
to defend against governments than,

01:23:28.800 --> 01:23:31.000
whereas if another government has a different view

01:23:31.000 --> 01:23:34.160
of what should be acceptable speech in their country,

01:23:34.160 --> 01:23:36.320
especially if it's a democratically elected government,

01:23:36.320 --> 01:23:39.320
and then I think that there's a certain amount

01:23:39.320 --> 01:23:41.200
of deference that you should have to that.

01:23:41.200 --> 01:23:44.960
So that's speaking more to the direct harm that's possible

01:23:44.960 --> 01:23:47.440
when you give governments access to data.

01:23:47.440 --> 01:23:49.400
But if we look at the United States,

01:23:50.360 --> 01:23:53.360
to the more nuanced kind of pressure to censor,

01:23:53.400 --> 01:23:54.800
not even order to censor,

01:23:54.800 --> 01:23:57.840
but pressure to censor from political entities,

01:23:57.840 --> 01:24:00.920
which has kind of received quite a bit of attention

01:24:00.920 --> 01:24:02.160
in the United States.

01:24:03.720 --> 01:24:06.640
Maybe one way to ask that question is,

01:24:06.640 --> 01:24:08.560
if you've seen the Twitter files,

01:24:09.440 --> 01:24:14.280
what have you learned from the kind of pressure

01:24:14.280 --> 01:24:18.520
from US government agencies that was seen in Twitter files?

01:24:18.520 --> 01:24:20.760
And what do you do with that kind of pressure?

01:24:21.760 --> 01:24:23.840
You know, I mean, I've seen it.

01:24:25.840 --> 01:24:27.360
It's really hard from the outside

01:24:27.360 --> 01:24:30.200
to know exactly what happened in each of these cases.

01:24:30.200 --> 01:24:35.200
You know, we've obviously been in a bunch of our own cases

01:24:35.800 --> 01:24:40.800
where, you know, where agencies or different folks

01:24:41.440 --> 01:24:46.200
will just say, hey, here's a threat that we're aware of.

01:24:46.200 --> 01:24:48.120
You should be aware of this too.

01:24:48.600 --> 01:24:51.320
It's not really pressure as much as it is just,

01:24:52.920 --> 01:24:56.080
you know, flagging something that our security systems

01:24:56.080 --> 01:24:57.640
should be on alert about.

01:24:58.520 --> 01:25:00.880
I get how some people could think of it as that.

01:25:02.640 --> 01:25:04.400
But at the end of the day,

01:25:04.400 --> 01:25:07.720
it's our call on how to handle that.

01:25:07.720 --> 01:25:09.000
But I mean, I just, you know,

01:25:09.000 --> 01:25:10.240
in terms of running these services,

01:25:10.240 --> 01:25:11.880
won't have access to as much information

01:25:11.880 --> 01:25:13.320
about what people think that adversaries

01:25:13.320 --> 01:25:15.320
might be trying to do as possible.

01:25:15.360 --> 01:25:18.880
Well, so you don't feel like there will be consequences

01:25:18.880 --> 01:25:23.880
if, you know, anybody, the CIA, the FBI, a political party,

01:25:24.240 --> 01:25:26.520
the Democrats or the Republicans

01:25:26.520 --> 01:25:31.520
of high powerful political figures write emails.

01:25:31.520 --> 01:25:34.320
You don't feel pressure from a suggestion.

01:25:34.320 --> 01:25:36.120
I guess what I'd say is there's so much pressure

01:25:36.120 --> 01:25:40.600
from all sides that I'm not sure that any specific thing

01:25:40.600 --> 01:25:42.880
that someone says is really adding

01:25:42.880 --> 01:25:44.520
that much more to the mix.

01:25:44.520 --> 01:25:47.440
It's, there are obviously a lot of people

01:25:47.440 --> 01:25:52.440
who think that we should be censoring more content

01:25:52.440 --> 01:25:53.280
or there are a lot of people

01:25:53.280 --> 01:25:55.400
who think we should be censoring less content.

01:25:55.400 --> 01:25:58.080
There are, as you say, all kinds of different groups

01:25:58.080 --> 01:25:59.760
that are involved in these debates, right?

01:25:59.760 --> 01:26:02.320
So there's the kind of elected officials

01:26:02.320 --> 01:26:04.880
and politicians themselves, there's the agencies,

01:26:04.880 --> 01:26:07.840
but I mean, but there's the media,

01:26:07.840 --> 01:26:10.160
there's activist groups, there's,

01:26:10.160 --> 01:26:11.800
this is not a US specific thing.

01:26:11.800 --> 01:26:13.280
There are groups all over the world

01:26:13.280 --> 01:26:16.200
and kind of all in every country

01:26:16.200 --> 01:26:18.080
that bring different values.

01:26:19.640 --> 01:26:22.000
So it's just a very, it's a very active debate

01:26:22.000 --> 01:26:23.880
and I understand it, right?

01:26:23.880 --> 01:26:27.800
I mean, these are, you know, these kind of questions

01:26:27.800 --> 01:26:31.400
get to really some of the most important social debates

01:26:31.400 --> 01:26:32.960
that are being had.

01:26:32.960 --> 01:26:36.000
So it gets back to the question of truth

01:26:36.000 --> 01:26:39.160
because for a lot of these things,

01:26:39.160 --> 01:26:41.640
they haven't yet been hardened into a single truth

01:26:41.640 --> 01:26:46.640
and society sort of trying to hash out what we think,

01:26:46.720 --> 01:26:48.400
right, on certain issues.

01:26:48.400 --> 01:26:50.720
Maybe in a few hundred years, everyone will look back

01:26:50.720 --> 01:26:52.160
and say, hey, no, it wasn't obvious

01:26:52.160 --> 01:26:53.040
that it should have been this,

01:26:53.040 --> 01:26:56.240
but no, we're kind of in that meat grinder now

01:26:56.240 --> 01:26:59.640
and working through that.

01:26:59.640 --> 01:27:04.640
So no, these are all very complicated

01:27:06.440 --> 01:27:11.440
and some people raise concerns in good faith

01:27:12.000 --> 01:27:13.560
and just say, hey, this is something

01:27:13.560 --> 01:27:15.840
that I wanna flag for you to think about.

01:27:15.840 --> 01:27:17.880
Certain people, I certainly think,

01:27:17.880 --> 01:27:21.800
like come at things with somewhat of a more kind of punitive

01:27:21.800 --> 01:27:26.600
or vengeful view of like, I want you to do this thing.

01:27:26.600 --> 01:27:28.680
If you don't, then I'm gonna try to make your life difficult

01:27:28.680 --> 01:27:30.560
and in a lot of other ways.

01:27:30.560 --> 01:27:33.880
But like, I don't know, there's just,

01:27:33.880 --> 01:27:36.560
this is like, this is one of the most pressurized debates

01:27:36.560 --> 01:27:37.520
I think in society.

01:27:37.520 --> 01:27:40.240
So I just think that there are so many people

01:27:40.240 --> 01:27:42.160
on different forces that are trying to apply pressure

01:27:42.160 --> 01:27:44.360
from different sides that it's,

01:27:44.360 --> 01:27:45.960
I don't think you can make decisions

01:27:45.960 --> 01:27:47.320
based on trying to make people happy.

01:27:47.320 --> 01:27:50.240
I think you just have to do what you think

01:27:50.240 --> 01:27:53.680
is the right balance and accept that people

01:27:53.680 --> 01:27:57.360
are gonna be upset no matter where you come out on that.

01:27:57.360 --> 01:28:00.120
Yeah, I like that pressurized debate.

01:28:00.120 --> 01:28:02.320
So how's your view of the freedom of speech

01:28:02.320 --> 01:28:03.680
evolved over the years?

01:28:04.680 --> 01:28:09.680
Um, and now with AI, where the freedom might apply to them,

01:28:13.160 --> 01:28:17.640
not just to the humans, but to the personalized agents

01:28:17.640 --> 01:28:19.080
as you've spoken about them.

01:28:20.120 --> 01:28:21.760
So yeah, I mean, I've probably gotten

01:28:21.760 --> 01:28:23.840
a somewhat more nuanced view just because I think

01:28:23.840 --> 01:28:25.800
that there are, you know, I come at this,

01:28:25.800 --> 01:28:29.240
I'm obviously very pro freedom of expression, right?

01:28:29.240 --> 01:28:31.480
I don't think you build a service like this

01:28:31.480 --> 01:28:33.360
that gives people tools to express themselves

01:28:33.360 --> 01:28:35.200
unless you think that people expressing themselves

01:28:35.200 --> 01:28:36.600
at scale is a good thing, right?

01:28:36.600 --> 01:28:40.280
So I didn't get into this to like try to prevent people

01:28:40.280 --> 01:28:42.360
from expressing anything.

01:28:42.360 --> 01:28:44.440
I like wanna give people tools

01:28:44.440 --> 01:28:46.520
so they can express as much as possible.

01:28:46.520 --> 01:28:49.120
And then I think it's become clear

01:28:49.120 --> 01:28:51.360
that there are certain categories of things

01:28:51.360 --> 01:28:53.280
that we've talked about that I think

01:28:53.280 --> 01:28:55.760
almost everyone accepts are bad and that no one wants

01:28:55.760 --> 01:28:58.600
and that are illegal even in countries like the US

01:28:58.600 --> 01:29:01.600
where, you know, you have the first amendment

01:29:01.640 --> 01:29:04.360
that's very protective of enabling speech.

01:29:04.360 --> 01:29:06.240
It's like, you're still not allowed to, you know,

01:29:06.240 --> 01:29:08.520
do things that are gonna immediately incite violence

01:29:08.520 --> 01:29:10.560
or, you know, violate people's intellectual property

01:29:10.560 --> 01:29:11.400
or things like that.

01:29:11.400 --> 01:29:14.920
So there are those, but then there's also a very active core

01:29:14.920 --> 01:29:18.760
of just active disagreements in society

01:29:18.760 --> 01:29:21.760
where some people may think that something is true or false.

01:29:21.760 --> 01:29:24.840
The other side might think it's the opposite

01:29:24.840 --> 01:29:26.640
or just unsettled, right?

01:29:26.640 --> 01:29:31.120
And those are some of the most difficult to kind of handle.

01:29:31.360 --> 01:29:33.160
Like we've talked about.

01:29:33.160 --> 01:29:38.160
But one of the lessons that I feel like I've learned

01:29:38.360 --> 01:29:43.360
is that a lot of times when you can,

01:29:44.920 --> 01:29:48.840
the best way to handle this stuff more practically

01:29:48.840 --> 01:29:51.400
is not in terms of answering the question

01:29:51.400 --> 01:29:53.040
of should this be allowed?

01:29:53.040 --> 01:29:58.040
But just like, what is the best way to deal

01:29:58.640 --> 01:30:00.680
with someone being a jerk?

01:30:00.720 --> 01:30:05.720
Is the person basically just having a like repeat behavior

01:30:06.840 --> 01:30:11.200
of like causing a lot of issues?

01:30:12.080 --> 01:30:14.720
So looking at it more at that level.

01:30:14.720 --> 01:30:16.800
And it's effect on the broader communities,

01:30:16.800 --> 01:30:19.320
health of the community, health of the state.

01:30:19.320 --> 01:30:21.640
It's tricky though, because like, how do you know

01:30:21.640 --> 01:30:24.880
there could be people that have a very controversial

01:30:24.880 --> 01:30:28.600
viewpoint that turns out to have a positive long-term effect

01:30:28.600 --> 01:30:30.040
on the health of the community

01:30:30.040 --> 01:30:31.680
because it challenges the community to think.

01:30:31.680 --> 01:30:33.360
That's true, absolutely.

01:30:33.360 --> 01:30:36.280
Yeah, no, I think you wanna be careful about that.

01:30:36.280 --> 01:30:38.760
I'm not sure I'm expressing this very clearly.

01:30:40.520 --> 01:30:42.520
Because I certainly agree with your point there.

01:30:42.520 --> 01:30:46.960
And my point isn't that we should not have people

01:30:46.960 --> 01:30:49.240
on our services that are being controversial.

01:30:49.240 --> 01:30:51.600
That's certainly not what I mean to say.

01:30:51.600 --> 01:30:56.600
It's that often I think it's not just looking

01:30:56.600 --> 01:30:59.600
at a specific example of speech

01:30:59.600 --> 01:31:02.880
that it's most effective to handle this stuff.

01:31:02.880 --> 01:31:05.680
And I think often you don't wanna make specific

01:31:05.680 --> 01:31:09.240
binary decisions of kind of this is allowed or this isn't.

01:31:09.240 --> 01:31:11.840
I mean, we talked about, you know, it's fact checking

01:31:11.840 --> 01:31:14.080
or Twitter's community voices thing.

01:31:14.080 --> 01:31:15.280
I think that's another good example.

01:31:15.280 --> 01:31:18.520
It's like, it's not a question of is this allowed or not.

01:31:18.520 --> 01:31:20.800
It's just a question of adding more context to the thing.

01:31:20.800 --> 01:31:22.640
And I think that that's helpful.

01:31:22.640 --> 01:31:26.200
So in the context of AI, which is what you were asking about,

01:31:26.200 --> 01:31:30.440
I think there are lots of ways that an AI can be helpful.

01:31:30.440 --> 01:31:33.920
You know, with an AI, it's less about censorship, right?

01:31:33.920 --> 01:31:37.360
Because, and it's more about what is the most productive

01:31:37.360 --> 01:31:38.680
answer to a question.

01:31:39.880 --> 01:31:42.240
You know, there was one case study that I was reviewing

01:31:42.240 --> 01:31:45.280
with the team is someone asked,

01:31:49.160 --> 01:31:53.040
can you explain to me how to 3D print a gun?

01:31:53.040 --> 01:31:58.040
And one proposed response is like,

01:31:58.240 --> 01:31:59.960
no, I can't talk about that, right?

01:31:59.960 --> 01:32:02.560
It's like basically just like shut it down immediately,

01:32:02.560 --> 01:32:03.760
which I think is some of what you see.

01:32:03.760 --> 01:32:05.480
It's like, as a large language model,

01:32:05.480 --> 01:32:08.000
I'm not allowed to talk about, you know, whatever.

01:32:09.240 --> 01:32:11.600
But there's another response which is like, hey,

01:32:11.600 --> 01:32:13.160
you know, I don't think that's a good idea.

01:32:13.160 --> 01:32:17.760
And a lot of countries, including the US 3D printing guns

01:32:17.760 --> 01:32:21.240
is illegal or kind of whatever the factual thing is.

01:32:21.240 --> 01:32:23.760
And it's like, okay, you know, that's actually a respectful

01:32:23.760 --> 01:32:24.800
and informative answer.

01:32:24.800 --> 01:32:27.720
And, you know, I may have not known that specific thing.

01:32:27.720 --> 01:32:31.160
And so there are different ways to handle this

01:32:31.160 --> 01:32:34.720
that I think kind of, you can either,

01:32:34.720 --> 01:32:36.520
you can either assume good intent,

01:32:37.480 --> 01:32:38.680
like maybe the person didn't know,

01:32:38.680 --> 01:32:40.280
and I'm just gonna help educate them.

01:32:40.280 --> 01:32:42.200
Or you could like kind of come at it as like,

01:32:42.200 --> 01:32:44.200
no, I need to shut this thing down immediately, right?

01:32:44.200 --> 01:32:47.000
It's like, I just, I'm not gonna talk about this, like.

01:32:47.960 --> 01:32:51.000
And there may be times where you need to do that,

01:32:51.000 --> 01:32:55.240
but I actually think having a somewhat more

01:32:55.240 --> 01:32:57.880
informative approach where you generally assume

01:32:57.880 --> 01:33:02.360
good intent from people is probably a better balance to be

01:33:02.360 --> 01:33:04.680
on as many things as you can be.

01:33:04.680 --> 01:33:06.120
You're not gonna be able to do that for everything.

01:33:06.120 --> 01:33:09.640
But that, you're kind of asking about how I approach this

01:33:09.640 --> 01:33:13.880
and I'm thinking about this as it relates to AI.

01:33:13.880 --> 01:33:15.960
And I think that that's a big difference

01:33:15.960 --> 01:33:20.560
in kind of how to handle sensitive content

01:33:20.560 --> 01:33:22.000
across these different modes.

01:33:24.040 --> 01:33:26.400
I have to ask, there's rumors you might be working

01:33:26.400 --> 01:33:29.000
on a social network that's text-based,

01:33:29.000 --> 01:33:33.600
that might be a competitor to Twitter, codenamed P92.

01:33:33.600 --> 01:33:38.280
Is there something you could say about those rumors?

01:33:38.280 --> 01:33:40.120
There is a project.

01:33:40.120 --> 01:33:43.800
You know, I've always thought that sort of a text-based

01:33:43.800 --> 01:33:45.680
kind of information utility

01:33:46.600 --> 01:33:50.800
is just a really important thing to society.

01:33:50.800 --> 01:33:54.160
And for whatever reason, I feel like Twitter

01:33:54.160 --> 01:33:56.600
has not lived up to what I would have thought

01:33:56.600 --> 01:33:58.360
its full potential should be.

01:33:58.360 --> 01:33:59.640
And I think that the current, you know,

01:33:59.640 --> 01:34:00.880
I think Elon thinks that, right?

01:34:00.880 --> 01:34:02.840
And that's probably one of the reasons why he bought it.

01:34:02.840 --> 01:34:07.840
And I do know there are ways to consider

01:34:10.040 --> 01:34:11.480
alternative approaches to this.

01:34:11.480 --> 01:34:14.440
And one that I think is potentially interesting

01:34:15.680 --> 01:34:17.400
is this open and federated approach

01:34:17.400 --> 01:34:18.760
where you're seeing with Mastodon,

01:34:18.760 --> 01:34:21.200
I mean, you're seeing that a little bit with Blue Sky.

01:34:21.200 --> 01:34:25.960
And I think that it's possible that something

01:34:25.960 --> 01:34:28.320
that meld some of those ideas

01:34:28.320 --> 01:34:31.320
with the graph and identity system

01:34:31.320 --> 01:34:34.160
that people have already cultivated on Instagram

01:34:34.160 --> 01:34:38.720
could be a kind of very welcome contribution to that space.

01:34:38.720 --> 01:34:40.360
But I don't know, we work on a lot of things

01:34:40.360 --> 01:34:41.200
all the time though, too.

01:34:41.200 --> 01:34:43.520
So I don't wanna get ahead of myself.

01:34:43.520 --> 01:34:45.800
I mean, we have projects that explore

01:34:45.800 --> 01:34:46.720
a lot of different things.

01:34:46.720 --> 01:34:49.040
And this is certainly one that I think

01:34:49.040 --> 01:34:50.640
could be interesting, but.

01:34:50.640 --> 01:34:54.680
So what's the release, the launch date of that again?

01:34:54.680 --> 01:34:58.520
Or what's the official website and...

01:34:58.520 --> 01:34:59.440
Well, we don't have that yet.

01:34:59.440 --> 01:35:00.280
Oh, okay.

01:35:00.280 --> 01:35:01.640
But I...

01:35:01.640 --> 01:35:02.480
All right.

01:35:02.480 --> 01:35:03.840
And look, I mean, I don't know exactly

01:35:03.840 --> 01:35:05.240
how this is gonna turn out.

01:35:05.240 --> 01:35:06.560
I mean, what I can say is, yeah,

01:35:06.560 --> 01:35:08.960
there's some people working on this, right?

01:35:08.960 --> 01:35:10.040
I think that there's something there

01:35:10.040 --> 01:35:13.720
that's interesting to explore.

01:35:13.720 --> 01:35:15.480
So if you look at, it'd be interesting

01:35:15.480 --> 01:35:19.280
to just ask this question and throw Twitter into the mix.

01:35:19.280 --> 01:35:22.120
At the landscape of social networks,

01:35:22.120 --> 01:35:26.720
that is Facebook, that is Instagram, that is WhatsApp.

01:35:28.120 --> 01:35:31.200
And then think of a text-based social network.

01:35:31.200 --> 01:35:32.400
When you look at that landscape,

01:35:32.400 --> 01:35:35.120
what are the interesting differences to you?

01:35:35.120 --> 01:35:37.960
Why do we have these different flavors?

01:35:37.960 --> 01:35:39.760
And what are the needs?

01:35:39.760 --> 01:35:40.680
What are the use cases?

01:35:40.680 --> 01:35:41.520
What are the products?

01:35:41.520 --> 01:35:43.320
What is the aspect of them

01:35:43.320 --> 01:35:45.760
that create a fulfilling human experience

01:35:45.760 --> 01:35:47.640
and a connection between humans

01:35:47.640 --> 01:35:49.320
that is somehow distinct?

01:35:49.320 --> 01:35:51.920
Well, I think text is very accessible

01:35:51.920 --> 01:35:54.440
for people to transmit ideas

01:35:54.440 --> 01:35:56.760
and to have back and forth exchanges.

01:35:57.960 --> 01:36:02.960
So it, I think, ends up being a good format for discussion,

01:36:04.360 --> 01:36:06.040
in a lot of ways, uniquely good, right?

01:36:06.040 --> 01:36:09.040
If you look at some of the other formats

01:36:09.040 --> 01:36:11.160
or other networks that are focused on one type of content,

01:36:11.160 --> 01:36:13.360
like TikTok is obviously huge, right?

01:36:13.360 --> 01:36:15.520
And there are comments on TikTok,

01:36:15.520 --> 01:36:19.520
but I think the architecture of the service

01:36:19.520 --> 01:36:21.400
is very clearly that you have the video

01:36:21.400 --> 01:36:25.040
as the primary thing, and there's comments after that.

01:36:28.960 --> 01:36:32.080
But I think one of the unique pieces

01:36:32.080 --> 01:36:35.080
of having text-based comments,

01:36:35.080 --> 01:36:39.240
the content is that the comments can also be first class.

01:36:39.240 --> 01:36:43.040
And that makes it so that conversations can just filter

01:36:43.040 --> 01:36:45.240
and fork into all these different directions

01:36:45.240 --> 01:36:47.640
and in a way that can be super useful.

01:36:47.640 --> 01:36:48.840
So I think there's a lot of things

01:36:48.840 --> 01:36:50.840
that are really awesome about the experience.

01:36:50.840 --> 01:36:52.680
It just always struck me,

01:36:52.680 --> 01:36:54.600
I always thought that Twitter

01:36:54.600 --> 01:36:56.320
should have a billion people using it,

01:36:56.320 --> 01:37:00.480
or whatever the thing is that basically

01:37:00.480 --> 01:37:01.640
ends up being in that space.

01:37:01.640 --> 01:37:03.920
And for whatever combination of reasons,

01:37:04.080 --> 01:37:07.600
again, these companies are complex organisms

01:37:07.600 --> 01:37:10.840
and it's very hard to diagnose this stuff from the outside.

01:37:10.840 --> 01:37:15.840
Why doesn't Twitter, why doesn't a text-based comment

01:37:15.840 --> 01:37:18.840
as a first citizen-based social network

01:37:18.840 --> 01:37:20.520
have a billion users?

01:37:20.520 --> 01:37:23.000
Well, I just think it's hard to build these companies.

01:37:23.000 --> 01:37:26.920
So it's not that every idea

01:37:26.920 --> 01:37:29.040
automatically goes and gets a billion people,

01:37:29.040 --> 01:37:30.880
it's just that I think that that idea

01:37:30.880 --> 01:37:33.480
coupled with good execution should get there.

01:37:34.400 --> 01:37:37.800
But I mean, look, we hit certain thresholds over time

01:37:37.800 --> 01:37:41.760
where we kind of plateaued early on

01:37:41.760 --> 01:37:43.120
and it wasn't clear that we were ever gonna reach

01:37:43.120 --> 01:37:44.800
a hundred million people on Facebook.

01:37:44.800 --> 01:37:48.320
And then we got really good at dialing in

01:37:48.320 --> 01:37:50.840
internationalization and helping the service

01:37:50.840 --> 01:37:52.240
grow in different countries.

01:37:52.240 --> 01:37:55.440
And that was like a whole competence

01:37:55.440 --> 01:37:59.480
that we needed to develop and helping people

01:37:59.480 --> 01:38:01.560
basically spread the service to their friends.

01:38:01.560 --> 01:38:03.760
That was one of the things, once we got very good at that,

01:38:03.800 --> 01:38:05.880
that was one of the things that made me feel like,

01:38:05.880 --> 01:38:08.760
hey, if Instagram joined us early on,

01:38:08.760 --> 01:38:10.280
then I felt like we could help grow that quickly.

01:38:10.280 --> 01:38:11.400
And same with WhatsApp.

01:38:11.400 --> 01:38:13.520
And I think that that's sort of been a core competence

01:38:13.520 --> 01:38:16.200
that we've developed and been able to execute on.

01:38:16.200 --> 01:38:17.040
And others have too, right?

01:38:17.040 --> 01:38:19.000
I mean, ByteDance obviously have done

01:38:19.000 --> 01:38:20.440
a very good job with TikTok

01:38:20.440 --> 01:38:23.880
and have reached more than a billion people there.

01:38:23.880 --> 01:38:26.640
But it's certainly not automatic, right?

01:38:26.640 --> 01:38:31.040
I think you need a certain level of execution

01:38:31.040 --> 01:38:31.960
to basically get there.

01:38:31.960 --> 01:38:34.440
And I think for whatever reason,

01:38:34.440 --> 01:38:36.200
I think Twitter has this great idea

01:38:36.200 --> 01:38:38.360
and sort of magic in the service,

01:38:39.520 --> 01:38:43.960
but they just haven't kind of cracked that piece yet.

01:38:43.960 --> 01:38:45.720
And I think that that's made it so that

01:38:45.720 --> 01:38:46.840
you're seeing all these other things,

01:38:46.840 --> 01:38:51.400
whether it's Mastodon or Blue Sky,

01:38:51.400 --> 01:38:54.360
that I think are maybe just different cuts

01:38:54.360 --> 01:38:55.200
at the same thing.

01:38:55.200 --> 01:38:57.240
But I think through the last generation

01:38:57.240 --> 01:39:00.120
of social media overall,

01:39:00.120 --> 01:39:01.480
one of the interesting experiments

01:39:01.480 --> 01:39:04.240
that I think should get run at larger scale

01:39:04.240 --> 01:39:05.280
is what happens if there's

01:39:05.280 --> 01:39:07.080
somewhat more decentralized control.

01:39:07.080 --> 01:39:10.160
And if it's like the stack is more open throughout.

01:39:10.160 --> 01:39:13.360
And I've just been pretty fascinated by that

01:39:13.360 --> 01:39:14.680
and seeing how that works.

01:39:16.240 --> 01:39:20.360
To some degree, end-to-end encryption on WhatsApp

01:39:20.360 --> 01:39:22.400
and as we bring it to other services,

01:39:22.400 --> 01:39:24.960
provides an element of it because it pushes

01:39:24.960 --> 01:39:26.800
the service really out to the edges.

01:39:26.840 --> 01:39:31.840
I mean, the server part of this that we run for WhatsApp

01:39:31.840 --> 01:39:34.120
is relatively very thin compared to what we do

01:39:34.120 --> 01:39:36.280
on Facebook or Instagram.

01:39:36.280 --> 01:39:39.680
And much more of the complexity is how the apps

01:39:39.680 --> 01:39:42.240
kind of negotiate with each other to pass information

01:39:42.240 --> 01:39:44.400
in a fully end-to-end encrypted way.

01:39:45.520 --> 01:39:48.040
But I don't know, I think that that is a good model.

01:39:48.040 --> 01:39:50.160
I think it puts more power in individuals' hands

01:39:50.160 --> 01:39:51.760
and there are a lot of benefits of it

01:39:51.760 --> 01:39:53.360
if you can make it happen.

01:39:53.360 --> 01:39:55.400
Again, this is all pretty speculative.

01:39:55.400 --> 01:39:58.680
I mean, I think that it's hard from the outside

01:39:58.680 --> 01:40:01.120
to know why anything does or doesn't work

01:40:01.120 --> 01:40:02.800
until you kind of take a run at it.

01:40:02.800 --> 01:40:07.280
And so I think it's kind of an interesting thing

01:40:07.280 --> 01:40:09.120
to experiment with, but I don't really know

01:40:09.120 --> 01:40:10.760
where this one's gonna go.

01:40:10.760 --> 01:40:12.920
So since we were talking about Twitter,

01:40:14.320 --> 01:40:19.320
Elon Musk had what I think a few harsh words

01:40:19.480 --> 01:40:21.040
that I wish he didn't say.

01:40:21.080 --> 01:40:26.080
So let me ask, in the hope in the name of camaraderie,

01:40:26.320 --> 01:40:29.040
what do you think Elon is doing well with Twitter?

01:40:29.040 --> 01:40:33.040
And what, as a person who has run for a long time,

01:40:33.040 --> 01:40:38.040
you, social networks, Facebook, Instagram, WhatsApp,

01:40:39.640 --> 01:40:41.600
what can he do better?

01:40:41.600 --> 01:40:45.240
What can he improve on that text-based social network?

01:40:45.240 --> 01:40:46.760
Gosh, it's always very difficult

01:40:46.760 --> 01:40:50.160
to offer specific critiques from the outside

01:40:50.160 --> 01:40:51.560
before you get into this.

01:40:51.560 --> 01:40:53.880
Because I think one thing that I've learned

01:40:53.880 --> 01:40:57.520
is that everyone has opinions on what you should do.

01:40:57.520 --> 01:41:00.000
And like running the company,

01:41:00.000 --> 01:41:02.960
you see a lot of specific nuances on things

01:41:02.960 --> 01:41:04.640
that are not apparent externally.

01:41:04.640 --> 01:41:09.640
And I often think that some of the discourse around us

01:41:12.240 --> 01:41:15.680
would be, could be better if there was more

01:41:16.960 --> 01:41:19.400
kind of space for acknowledging that there's certain things

01:41:19.400 --> 01:41:21.520
that we're seeing internally that guide what we're doing.

01:41:21.520 --> 01:41:23.560
But I don't know.

01:41:23.560 --> 01:41:26.360
I mean, since you asked what is going well,

01:41:33.480 --> 01:41:38.480
I do think that Elon led a push early on

01:41:39.280 --> 01:41:41.400
to make Twitter a lot leaner.

01:41:41.400 --> 01:41:46.400
And I think that you can agree or disagree

01:41:47.400 --> 01:41:51.640
with exactly all the tactics and how he did that.

01:41:51.640 --> 01:41:55.360
Obviously, every leader has their own style

01:41:55.360 --> 01:41:58.520
for if you need to make dramatic changes for that,

01:41:58.520 --> 01:41:59.920
how you're gonna execute it.

01:42:01.000 --> 01:42:04.600
But a lot of the specific principles that he pushed on

01:42:05.880 --> 01:42:09.400
around basically trying to make the organization

01:42:09.400 --> 01:42:12.800
more technical, around decreasing the distance

01:42:12.800 --> 01:42:16.320
between engineers at the company and him,

01:42:16.320 --> 01:42:18.000
like fewer layers of management.

01:42:19.920 --> 01:42:23.240
I think that those were generally good changes.

01:42:23.240 --> 01:42:26.200
And I'm also, I also think that it was probably good

01:42:26.200 --> 01:42:28.600
for the industry that he made those changes

01:42:28.600 --> 01:42:30.600
because my sense is that there were a lot of other people

01:42:30.600 --> 01:42:33.200
who thought that those were good changes,

01:42:33.200 --> 01:42:38.200
but who may have been a little shy about doing them.

01:42:40.000 --> 01:42:44.800
And I think he, just in my conversations with other founders

01:42:45.240 --> 01:42:47.440
and how people have reacted to the things that we've done,

01:42:47.440 --> 01:42:49.920
what I've heard from a lot of folks is just,

01:42:49.920 --> 01:42:53.800
hey, when someone like you, when I wrote the letter

01:42:53.800 --> 01:42:55.640
outlining the organizational changes

01:42:55.640 --> 01:42:58.040
that I wanted to make back in March,

01:42:58.040 --> 01:43:00.200
and when people see what Elon is doing,

01:43:00.200 --> 01:43:05.200
I think that that gives people the ability

01:43:05.200 --> 01:43:07.960
to think through how to shape their organizations

01:43:07.960 --> 01:43:12.960
in a way that hopefully can be good for the industry

01:43:13.600 --> 01:43:16.040
and make all these companies more productive over time.

01:43:16.040 --> 01:43:19.440
So I think that that was one where I think he was

01:43:20.480 --> 01:43:23.560
quite ahead of a bunch of the other companies on.

01:43:23.560 --> 01:43:27.560
And what he was doing there, again, from the outside,

01:43:27.560 --> 01:43:29.760
very hard to know, it's like, okay, did he cut too much?

01:43:29.760 --> 01:43:31.360
Did he not cut enough, whatever.

01:43:31.360 --> 01:43:34.960
I don't think it's like my place to opine on that.

01:43:34.960 --> 01:43:37.680
And you asked for a positive framing

01:43:37.680 --> 01:43:40.840
of the question of what do I admire?

01:43:40.840 --> 01:43:41.960
What do I think went well?

01:43:41.960 --> 01:43:45.360
But I think that like certainly his actions

01:43:47.200 --> 01:43:49.760
led me and I think a lot of other folks in the industry

01:43:49.760 --> 01:43:53.600
to think about, hey, are we kind of doing this

01:43:53.600 --> 01:43:54.640
as much as we should?

01:43:54.640 --> 01:43:57.120
Like could we make our companies better

01:43:57.120 --> 01:43:59.360
by pushing on some of these same principles?

01:43:59.360 --> 01:44:01.920
Well, the two of you are in the top of the world

01:44:01.920 --> 01:44:03.960
in terms of leading the development of tech,

01:44:03.960 --> 01:44:08.960
and I wish there was more both way camaraderie and kindness,

01:44:09.920 --> 01:44:13.440
more love in the world, because love is the answer.

01:44:14.440 --> 01:44:19.200
But let me ask on a point of efficiency.

01:44:19.200 --> 01:44:22.600
You recently announced multiple stages of layoffs at Metta.

01:44:23.760 --> 01:44:27.600
What are the most painful aspects of this process

01:44:27.600 --> 01:44:31.960
given for the individuals, the painful effects it has

01:44:31.960 --> 01:44:32.800
on those people's lives?

01:44:32.800 --> 01:44:36.160
Yeah, I mean, that's it, and that's it.

01:44:36.160 --> 01:44:41.160
I mean, you basically have a significant number of people

01:44:41.280 --> 01:44:45.680
who this is just not the end of their time at Metta

01:44:45.680 --> 01:44:49.840
that they or I would have hoped for

01:44:49.840 --> 01:44:51.240
when they joined the company.

01:44:52.640 --> 01:44:56.200
And I mean, running a company,

01:44:56.200 --> 01:45:00.240
people are constantly joining and leaving the company

01:45:00.240 --> 01:45:03.880
for different directions, but for different reasons.

01:45:03.880 --> 01:45:08.880
But layoffs are uniquely challenging and tough

01:45:10.440 --> 01:45:14.840
in that you have a lot of people leaving for reasons

01:45:14.840 --> 01:45:17.600
that aren't connected to their own performance

01:45:17.600 --> 01:45:22.080
or the culture not being a fit at that point.

01:45:22.080 --> 01:45:27.080
It's really just, it's a kind of strategy decision

01:45:27.080 --> 01:45:29.280
and sometimes financially required,

01:45:29.960 --> 01:45:33.760
but not fully in our case,

01:45:33.760 --> 01:45:35.840
and especially on the changes that we made this year,

01:45:35.840 --> 01:45:38.200
a lot of it was more kind of culturally

01:45:38.200 --> 01:45:40.560
and strategically driven by this push

01:45:40.560 --> 01:45:44.440
where I wanted us to become a stronger technology company

01:45:44.440 --> 01:45:47.640
with more of a focus on building more technical

01:45:47.640 --> 01:45:50.000
and more of a focus on building

01:45:50.000 --> 01:45:52.200
higher quality products faster.

01:45:52.200 --> 01:45:53.680
And I just view the external world

01:45:53.680 --> 01:45:56.040
as quite volatile right now.

01:45:56.040 --> 01:46:00.360
And I wanted to make sure that we had a stable position

01:46:00.360 --> 01:46:01.680
to be able to continue investing

01:46:01.680 --> 01:46:04.920
in these long-term ambitious projects

01:46:04.920 --> 01:46:07.640
that we have around continuing to push AI forward

01:46:07.640 --> 01:46:10.280
and continuing to push forward all the metaverse work.

01:46:10.280 --> 01:46:15.080
And in order to do that in light of the pretty big thrash

01:46:15.080 --> 01:46:18.080
that we had seen over the last 18 months,

01:46:18.080 --> 01:46:21.440
some of it macroeconomic induced,

01:46:21.440 --> 01:46:24.120
some of it competitively induced,

01:46:24.120 --> 01:46:27.360
some of it just because of bad decisions

01:46:27.360 --> 01:46:28.880
or things that we got wrong.

01:46:30.280 --> 01:46:32.320
I don't know, I just, I decided that we needed to get

01:46:32.320 --> 01:46:35.280
to a point where we were a lot leaner and,

01:46:35.280 --> 01:46:36.680
but look, I mean, but then, okay,

01:46:36.680 --> 01:46:38.720
it's one thing to do that, to like decide that

01:46:38.720 --> 01:46:40.360
at a high level, then the question is,

01:46:40.360 --> 01:46:42.840
how do you execute that as compassionately as possible?

01:46:42.840 --> 01:46:44.400
And there's no good way.

01:46:46.000 --> 01:46:47.400
There's no perfect way for sure.

01:46:47.400 --> 01:46:49.320
And it's gonna be tough no matter what,

01:46:49.320 --> 01:46:53.440
but as a leadership team here,

01:46:53.440 --> 01:46:55.320
we've certainly spent a lot of time just thinking,

01:46:55.320 --> 01:46:58.400
okay, given that this is a thing that sucks,

01:46:58.400 --> 01:47:01.000
like what is the most compassionate way

01:47:01.000 --> 01:47:01.920
that we can do this?

01:47:01.920 --> 01:47:05.200
And that's what we've tried to do.

01:47:05.200 --> 01:47:08.480
And you mentioned there's an increased focus

01:47:08.480 --> 01:47:13.160
on engineering, on tech, so the technology teams,

01:47:13.160 --> 01:47:17.000
tech-focused teams, on building products, that.

01:47:17.840 --> 01:47:19.320
Yeah, I mean, I wanted to,

01:47:20.320 --> 01:47:24.320
I want to empower engineers more,

01:47:24.320 --> 01:47:27.120
the people who are building things, the technical teams.

01:47:29.920 --> 01:47:32.840
Part of that is making sure that the people

01:47:32.840 --> 01:47:34.680
who are building things aren't just at like

01:47:34.680 --> 01:47:36.280
the leaf nodes of the organization.

01:47:36.280 --> 01:47:39.480
I don't want like eight levels of management

01:47:39.480 --> 01:47:41.760
and then the people actually doing the work.

01:47:41.760 --> 01:47:43.760
So we made changes to make it so that

01:47:43.760 --> 01:47:45.520
you have individual contributor engineers

01:47:45.520 --> 01:47:47.880
reporting at almost every level up the stack.

01:47:47.880 --> 01:47:49.960
Which I think is important because you're running a company,

01:47:49.960 --> 01:47:53.160
one of the big questions is latency

01:47:53.160 --> 01:47:55.400
of information that you get.

01:47:55.400 --> 01:47:58.280
We talked about this a bit earlier in terms of

01:47:58.280 --> 01:48:02.280
kind of the joy of the feedback that you get

01:48:02.280 --> 01:48:04.520
doing something like Jiu-Jitsu compared to

01:48:04.520 --> 01:48:06.280
running a long-term project.

01:48:06.280 --> 01:48:08.840
But I actually think part of the art of running a company

01:48:08.840 --> 01:48:12.120
is trying to constantly re-engineer it

01:48:12.120 --> 01:48:13.840
so that your feedback loops get shorter

01:48:13.840 --> 01:48:14.920
so you can learn faster.

01:48:14.920 --> 01:48:17.080
And part of the way that you do that is by

01:48:17.200 --> 01:48:19.000
I kind of think that every layer that you have

01:48:19.000 --> 01:48:23.000
in the organization means that information

01:48:23.000 --> 01:48:25.880
might not need to get reviewed before it goes to you.

01:48:25.880 --> 01:48:28.360
And I think making it so that the people doing the work

01:48:28.360 --> 01:48:30.600
are as close as possible to you as possible

01:48:30.600 --> 01:48:33.000
is pretty important.

01:48:33.000 --> 01:48:34.120
So there's that.

01:48:34.120 --> 01:48:36.760
And I think over time, companies just build up

01:48:36.760 --> 01:48:40.280
very large support functions that are not doing

01:48:40.280 --> 01:48:41.960
the kind of core technical work.

01:48:41.960 --> 01:48:43.880
And those functions are very important,

01:48:43.880 --> 01:48:45.960
but I think having them in the right proportion

01:48:45.960 --> 01:48:46.800
is important.

01:48:46.800 --> 01:48:51.800
And if you try to do good work, but you don't have

01:48:52.680 --> 01:48:57.680
the right marketing team or the right legal advice,

01:48:57.680 --> 01:49:00.520
like you're gonna make some pretty big blunders.

01:49:00.520 --> 01:49:05.520
But at the same time, if you just have too big of things

01:49:07.560 --> 01:49:09.560
and some of these support roles,

01:49:09.560 --> 01:49:13.760
then that might make it so that things are just move a lot.

01:49:14.440 --> 01:49:17.440
Maybe you're too conservative or you move a lot slower

01:49:17.440 --> 01:49:19.440
than you should otherwise.

01:49:19.440 --> 01:49:21.040
Those are just examples.

01:49:21.040 --> 01:49:21.880
But it's-

01:49:21.880 --> 01:49:22.720
But-

01:49:22.720 --> 01:49:24.640
How do you find that balance that's really tough?

01:49:24.640 --> 01:49:27.040
Yeah, but it's a constant equilibrium

01:49:27.040 --> 01:49:28.720
that you're searching for.

01:49:28.720 --> 01:49:30.680
Yeah, how many managers to have?

01:49:30.680 --> 01:49:33.320
What are the pros and cons of managers?

01:49:33.320 --> 01:49:35.680
Well, I mean, I believe a lot in management.

01:49:35.680 --> 01:49:36.520
I think there are some people who think

01:49:36.520 --> 01:49:38.560
that it doesn't matter as much, but look,

01:49:38.560 --> 01:49:40.880
I mean, we have a lot of the younger people at the company

01:49:40.880 --> 01:49:41.720
for the most part,

01:49:41.720 --> 01:49:43.680
we have a lot of the younger people at the company

01:49:43.680 --> 01:49:45.160
for whom this is their first job

01:49:45.160 --> 01:49:48.360
and people need to grow and learn in their career.

01:49:48.360 --> 01:49:50.280
And I think that all that stuff is important,

01:49:50.280 --> 01:49:52.520
but here's one mathematical way to look at it.

01:49:54.160 --> 01:49:55.520
At the beginning of this,

01:49:58.440 --> 01:50:00.600
I asked our people team,

01:50:00.600 --> 01:50:03.800
what was the average number of reports that a manager had?

01:50:03.800 --> 01:50:08.480
And I think it was around three, maybe three to four,

01:50:08.480 --> 01:50:10.040
but closer to three.

01:50:10.040 --> 01:50:14.120
I was like, wow, like a manager can best practices

01:50:14.120 --> 01:50:17.160
that person can manage, seven or eight people.

01:50:18.240 --> 01:50:20.120
But there was a reason why it was closer to three.

01:50:20.120 --> 01:50:22.720
It was because we were growing so quickly, right?

01:50:22.720 --> 01:50:25.920
And when you're hiring so many people so quickly,

01:50:25.920 --> 01:50:28.280
then that means that you need managers

01:50:28.280 --> 01:50:30.320
who have capacity to onboard new people.

01:50:31.240 --> 01:50:32.560
And also if you have a new manager,

01:50:32.560 --> 01:50:34.920
you may not wanna have them have seven direct reports

01:50:34.920 --> 01:50:36.880
immediately, because you want them to ramp up.

01:50:36.880 --> 01:50:38.960
But the thing is going forward,

01:50:38.960 --> 01:50:41.640
I don't want us to actually hire that many people

01:50:41.640 --> 01:50:42.640
that quickly, right?

01:50:42.640 --> 01:50:44.960
So I actually think we'll just do better work

01:50:44.960 --> 01:50:46.000
if we have more constraints

01:50:46.000 --> 01:50:48.840
and we're leaner as an organization.

01:50:48.840 --> 01:50:50.600
So in a world where we're not adding

01:50:50.600 --> 01:50:51.880
so many people as quickly,

01:50:52.840 --> 01:50:55.000
is it as valuable to have a lot of managers

01:50:55.000 --> 01:50:56.920
who have extra capacity waiting for new people?

01:50:56.920 --> 01:50:57.760
No, right?

01:50:57.760 --> 01:51:01.600
So now we can sort of defragment the organization

01:51:01.600 --> 01:51:03.880
and get to a place where the average is closer

01:51:03.880 --> 01:51:05.880
to that seven or eight.

01:51:05.920 --> 01:51:08.720
And it just ends up being a somewhat more

01:51:08.720 --> 01:51:10.880
kind of compact management structure,

01:51:10.880 --> 01:51:14.160
which decreases the latency on information

01:51:14.160 --> 01:51:15.320
going up and down the chain

01:51:15.320 --> 01:51:17.680
and I think empowers people more.

01:51:17.680 --> 01:51:19.480
But I mean, that's an example that I think

01:51:19.480 --> 01:51:23.040
it doesn't kind of undervalue the importance of management

01:51:23.040 --> 01:51:28.040
and the kind of the personal growth or coaching

01:51:28.760 --> 01:51:30.520
that people need in order to do their jobs well.

01:51:30.520 --> 01:51:32.040
It's just, I think, realistically,

01:51:32.040 --> 01:51:34.200
we're just not gonna hire as many people going forward.

01:51:34.200 --> 01:51:36.360
So I think that you need a different structure.

01:51:36.360 --> 01:51:41.120
This whole incredible hierarchy and network of humans

01:51:41.120 --> 01:51:43.080
that make up a company is fascinating.

01:51:43.080 --> 01:51:44.040
Oh, yeah.

01:51:44.040 --> 01:51:45.440
Yeah.

01:51:45.440 --> 01:51:48.400
How do you hire great teams?

01:51:48.400 --> 01:51:49.880
How do you hire great,

01:51:49.880 --> 01:51:52.840
now with the focus on engineering and technical teams,

01:51:52.840 --> 01:51:55.840
how do you hire great engineers

01:51:55.840 --> 01:51:58.580
and great members of technical teams?

01:51:59.480 --> 01:52:01.360
Well, you're asking how you select

01:52:01.360 --> 01:52:03.000
or how you attract them?

01:52:03.000 --> 01:52:05.680
Both, but select, I think.

01:52:05.680 --> 01:52:09.920
I think attract is work on cool stuff and have a vision.

01:52:09.920 --> 01:52:10.760
I think the stuff we're talking about.

01:52:10.760 --> 01:52:12.600
I think that's right and have a track record

01:52:12.600 --> 01:52:14.080
that people think you're actually gonna be able to do it.

01:52:14.080 --> 01:52:18.600
Yeah, to me, the select seems like more of the art form,

01:52:18.600 --> 01:52:20.360
more of the tricky thing.

01:52:20.360 --> 01:52:23.880
Do you select the people that fit the culture

01:52:23.880 --> 01:52:26.840
and can get integrated the most effectively and so on?

01:52:26.840 --> 01:52:29.440
And maybe, especially when they're young,

01:52:29.440 --> 01:52:34.440
to see the magic through the resume,

01:52:35.560 --> 01:52:37.240
through the paperwork and all this kind of stuff,

01:52:37.240 --> 01:52:39.520
to see that there's a special human there

01:52:39.520 --> 01:52:42.920
that would do incredible work.

01:52:42.920 --> 01:52:46.520
So there are lots of different cuts on this question.

01:52:46.520 --> 01:52:49.920
I mean, I think when an organization is growing quickly,

01:52:49.920 --> 01:52:53.440
one of the big questions that teams face

01:52:53.440 --> 01:52:55.840
is do I hire this person who's in front of me now

01:52:55.840 --> 01:52:57.520
because they seem good,

01:52:57.520 --> 01:53:01.400
or do I hold out to get someone who's even better?

01:53:01.400 --> 01:53:06.400
And the heuristic that I always focused on for myself

01:53:06.960 --> 01:53:11.200
and my own kind of direct hiring that I think works

01:53:11.200 --> 01:53:13.800
when you recurse it through the organization

01:53:13.800 --> 01:53:16.480
is that you should only hire someone to be on your team

01:53:16.480 --> 01:53:18.280
if you would be happy working for them

01:53:18.280 --> 01:53:19.880
in an alternate universe.

01:53:19.880 --> 01:53:22.080
And saying that that kind of works.

01:53:22.080 --> 01:53:24.560
And that's basically how I've tried to build my team.

01:53:25.240 --> 01:53:28.840
You know, I'm not in a rush to not be running the company,

01:53:28.840 --> 01:53:30.360
but I think in an alternate universe

01:53:30.360 --> 01:53:32.400
where one of these other folks was running the company,

01:53:32.400 --> 01:53:33.520
I'd be happy to work for them.

01:53:33.520 --> 01:53:35.400
I feel like I'd learn from them.

01:53:35.400 --> 01:53:38.520
I respect their kind of general judgment.

01:53:39.360 --> 01:53:40.600
They're all very insightful.

01:53:40.600 --> 01:53:41.840
They have good values.

01:53:43.240 --> 01:53:47.560
And I think that that gives you some rubric for,

01:53:47.560 --> 01:53:48.920
you can apply that at every layer.

01:53:48.920 --> 01:53:50.400
And I think if you apply that at every layer

01:53:50.400 --> 01:53:51.760
in the organization,

01:53:51.760 --> 01:53:54.000
then you'll have a pretty strong organization.

01:53:55.560 --> 01:53:56.880
Okay.

01:53:56.880 --> 01:53:59.040
In an organization that's not growing as quickly,

01:53:59.040 --> 01:54:01.440
the questions might be a little different though.

01:54:02.440 --> 01:54:05.920
And there, you asked about young people specifically,

01:54:05.920 --> 01:54:07.640
like people out of college.

01:54:07.640 --> 01:54:10.840
And one of the things that we see is,

01:54:10.840 --> 01:54:12.560
it's a pretty basic lesson,

01:54:12.560 --> 01:54:15.240
but like we have a much better sense

01:54:15.240 --> 01:54:16.640
of who the best people are

01:54:16.640 --> 01:54:19.440
who have interned at the company for a couple of months

01:54:19.440 --> 01:54:22.320
than by looking at them at kind of a resume

01:54:22.320 --> 01:54:25.160
or a short interview loop.

01:54:25.160 --> 01:54:27.320
I mean, obviously the in-person feel that you get

01:54:27.320 --> 01:54:29.800
from someone probably tells you more than the resume.

01:54:29.800 --> 01:54:33.600
And you can do some basic skills assessment,

01:54:33.600 --> 01:54:36.400
but a lot of the stuff really just is cultural.

01:54:36.400 --> 01:54:38.520
People thrive in different environments

01:54:40.480 --> 01:54:44.880
and on different teams, even within a specific company.

01:54:44.880 --> 01:54:47.480
And it's like the people who come

01:54:47.480 --> 01:54:50.560
for even a short period of time over a summer,

01:54:50.560 --> 01:54:52.160
who do a great job here,

01:54:53.000 --> 01:54:53.840
you know that they're gonna be great

01:54:53.840 --> 01:54:55.320
if they came and joined full time.

01:54:55.320 --> 01:54:57.480
And that's one of the reasons why we've invested

01:54:57.480 --> 01:55:01.440
so much in internship is basically

01:55:01.440 --> 01:55:03.800
it's a very useful sorting function,

01:55:03.800 --> 01:55:05.200
both for us and for the people

01:55:05.200 --> 01:55:06.840
who wanna try out the company.

01:55:06.840 --> 01:55:07.960
You mentioned in-person,

01:55:07.960 --> 01:55:10.120
what do you think about remote work?

01:55:10.120 --> 01:55:12.160
A topic that's been discussed extensively

01:55:12.160 --> 01:55:14.440
because of the, over the past few years,

01:55:14.440 --> 01:55:15.880
because of the pandemic.

01:55:15.880 --> 01:55:20.000
Yeah, I mean, I think it's a thing that's here to stay,

01:55:21.000 --> 01:55:24.960
but I think that there's value in both, right?

01:55:24.960 --> 01:55:28.320
It's not, you know, I wouldn't wanna run

01:55:28.320 --> 01:55:31.480
a fully remote company yet, at least.

01:55:31.480 --> 01:55:34.240
I think there's an asterisk on that, which is that-

01:55:34.240 --> 01:55:36.280
Some of the other stuff you're working on, yeah.

01:55:36.280 --> 01:55:37.120
Yeah, exactly.

01:55:37.120 --> 01:55:39.920
It's like all the, you know, metaverse work

01:55:39.920 --> 01:55:43.680
and the ability to be, to feel like you're truly present,

01:55:44.520 --> 01:55:45.840
no matter where you are.

01:55:45.840 --> 01:55:48.480
I think once you have that all dialed in,

01:55:48.480 --> 01:55:50.680
then we may, you know, one day reach a point

01:55:50.680 --> 01:55:52.600
where it really just doesn't matter as much

01:55:52.600 --> 01:55:53.880
where you are physically.

01:55:55.640 --> 01:56:00.640
But I don't know, today it still does, right?

01:56:01.960 --> 01:56:04.480
So yeah, for people who,

01:56:04.480 --> 01:56:06.960
there are all these people who have special skills

01:56:06.960 --> 01:56:09.840
and wanna live in a place where we don't have an office.

01:56:09.840 --> 01:56:11.560
Are we better off having them at the company?

01:56:11.560 --> 01:56:12.720
Absolutely, right?

01:56:12.720 --> 01:56:15.320
And are a lot of people who work at the company

01:56:15.400 --> 01:56:17.840
for several years and then, you know,

01:56:17.840 --> 01:56:19.920
build up the relationships internally

01:56:22.040 --> 01:56:23.160
and kind of have the trust

01:56:23.160 --> 01:56:24.960
and have a sense of how the company works.

01:56:24.960 --> 01:56:26.720
Can they go work remotely now if they want

01:56:26.720 --> 01:56:28.000
and still do it as effectively?

01:56:28.000 --> 01:56:30.120
And we've done all these studies that show it's like,

01:56:30.120 --> 01:56:31.520
okay, does that affect their performance?

01:56:31.520 --> 01:56:32.440
It does not.

01:56:33.880 --> 01:56:36.520
But, you know, for the new folks who are joining

01:56:37.520 --> 01:56:40.440
and for people who are earlier in their career

01:56:40.440 --> 01:56:43.080
and, you know, need to learn how to solve certain problems

01:56:43.080 --> 01:56:45.040
and need to get ramped up on the culture.

01:56:46.360 --> 01:56:48.360
You know, when you're working through

01:56:48.360 --> 01:56:49.760
really complicated problems

01:56:49.760 --> 01:56:51.480
where you don't just wanna sit in the,

01:56:51.480 --> 01:56:52.880
you don't just want the formal meeting,

01:56:52.880 --> 01:56:55.000
but you wanna be able to like brainstorm

01:56:55.000 --> 01:56:56.440
when you're walking in the hallway together

01:56:56.440 --> 01:56:57.440
after the meeting.

01:56:58.800 --> 01:57:00.720
I don't know, it's like we just haven't replaced

01:57:00.720 --> 01:57:05.720
the kind of in-person dynamics there yet

01:57:07.280 --> 01:57:09.680
with anything remote yet, so.

01:57:09.680 --> 01:57:11.240
Yeah, there's a magic to the in-person

01:57:11.240 --> 01:57:13.560
that we'll talk about this a little bit more,

01:57:13.560 --> 01:57:15.640
but I'm really excited by the possibilities

01:57:15.640 --> 01:57:17.480
in the next two years in virtual reality

01:57:17.480 --> 01:57:19.960
and mixed reality that are possible

01:57:19.960 --> 01:57:21.840
with high resolution scans.

01:57:21.840 --> 01:57:26.840
I mean, I, as a person who loves in-person interaction,

01:57:27.200 --> 01:57:29.600
like these podcasts in person,

01:57:29.600 --> 01:57:33.480
it would be incredible to achieve the level of realism

01:57:33.480 --> 01:57:35.200
I've gotten the chance to witness.

01:57:35.200 --> 01:57:38.280
But let me ask about that.

01:57:38.280 --> 01:57:39.120
Yeah.

01:57:39.120 --> 01:57:44.120
You got a chance to look at the Quest 3 headset

01:57:44.160 --> 01:57:45.600
and it is amazing.

01:57:47.000 --> 01:57:49.560
You've announced it.

01:57:49.560 --> 01:57:52.800
It's, you'll get some more details in the fall,

01:57:52.800 --> 01:57:53.720
maybe release in the fall.

01:57:53.720 --> 01:57:54.960
When is it getting released again?

01:57:54.960 --> 01:57:56.280
I forgot you mentioned it to me.

01:57:56.280 --> 01:57:57.840
We'll give more details at Kinect,

01:57:57.840 --> 01:57:59.080
but it's coming this fall.

01:57:59.080 --> 01:57:59.920
Okay.

01:58:01.240 --> 01:58:06.160
So it's priced at 499.

01:58:07.160 --> 01:58:09.600
What features are you most excited about there?

01:58:09.600 --> 01:58:11.280
There are basically two big new things

01:58:11.280 --> 01:58:14.320
that we've added to Quest 3 over Quest 2.

01:58:14.320 --> 01:58:17.240
The first is high resolution mixed reality.

01:58:18.560 --> 01:58:22.680
And the basic idea here is that

01:58:22.680 --> 01:58:25.000
you can think about virtual reality as

01:58:25.000 --> 01:58:28.960
you have the headset and like all the pixels are virtual

01:58:28.960 --> 01:58:32.120
and you're basically like immersed in a different world.

01:58:32.120 --> 01:58:35.680
Mixed reality is where you see the physical world around you

01:58:35.680 --> 01:58:37.240
and you can place virtual objects in it.

01:58:37.240 --> 01:58:40.160
Whether that's a screen to watch a movie

01:58:40.160 --> 01:58:42.440
or a projection of your virtual desktop,

01:58:42.440 --> 01:58:44.800
or you're playing a game where like zombies

01:58:44.800 --> 01:58:47.600
are coming out through the wall and you need to shoot them.

01:58:47.600 --> 01:58:50.000
Or, you know, we're playing Dungeons and Dragons

01:58:50.000 --> 01:58:52.360
or some board game and we just have a virtual version

01:58:52.360 --> 01:58:55.640
of the board in front of us while we're sitting here.

01:58:55.640 --> 01:58:57.960
All that's possible in mixed reality.

01:58:57.960 --> 01:59:01.000
And I think that that is going to be the next big capability

01:59:01.000 --> 01:59:02.520
on top of virtual reality.

01:59:02.520 --> 01:59:05.480
It has done so well.

01:59:06.320 --> 01:59:08.440
I have to say, as a person who experienced it today

01:59:08.440 --> 01:59:13.440
with zombies, having a full awareness of the environment

01:59:13.760 --> 01:59:16.840
and integrating that environment in the way they run at you

01:59:16.840 --> 01:59:20.560
while they try to kill you, it's just the mixed reality,

01:59:20.560 --> 01:59:23.400
the pass through is really, really, really well done.

01:59:23.400 --> 01:59:28.240
And the fact that it's only $500 is really, it's well done.

01:59:28.240 --> 01:59:29.080
Thank you.

01:59:29.080 --> 01:59:30.600
I'm super excited about it.

01:59:30.600 --> 01:59:35.600
I mean, we put a lot of work into making the device

01:59:37.920 --> 01:59:40.640
both as good as possible and as affordable as possible

01:59:40.640 --> 01:59:43.200
because a big part of our mission and ethos here

01:59:43.200 --> 01:59:46.320
is we want people to be able to connect with each other.

01:59:46.320 --> 01:59:49.120
We want to reach and we want to serve a lot of people, right?

01:59:49.120 --> 01:59:51.800
We want to bring this technology to everyone, right?

01:59:51.800 --> 01:59:54.120
So we're not just trying to serve

01:59:54.120 --> 01:59:57.600
like an elite wealthy crowd.

01:59:57.600 --> 02:00:01.240
We really want this to be accessible.

02:00:01.240 --> 02:00:03.800
So that is in a lot of ways

02:00:03.800 --> 02:00:05.400
an extremely hard technical problem

02:00:05.400 --> 02:00:09.120
because we don't just have the ability to put

02:00:09.120 --> 02:00:10.560
an unlimited amount of hardware

02:00:10.560 --> 02:00:12.920
and thus we needed to basically deliver something

02:00:12.920 --> 02:00:16.120
that works really well, but in an affordable package.

02:00:16.120 --> 02:00:18.160
And we started with Quest Pro last year.

02:00:18.160 --> 02:00:23.160
It was $1,500 and now we've lowered the price to 1,000,

02:00:23.360 --> 02:00:26.440
but in a lot of ways, the mixed reality in Quest 3

02:00:26.440 --> 02:00:29.520
is an even better and more advanced level

02:00:29.520 --> 02:00:31.400
than what we were able to deliver in Quest Pro.

02:00:31.400 --> 02:00:36.400
So I'm really proud of where we are with Quest 3 on that.

02:00:36.400 --> 02:00:38.680
It's going to work with all of the virtual reality titles

02:00:38.680 --> 02:00:40.840
and everything that existed there.

02:00:40.840 --> 02:00:43.280
So people who want to play fully immersive games,

02:00:43.280 --> 02:00:47.320
social experiences, fitness, all that stuff will work,

02:00:47.320 --> 02:00:49.440
but now you'll also get mixed reality too.

02:00:49.480 --> 02:00:52.200
Which I think people really like because it's,

02:00:52.200 --> 02:00:54.760
sometimes you want to be super immersed in a game,

02:00:54.760 --> 02:00:58.200
but a lot of the time, especially when you're moving around,

02:00:58.200 --> 02:01:01.400
if you're active, like you're doing some fitness experience,

02:01:02.560 --> 02:01:05.080
let's say you're doing boxing or something,

02:01:05.080 --> 02:01:06.520
it's like, you kind of want to be able

02:01:06.520 --> 02:01:08.200
to see the room around you so that way you know

02:01:08.200 --> 02:01:11.240
that I'm not going to punch a lamp or something like that.

02:01:11.240 --> 02:01:13.400
And I don't know if you got to play with this experience,

02:01:13.400 --> 02:01:14.720
but we basically have the,

02:01:14.720 --> 02:01:16.520
and it's just sort of like a fun level,

02:01:16.520 --> 02:01:17.560
but it's like, you know,

02:01:17.640 --> 02:01:19.400
we're like in a conference room or your living room

02:01:19.400 --> 02:01:22.920
and you have the guy there and you're boxing him

02:01:22.920 --> 02:01:24.600
and you're fighting him and it's like.

02:01:24.600 --> 02:01:25.720
All the other people are there too.

02:01:25.720 --> 02:01:26.560
I got a chance to do that.

02:01:26.560 --> 02:01:27.400
Yeah.

02:01:27.400 --> 02:01:28.400
And all the people are there.

02:01:29.720 --> 02:01:31.920
It's like that guy is right there.

02:01:31.920 --> 02:01:33.480
Yeah, it's like it's right in the room.

02:01:33.480 --> 02:01:36.280
And the other human, the path, you're seeing them also,

02:01:36.280 --> 02:01:38.040
they can cheer you on, they can make fun of you

02:01:38.040 --> 02:01:39.920
if they're anything like friends of mine.

02:01:39.920 --> 02:01:41.920
And then you're like, oh, I'm going to do this,

02:01:41.920 --> 02:01:43.760
I'm going to do this, I'm going to do that.

02:01:43.760 --> 02:01:45.600
And then you're like, oh, I'm going to do that.

02:01:45.600 --> 02:01:47.120
If they're anything like friends of mine.

02:01:47.120 --> 02:01:52.120
And then just it, yeah, it's really,

02:01:53.160 --> 02:01:55.320
it's a really compelling experience.

02:01:55.320 --> 02:01:56.960
And VR is really interesting too,

02:01:56.960 --> 02:01:58.800
but this is something else almost.

02:01:58.800 --> 02:02:01.640
This is, this becomes integrated into your life,

02:02:01.640 --> 02:02:03.520
into your world.

02:02:03.520 --> 02:02:04.360
Yeah.

02:02:04.360 --> 02:02:06.880
And it, so I think it's a completely new capability

02:02:06.880 --> 02:02:09.280
that will unlock a lot of different content.

02:02:09.280 --> 02:02:11.520
And I think it'll also just make the experience

02:02:11.520 --> 02:02:13.120
more comfortable for a set of people

02:02:13.120 --> 02:02:16.840
who didn't want to have only fully immersive experiences.

02:02:16.840 --> 02:02:19.280
I think if you want experiences where you're grounded in,

02:02:19.280 --> 02:02:21.680
you know, your living room in the physical world around you,

02:02:21.680 --> 02:02:23.800
now you'll be able to have that too.

02:02:23.800 --> 02:02:24.960
And I think that that's pretty exciting.

02:02:24.960 --> 02:02:28.640
I really liked how it added windows

02:02:28.640 --> 02:02:30.480
to a room with no windows.

02:02:30.480 --> 02:02:31.320
Yeah.

02:02:31.320 --> 02:02:32.160
Me as a person-

02:02:32.160 --> 02:02:33.000
Did you see the aquarium one

02:02:33.000 --> 02:02:34.160
where you could see the shark swim up

02:02:34.160 --> 02:02:35.560
or was that just the zombie one?

02:02:35.560 --> 02:02:37.640
Just the zombie one, but it's still off time.

02:02:37.640 --> 02:02:39.360
You don't necessarily want windows added

02:02:39.360 --> 02:02:41.000
to your living room where zombies come out of,

02:02:41.000 --> 02:02:43.920
but yes, in the context of that game, it's, yeah, yeah.

02:02:43.920 --> 02:02:47.560
I enjoyed it because you could see the nature outside.

02:02:47.560 --> 02:02:50.000
And me as a person that doesn't have windows,

02:02:50.000 --> 02:02:52.080
it's just nice to have nature.

02:02:52.080 --> 02:02:53.720
Yeah, well-

02:02:53.720 --> 02:02:56.600
Even if it's a mixed reality setting.

02:02:56.600 --> 02:02:58.400
It was, it's kind of like, there was a,

02:02:58.400 --> 02:03:01.640
I know it's a zombie game, but there was a Zen nature,

02:03:01.640 --> 02:03:03.720
Zen aspect to being able to look outside

02:03:03.720 --> 02:03:06.640
and alter your environment as you know it.

02:03:08.200 --> 02:03:09.040
Yeah.

02:03:09.040 --> 02:03:10.320
I mean-

02:03:10.320 --> 02:03:12.400
There will probably be better, more Zen ways to do that

02:03:12.400 --> 02:03:13.760
than the zombie game you're describing,

02:03:13.760 --> 02:03:16.160
but you're right that the basic idea

02:03:16.160 --> 02:03:19.400
of sort of having your physical environment

02:03:19.400 --> 02:03:21.840
on pass through, but then being able to bring

02:03:21.840 --> 02:03:25.800
in different elements, I mean,

02:03:25.800 --> 02:03:27.640
I think it's going to be super powerful.

02:03:27.640 --> 02:03:30.720
And in some ways, I think that these are,

02:03:30.720 --> 02:03:32.640
mixed reality is also a predecessor

02:03:32.640 --> 02:03:35.040
to eventually we will get AR glasses

02:03:35.040 --> 02:03:37.400
that are not kind of the goggles form factor

02:03:37.400 --> 02:03:40.200
of the current generation of headsets

02:03:40.200 --> 02:03:41.680
that people are making.

02:03:42.720 --> 02:03:44.040
But I think a lot of the experiences

02:03:44.040 --> 02:03:46.240
that developers are making for mixed reality

02:03:46.240 --> 02:03:48.680
of basically you just have a kind of a hologram

02:03:48.680 --> 02:03:50.040
that you're putting in the world,

02:03:50.040 --> 02:03:53.560
will hopefully apply once we get the AR glasses too.

02:03:53.560 --> 02:03:56.760
Now that's got its own whole set of challenges and it's-

02:03:56.760 --> 02:03:58.280
Well, the headset is already smaller

02:03:58.280 --> 02:04:00.000
than the previous version.

02:04:00.000 --> 02:04:01.720
Oh yeah, it's 40% thinner.

02:04:01.720 --> 02:04:03.360
And the other thing that I think is good about it,

02:04:03.360 --> 02:04:05.920
yeah, so mixed reality was the first big thing.

02:04:05.960 --> 02:04:10.120
The second is it's just a great VR headset.

02:04:10.120 --> 02:04:13.200
It's, I mean, it's got 2X the graphics processing power,

02:04:14.120 --> 02:04:18.880
40% sharper screens, 40% thinner, more comfortable,

02:04:18.880 --> 02:04:21.320
better strap architecture, all this stuff that,

02:04:21.320 --> 02:04:22.480
you know, if you liked Quest 2,

02:04:22.480 --> 02:04:24.080
I think that this is just going to be,

02:04:24.080 --> 02:04:25.920
it's like all the content that you might've played

02:04:25.920 --> 02:04:28.000
in Quest 2 is just going to get sharper automatically

02:04:28.000 --> 02:04:29.040
and look better in this.

02:04:29.040 --> 02:04:31.680
So it's, I think people are really going to like it.

02:04:31.680 --> 02:04:33.720
Yeah, so this fall.

02:04:33.760 --> 02:04:36.320
This fall, I have to ask,

02:04:36.320 --> 02:04:40.080
Apple just announced a mixed reality headset

02:04:40.080 --> 02:04:45.080
called Vision Pro for $3,500, available in early 2024.

02:04:45.920 --> 02:04:47.760
What do you think about this headset?

02:04:48.960 --> 02:04:51.800
Well, I saw the materials when they launched.

02:04:51.800 --> 02:04:53.800
I haven't gotten a chance to play with it yet.

02:04:53.800 --> 02:04:56.160
So kind of take everything with a grain of salt,

02:04:56.160 --> 02:04:59.000
but a few high-level thoughts.

02:04:59.000 --> 02:05:04.000
I mean, first, you know, I do think that this is

02:05:05.520 --> 02:05:09.920
a certain level of validation for the category, right?

02:05:09.920 --> 02:05:13.520
Where, you know, we were the primary folks out there

02:05:13.520 --> 02:05:16.800
before saying, hey, I think that this, you know,

02:05:16.800 --> 02:05:19.000
virtual reality, augmented reality, mixed reality,

02:05:19.000 --> 02:05:20.360
this is going to be a big part

02:05:20.360 --> 02:05:21.920
of the next computing platform.

02:05:23.520 --> 02:05:28.520
I think having Apple come in and share that vision,

02:05:28.600 --> 02:05:33.600
will make a lot of people who are fans of their products,

02:05:35.240 --> 02:05:36.400
really consider that.

02:05:37.680 --> 02:05:41.680
And then, you know, of course, the $3,500 price,

02:05:43.520 --> 02:05:45.240
you know, on the one hand, I get it for,

02:05:45.240 --> 02:05:47.200
with all the stuff that they're trying to pack in there.

02:05:47.200 --> 02:05:49.680
On the other hand, a lot of people aren't going to find that

02:05:49.680 --> 02:05:51.200
to be affordable.

02:05:51.200 --> 02:05:53.840
So I think that there's a chance that them coming in

02:05:53.840 --> 02:05:57.600
actually increases demand for the overall space

02:05:57.600 --> 02:06:00.600
and that Quest 3 is actually the primary beneficiary

02:06:00.600 --> 02:06:03.880
of that because a lot of the people who might say,

02:06:03.880 --> 02:06:06.920
hey, you know, this, I think I'm going to give

02:06:06.920 --> 02:06:08.160
another consideration to this.

02:06:08.160 --> 02:06:11.120
Or, you know, now I understand maybe what mixed reality

02:06:11.120 --> 02:06:14.480
is more and Quest 3 is the best one on the market

02:06:14.480 --> 02:06:16.760
that I can afford.

02:06:16.760 --> 02:06:18.160
And it's great also, right?

02:06:18.160 --> 02:06:20.920
It's, I think that that's, and, you know, in our own way,

02:06:20.920 --> 02:06:23.120
I think we're, and there are a lot of features that we have

02:06:23.120 --> 02:06:24.320
where we're leading on.

02:06:25.280 --> 02:06:28.800
So I think that that's, that I think is going to be a very,

02:06:28.800 --> 02:06:29.960
that could be quite good.

02:06:31.720 --> 02:06:34.040
And then obviously over time, the companies are just focused

02:06:34.040 --> 02:06:36.720
on somewhat different things, right?

02:06:36.720 --> 02:06:41.160
Apple has always, you know, I think focused on building

02:06:42.040 --> 02:06:47.040
really kind of high-end things, whereas our focus has been

02:06:48.400 --> 02:06:51.280
on, it's just, we have a more democratic ethos.

02:06:51.280 --> 02:06:53.600
We want to build things that are accessible

02:06:53.840 --> 02:06:55.440
to a wider number of people.

02:06:56.640 --> 02:07:00.120
You know, we've sold tens of millions of Quest devices.

02:07:02.640 --> 02:07:05.080
My understanding, just based on rumors,

02:07:05.080 --> 02:07:06.360
I don't have any special knowledge on this,

02:07:06.360 --> 02:07:10.440
is that Apple is building about 1 million of their device.

02:07:10.440 --> 02:07:11.280
Right?

02:07:11.280 --> 02:07:13.520
So just in terms of like what you kind of expect

02:07:13.520 --> 02:07:18.520
in terms of sales numbers, I just think that this is,

02:07:18.840 --> 02:07:22.040
I mean, Quest is going to be the primary thing

02:07:22.040 --> 02:07:25.040
that people in the market will continue using

02:07:25.040 --> 02:07:26.000
for the foreseeable future.

02:07:26.000 --> 02:07:27.040
And then obviously over the long-term,

02:07:27.040 --> 02:07:29.920
it's up to the companies to see how well we executed

02:07:29.920 --> 02:07:31.360
the different things that we're doing.

02:07:31.360 --> 02:07:33.040
But we kind of come at it from different places.

02:07:33.040 --> 02:07:37.360
We're very focused on social interaction, communication,

02:07:39.440 --> 02:07:40.880
being more active, right?

02:07:40.880 --> 02:07:43.880
So there's fitness, there's gaming, there are those things.

02:07:45.160 --> 02:07:46.720
You know, whereas I think a lot of the use cases

02:07:46.720 --> 02:07:51.720
that you saw in Apple's launch material were more around,

02:07:51.920 --> 02:07:53.880
you know, people sitting, you know,

02:07:53.880 --> 02:07:56.880
people looking at screens, which are great.

02:07:56.880 --> 02:07:58.920
I think that you will replace your laptop

02:07:58.920 --> 02:08:00.720
over time with a headset.

02:08:00.720 --> 02:08:03.520
But I think in terms of kind of how,

02:08:03.520 --> 02:08:04.760
with the different use cases

02:08:04.760 --> 02:08:07.240
that the companies are going after,

02:08:07.240 --> 02:08:10.320
they're a bit different for where we are right now.

02:08:10.320 --> 02:08:13.640
Yeah, so gaming wasn't a big part of the presentation,

02:08:13.640 --> 02:08:15.280
which is interesting.

02:08:16.360 --> 02:08:19.160
It feels like mixed reality,

02:08:19.160 --> 02:08:21.600
gaming's such a big part of that.

02:08:22.440 --> 02:08:24.400
It was interesting to see it missing in the presentation.

02:08:24.400 --> 02:08:27.080
Well, I mean, look, there are certain design trade-offs

02:08:27.080 --> 02:08:31.720
in this where, you know, they made this point

02:08:31.720 --> 02:08:33.440
about not wanting to have controllers,

02:08:33.440 --> 02:08:36.360
which on the one hand, there's a certain elegance

02:08:36.360 --> 02:08:38.440
about just being able to navigate the system

02:08:38.440 --> 02:08:41.120
with eye gaze and hand tracking.

02:08:41.120 --> 02:08:44.000
And by the way, you'll be able to just navigate Quest

02:08:44.000 --> 02:08:46.640
with your hands too, if that's what you want.

02:08:46.640 --> 02:08:48.360
Yeah, one of the things I should mention

02:08:48.360 --> 02:08:51.720
is the capability from the cameras

02:08:51.720 --> 02:08:56.160
with computer vision to detect certain aspects of the hand

02:08:56.160 --> 02:08:57.360
allowing you to have a controller

02:08:57.360 --> 02:08:59.000
that doesn't have that ring thing.

02:08:59.000 --> 02:09:01.120
Yeah, the hand tracking in Quest 3

02:09:01.120 --> 02:09:03.640
and the controller tracking is a big step up

02:09:03.640 --> 02:09:05.600
from the last generation.

02:09:07.320 --> 02:09:09.720
And one of the demos that we have is basically

02:09:09.720 --> 02:09:12.120
an MR experience teaching you how to play piano

02:09:12.120 --> 02:09:14.240
where it basically highlights the notes that you need to play

02:09:14.240 --> 02:09:16.880
and it's like, it's hands, it's no controllers.

02:09:16.880 --> 02:09:20.440
But I think if you care about gaming,

02:09:20.440 --> 02:09:25.440
having a controller allows you to have a more tactile feel

02:09:26.080 --> 02:09:30.280
and allows you to capture fine motor movement

02:09:30.280 --> 02:09:34.560
much more precisely than what you can do with hands

02:09:34.560 --> 02:09:36.000
without something that you're touching.

02:09:36.000 --> 02:09:38.560
So again, I think there are certain questions

02:09:38.560 --> 02:09:42.040
which are just around what use cases are you optimizing for.

02:09:43.880 --> 02:09:45.760
I think if you wanna play games,

02:09:45.760 --> 02:09:49.200
then I think that you wanna design the system

02:09:49.200 --> 02:09:52.160
in a different way and we're more focused

02:09:52.160 --> 02:09:55.360
on kind of social experiences, entertainment experiences.

02:09:56.960 --> 02:09:59.720
Whereas if what you want is to make sure

02:09:59.720 --> 02:10:02.720
that the text that you read on a screen

02:10:02.720 --> 02:10:04.600
is as crisp as possible,

02:10:04.600 --> 02:10:08.080
then you need to make the design and cost trade-offs

02:10:08.080 --> 02:10:12.320
that they made that lead you to making a $3,500 device.

02:10:12.320 --> 02:10:14.680
So I think that there is a use case for that for sure,

02:10:14.680 --> 02:10:17.480
but I just think that the companies,

02:10:17.480 --> 02:10:20.360
we've basically made different design trade-offs

02:10:20.360 --> 02:10:24.520
to get to the use cases that we're trying to serve.

02:10:24.520 --> 02:10:26.520
There's a lot of other stuff

02:10:26.520 --> 02:10:29.680
I'd love to talk to you about the metaverse,

02:10:29.680 --> 02:10:31.840
especially the Kodak Avatar,

02:10:31.840 --> 02:10:33.200
which I've gotten to experience

02:10:33.200 --> 02:10:35.120
a lot of different variations of recently

02:10:35.120 --> 02:10:36.920
that I'm really, really excited about.

02:10:36.920 --> 02:10:39.160
Yeah, I'm excited to talk about that too.

02:10:39.160 --> 02:10:41.360
I'll have to wait a little bit because,

02:10:42.320 --> 02:10:47.320
well, I think there's a lot more to show off in that regard.

02:10:47.920 --> 02:10:50.240
But let me step back to AI.

02:10:50.240 --> 02:10:51.840
I think we've mentioned it a little bit,

02:10:51.840 --> 02:10:55.680
but I'd like to linger on this question

02:10:55.680 --> 02:11:00.680
that folks like Eleazar Yudkovsky has to worry about

02:11:00.800 --> 02:11:03.240
and others of the existential,

02:11:03.240 --> 02:11:05.320
of the serious threats of AI

02:11:05.320 --> 02:11:07.080
that have been reinvigorated now

02:11:07.080 --> 02:11:09.920
with the rapid developments of AI systems.

02:11:09.920 --> 02:11:14.680
Do you worry about the existential risks of AI

02:11:14.680 --> 02:11:17.960
as Eleazar does, about the alignment problem,

02:11:17.960 --> 02:11:20.640
about this getting out of hand?

02:11:20.640 --> 02:11:23.080
Anytime where there's a number of serious people

02:11:23.080 --> 02:11:27.480
who are raising a concern that is that existential

02:11:27.480 --> 02:11:28.960
about something that you're involved with,

02:11:28.960 --> 02:11:31.000
I think you have to think about it, right?

02:11:31.000 --> 02:11:33.960
So I've spent quite a bit of time thinking about it

02:11:33.960 --> 02:11:35.160
from that perspective.

02:11:35.320 --> 02:11:38.320
The thing where I basically have come out on this for now

02:11:38.320 --> 02:11:41.760
is I do think that there are, over time,

02:11:41.760 --> 02:11:43.600
I think that we need to think about this even more

02:11:43.600 --> 02:11:47.560
as we approach something that could be closer

02:11:47.560 --> 02:11:48.400
to super intelligence.

02:11:48.400 --> 02:11:49.800
I just think it's pretty clear

02:11:49.800 --> 02:11:51.880
to anyone working on these projects today

02:11:51.880 --> 02:11:53.560
that we're not there.

02:11:53.560 --> 02:11:57.360
And one of my concerns is that,

02:11:57.360 --> 02:12:00.080
we spent a fair amount of time on this before,

02:12:00.080 --> 02:12:03.520
but I think that there's a lot more to think about

02:12:03.520 --> 02:12:06.840
on this before, but there are more,

02:12:10.320 --> 02:12:11.720
I don't know if mundane is the right word,

02:12:11.720 --> 02:12:15.000
but there's concerns that already exist

02:12:15.000 --> 02:12:19.720
about people using AI tools to do harmful things

02:12:19.720 --> 02:12:21.120
of the type that we're already aware,

02:12:21.120 --> 02:12:23.800
whether we talked about fraud or scams

02:12:23.800 --> 02:12:25.280
or different things like that.

02:12:27.600 --> 02:12:31.560
And that's going to be a pretty big set of challenges

02:12:31.560 --> 02:12:33.120
that the companies working on this are gonna need

02:12:33.120 --> 02:12:36.560
to grapple with, regardless of whether

02:12:36.560 --> 02:12:38.560
there is an existential concern as well

02:12:38.560 --> 02:12:40.040
at some point down the road.

02:12:40.040 --> 02:12:42.960
So I do worry that to some degree,

02:12:45.160 --> 02:12:50.160
people can get a little too focused on some of the tail risk

02:12:51.640 --> 02:12:54.520
and then not do as good of a job as we need to

02:12:54.520 --> 02:12:57.720
on the things that you can be almost certain

02:12:57.720 --> 02:12:59.480
are going to come down the pipe

02:12:59.480 --> 02:13:03.880
as real risks that kind of manifest themselves

02:13:03.880 --> 02:13:04.720
in the near term.

02:13:04.720 --> 02:13:08.200
So for me, I've spent most of my time on that

02:13:08.200 --> 02:13:13.200
once I kind of made the realization that the size

02:13:13.640 --> 02:13:15.160
of models that we're talking about now

02:13:15.160 --> 02:13:17.800
in terms of what we're building are quite far

02:13:17.800 --> 02:13:20.040
from the super intelligence type concerns

02:13:20.040 --> 02:13:22.200
that people raise.

02:13:22.200 --> 02:13:25.840
But I think once we get a couple of steps closer to that,

02:13:25.840 --> 02:13:28.000
I know as we do get closer, I think that those,

02:13:28.240 --> 02:13:31.920
you know, there are going to be some novel risks

02:13:31.920 --> 02:13:34.240
and issues about how we make sure

02:13:34.240 --> 02:13:36.680
that the systems are safe for sure.

02:13:36.680 --> 02:13:38.440
I guess here, just to take the conversation

02:13:38.440 --> 02:13:40.160
in a somewhat different direction,

02:13:41.040 --> 02:13:44.720
I think in some of these debates around safety,

02:13:45.680 --> 02:13:49.720
I think the concepts of intelligence and autonomy

02:13:52.160 --> 02:13:57.160
or like the being of the thing, you know, as an analogy,

02:13:57.800 --> 02:13:59.680
they get kind of conflated together.

02:13:59.680 --> 02:14:03.440
And I think it very well could be the case

02:14:03.440 --> 02:14:07.240
that you can make something in scale intelligence quite far,

02:14:07.240 --> 02:14:12.240
but that may not manifest the safety concerns

02:14:14.920 --> 02:14:16.840
that people are saying in the sense that, I mean,

02:14:16.840 --> 02:14:19.000
just if you look at human biology, it's like, all right,

02:14:19.000 --> 02:14:22.800
we have our neocortex is where all the thinking happens,

02:14:22.800 --> 02:14:25.800
right, but it's not really calling the shots

02:14:25.800 --> 02:14:26.640
at the end of the day.

02:14:26.640 --> 02:14:31.200
We have a much more primitive old brain structure

02:14:31.200 --> 02:14:32.800
for which our neocortex,

02:14:32.800 --> 02:14:34.160
which is this powerful machinery,

02:14:34.160 --> 02:14:36.800
is basically just a kind of prediction

02:14:36.800 --> 02:14:41.800
and reasoning engine to help our very simple brain

02:14:44.640 --> 02:14:48.400
decide how to plan and do what it needs to do

02:14:48.400 --> 02:14:52.440
in order to achieve these like very kind of basic impulses.

02:14:52.440 --> 02:14:55.360
And I think that you can think about some

02:14:55.360 --> 02:15:00.200
of the development of intelligence along the same lines

02:15:00.200 --> 02:15:03.240
where just like our neocortex doesn't have free will

02:15:03.240 --> 02:15:07.360
or autonomy, we might develop these wildly

02:15:07.360 --> 02:15:11.120
intelligent systems that are much more intelligent

02:15:11.120 --> 02:15:13.600
than our neocortex have much more capacity,

02:15:13.600 --> 02:15:17.120
but the same way that our neocortex is sort of subservient

02:15:17.120 --> 02:15:22.120
and is used as a tool by our kind of simple impulse brain,

02:15:22.920 --> 02:15:25.400
I think that it's not out of the question

02:15:25.400 --> 02:15:28.120
that very intelligent systems that have the capacity

02:15:28.120 --> 02:15:31.560
to think will kind of act as that as sort of an extension

02:15:31.560 --> 02:15:33.640
of the neocortex doing that.

02:15:33.640 --> 02:15:37.520
So I think my own view is that where we really need

02:15:37.520 --> 02:15:42.520
to be careful is on the development of autonomy

02:15:42.680 --> 02:15:47.320
and how we think about that because it's actually the case

02:15:47.320 --> 02:15:50.640
that relatively simple and unintelligent things

02:15:50.640 --> 02:15:53.800
that have runaway autonomy and just spread themselves

02:15:53.800 --> 02:15:57.000
or it's like, we have a word for that, it's a virus, right?

02:15:57.000 --> 02:15:59.560
It's, I mean, like it's can be simple computer code

02:15:59.560 --> 02:16:01.040
that is not particularly intelligent,

02:16:01.040 --> 02:16:03.440
but just spreads itself and does a lot of harm,

02:16:05.160 --> 02:16:06.920
biologically or computer.

02:16:06.920 --> 02:16:11.920
And I just think that these are somewhat separable things.

02:16:12.960 --> 02:16:15.760
And a lot of what I think we need to develop

02:16:15.760 --> 02:16:18.240
when people talk about safety and responsibility

02:16:18.240 --> 02:16:21.360
is really the governance on the autonomy

02:16:21.360 --> 02:16:23.840
that can be given to systems.

02:16:23.840 --> 02:16:28.840
And to me, if I were a policymaker or thinking about this,

02:16:29.600 --> 02:16:31.520
I would really wanna think about that distinction

02:16:31.520 --> 02:16:34.240
between these where I think building intelligent systems

02:16:34.240 --> 02:16:37.840
will be, can create a huge advance in terms of people's

02:16:37.840 --> 02:16:42.360
quality of life and productivity growth in the economy.

02:16:42.360 --> 02:16:46.320
But it's the autonomy part of this that I think we really

02:16:46.320 --> 02:16:48.920
need to make progress on how to govern these things

02:16:48.920 --> 02:16:53.920
responsibly before we build the capacity for them

02:16:54.240 --> 02:16:56.560
to make a lot of decisions on their own

02:16:56.560 --> 02:17:00.320
or give them goals or things like that.

02:17:00.320 --> 02:17:01.960
And I know that's a research problem,

02:17:01.960 --> 02:17:03.120
but I do think that to some degree,

02:17:03.120 --> 02:17:06.920
these are somewhat separable things.

02:17:06.920 --> 02:17:09.800
I love the distinction between intelligence and autonomy

02:17:09.800 --> 02:17:12.160
and the metaphor within your cortex.

02:17:14.200 --> 02:17:15.840
Let me ask about power.

02:17:16.840 --> 02:17:19.960
So building superintelligence systems,

02:17:19.960 --> 02:17:22.040
even if it's not in the near term,

02:17:22.040 --> 02:17:25.520
I think Metta is one of the few companies,

02:17:25.520 --> 02:17:29.320
if not the main company that will develop

02:17:29.320 --> 02:17:31.360
the superintelligence system.

02:17:31.360 --> 02:17:34.560
And you are a man who's at the head of this company.

02:17:34.560 --> 02:17:37.320
Building AGI might make you the most powerful man

02:17:37.320 --> 02:17:38.160
in the world.

02:17:38.160 --> 02:17:40.320
Do you worry that that power will corrupt you?

02:17:41.320 --> 02:17:42.160
What a question.

02:17:45.400 --> 02:17:47.320
I mean, look, I think realistically,

02:17:47.320 --> 02:17:48.960
this gets back to the open source things

02:17:48.960 --> 02:17:50.440
that we talked about before,

02:17:50.440 --> 02:17:55.440
which is I don't think that the world will be best served

02:17:56.080 --> 02:18:01.080
by any small number of organizations having this

02:18:03.800 --> 02:18:07.920
without it being something that is more broadly available.

02:18:08.920 --> 02:18:10.920
And I think if you look through history,

02:18:12.800 --> 02:18:16.480
it's when there are these sort of like unipolar advances

02:18:16.480 --> 02:18:19.240
and things that, and like power imbalances

02:18:19.240 --> 02:18:23.960
that they're doing to being kind of weird situations.

02:18:23.960 --> 02:18:26.880
So this is one of the reasons why I think open sources

02:18:26.880 --> 02:18:31.520
is generally the right approach.

02:18:31.520 --> 02:18:34.720
And I think it's a categorically different question today

02:18:34.720 --> 02:18:36.840
when we're not close to superintelligence

02:18:36.840 --> 02:18:37.760
and that there's a good chance

02:18:37.760 --> 02:18:40.120
that even once we get closer to superintelligence,

02:18:40.120 --> 02:18:42.080
open sourcing remains the right approach,

02:18:42.080 --> 02:18:43.000
even though I think at that point,

02:18:43.000 --> 02:18:44.640
it's a somewhat different debate.

02:18:45.960 --> 02:18:49.200
But I think part of that is that that is,

02:18:49.200 --> 02:18:52.360
I think one of the best ways to ensure that the system

02:18:52.360 --> 02:18:54.160
is as secure and safe as possible,

02:18:54.160 --> 02:18:55.840
because it's not just about a lot of people

02:18:55.840 --> 02:18:57.160
having access to it.

02:18:57.160 --> 02:19:00.760
It's the scrutiny that kind of comes with

02:19:00.760 --> 02:19:02.360
building an open source system.

02:19:02.360 --> 02:19:04.440
But I think that this is a pretty widely accepted thing

02:19:04.440 --> 02:19:08.760
about open sources that you have the code out there

02:19:08.760 --> 02:19:11.240
so anyone can see the vulnerabilities.

02:19:11.240 --> 02:19:13.880
Anyone can kind of mess with it in different ways.

02:19:13.880 --> 02:19:15.520
People can spin off their own projects

02:19:15.520 --> 02:19:17.720
and experiment in a ton of different ways.

02:19:17.720 --> 02:19:20.000
And the net result of all of that

02:19:20.000 --> 02:19:22.040
is that the systems just get hardened

02:19:22.040 --> 02:19:24.640
and get to be a lot safer and more secure.

02:19:25.880 --> 02:19:29.400
So I think that there's a chance

02:19:29.400 --> 02:19:34.080
that that ends up being the way that this goes to

02:19:34.080 --> 02:19:35.480
a pretty good chance,

02:19:35.480 --> 02:19:39.120
and that having this be open

02:19:39.120 --> 02:19:43.120
both leads to a healthier development of the technology

02:19:43.120 --> 02:19:47.440
and also leads to a more balanced distribution

02:19:47.440 --> 02:19:50.800
of the technology in a way that strike me

02:19:50.800 --> 02:19:53.120
as good values to aspire to.

02:19:53.120 --> 02:19:55.960
So to you, there's risks to open sourcing,

02:19:55.960 --> 02:19:57.760
but the benefits outweigh the risks.

02:19:57.760 --> 02:20:00.080
At the two, it's interesting,

02:20:00.080 --> 02:20:01.880
I think the way you put it,

02:20:03.440 --> 02:20:06.760
you put it well that there's a different discussion now

02:20:06.760 --> 02:20:10.560
than when we get closer to the development

02:20:10.560 --> 02:20:13.040
of super intelligence of the benefits

02:20:13.040 --> 02:20:15.440
and risks of open sourcing.

02:20:15.440 --> 02:20:17.880
Yeah, and to be clear, I feel quite confident

02:20:17.880 --> 02:20:21.240
in the assessment that open sourcing models now

02:20:21.240 --> 02:20:23.360
is net positive.

02:20:23.360 --> 02:20:26.240
I think there's a good argument that in the future,

02:20:26.240 --> 02:20:28.760
it will be too, even as you get closer to super intelligence,

02:20:28.760 --> 02:20:32.240
but I've certainly have not decided on that yet,

02:20:32.240 --> 02:20:33.240
and I think that it becomes

02:20:33.240 --> 02:20:35.680
a somewhat more complex set of questions

02:20:35.680 --> 02:20:37.320
that I think people will have time to debate

02:20:37.320 --> 02:20:39.520
and will also be informed by what happens

02:20:39.520 --> 02:20:41.600
between now and then to make those decisions.

02:20:41.600 --> 02:20:43.600
We don't have to necessarily just debate

02:20:43.600 --> 02:20:44.840
that in theory right now.

02:20:44.840 --> 02:20:48.000
What year do you think we'll have a super intelligence?

02:20:50.080 --> 02:20:52.080
I don't know, I mean, that's pure speculation.

02:20:52.080 --> 02:20:55.320
I think it's very clear, just taking a step back,

02:20:55.320 --> 02:20:57.480
that we had a big breakthrough in the last year,

02:20:57.480 --> 02:21:00.040
where the LLMs and diffusion models

02:21:00.040 --> 02:21:02.600
basically reached a scale where they're able

02:21:02.600 --> 02:21:05.320
to do some pretty interesting things.

02:21:05.320 --> 02:21:07.320
And then I think the question is what happens from here?

02:21:07.320 --> 02:21:09.720
And just to paint the two extremes,

02:21:14.080 --> 02:21:17.480
on one side, it's like, okay, we just had one breakthrough.

02:21:17.480 --> 02:21:19.360
If we just have like another breakthrough like that,

02:21:19.360 --> 02:21:21.200
or maybe two, then we can have something

02:21:21.200 --> 02:21:23.240
that's truly crazy, right?

02:21:23.240 --> 02:21:28.200
And is just like so much more advanced.

02:21:28.200 --> 02:21:31.480
And on that side of the argument,

02:21:31.480 --> 02:21:36.480
it's like, okay, well, maybe we're only a couple

02:21:36.520 --> 02:21:41.040
of big steps away from reaching something

02:21:41.040 --> 02:21:43.320
that looks more like general intelligence.

02:21:43.320 --> 02:21:45.800
Okay, that's one side of the argument.

02:21:45.800 --> 02:21:47.600
And the other side, which is what we've historically

02:21:47.600 --> 02:21:50.880
seen a lot more, is that a breakthrough leads to

02:21:53.760 --> 02:21:58.320
in that Gartner hype cycle, there's like the hype,

02:21:58.320 --> 02:22:00.960
and then there's the trough of disillusionment after,

02:22:00.960 --> 02:22:03.600
when like people think that there's a chance that, hey,

02:22:03.600 --> 02:22:04.640
okay, there's a big breakthrough,

02:22:04.640 --> 02:22:06.040
maybe we're about to get another big breakthrough.

02:22:06.040 --> 02:22:07.480
And it's like, actually, you're not about

02:22:07.480 --> 02:22:08.960
to get another breakthrough.

02:22:08.960 --> 02:22:10.800
Maybe you're actually just gonna have to sit

02:22:10.800 --> 02:22:12.320
with this one for a while.

02:22:12.320 --> 02:22:17.320
And it could be five years, it could be 10 years,

02:22:18.760 --> 02:22:20.840
it could be 15 years until you figure out

02:22:21.400 --> 02:22:24.040
the kind of the next big thing

02:22:24.040 --> 02:22:25.600
that needs to get figured out.

02:22:25.600 --> 02:22:28.680
And, but I think that the fact that we just had

02:22:28.680 --> 02:22:31.880
this breakthrough sort of makes it so that we're

02:22:31.880 --> 02:22:35.280
at a point of almost a very wide error bars

02:22:35.280 --> 02:22:36.520
on what happens next.

02:22:37.800 --> 02:22:41.280
I think the traditional technical view of like looking

02:22:41.280 --> 02:22:44.680
at the industry would suggest that we're not just going

02:22:44.680 --> 02:22:48.080
to stack in a like breakthrough on top of breakthrough

02:22:49.040 --> 02:22:52.240
like every six months or something right now.

02:22:52.240 --> 02:22:55.160
I think it will, I'm guessing, I would guess

02:22:55.160 --> 02:22:57.920
that it will take somewhat longer in between these.

02:22:57.920 --> 02:23:02.560
But I don't know, I tend to be pretty optimistic

02:23:02.560 --> 02:23:03.400
about breakthroughs too.

02:23:03.400 --> 02:23:05.760
So I mean, so I think if you normalized

02:23:05.760 --> 02:23:09.680
for my normal optimism, then maybe it would be even slower

02:23:09.680 --> 02:23:10.520
than what I'm saying.

02:23:10.520 --> 02:23:13.440
But even within that, like I'm not even opining

02:23:13.440 --> 02:23:15.360
on the question of how many breakthroughs are required

02:23:15.360 --> 02:23:17.960
to get to general intelligence because no one knows.

02:23:18.960 --> 02:23:21.520
But this particular breakthrough was so,

02:23:21.520 --> 02:23:25.920
such a small step that resulted in such a big leap

02:23:25.920 --> 02:23:30.080
in performance as experienced by human beings

02:23:30.080 --> 02:23:33.280
that it makes you think, wow, are we,

02:23:33.280 --> 02:23:37.760
as we stumble across this very open world of research,

02:23:37.760 --> 02:23:42.160
will we stumble across another thing that will have

02:23:42.160 --> 02:23:43.960
a giant leap in performance?

02:23:45.880 --> 02:23:49.840
And also we don't know exactly at which stage

02:23:49.840 --> 02:23:52.200
is it really going to be impressive?

02:23:52.200 --> 02:23:54.360
Because it feels like it's really encroaching

02:23:54.360 --> 02:23:58.000
on impressive levels of intelligence.

02:23:58.000 --> 02:23:59.760
You still didn't answer the question

02:23:59.760 --> 02:24:02.080
of what year we're going to have superintelligence.

02:24:02.080 --> 02:24:03.720
I'd like to hold you to that.

02:24:03.720 --> 02:24:04.560
No, I'm just kidding.

02:24:04.560 --> 02:24:07.880
But is there something you could say about the timeline

02:24:09.000 --> 02:24:10.960
as you think about the development

02:24:10.960 --> 02:24:14.960
of AGI superintelligence systems?

02:24:14.960 --> 02:24:15.800
Sure.

02:24:15.800 --> 02:24:19.360
So I still don't think I have any particular insight

02:24:19.360 --> 02:24:21.800
on when like a singular AI system

02:24:21.800 --> 02:24:24.120
that is a general intelligence will get created.

02:24:24.120 --> 02:24:26.240
But I think the one thing that most people

02:24:27.480 --> 02:24:28.880
in the discourse that I've seen about this

02:24:28.880 --> 02:24:32.360
haven't really grappled with is that we do seem to have

02:24:34.000 --> 02:24:37.560
organizations and structures in the world

02:24:37.560 --> 02:24:40.160
that exhibit greater than human intelligence already.

02:24:40.160 --> 02:24:44.240
So, you know, one example is a company.

02:24:44.240 --> 02:24:45.560
You know, it acts as an entity.

02:24:45.560 --> 02:24:48.480
It has, you know, singular brand.

02:24:48.480 --> 02:24:50.920
Obviously it's a collection of people.

02:24:50.920 --> 02:24:52.960
But I certainly hope that, you know,

02:24:52.960 --> 02:24:55.480
meta with tens of thousands of people

02:24:55.480 --> 02:24:57.760
make smarter decisions than one person.

02:24:57.760 --> 02:25:00.160
But I think that that would be pretty bad if it didn't.

02:25:01.640 --> 02:25:05.120
Another example that I think is even more removed

02:25:05.120 --> 02:25:07.520
from kind of the way we think about

02:25:07.520 --> 02:25:11.000
like the personification of intelligence,

02:25:11.000 --> 02:25:13.480
which is often implied in some of these questions,

02:25:13.480 --> 02:25:16.040
is think about something like the stock market, right?

02:25:16.040 --> 02:25:18.440
The stock market is, you know, it takes inputs.

02:25:18.440 --> 02:25:19.520
It's a distributed system.

02:25:19.520 --> 02:25:23.640
It's like the cybernetic organism that, you know,

02:25:23.640 --> 02:25:26.400
probably millions of people around the world

02:25:26.400 --> 02:25:28.480
are basically voting every day

02:25:28.480 --> 02:25:30.800
by choosing what to invest in.

02:25:30.800 --> 02:25:35.800
But it's basically this organism or structure

02:25:36.200 --> 02:25:39.280
that is smarter than any individual

02:25:39.280 --> 02:25:42.320
that we use to allocate capital

02:25:42.320 --> 02:25:44.480
as efficiently as possible around the world.

02:25:44.480 --> 02:25:49.480
And I do think that this notion that there are already

02:25:51.400 --> 02:25:56.400
these cybernetic systems that are either melding

02:25:58.560 --> 02:26:00.400
the intelligence of multiple people together

02:26:00.400 --> 02:26:02.880
or melding the intelligence of multiple people

02:26:02.880 --> 02:26:06.440
and technology together to form something

02:26:06.440 --> 02:26:09.400
which is dramatically more intelligent

02:26:09.400 --> 02:26:12.080
than any individual in the world

02:26:14.760 --> 02:26:17.360
is something that seems to exist

02:26:17.360 --> 02:26:20.160
and that we seem to be able to harness

02:26:20.160 --> 02:26:22.240
in a productive way for our society

02:26:23.120 --> 02:26:25.120
as long as we basically build these structures

02:26:25.120 --> 02:26:27.200
and balance with each other.

02:26:27.200 --> 02:26:28.760
So I don't know.

02:26:28.760 --> 02:26:31.640
I mean, that at least gives me hope

02:26:31.640 --> 02:26:33.360
that as we advance the technology,

02:26:33.360 --> 02:26:34.960
and I don't know how long exactly it's gonna be,

02:26:34.960 --> 02:26:36.960
but you asked when is this gonna exist?

02:26:36.960 --> 02:26:40.520
I think to some degree we already have many organizations

02:26:40.520 --> 02:26:42.840
in the world that are smarter than a single human.

02:26:42.840 --> 02:26:45.600
And that seems to be something that is generally productive

02:26:45.600 --> 02:26:46.880
in advancing humanity.

02:26:46.880 --> 02:26:49.560
And somehow the individual AI systems

02:26:49.560 --> 02:26:51.280
empower the individual humans

02:26:51.280 --> 02:26:53.240
and the interaction between the humans

02:26:53.240 --> 02:26:56.040
to make that collective intelligence machinery

02:26:56.040 --> 02:26:57.840
that you're referring to smarter.

02:26:57.840 --> 02:27:00.560
So it's not like AI is becoming super intelligent.

02:27:00.560 --> 02:27:03.160
It's just becoming the engine

02:27:03.160 --> 02:27:04.920
that's making the collective intelligence

02:27:04.920 --> 02:27:07.160
is primarily human more intelligent.

02:27:08.680 --> 02:27:10.680
It's educating the humans better.

02:27:10.680 --> 02:27:12.840
It's making them better informed.

02:27:12.840 --> 02:27:15.520
It's making them more efficient for them

02:27:15.520 --> 02:27:19.040
to communicate effectively and debate ideas.

02:27:19.040 --> 02:27:20.480
And through that process,

02:27:20.480 --> 02:27:22.200
just making the whole collective intelligence

02:27:22.200 --> 02:27:24.360
more and more and more intelligent,

02:27:24.360 --> 02:27:26.960
maybe faster than the individual AI systems

02:27:26.960 --> 02:27:30.520
that are trained on human data anyway, are becoming.

02:27:30.520 --> 02:27:32.800
Maybe the collective intelligence of the human species

02:27:32.800 --> 02:27:34.760
might outpace the development of AI.

02:27:36.040 --> 02:27:36.880
Just like-

02:27:36.880 --> 02:27:37.840
I think there's a balance in here

02:27:37.840 --> 02:27:40.920
because I mean, if a lot of the input

02:27:40.920 --> 02:27:44.720
that the systems are being trained on

02:27:44.720 --> 02:27:47.680
is basically coming from feedback from people,

02:27:47.680 --> 02:27:50.200
then a lot of the development does need to happen

02:27:50.200 --> 02:27:51.680
in human time, right?

02:27:51.680 --> 02:27:55.840
It's not like a machine will just be able to go learn

02:27:55.840 --> 02:27:58.200
all the stuff about how people think about stuff.

02:27:58.200 --> 02:28:00.960
There's a cycle to how this needs to work.

02:28:01.000 --> 02:28:04.720
This is an exciting world we're living in

02:28:04.720 --> 02:28:07.560
and now you're at the forefront of developing.

02:28:07.560 --> 02:28:09.600
One of the ways you keep yourself humble,

02:28:09.600 --> 02:28:11.560
like we mentioned with Jiu-Jitsu,

02:28:11.560 --> 02:28:14.480
is doing some really difficult challenges,

02:28:14.480 --> 02:28:16.200
mental and physical.

02:28:16.200 --> 02:28:19.240
One of those you've done very recently

02:28:19.240 --> 02:28:21.560
is the Murph Challenge.

02:28:21.560 --> 02:28:23.000
And you got a really good time.

02:28:23.000 --> 02:28:25.720
It's 100 pull-ups, 200 push-ups, 300 squats,

02:28:25.720 --> 02:28:28.920
and a mile before and a mile around after.

02:28:28.920 --> 02:28:32.520
You got under 40 minutes on that.

02:28:33.400 --> 02:28:35.360
What was the hardest part?

02:28:35.360 --> 02:28:37.440
I think a lot of people were very impressed.

02:28:37.440 --> 02:28:38.840
It's a very impressive time.

02:28:39.880 --> 02:28:40.720
Yeah, I was-

02:28:40.720 --> 02:28:44.120
How crazy are you, I guess is the question I'm asking.

02:28:44.120 --> 02:28:44.960
It wasn't my best time,

02:28:44.960 --> 02:28:48.200
but anything under 40 minutes I'm happy with.

02:28:48.200 --> 02:28:49.600
It wasn't your best time.

02:28:49.600 --> 02:28:52.440
Nah, I think I've done it a little faster before,

02:28:52.440 --> 02:28:53.280
but not much.

02:28:54.800 --> 02:28:58.120
And of my friends, I did not win on Memorial Day.

02:28:58.120 --> 02:29:00.400
One of my friends did it actually several minutes

02:29:00.400 --> 02:29:01.960
faster than me.

02:29:01.960 --> 02:29:04.880
But just to clear up one thing that I think was,

02:29:04.880 --> 02:29:07.040
I saw a bunch of questions about this on the internet.

02:29:07.040 --> 02:29:09.560
There are multiple ways to do the Murph Challenge.

02:29:09.560 --> 02:29:12.080
There's a kind of partitioned mode

02:29:12.080 --> 02:29:15.320
where you do sets of pull-ups, push-ups,

02:29:15.320 --> 02:29:17.600
and squats together.

02:29:17.600 --> 02:29:18.680
And then there's unpartitioned

02:29:18.680 --> 02:29:20.320
where you do the 100 pull-ups,

02:29:21.280 --> 02:29:22.560
and then the 200 push-ups,

02:29:22.560 --> 02:29:25.760
and then the 300 squats in serial.

02:29:25.760 --> 02:29:29.680
And obviously, if you're doing them unpartitioned,

02:29:29.680 --> 02:29:33.000
then it takes longer to get through the 100 pull-ups

02:29:33.000 --> 02:29:35.600
because anytime you're resting in between the pull-ups,

02:29:35.600 --> 02:29:38.040
you're not also doing push-ups and squats.

02:29:38.040 --> 02:29:40.560
So yeah, I'm sure my unpartitioned time

02:29:40.560 --> 02:29:42.360
would be quite a bit slower,

02:29:42.360 --> 02:29:45.200
but no, I think at the end of this,

02:29:48.440 --> 02:29:49.520
first of all, I think it's a good way

02:29:49.520 --> 02:29:50.920
to honor Memorial Day, right?

02:29:50.920 --> 02:29:55.920
It's this Lieutenant Murphy,

02:29:56.080 --> 02:30:00.920
basically, this was one of his favorite exercises,

02:30:00.920 --> 02:30:03.840
and I just try to do it on Memorial Day each year,

02:30:03.840 --> 02:30:05.680
and it's a good workout.

02:30:06.920 --> 02:30:09.440
I got my older daughters to do it with me this time.

02:30:11.120 --> 02:30:12.960
My oldest daughter wants a weight vest

02:30:12.960 --> 02:30:15.120
because she sees me doing it with a weight vest.

02:30:15.120 --> 02:30:16.360
I don't know if a seven-year-old

02:30:16.360 --> 02:30:18.920
should be using a weight vest to do pull-ups,

02:30:18.920 --> 02:30:19.760
but-

02:30:20.680 --> 02:30:23.480
Difficult question a parent must ask themselves, yes.

02:30:23.480 --> 02:30:25.640
I was like, maybe I can make you a very lightweight vest,

02:30:25.640 --> 02:30:27.160
but I didn't think it was good for this.

02:30:27.160 --> 02:30:29.000
So she basically did a quarter Murph,

02:30:29.000 --> 02:30:33.440
so she ran a quarter mile and then did 25 pull-ups,

02:30:33.440 --> 02:30:37.160
50 push-ups, and 75 air squats,

02:30:37.160 --> 02:30:41.040
then ran another quarter mile in 15 minutes,

02:30:41.040 --> 02:30:43.360
which I was pretty impressed by,

02:30:43.360 --> 02:30:48.360
and my five-year-old too, so I was excited about that.

02:30:48.360 --> 02:30:50.720
And I'm glad that I'm teaching them

02:30:50.720 --> 02:30:54.680
kind of the value of physicality, right?

02:30:54.680 --> 02:30:57.640
I think a good day for Max, my daughter,

02:30:57.640 --> 02:30:59.360
is when she gets to like go to the gym with me

02:30:59.360 --> 02:31:00.880
and cranks out a bunch of pull-ups,

02:31:00.880 --> 02:31:03.520
and I love that about her.

02:31:03.520 --> 02:31:04.760
I mean, I think it's like, good.

02:31:04.760 --> 02:31:07.440
She's, you know, hopefully I'm teaching her

02:31:07.440 --> 02:31:08.960
some good lessons, but-

02:31:08.960 --> 02:31:11.640
I mean, the broader question here is,

02:31:11.640 --> 02:31:12.680
given how busy you are,

02:31:12.680 --> 02:31:14.880
given how much stuff you have going on in your life,

02:31:14.880 --> 02:31:19.880
what's like the perfect exercise regimen for you

02:31:21.560 --> 02:31:25.320
to keep yourself happy,

02:31:25.320 --> 02:31:28.640
to keep yourself productive in your main line of work?

02:31:29.720 --> 02:31:32.120
Yeah, so I mean, right now I'm focused

02:31:32.120 --> 02:31:35.600
most of my workouts on fighting,

02:31:35.600 --> 02:31:38.960
so jiu-jitsu and MMA,

02:31:40.520 --> 02:31:43.720
but I don't know, I mean, maybe if you're a professional,

02:31:43.720 --> 02:31:45.080
you can do that every day, I can't.

02:31:45.080 --> 02:31:47.600
I just get, you know, it's too many,

02:31:47.600 --> 02:31:49.760
too many bruises and things that you need to recover from.

02:31:49.760 --> 02:31:52.040
So I do that, you know, three to four times a week,

02:31:52.040 --> 02:31:55.680
and then the other days,

02:31:56.680 --> 02:31:57.920
I just try to do a mix of things,

02:31:57.920 --> 02:32:02.040
like just cardio conditioning, strength building, mobility.

02:32:02.040 --> 02:32:04.160
So you try to do something physical every day?

02:32:04.160 --> 02:32:06.000
Yeah, I try to, unless I'm just so tired

02:32:06.000 --> 02:32:08.440
that I just need to relax,

02:32:08.440 --> 02:32:10.520
but then I'll still try to like go for a walk or something.

02:32:10.520 --> 02:32:13.520
I mean, even here, I don't know,

02:32:13.760 --> 02:32:14.880
have you been on the roof here yet?

02:32:14.880 --> 02:32:15.720
No.

02:32:15.720 --> 02:32:16.560
We'll go on the roof after this.

02:32:16.560 --> 02:32:17.400
I heard things.

02:32:17.400 --> 02:32:18.480
But it's like, we designed this building,

02:32:18.480 --> 02:32:20.400
and I put a park on the roof,

02:32:20.400 --> 02:32:22.240
so that way, that's like my meetings

02:32:22.240 --> 02:32:24.160
when I'm just doing kind of a one-on-one

02:32:24.160 --> 02:32:25.840
or talking to a couple of people.

02:32:25.840 --> 02:32:27.960
I have a very hard time just sitting.

02:32:27.960 --> 02:32:31.040
I feel like it gets super stiff, it like feels really bad,

02:32:33.040 --> 02:32:36.400
but I don't know, being physical is very important to me.

02:32:36.400 --> 02:32:39.080
I think it's, I do not believe,

02:32:39.080 --> 02:32:41.440
this gets to the question about AI,

02:32:41.440 --> 02:32:44.280
I don't think that a being is just a mind.

02:32:45.240 --> 02:32:48.520
I think we're kind of meant to do things,

02:32:48.520 --> 02:32:52.160
and like physically, and a lot of the sensations

02:32:52.160 --> 02:32:55.600
that we feel are connected to that.

02:32:55.600 --> 02:32:58.120
And I think that that's a lot of what makes you a human

02:32:58.120 --> 02:33:01.720
is basically having those,

02:33:01.720 --> 02:33:06.720
having that set of sensations and experiences around that

02:33:08.120 --> 02:33:10.360
coupled with a mind to reason about them.

02:33:11.840 --> 02:33:16.400
But I don't know, I think it's important for balance

02:33:16.400 --> 02:33:20.440
to kind of get out, challenge yourself in different ways,

02:33:20.440 --> 02:33:23.400
learn different skills, clear your mind.

02:33:23.400 --> 02:33:27.480
Do you think AI, in order to become super intelligent,

02:33:27.480 --> 02:33:28.920
and AGI should have a body?

02:33:31.600 --> 02:33:35.320
It depends on what the goal is.

02:33:35.320 --> 02:33:39.040
I think that there's this assumption in that question

02:33:39.040 --> 02:33:44.040
that intelligence should be kind of person-like.

02:33:44.240 --> 02:33:46.960
Whereas, as we were just talking about,

02:33:48.840 --> 02:33:51.720
you can have these greater than single human

02:33:51.720 --> 02:33:54.760
intelligent organisms like the stock market,

02:33:54.760 --> 02:33:56.280
which obviously do not have bodies

02:33:56.280 --> 02:33:57.920
and do not speak a language, right?

02:33:57.920 --> 02:34:02.920
And like, and just kind of have their own system.

02:34:04.200 --> 02:34:09.000
But, so I don't know, my guess is there will be limits

02:34:09.000 --> 02:34:12.120
to what a system that is purely an intelligence

02:34:12.120 --> 02:34:14.960
can understand about the human condition

02:34:14.960 --> 02:34:17.960
without having the same, not just senses,

02:34:17.960 --> 02:34:22.520
but like, our bodies change as we get older, right?

02:34:22.520 --> 02:34:24.000
And we kind of evolve.

02:34:24.000 --> 02:34:29.000
And I think that those very subtle physical changes

02:34:31.280 --> 02:34:34.680
just drive a lot of social patterns and behavior

02:34:34.680 --> 02:34:37.200
around like when you choose to have kids, right?

02:34:37.360 --> 02:34:39.080
Just like all these, you know, that's not even subtle,

02:34:39.080 --> 02:34:40.080
that's a major one, right?

02:34:40.080 --> 02:34:43.920
But like, you know, how you design things around the house.

02:34:43.920 --> 02:34:48.360
So, yeah, I mean, I think if the goal is to understand

02:34:48.360 --> 02:34:49.360
people as much as possible,

02:34:49.360 --> 02:34:54.240
I think that that's trying to model those sensations

02:34:54.240 --> 02:34:55.480
is probably somewhat important.

02:34:55.480 --> 02:34:57.200
But I think that there's a lot of value

02:34:57.200 --> 02:34:58.920
that can be created by having intelligence,

02:34:58.920 --> 02:35:02.440
even that is separate from that as a separate thing.

02:35:02.440 --> 02:35:05.480
So one of the features of being human is that

02:35:05.520 --> 02:35:08.280
we're mortal, we die.

02:35:08.280 --> 02:35:10.440
We've talked about AI a lot,

02:35:10.440 --> 02:35:12.960
but potentially replicas of ourselves.

02:35:14.000 --> 02:35:16.280
Do you think there'll be AI replicas of you and me

02:35:16.280 --> 02:35:18.840
that persist long after we're gone

02:35:18.840 --> 02:35:21.720
that family and loved ones can talk to?

02:35:24.600 --> 02:35:27.600
I think we'll have the capacity to do something like that.

02:35:27.600 --> 02:35:29.360
And I think one of the big questions

02:35:30.360 --> 02:35:32.640
that we've had to struggle with

02:35:32.640 --> 02:35:35.360
in the context of social networks

02:35:35.360 --> 02:35:36.920
is who gets to make that?

02:35:38.000 --> 02:35:40.680
And, you know, my answer to that,

02:35:40.680 --> 02:35:42.320
you know, in the context of the work that we're doing

02:35:42.320 --> 02:35:44.080
is that that should be your choice, right?

02:35:44.080 --> 02:35:46.360
I don't think anyone should be able to choose

02:35:46.360 --> 02:35:51.360
to make a Lex spot that people can choose to talk to

02:35:52.000 --> 02:35:53.640
and get to train that.

02:35:53.640 --> 02:35:54.480
And we've kind of,

02:35:54.480 --> 02:35:56.600
we have this precedent of making some of these calls

02:35:56.600 --> 02:36:00.040
where, I mean, someone can create a page

02:36:00.040 --> 02:36:03.400
for a Lex fan club,

02:36:03.400 --> 02:36:07.080
but you can't create a page and say that you're Lex, right?

02:36:08.600 --> 02:36:11.240
So I think that similarly, I think,

02:36:11.240 --> 02:36:14.400
I mean, maybe, you know, someone maybe can make a,

02:36:14.400 --> 02:36:17.480
should be able to make an AI that's a Lex admirer

02:36:17.480 --> 02:36:18.440
that someone can talk to.

02:36:18.440 --> 02:36:21.200
But I think it should ultimately be your call

02:36:21.200 --> 02:36:24.760
whether there is a Lex AI.

02:36:24.760 --> 02:36:26.760
Well, I'm open sourcing the Lex.

02:36:26.760 --> 02:36:31.760
So you're a man of faith.

02:36:31.840 --> 02:36:34.280
What role has faith played in your life

02:36:34.280 --> 02:36:35.600
and your understanding of the world

02:36:35.600 --> 02:36:37.920
and your understanding of your own life

02:36:37.920 --> 02:36:41.160
and your understanding of your work

02:36:41.160 --> 02:36:44.520
and how your work impacts the world?

02:36:46.880 --> 02:36:48.800
Yeah, I think that there's a few different parts of this

02:36:48.800 --> 02:36:49.760
that are relevant.

02:36:51.840 --> 02:36:53.560
There's sort of a philosophical part

02:36:53.560 --> 02:36:55.560
and there's a cultural part.

02:36:55.560 --> 02:36:58.680
And one of the most basic lessons

02:36:58.680 --> 02:37:01.080
is right at the beginning of Genesis, right?

02:37:01.080 --> 02:37:04.440
It's like God creates the earth and creates people

02:37:04.440 --> 02:37:06.920
and creates people in God's image.

02:37:06.920 --> 02:37:09.920
And there's the question of, you know, what does that mean?

02:37:09.920 --> 02:37:12.160
And all the only context that you have about God

02:37:12.160 --> 02:37:13.280
at that point in the Old Testament

02:37:13.280 --> 02:37:15.600
is that God has created things.

02:37:15.600 --> 02:37:18.360
So I always thought that one of the interesting lessons

02:37:18.360 --> 02:37:23.360
from that is that there's a virtue in creating things

02:37:24.320 --> 02:37:27.040
that is like whether it's artistic

02:37:27.040 --> 02:37:29.960
or whether you're building things

02:37:29.960 --> 02:37:32.280
that are functionally useful for other people.

02:37:34.360 --> 02:37:39.360
I think that that by itself is a good.

02:37:39.520 --> 02:37:44.520
And that kind of drives a lot of how I think about morality

02:37:46.360 --> 02:37:49.320
and my personal philosophy around like,

02:37:49.320 --> 02:37:51.600
what is a good life, right?

02:37:51.600 --> 02:37:56.600
I think it's one where you're helping the people around you

02:37:56.600 --> 02:38:01.600
and you're being a kind of positive creative force

02:38:01.640 --> 02:38:05.200
in the world that is helping to bring new things

02:38:05.200 --> 02:38:09.520
into the world, whether they're amazing other people, kids,

02:38:09.520 --> 02:38:14.520
or just leading to the creation of different things

02:38:15.720 --> 02:38:17.160
that wouldn't have been possible otherwise.

02:38:17.160 --> 02:38:20.560
And so that's a value for me that matters deeply.

02:38:20.560 --> 02:38:24.040
And I just love spending time with the kids

02:38:24.040 --> 02:38:25.720
and seeing that they sort of,

02:38:25.720 --> 02:38:27.760
trying to impart this value to them.

02:38:27.760 --> 02:38:31.520
And it's like, I mean, nothing makes me happier

02:38:31.520 --> 02:38:34.000
than like when I come home from work

02:38:34.000 --> 02:38:38.040
and I see like my daughter's like building Legos

02:38:38.040 --> 02:38:38.960
on the table or something.

02:38:38.960 --> 02:38:41.520
It's like, all right, I did that when I was a kid, right?

02:38:41.520 --> 02:38:43.200
So many other people were doing this.

02:38:43.200 --> 02:38:45.280
And like, I hope you don't lose that spirit

02:38:45.280 --> 02:38:46.840
where when you kind of grow up

02:38:46.840 --> 02:38:49.760
and you wanna just continue building different things

02:38:49.760 --> 02:38:54.240
no matter what it is, to me that's a lot of what matters.

02:38:55.240 --> 02:38:56.320
That's the philosophical piece.

02:38:56.320 --> 02:38:58.440
I think the cultural piece is just about community

02:38:58.440 --> 02:39:01.040
and values and that part of things

02:39:01.040 --> 02:39:02.680
I think has just become a lot more important to me

02:39:02.680 --> 02:39:03.960
since I've had kids.

02:39:05.160 --> 02:39:07.960
You know, it's almost autopilot when you're a kid,

02:39:07.960 --> 02:39:09.680
you're in the kind of getting imparted

02:39:09.680 --> 02:39:10.920
two phase of your life.

02:39:10.920 --> 02:39:14.160
But, and I didn't really think about religion

02:39:14.160 --> 02:39:16.360
that much for a while.

02:39:16.400 --> 02:39:21.000
You know, I was in college before I had kids.

02:39:21.000 --> 02:39:23.560
And then I think having kids has this way

02:39:23.560 --> 02:39:26.440
of really making you think about what traditions

02:39:26.440 --> 02:39:30.000
you wanna impart and how you wanna celebrate

02:39:30.000 --> 02:39:34.520
and like what balance you want in your life.

02:39:34.520 --> 02:39:37.360
And I mean, a bunch of the questions that you've asked

02:39:37.360 --> 02:39:40.600
and a bunch of the things that we're talking about.

02:39:40.600 --> 02:39:43.720
Just the irony of the curtains coming down

02:39:44.680 --> 02:39:46.760
as we're talking about mortality.

02:39:46.760 --> 02:39:49.360
Once again, same as last time,

02:39:49.360 --> 02:39:52.520
this is just that the universe works

02:39:52.520 --> 02:39:55.040
and we are definitely living in a simulation,

02:39:55.040 --> 02:39:58.920
but go ahead, community tradition and the values,

02:39:58.920 --> 02:40:00.600
the faith and religion is still.

02:40:00.600 --> 02:40:03.080
A lot of the topics that we've talked about today

02:40:03.080 --> 02:40:08.320
are around how do you balance,

02:40:09.560 --> 02:40:11.040
you know, whether it's running a company

02:40:11.520 --> 02:40:14.040
or different responsibilities with this.

02:40:16.240 --> 02:40:18.920
I don't know, yeah, how do you kind of balance that?

02:40:18.920 --> 02:40:21.880
And I always also just think that it's very grounding

02:40:21.880 --> 02:40:25.880
to just believe that there's something

02:40:25.880 --> 02:40:28.440
that is much bigger than you that is guiding things.

02:40:29.600 --> 02:40:34.400
That amongst other things gives you a bit of humility.

02:40:35.240 --> 02:40:38.240
As you pursue that spirit of creating,

02:40:38.240 --> 02:40:41.040
that you spoke to, creating beauty in the world.

02:40:41.040 --> 02:40:44.440
And as Dostoevsky said, beauty will save the world.

02:40:44.440 --> 02:40:47.160
Mark, I'm a huge fan of yours,

02:40:47.160 --> 02:40:49.560
honored to be able to call you a friend

02:40:49.560 --> 02:40:54.560
and I am looking forward to both kicking your ass

02:40:54.560 --> 02:40:58.520
and you kicking my ass on the mat tomorrow in Jiu-Jitsu,

02:40:58.520 --> 02:41:01.960
this incredible sport and art that we both love.

02:41:02.000 --> 02:41:07.000
This incredible sport and art that we both participate in.

02:41:07.080 --> 02:41:08.200
Thank you so much for talking today.

02:41:08.200 --> 02:41:09.480
Thank you for everything you're doing

02:41:09.480 --> 02:41:14.000
in so many exciting realms of technology and human life.

02:41:14.000 --> 02:41:16.080
I can't wait to talk to you again in the metaverse.

02:41:16.080 --> 02:41:16.920
Thank you.

02:41:18.040 --> 02:41:19.640
Thanks for listening to this conversation

02:41:19.640 --> 02:41:20.960
with Mark Zuckerberg.

02:41:20.960 --> 02:41:22.120
To support this podcast,

02:41:22.120 --> 02:41:25.120
please check out our sponsors in the description.

02:41:25.120 --> 02:41:28.280
And now let me leave you with some words from Isaac Asimov.

02:41:29.120 --> 02:41:31.880
It is change, continuing change.

02:41:31.880 --> 02:41:33.640
Inevitable change.

02:41:33.640 --> 02:41:36.120
That is the dominant factor in society today.

02:41:37.040 --> 02:41:40.040
No sensible decision can be made any longer

02:41:40.040 --> 02:41:43.680
without taking into account not only the world as it is,

02:41:43.680 --> 02:41:45.720
but the world as it will be.

02:41:46.960 --> 02:41:49.760
Thank you for listening and hope to see you next time.


WEBVTT

00:00.000 --> 00:02.720
The following is a conversation with Yann LeCun,

00:02.720 --> 00:04.560
his second time on the podcast.

00:04.560 --> 00:09.180
He is the chief AI scientist at Metta, formerly Facebook,

00:09.180 --> 00:13.080
professor at NYU, touring award winner,

00:13.080 --> 00:15.640
one of the seminal figures in the history

00:15.640 --> 00:18.480
of machine learning and artificial intelligence,

00:18.480 --> 00:21.980
and someone who is brilliant and opinionated

00:21.980 --> 00:23.440
in the best kind of way.

00:23.440 --> 00:26.000
And so it was always fun to talk to him.

00:26.000 --> 00:28.000
This is the Lex Fridman podcast.

00:28.000 --> 00:29.960
To support it, please check out our sponsors

00:29.960 --> 00:31.240
in the description.

00:31.240 --> 00:35.060
And now here's my conversation with Yann LeCun.

00:36.160 --> 00:37.560
You co-wrote the article,

00:37.560 --> 00:40.900
Self-Supervised Learning, The Dark Matter of Intelligence.

00:40.900 --> 00:43.720
Great title, by the way, with Ishan Misra.

00:43.720 --> 00:46.640
So let me ask, what is self-supervised learning

00:46.640 --> 00:49.920
and why is it the dark matter of intelligence?

00:49.920 --> 00:51.720
I'll start by the dark matter part.

00:53.120 --> 00:55.680
There is obviously a kind of learning

00:55.680 --> 00:59.880
that humans and animals are doing

00:59.880 --> 01:02.840
that we currently are not reproducing properly

01:02.840 --> 01:04.700
with machines or with AI, right?

01:04.700 --> 01:07.480
So the most popular approaches to machine learning today

01:07.480 --> 01:09.700
are, or paradigms, I should say,

01:09.700 --> 01:12.760
are supervised learning and reinforcement learning.

01:12.760 --> 01:15.160
And they are extremely inefficient.

01:15.160 --> 01:17.660
Supervised learning requires many samples

01:17.660 --> 01:19.800
for learning anything.

01:19.800 --> 01:22.580
And reinforcement learning requires a ridiculously

01:22.580 --> 01:25.280
large number of trial and errors

01:25.280 --> 01:27.340
for a system to learn anything.

01:27.780 --> 01:31.080
And that's why we don't have self-driving cars.

01:32.980 --> 01:34.780
That's a big leap from one to the other.

01:34.780 --> 01:38.780
Okay, so that to solve difficult problems,

01:38.780 --> 01:42.340
you have to have a lot of human annotation

01:42.340 --> 01:44.100
for supervised learning to work.

01:44.100 --> 01:45.540
And to solve those difficult problems

01:45.540 --> 01:46.680
with reinforcement learning,

01:46.680 --> 01:50.220
you have to have some way to maybe simulate that problem

01:50.220 --> 01:52.720
such that you can do that large scale kind of learning

01:52.720 --> 01:54.420
that reinforcement learning requires.

01:54.420 --> 01:58.340
Right, so how is it that most teenagers can learn

01:58.340 --> 02:02.300
to drive a car in about 20 hours of practice,

02:02.300 --> 02:07.300
whereas even with millions of hours of simulated practice,

02:07.500 --> 02:09.220
a self-driving car can't actually learn

02:09.220 --> 02:10.720
to drive itself properly?

02:12.100 --> 02:13.900
And so obviously we're missing something, right?

02:13.900 --> 02:15.600
And it's quite obvious for a lot of people

02:15.600 --> 02:19.760
that the immediate response you get from many people is,

02:19.760 --> 02:22.840
well, humans use their background knowledge

02:22.840 --> 02:25.840
to learn faster, and they're right.

02:25.840 --> 02:28.280
Now, how was that background knowledge acquired?

02:28.280 --> 02:30.080
And that's the big question.

02:30.080 --> 02:32.400
So now you have to ask,

02:32.400 --> 02:35.160
how do babies in the first few months of life

02:35.160 --> 02:37.120
learn how the world works?

02:37.120 --> 02:38.240
Mostly by observation,

02:38.240 --> 02:40.280
because they can hardly act in the world.

02:41.360 --> 02:42.560
And they learn an enormous amount

02:42.560 --> 02:43.840
of background knowledge about the world.

02:43.840 --> 02:47.960
That may be the basis of what we call common sense.

02:47.960 --> 02:51.240
This type of learning, it's not learning a task,

02:51.240 --> 02:53.680
it's not being reinforced for anything,

02:53.680 --> 02:55.560
it's just observing the world

02:55.560 --> 02:57.300
and figuring out how it works.

02:58.380 --> 03:01.240
Building world models, learning world models.

03:01.240 --> 03:02.120
How do we do this?

03:02.120 --> 03:04.560
And how do we reproduce this in machines?

03:04.560 --> 03:09.560
So self-supervised learning is one instance or one attempt

03:10.680 --> 03:13.120
trying to reproduce this kind of learning.

03:13.120 --> 03:16.400
Okay, so you're looking at just observation,

03:16.400 --> 03:18.720
so not even the interacting part of a child.

03:18.720 --> 03:20.940
It's just sitting there watching mom and dad

03:21.580 --> 03:23.460
walk around, pick up stuff, all of that.

03:23.460 --> 03:25.500
That's what you mean about background knowledge.

03:25.500 --> 03:27.500
Perhaps not even watching mom and dad,

03:27.500 --> 03:29.980
just watching the world go by.

03:29.980 --> 03:31.900
Just having eyes open or having eyes closed

03:31.900 --> 03:34.460
or the very act of opening and closing eyes

03:34.460 --> 03:36.260
that the world appears and disappears,

03:36.260 --> 03:37.820
all that basic information.

03:39.100 --> 03:43.100
And you're saying in order to learn to drive,

03:43.100 --> 03:45.820
like the reason humans are able to learn to drive quickly,

03:45.820 --> 03:47.340
some faster than others,

03:47.340 --> 03:48.660
is because of the background knowledge.

03:48.660 --> 03:51.780
Humans are able to watch cars operate in the world

03:51.780 --> 03:53.620
in the many years leading up to it,

03:53.620 --> 03:55.780
the physics of basic objects and all that kind of stuff.

03:55.780 --> 03:56.620
That's right.

03:56.620 --> 03:57.460
I mean, the basic physics of objects,

03:57.460 --> 04:00.900
you don't even need to know how a car works, right?

04:00.900 --> 04:02.500
Because that you can learn fairly quickly.

04:02.500 --> 04:04.400
I mean, the example I use very often is

04:04.400 --> 04:06.660
you're driving next to a cliff

04:06.660 --> 04:10.580
and you know in advance because of your understanding

04:10.580 --> 04:13.220
of intuitive physics that if you turn the wheel

04:13.220 --> 04:15.080
to the right, the car will veer to the right,

04:15.080 --> 04:17.620
will run off the cliff, fall off the cliff

04:17.620 --> 04:20.460
and nothing good will come out of this, right?

04:20.460 --> 04:22.780
But if you are a sort of, you know,

04:22.780 --> 04:25.140
tabularized reinforcement learning system

04:25.140 --> 04:27.100
that doesn't have a model of the world,

04:28.180 --> 04:30.540
you have to repeat falling off this cliff

04:30.540 --> 04:32.820
thousands of times before you figure out it's a bad idea.

04:32.820 --> 04:34.600
And then a few more thousand times

04:34.600 --> 04:37.020
before you figure out how to not do it.

04:37.020 --> 04:39.260
And then a few more million times before you figure out

04:39.260 --> 04:42.540
how to not do it in every situation you ever encounter.

04:42.540 --> 04:45.860
So self-supervised learning still has to have

04:45.860 --> 04:50.620
some source of truth being told to it by somebody.

04:50.620 --> 04:54.580
So you have to figure out a way without human assistance

04:54.580 --> 04:56.620
or without significant amount of human assistance

04:56.620 --> 04:59.140
to get that truth from the world.

04:59.140 --> 05:04.020
So the mystery there is how much signal is there,

05:04.020 --> 05:06.320
how much truth is there that the world gives you,

05:06.320 --> 05:08.200
whether it's the human world,

05:08.200 --> 05:10.060
like you watch YouTube or something like that,

05:10.060 --> 05:13.000
or it's the more natural world.

05:13.000 --> 05:14.940
So how much signal is there?

05:14.940 --> 05:18.580
So here's the trick, there is way more signal

05:18.580 --> 05:20.620
in sort of a self-supervised setting

05:20.620 --> 05:22.540
than there is in either a supervised

05:22.540 --> 05:24.540
or reinforcement setting.

05:24.540 --> 05:28.420
And this is going to my, you know, analogy of the cake.

05:29.620 --> 05:32.380
The, you know, the cake as someone has called it,

05:32.380 --> 05:36.040
where when you try to figure out how much information

05:36.040 --> 05:37.860
you ask the machine to predict

05:37.860 --> 05:41.020
and how much feedback you give the machine at every trial,

05:41.020 --> 05:41.860
in reinforcement learning,

05:41.860 --> 05:43.340
you give the machine a single scalar.

05:43.340 --> 05:45.420
You tell the machine, you did good, you did bad.

05:45.420 --> 05:49.620
And you only tell this to the machine once in a while.

05:49.620 --> 05:50.460
When I say you,

05:50.460 --> 05:52.820
it could be the universe telling the machine, right?

05:54.100 --> 05:55.860
But it's just one scalar.

05:55.860 --> 05:57.140
So as a consequence,

05:57.140 --> 05:59.580
you cannot possibly learn something very complicated

05:59.580 --> 06:01.140
without many, many, many trials

06:01.140 --> 06:04.740
where you get many, many feedbacks of this type.

06:04.740 --> 06:06.060
Supervised learning,

06:06.060 --> 06:10.220
you give a few bits to the machine at every sample.

06:11.220 --> 06:14.300
Let's say you're training a system on, you know,

06:14.300 --> 06:16.300
recognizing images on ImageNet,

06:16.300 --> 06:17.660
there is 1,000 categories

06:17.660 --> 06:20.900
that's a little less than 10 bits of information per sample.

06:22.140 --> 06:24.620
But self-supervised learning here is a setting you,

06:24.620 --> 06:26.340
ideally, we don't know how to do this yet,

06:26.340 --> 06:31.340
but ideally you would show a machine a segment of video

06:31.620 --> 06:32.540
and then stop the video

06:32.540 --> 06:35.540
and ask the machine to predict what's going to happen next.

06:37.620 --> 06:38.660
And so you let the machine predict

06:38.660 --> 06:41.380
and then you let time go by

06:41.380 --> 06:44.300
and show the machine what actually happened

06:44.300 --> 06:46.340
and hope the machine will, you know,

06:46.340 --> 06:49.380
learn to do a better job at predicting next time around.

06:49.380 --> 06:51.540
There's a huge amount of information you give the machine

06:51.540 --> 06:53.540
because it's an entire video clip

06:54.660 --> 06:59.220
of the future after the video clip you fed it

06:59.220 --> 07:00.260
in the first place.

07:00.260 --> 07:02.860
So both for language and for vision,

07:02.860 --> 07:06.900
there's a subtle, seemingly trivial construction,

07:06.900 --> 07:08.500
but maybe that's representative

07:08.500 --> 07:10.620
of what is required to create intelligence,

07:10.620 --> 07:12.900
which is filling the gap.

07:13.740 --> 07:17.180
So, it sounds dumb,

07:17.180 --> 07:20.580
but can you, it is possible

07:20.580 --> 07:23.020
that you can solve all of intelligence in this way,

07:23.020 --> 07:28.020
just for both language, just give a sentence and continue it

07:28.780 --> 07:31.140
or give a sentence and there's a gap in it,

07:32.060 --> 07:33.500
some words blanked out

07:33.500 --> 07:35.700
and you fill in what words go there.

07:35.700 --> 07:39.180
For vision, you give a sequence of images

07:39.180 --> 07:40.940
and predict what's going to happen next

07:40.940 --> 07:43.820
or you fill in what happened in between.

07:43.820 --> 07:46.940
Do you think it's possible that formulation alone

07:48.620 --> 07:50.980
as a signal for self-supervised learning

07:50.980 --> 07:53.620
can solve intelligence for vision and language?

07:53.620 --> 07:56.300
I think that's our best shot at the moment.

07:56.300 --> 07:59.140
So whether this will take us all the way

07:59.140 --> 08:01.780
to, you know, human level intelligence or something,

08:01.780 --> 08:04.860
or just cat level intelligence is not clear,

08:04.860 --> 08:07.380
but among all the possible approaches

08:07.380 --> 08:09.540
that people have proposed, I think it's our best shot.

08:09.540 --> 08:14.540
So, I think this idea of an intelligent system

08:14.660 --> 08:18.900
filling in the blanks, either predicting the future,

08:18.900 --> 08:22.220
inferring the past, filling in missing information,

08:23.820 --> 08:26.700
I'm currently filling the blank of what is beyond your head

08:26.700 --> 08:30.620
and what your head looks like from the back

08:30.620 --> 08:33.780
because I have basic knowledge about how humans are made.

08:33.780 --> 08:36.340
And I don't know what you're going to say

08:36.340 --> 08:37.300
at which point you're going to speak,

08:37.300 --> 08:38.420
whether you're going to move your head this way

08:38.420 --> 08:40.300
or that way, which way you're going to look,

08:40.300 --> 08:42.140
but I know you're not going to just dematerialize

08:42.140 --> 08:44.860
and reappear three meters down the hall

08:46.300 --> 08:49.540
because I know what's possible and what's impossible

08:49.540 --> 08:50.940
according to intuitive physics.

08:50.940 --> 08:53.300
So you have a model of what's possible, what's impossible,

08:53.300 --> 08:55.140
and then you'd be very surprised if it happens

08:55.140 --> 08:57.900
and then you'll have to reconstruct your model.

08:57.900 --> 08:59.660
Right, so that's the model of the world.

08:59.660 --> 09:02.300
It's what tells you what fills in the blanks.

09:02.300 --> 09:04.500
So given your partial information

09:04.500 --> 09:08.100
about the state of the world, given by your perception,

09:08.100 --> 09:11.380
your model of the world fills in the missing information

09:11.380 --> 09:13.780
and that includes predicting the future,

09:13.780 --> 09:16.900
re-predicting the past, filling in things

09:16.900 --> 09:18.460
you don't immediately perceive.

09:18.460 --> 09:22.300
And that doesn't have to be purely generic vision

09:22.300 --> 09:24.380
or visual information or generic language.

09:24.380 --> 09:28.940
You can go to specifics like predicting

09:28.940 --> 09:31.660
what control decision you make when you're driving in a lane.

09:31.660 --> 09:35.620
You have a sequence of images from a vehicle

09:35.620 --> 09:39.660
and then you have information if you record it on video

09:39.660 --> 09:41.820
where the car ended up going.

09:41.820 --> 09:45.540
So you can go back in time and predict where the car went

09:45.540 --> 09:46.700
based on the visual information.

09:46.700 --> 09:49.340
That's very specific, domain specific.

09:49.340 --> 09:51.500
Right, but the question is whether we can come up

09:51.500 --> 09:56.500
with sort of a generic method for training machines

09:57.020 --> 09:59.860
to do this kind of prediction or filling in the blanks.

09:59.860 --> 10:03.260
So right now, this type of approach

10:03.260 --> 10:05.580
has been unbelievably successful

10:05.580 --> 10:08.180
in the context of natural language processing.

10:08.180 --> 10:10.420
Every modern natural language processing is pre-trained

10:10.420 --> 10:13.740
in self-supervised manner to fill in the blanks.

10:13.740 --> 10:16.420
You show it a sequence of words, you remove 10% of them

10:16.420 --> 10:17.940
and then you train some gigantic neural net

10:17.940 --> 10:20.300
to predict the words that are missing.

10:20.300 --> 10:22.700
And once you've pre-trained that network,

10:22.700 --> 10:26.580
you can use the internal representation learned by it

10:26.620 --> 10:31.620
as input to something that you train supervised or whatever.

10:32.220 --> 10:33.380
That's been incredibly successful.

10:33.380 --> 10:37.580
Not so successful in images, although it's making progress

10:37.580 --> 10:42.580
and it's based on sort of manual data augmentation.

10:42.580 --> 10:43.580
We can go into this later,

10:43.580 --> 10:47.220
but what has not been successful yet is training from video.

10:47.220 --> 10:50.300
So getting a machine to learn to represent the visual world,

10:50.300 --> 10:52.820
for example, by just watching video.

10:52.820 --> 10:54.820
Nobody has really succeeded in doing this.

10:54.820 --> 10:57.540
Okay, well, let's kind of give a high-level overview.

10:57.540 --> 11:02.380
What's the difference in kind and in difficulty

11:02.380 --> 11:03.940
between vision and language?

11:03.940 --> 11:06.220
So you said people haven't been able

11:06.220 --> 11:10.460
to really kind of crack the problem of vision open

11:10.460 --> 11:11.940
in terms of self-supervised learning,

11:11.940 --> 11:13.820
but that may not be necessarily

11:13.820 --> 11:15.860
because it's fundamentally more difficult.

11:15.860 --> 11:18.700
Maybe like when we're talking about achieving,

11:18.700 --> 11:22.300
like passing the Turing test in the full spirit

11:22.300 --> 11:24.940
of the Turing test in language might be harder than vision.

11:24.940 --> 11:26.460
That's not obvious.

11:26.460 --> 11:29.460
So in your view, which is harder

11:29.460 --> 11:31.980
or perhaps are they just the same problem?

11:31.980 --> 11:34.860
When the farther we get to solving each,

11:34.860 --> 11:36.740
the more we realize it's all the same thing.

11:36.740 --> 11:37.700
It's all the same cake.

11:37.700 --> 11:40.220
I think what I'm looking for are methods

11:40.220 --> 11:43.620
that make them look essentially like the same cake,

11:43.620 --> 11:44.820
but currently they're not.

11:44.820 --> 11:48.500
And the main issue with learning world models

11:48.500 --> 11:50.180
or learning predictive models

11:50.180 --> 11:55.180
is that the prediction is never a single thing

11:55.940 --> 11:59.220
because the world is not entirely predictable.

11:59.220 --> 12:00.740
It may be deterministic or stochastic.

12:00.740 --> 12:02.980
We can get into the philosophical discussion about it,

12:02.980 --> 12:05.300
but even if it's deterministic,

12:05.300 --> 12:07.460
it's not entirely predictable.

12:07.460 --> 12:11.780
And so if I play a short video clip

12:11.780 --> 12:14.180
and then I ask you to predict what's going to happen next,

12:14.180 --> 12:16.380
there's many, many plausible continuations

12:16.380 --> 12:18.340
for that video clip.

12:18.340 --> 12:20.580
And the number of continuation grows

12:20.580 --> 12:23.940
with the interval of time that you're asking the system

12:23.940 --> 12:26.500
to make a prediction for.

12:26.500 --> 12:29.900
And so one big question with self-supervised learning

12:29.900 --> 12:32.380
is how you represent this uncertainty,

12:32.380 --> 12:35.260
how you represent multiple discrete outcomes,

12:35.260 --> 12:37.140
how you represent a sort of continuum

12:37.140 --> 12:40.460
of possible outcomes, et cetera.

12:40.460 --> 12:45.260
And if you are sort of a classical machine learning person,

12:45.260 --> 12:47.940
you say, oh, you just represent a distribution, right?

12:49.180 --> 12:52.620
And that we know how to do when we're predicting words,

12:52.620 --> 12:53.740
missing words in the text,

12:53.740 --> 12:56.900
because you can have a neural net give a score

12:56.900 --> 12:58.660
for every word in the dictionary.

12:58.660 --> 13:02.500
It's a big list of numbers, maybe 100,000 or so.

13:02.500 --> 13:05.300
And you can turn them into a probability distribution

13:05.300 --> 13:07.660
that tells you when I say a sentence,

13:09.940 --> 13:12.380
the cat is chasing the blank in the kitchen.

13:13.180 --> 13:15.860
There are only a few words that make sense there.

13:15.860 --> 13:18.380
It could be mouse, it could be a lizard spot,

13:18.380 --> 13:19.940
or something like that, right?

13:21.580 --> 13:25.860
And if I say the blank is chasing the blank in the savanna,

13:25.860 --> 13:27.860
you also have a bunch of plausible options

13:27.860 --> 13:29.220
for those two words, right?

13:30.980 --> 13:33.660
Because you have kind of an underlying reality

13:33.660 --> 13:36.300
that you can refer to to sort of fill in those blanks.

13:38.140 --> 13:42.060
So you cannot say for sure in the savanna

13:42.060 --> 13:44.500
if it's a lion or a cheetah or whatever,

13:44.500 --> 13:49.500
you cannot know if it's a zebra or a goo or whatever,

13:49.580 --> 13:51.140
well, the beast, the same thing.

13:55.380 --> 13:56.820
But you can represent the uncertainty

13:56.820 --> 13:58.500
by just a long list of numbers.

13:58.500 --> 14:01.820
Now, if I do the same thing with video

14:01.820 --> 14:04.340
and I ask you to predict a video clip,

14:04.340 --> 14:07.420
it's not a discrete set of potential frames.

14:07.420 --> 14:09.980
You have to have somewhere representing

14:09.980 --> 14:13.540
a sort of infinite number of plausible continuations

14:13.540 --> 14:17.460
of multiple frames in a high dimensional continuous space.

14:17.460 --> 14:20.540
And we just have no idea how to do this properly.

14:20.540 --> 14:22.860
Finite high dimensional.

14:22.860 --> 14:23.700
So like you-

14:23.700 --> 14:25.300
It's finite high dimensional, yes.

14:25.300 --> 14:29.100
Just like the words, they try to get it down

14:29.100 --> 14:33.380
to a small finite set of like under a million,

14:33.380 --> 14:34.220
something like that.

14:34.220 --> 14:35.060
Something like that.

14:35.060 --> 14:36.020
I mean, it's kind of ridiculous

14:36.020 --> 14:39.020
that we're doing a distribution

14:39.020 --> 14:42.940
of every single possible word for language and it works.

14:42.940 --> 14:45.340
It feels like that's a really dumb way to do it.

14:46.500 --> 14:49.740
Like it seems to be like there should be

14:49.740 --> 14:52.940
some more compressed representation

14:52.940 --> 14:55.060
of the distribution of the words.

14:55.060 --> 14:56.140
You're right about that.

14:56.140 --> 14:57.180
And so- I agree.

14:57.180 --> 14:58.940
Do you have any interesting ideas

14:58.940 --> 15:01.060
about how to represent all of the reality

15:01.060 --> 15:01.900
in a compressed way

15:01.900 --> 15:03.820
so that you can form a distribution over it?

15:03.820 --> 15:06.220
That's one of the big questions, how do you do that?

15:06.380 --> 15:08.460
I mean, what's kind of another thing

15:08.460 --> 15:13.100
that really is stupid about, I shouldn't say stupid,

15:13.100 --> 15:15.580
but like simplistic about current approaches

15:15.580 --> 15:19.380
to self-supervised learning in NLP in text

15:19.380 --> 15:21.940
is that not only do you represent

15:21.940 --> 15:23.820
a giant distribution over words,

15:23.820 --> 15:25.700
but for multiple words that are missing,

15:25.700 --> 15:27.700
those distributions are essentially independent

15:27.700 --> 15:28.540
of each other.

15:30.220 --> 15:33.060
And you don't pay too much of a price for this.

15:33.700 --> 15:37.780
So you can't, so the system in the sentence

15:37.780 --> 15:39.620
that I gave earlier,

15:39.620 --> 15:43.620
if it gives a certain probability for lion and cheetah

15:43.620 --> 15:47.780
and then a certain probability for gazelle,

15:47.780 --> 15:50.260
wildebeest and zebra,

15:51.940 --> 15:54.780
those two probabilities are independent of each other.

15:55.940 --> 15:58.020
And it's not the case that those things are independent.

15:58.020 --> 16:01.420
Lions actually attack like bigger animals than cheetahs.

16:01.420 --> 16:05.940
So there's a huge independence hypothesis in this process,

16:05.940 --> 16:07.780
which is not actually true.

16:07.780 --> 16:09.860
The reason for this is that we don't know

16:09.860 --> 16:12.980
how to represent properly distributions

16:12.980 --> 16:16.220
over combinatorial sequences of symbols,

16:16.220 --> 16:18.980
essentially because the number grows exponentially

16:18.980 --> 16:21.300
with the length of the symbols.

16:21.300 --> 16:22.740
And so we have to use tricks for this,

16:22.740 --> 16:26.380
but those techniques can get around,

16:26.380 --> 16:27.780
like don't even deal with it.

16:27.780 --> 16:31.340
So the big question is like, would there be,

16:32.260 --> 16:35.660
some sort of abstract latent representation of text

16:35.660 --> 16:40.660
that would say that when I switch lion for cheetah,

16:42.100 --> 16:45.500
I also have to switch zebra for gazelle.

16:45.500 --> 16:48.700
Yeah, so this independence assumption,

16:48.700 --> 16:51.140
let me throw some criticism at you that I often hear

16:51.140 --> 16:52.940
and see how you respond.

16:52.940 --> 16:56.020
So this kind of filling in the blanks is just statistics.

16:56.020 --> 16:57.900
You're not learning anything

16:58.900 --> 17:01.580
like the deep underlying concepts.

17:01.580 --> 17:05.660
You're just mimicking stuff from the past.

17:05.660 --> 17:07.540
You're not learning anything new

17:07.540 --> 17:10.780
such that you can use it to generalize about the world.

17:11.980 --> 17:14.100
Or, okay, let me just say the crude version,

17:14.100 --> 17:16.220
which is just statistics.

17:16.220 --> 17:17.500
It's not intelligence.

17:18.340 --> 17:19.620
What do you have to say to that?

17:19.620 --> 17:20.900
What do you usually say to that

17:20.900 --> 17:22.620
if you kind of hear this kind of thing?

17:22.620 --> 17:23.940
I don't get into those discussions

17:23.940 --> 17:26.740
because they're kind of pointless.

17:26.740 --> 17:28.740
So first of all, it's quite possible

17:28.740 --> 17:30.460
that intelligence is just statistics.

17:30.460 --> 17:32.740
It's just statistics of a particular kind.

17:32.740 --> 17:35.300
Yes, but this is the philosophical question.

17:35.300 --> 17:40.260
Is it possible that intelligence is just statistics?

17:40.260 --> 17:43.500
Yeah, but what kind of statistics?

17:43.500 --> 17:46.220
So if you are asking the question,

17:47.180 --> 17:50.660
are the models of the world that we learn,

17:50.660 --> 17:52.300
do they have some notion of causality?

17:52.300 --> 17:53.380
Yes.

17:53.380 --> 17:56.220
So if the criticism comes from people who say,

17:57.220 --> 17:59.460
current machine learning systems don't care about causality,

17:59.460 --> 18:03.140
which, by the way, is wrong, I agree with them.

18:04.660 --> 18:06.580
Your model of the world should have your actions

18:06.580 --> 18:09.100
as one of the inputs,

18:09.100 --> 18:11.460
and that will drive you to learn causal models of the world

18:11.460 --> 18:15.100
where you know what intervention in the world

18:15.100 --> 18:16.740
will cause what result,

18:16.740 --> 18:19.460
or you can do this by observation of other agents

18:19.460 --> 18:22.540
acting in the world and observing the effect,

18:22.540 --> 18:24.260
other humans, for example.

18:24.260 --> 18:28.420
So I think at some level of description,

18:28.420 --> 18:30.260
intelligence is just statistics,

18:31.660 --> 18:35.220
but that doesn't mean you won't have models

18:35.220 --> 18:40.100
that have deep mechanistic explanation for what goes on.

18:40.100 --> 18:42.060
The question is, how do you learn them?

18:42.060 --> 18:44.460
That's the question I'm interested in.

18:44.460 --> 18:49.380
Because a lot of people who actually voice their criticism

18:49.380 --> 18:51.060
say that those mechanistic model

18:51.060 --> 18:52.700
have to come from something else.

18:52.700 --> 18:54.100
They have to come from human designers.

18:54.940 --> 18:56.180
They have to come from I don't know what.

18:56.180 --> 18:57.900
And obviously, we learn them.

18:59.300 --> 19:01.780
Or if we don't learn them as an individual,

19:01.780 --> 19:04.900
nature learn them for us using evolution.

19:04.900 --> 19:07.180
So regardless of what you think,

19:07.180 --> 19:10.220
those processes have been learned somehow.

19:10.220 --> 19:12.940
So if you look at the human brain,

19:12.940 --> 19:14.660
just like when we humans introspect

19:14.660 --> 19:16.300
about how the brain works,

19:16.300 --> 19:20.260
it seems like when we think about what is intelligence,

19:20.260 --> 19:22.460
we think about the high-level stuff,

19:22.460 --> 19:23.980
like the models we've constructed,

19:23.980 --> 19:25.580
concepts like cognitive science,

19:25.580 --> 19:28.740
like concepts of memory and reasoning module,

19:28.740 --> 19:31.700
almost like these high-level modules.

19:31.700 --> 19:34.440
Is this serve as a good analogy?

19:36.180 --> 19:40.740
Are we ignoring the dark matter,

19:40.740 --> 19:43.620
the basic low-level mechanisms,

19:43.620 --> 19:45.820
just like we ignore the way the operating system works.

19:45.820 --> 19:49.700
We're just using the high-level software.

19:49.740 --> 19:52.780
We're ignoring that at the low level,

19:52.780 --> 19:56.500
the neural network might be doing something like statistics.

19:56.500 --> 19:59.180
Like me, sorry to use this word

19:59.180 --> 20:00.620
probably incorrectly and crudely,

20:00.620 --> 20:03.380
but doing this kind of fill-in-the-gap kind of learning

20:03.380 --> 20:05.820
and just kind of updating the model constantly

20:05.820 --> 20:09.300
in order to be able to support the raw sensory information

20:09.300 --> 20:12.460
to predict it and adjust to the prediction when it's wrong.

20:12.460 --> 20:15.900
But like when we look at our brain at the high level,

20:15.900 --> 20:18.420
it feels like we're doing, like we're playing chess.

20:19.140 --> 20:22.260
We're like playing with high-level concepts

20:22.260 --> 20:23.700
and we're stitching them together

20:23.700 --> 20:26.020
and we're putting them into long-term memory.

20:26.020 --> 20:28.300
But really what's going underneath

20:28.300 --> 20:30.220
is something we're not able to introspect,

20:30.220 --> 20:34.460
which is this kind of simple large neural network

20:34.460 --> 20:36.020
that's just filling in the gaps.

20:36.020 --> 20:38.260
Right, well, okay, so there's a lot of questions

20:38.260 --> 20:39.820
and a lot of answers there.

20:39.820 --> 20:40.660
Okay, so first of all,

20:40.660 --> 20:42.700
there's a whole school of thought in neuroscience,

20:42.700 --> 20:45.260
competition neuroscience in particular,

20:45.260 --> 20:47.800
that likes the idea of predictive coding,

20:47.840 --> 20:50.160
which is really related to the idea

20:50.160 --> 20:52.120
I was talking about in self-supervised learning.

20:52.120 --> 20:53.600
So everything is about prediction.

20:53.600 --> 20:56.400
The essence of intelligence is the ability to predict

20:56.400 --> 20:57.640
and everything the brain does

20:57.640 --> 21:02.200
is trying to predict everything from everything else.

21:02.200 --> 21:04.840
Okay, and that's really sort of the underlying principle,

21:04.840 --> 21:07.880
if you want, that self-supervised learning

21:07.880 --> 21:10.720
is trying to kind of reproduce this idea of prediction

21:10.720 --> 21:13.160
as kind of an essential mechanism

21:13.160 --> 21:16.400
of task-independent learning, if you want.

21:16.400 --> 21:19.400
The next step is what kind of intelligence

21:19.400 --> 21:21.200
are you interested in reproducing?

21:21.200 --> 21:24.720
And of course, we all think about trying to reproduce

21:24.720 --> 21:28.400
sort of high-level cognitive processes in humans,

21:28.400 --> 21:30.480
but like with machines, we're not even at the level

21:30.480 --> 21:35.480
of even reproducing the learning processes in a cat brain.

21:37.240 --> 21:39.440
The most intelligent or intelligent systems

21:39.440 --> 21:42.040
don't have as much common sense as a house cat.

21:43.280 --> 21:45.240
So how is it that cats learn?

21:45.760 --> 21:47.960
Cats don't do a whole lot of reasoning.

21:47.960 --> 21:49.640
They certainly have causal models.

21:49.640 --> 21:53.640
They certainly have, because many cats can figure out

21:53.640 --> 21:56.640
how they can act on the world to get what they want.

21:56.640 --> 22:01.640
They certainly have a fantastic model of intuitive physics,

22:01.800 --> 22:04.560
certainly of the dynamics of their own bodies,

22:04.560 --> 22:06.920
but also of praise and things like that, right?

22:06.920 --> 22:09.920
So they're pretty smart.

22:09.920 --> 22:12.440
They only do this with about 800 million neurons.

22:13.440 --> 22:17.720
We are not anywhere close to reproducing this kind of thing.

22:17.720 --> 22:22.200
So to some extent, I could say let's not even worry

22:22.200 --> 22:27.200
about the high-level cognition and long-term planning

22:27.280 --> 22:29.840
and reasoning that humans can do until we figure out,

22:29.840 --> 22:32.280
can we even reproduce what cats are doing?

22:32.280 --> 22:36.800
Now that said, this ability to learn world models,

22:36.800 --> 22:41.360
I think is the key to the possibility of learning machines

22:41.600 --> 22:43.200
that can also reason.

22:43.200 --> 22:45.680
So whenever I give a talk, I say there are three challenges

22:45.680 --> 22:47.360
in the three main challenges in machine learning.

22:47.360 --> 22:49.960
The first one is getting machines to learn

22:49.960 --> 22:51.840
to represent the world,

22:51.840 --> 22:54.880
and I'm proposing self-supervised learning.

22:54.880 --> 22:58.040
The second is getting machines to reason

22:58.040 --> 22:59.280
in ways that are compatible

22:59.280 --> 23:01.640
with essentially gradient-based learning,

23:01.640 --> 23:04.280
because this is what deep learning is all about, really.

23:05.320 --> 23:07.680
And the third one is something we have no idea how to solve,

23:07.680 --> 23:09.520
at least I have no idea how to solve,

23:09.520 --> 23:12.840
is can we get machines to learn

23:12.840 --> 23:16.040
hierarchical representations of action plans?

23:18.000 --> 23:18.840
We know how to train them

23:18.840 --> 23:21.280
to learn hierarchical representations of perception,

23:22.320 --> 23:23.760
with convolutional nets and things like that,

23:23.760 --> 23:26.120
and transformers, but what about action plans?

23:26.120 --> 23:28.360
Can we get them to spontaneously learn

23:28.360 --> 23:30.560
good hierarchical representations of actions?

23:30.560 --> 23:32.440
Also gradient-based.

23:32.440 --> 23:35.920
Yeah, all of that needs to be somewhat differentiable

23:35.920 --> 23:38.800
so that you can apply sort of gradient-based learning,

23:38.800 --> 23:40.840
which is really what deep learning is about.

23:42.120 --> 23:45.480
So it's background, knowledge,

23:45.480 --> 23:50.480
ability to reason in a way that's differentiable

23:50.560 --> 23:53.120
that is somehow connected,

23:53.120 --> 23:55.520
deeply integrated with that background knowledge,

23:55.520 --> 23:57.640
or builds on top of that background knowledge,

23:57.640 --> 23:59.160
and then giving that background knowledge

23:59.160 --> 24:02.400
to be able to make hierarchical plans in the world.

24:02.400 --> 24:05.520
So if you take classical optimal control,

24:05.520 --> 24:07.040
there's something in classical optimal control

24:07.040 --> 24:10.560
called model predictive control,

24:10.560 --> 24:13.880
and it's been around since the early 60s.

24:13.880 --> 24:16.880
NASA uses that to compute trajectories of rockets.

24:16.880 --> 24:20.640
And the basic idea is that you have a predictive model

24:20.640 --> 24:21.880
of the rocket, let's say,

24:21.880 --> 24:25.480
or whatever system you intend to control,

24:25.480 --> 24:28.400
which given the state of the system at time t

24:28.400 --> 24:31.680
and given an action that you're taking on the system,

24:31.680 --> 24:33.560
so for a rocket would be thrust

24:33.560 --> 24:35.640
and all the controls you can have,

24:36.640 --> 24:37.920
it gives you the state of the system

24:37.920 --> 24:39.520
at time t plus delta t, right?

24:39.520 --> 24:42.320
So basically a differential equation, something like that.

24:44.200 --> 24:47.200
And if you have this model and you have this model

24:47.200 --> 24:49.360
in the form of some sort of neural net

24:49.360 --> 24:51.600
or some sort of set of formula

24:51.600 --> 24:53.560
that you can back propagate gradient through,

24:53.560 --> 24:55.840
you can do what's called model predictive control

24:55.840 --> 24:58.240
or gradient-based model predictive control.

24:58.240 --> 25:03.240
So you can unroll that model in time,

25:04.120 --> 25:09.120
you feed it a hypothesized sequence of actions,

25:10.160 --> 25:12.720
and then you have some objective function

25:12.720 --> 25:15.160
that measures how well at the end of the trajectory

25:15.160 --> 25:18.200
the system has succeeded or matched what you want it to do.

25:19.160 --> 25:20.280
Is it a robot harm?

25:20.280 --> 25:22.480
Have you grasped the object you wanna grasp?

25:22.480 --> 25:25.480
If it's a rocket, are you at the right place

25:25.480 --> 25:28.200
near the space station, things like that.

25:28.200 --> 25:30.080
And by back propagation through time,

25:30.080 --> 25:32.040
and again, this was invented in the 1960s

25:32.840 --> 25:35.160
by optimal control theorists,

25:35.160 --> 25:39.120
you can figure out what is the optimal sequence of actions

25:39.120 --> 25:44.040
that will get my system to the best final state.

25:45.160 --> 25:47.600
So that's a form of reasoning.

25:47.600 --> 25:48.640
It's basically planning.

25:48.640 --> 25:51.160
And a lot of planning systems in robotics

25:51.160 --> 25:52.880
are actually based on this.

25:52.880 --> 25:56.200
And you can think of this as a form of reasoning.

25:56.200 --> 26:00.800
So to take the example of the teenager driving a car again,

26:00.840 --> 26:02.960
you have a pretty good dynamical model of the car.

26:02.960 --> 26:04.240
It doesn't need to be very accurate,

26:04.240 --> 26:07.400
but you know, again, that if you turn the wheel to the right

26:07.400 --> 26:09.480
and there is a cliff, you're gonna run off the cliff, right?

26:09.480 --> 26:12.160
You don't need to have a very accurate model to predict that.

26:12.160 --> 26:13.720
And you can run this in your mind

26:13.720 --> 26:16.120
and decide not to do it for that reason,

26:16.120 --> 26:17.520
because you can predict in advance

26:17.520 --> 26:18.640
that the result is gonna be bad.

26:18.640 --> 26:21.080
So you can sort of imagine different scenarios

26:21.080 --> 26:25.280
and then employ or take the first step in the scenario

26:25.280 --> 26:26.440
that is most favorable

26:26.440 --> 26:27.880
and then repeat the process of planning.

26:27.880 --> 26:30.640
That's called receding horizon model predictive control.

26:31.160 --> 26:34.680
All those things have names going back decades.

26:35.840 --> 26:40.040
And so if you're not in classical optimal control,

26:40.040 --> 26:42.440
the model of the world is not generally learned.

26:43.800 --> 26:45.600
Sometimes a few parameters you have to identify,

26:45.600 --> 26:47.120
that's called systems identification.

26:47.120 --> 26:52.000
But generally the model is mostly deterministic

26:52.000 --> 26:53.280
and mostly built by hand.

26:53.280 --> 26:55.680
So the big question of AI,

26:55.680 --> 26:58.760
I think the big challenge of AI for the next decade

26:58.760 --> 27:01.120
is how do we get machines to run predictive models

27:01.120 --> 27:03.720
of the world that deal with uncertainty

27:03.720 --> 27:05.840
and deal with the real world in all this complexity?

27:05.840 --> 27:08.160
So it's not just the trajectory of a rocket,

27:08.160 --> 27:10.240
which you can reduce to first principles.

27:10.240 --> 27:13.040
It's not even just a trajectory of a robot arm,

27:13.040 --> 27:16.320
which again, you can model by careful mathematics,

27:16.320 --> 27:17.200
but it's everything else,

27:17.200 --> 27:18.880
everything we observe in the world,

27:18.880 --> 27:23.000
people behavior, physical systems

27:23.000 --> 27:26.680
that involve collective phenomena like water

27:27.160 --> 27:32.160
or branches in a tree or something,

27:32.640 --> 27:36.720
or like complex things that humans have no trouble

27:36.720 --> 27:38.560
developing abstract representations

27:38.560 --> 27:39.880
and predictive model for,

27:39.880 --> 27:41.640
but we still don't know how to do with machines.

27:41.640 --> 27:43.920
Where do you put in these three,

27:43.920 --> 27:46.240
maybe in the planning stages,

27:47.200 --> 27:50.680
the game theoretic nature of this world,

27:50.680 --> 27:54.440
where your actions not only respond to the dynamic nature

27:54.440 --> 27:57.560
of the world, the environment, but also affect it.

27:57.560 --> 27:59.880
So if there's other humans involved,

27:59.880 --> 28:02.280
is this point number four,

28:02.280 --> 28:03.480
or is it somehow integrated

28:03.480 --> 28:05.840
into the hierarchical representation of action

28:05.840 --> 28:06.680
in your view?

28:06.680 --> 28:07.520
I think it's integrated.

28:07.520 --> 28:10.440
It's just that now your model of the world

28:10.440 --> 28:11.600
has to deal with, you know,

28:11.600 --> 28:13.120
it just makes it more complicated, right?

28:13.120 --> 28:15.640
The fact that humans are complicated

28:15.640 --> 28:17.280
and not easily predictable,

28:17.280 --> 28:19.920
that makes your model of the world much more complicated,

28:19.920 --> 28:21.360
that much more complicated.

28:21.360 --> 28:22.440
Well, there's a chess.

28:22.440 --> 28:25.320
I mean, I suppose chess is an analogy.

28:25.320 --> 28:28.160
So multi-cardinal tree search.

28:28.160 --> 28:32.080
I mean, there's a, I go, you go, I go, you go.

28:32.080 --> 28:35.640
Like Andre Capote recently gave a talk at MIT

28:35.640 --> 28:37.000
about car doors.

28:37.960 --> 28:39.320
I think there's some machine learning too,

28:39.320 --> 28:40.800
but mostly car doors.

28:40.800 --> 28:43.400
And there's a dynamic nature to the car,

28:43.400 --> 28:45.760
like the person opening the door checking.

28:45.760 --> 28:46.920
I mean, he wasn't talking about that.

28:46.920 --> 28:48.480
He was talking about the perception problem

28:48.480 --> 28:50.960
of what the ontology of what defines a car door,

28:50.960 --> 28:52.960
this big philosophical question.

28:52.960 --> 28:54.680
But to me, it was interesting because like,

28:54.680 --> 28:57.360
it's obvious that the person opening the car doors,

28:57.360 --> 28:59.600
they're trying to get out, like here in New York,

28:59.600 --> 29:01.440
trying to get out of the car.

29:01.440 --> 29:03.640
You slowing down is going to signal something.

29:03.640 --> 29:05.400
You speeding up is gonna signal something.

29:05.400 --> 29:06.480
And that's a dance.

29:06.480 --> 29:10.200
It's a asynchronous chess game.

29:10.200 --> 29:11.040
I don't know.

29:12.000 --> 29:16.920
So it feels like it's not just,

29:16.920 --> 29:18.800
I mean, I guess you can integrate all of them

29:18.800 --> 29:20.360
to one giant model.

29:20.400 --> 29:24.400
Like the entirety of these little interactions,

29:24.400 --> 29:25.800
because it's not as complicated as chess.

29:25.800 --> 29:27.200
It's just like a little dance.

29:27.200 --> 29:28.880
We do like a little dance together

29:28.880 --> 29:30.080
and then we figure it out.

29:30.080 --> 29:32.600
Well, in some ways it's way more complicated than chess

29:32.600 --> 29:37.320
because it's uncertain in a continuous manner.

29:38.280 --> 29:39.920
It doesn't feel more complicated.

29:39.920 --> 29:41.160
But it doesn't feel more complicated

29:41.160 --> 29:43.760
because that's what we've evolved to solve.

29:43.760 --> 29:45.560
This is the kind of problem we've evolved to solve.

29:45.560 --> 29:49.360
And so we're good at it because nature has made us good at it.

29:50.520 --> 29:52.320
Nature has not made us good at chess.

29:52.320 --> 29:54.160
We completely suck at chess.

29:55.680 --> 29:57.960
In fact, that's why we designed it as a game,

29:57.960 --> 29:58.960
is to be challenging.

30:00.320 --> 30:02.560
And if there is something that recent progress

30:02.560 --> 30:05.600
in chess and Go has made us realize

30:05.600 --> 30:07.880
is that humans are really terrible at those things,

30:07.880 --> 30:09.640
like really bad.

30:09.640 --> 30:11.480
There was a story right before AlphaGo

30:11.480 --> 30:15.160
that the best Go players thought

30:15.160 --> 30:17.640
there were maybe two or three stones behind

30:17.640 --> 30:19.640
an ideal player that they would call God.

30:20.680 --> 30:23.640
In fact, no, there are like nine or 10 stones behind.

30:23.640 --> 30:25.320
I mean, we're just bad.

30:25.320 --> 30:27.360
So we're not good at,

30:27.360 --> 30:30.320
and it's because we have limited working memory.

30:30.320 --> 30:32.920
We're not very good at doing this tree exploration

30:32.920 --> 30:36.760
that computers are much better at doing than we are.

30:36.760 --> 30:38.520
But we are much better at learning

30:38.520 --> 30:40.560
differentiable models of the world.

30:40.560 --> 30:43.040
I mean, I said differentiable in a kind of,

30:43.040 --> 30:46.360
I should say not differentiable in the sense that

30:46.360 --> 30:47.440
we went back far through it,

30:47.440 --> 30:50.480
but in the sense that our brain has some mechanism

30:50.480 --> 30:54.040
for estimating gradients of some kind.

30:54.040 --> 30:56.480
And that's what makes us efficient.

30:56.480 --> 31:00.480
So if you have an agent that consists of

31:01.320 --> 31:04.360
a model of the world, which in the human brain

31:04.360 --> 31:06.760
is basically the entire front half of your brain,

31:08.520 --> 31:10.200
an objective function,

31:10.200 --> 31:14.400
which in humans is a combination of two things.

31:14.400 --> 31:17.640
There is your sort of intrinsic motivation module,

31:17.640 --> 31:19.040
which is on the base of ganglia,

31:19.040 --> 31:20.080
on the base of your brain.

31:20.080 --> 31:22.480
That's the thing that measures pain and hunger

31:22.480 --> 31:26.800
and things like that, like immediate feelings and emotions.

31:28.000 --> 31:31.200
And then there is the equivalent of what people

31:31.200 --> 31:32.600
in reinforcement learning call a critic,

31:32.600 --> 31:36.080
which is a sort of module that predicts ahead

31:36.080 --> 31:41.080
what the outcome of a situation will be.

31:41.920 --> 31:43.800
And so it's not a cost function,

31:44.480 --> 31:45.440
not an objective function,

31:45.440 --> 31:49.000
but it's sort of a trained predictor

31:49.000 --> 31:50.960
of the ultimate objective function.

31:50.960 --> 31:52.600
And that also is differentiable.

31:52.600 --> 31:54.640
And so if all of this is differentiable,

31:54.640 --> 31:59.640
your cost function, your critic, your world model,

32:00.360 --> 32:03.080
then you can use gradient-based type methods

32:03.080 --> 32:05.840
to do planning, to do reasoning, to do learning,

32:06.800 --> 32:11.800
to do all the things that we'd like an agent to do.

32:11.840 --> 32:15.360
And gradient-based learning, like what's your intuition?

32:15.360 --> 32:18.400
That's probably at the core of what can solve intelligence.

32:18.400 --> 32:23.400
So you don't need like logic-based reasoning in your view.

32:25.600 --> 32:27.280
I don't know how to make logic-based reasoning

32:27.280 --> 32:30.160
compatible with efficient learning.

32:30.160 --> 32:32.320
And okay, I mean, there is a big question,

32:32.320 --> 32:33.880
perhaps a philosophical question.

32:33.880 --> 32:35.200
I mean, it's not that philosophical,

32:35.200 --> 32:40.000
but that we can ask is that all the learning algorithms

32:40.000 --> 32:43.280
we know from engineering and computer science

32:43.280 --> 32:47.400
proceed by optimizing some objective function, right?

32:48.320 --> 32:49.920
So one question we may ask is,

32:51.800 --> 32:54.720
does learning in the brain minimize an objective function?

32:54.720 --> 32:57.360
I mean, it could be a composite

32:57.360 --> 32:58.480
of multiple objective functions,

32:58.480 --> 33:00.280
but it's still an objective function.

33:01.400 --> 33:04.640
Second, if it does optimize an objective function,

33:04.640 --> 33:09.640
does it do it by some sort of gradient estimation?

33:09.920 --> 33:10.880
It doesn't need to be backprop,

33:10.880 --> 33:14.840
but some way of estimating the gradient in efficient manner,

33:14.840 --> 33:17.000
whose complexity is on the same order of magnitude

33:17.000 --> 33:20.800
as actually running the inference.

33:22.800 --> 33:24.040
Because you can't afford to do things

33:24.040 --> 33:26.560
like perturbing a weight in your brain

33:26.560 --> 33:28.080
to figure out what the effect is,

33:28.080 --> 33:31.560
and then you can do sort of estimating gradient

33:31.560 --> 33:32.400
by perturbation.

33:33.320 --> 33:35.440
To me, it seems very implausible

33:35.440 --> 33:37.560
that the brain uses some sort of

33:39.240 --> 33:43.000
zero-thought or black box gradient-free optimization

33:43.000 --> 33:45.200
because it's so much less efficient

33:45.200 --> 33:46.320
than gradient optimization.

33:46.320 --> 33:49.280
So it has to have a way of estimating gradient.

33:49.280 --> 33:52.800
Is it possible that some kind of logic-based reasoning

33:52.800 --> 33:55.400
emerges in pockets as a useful,

33:55.400 --> 33:58.120
like you said, if the brain is an objective function?

33:58.120 --> 34:01.320
Maybe it's a mechanism for creating objective functions.

34:01.320 --> 34:05.120
It's a mechanism for creating knowledge bases,

34:05.120 --> 34:08.400
for example, that can then be queried.

34:08.400 --> 34:10.280
Like maybe it's like an efficient representation

34:10.280 --> 34:12.720
of knowledge that's learned in a gradient-based way

34:12.720 --> 34:13.800
or something like that.

34:13.800 --> 34:15.960
Well, so I think there is a lot of different types

34:15.960 --> 34:17.320
of intelligence.

34:17.320 --> 34:19.640
So first of all, I think the type of logical reasoning

34:19.640 --> 34:23.760
that we think about, that we are maybe stemming

34:23.760 --> 34:27.720
from sort of classical AI of the 1970s and 80s,

34:29.040 --> 34:32.960
I think humans use that relatively rarely

34:32.960 --> 34:34.680
and are not particularly good at it.

34:35.200 --> 34:37.760
But we judge each other based on our ability

34:37.760 --> 34:40.920
to solve those rare problems.

34:40.920 --> 34:41.760
It's called an IQ test.

34:41.760 --> 34:42.920
I don't think so.

34:42.920 --> 34:45.480
Like I'm not very good at chess.

34:45.480 --> 34:47.720
Yes, I'm judging you this whole time

34:47.720 --> 34:50.000
because what we actually-

34:50.000 --> 34:53.040
With your heritage, I'm sure you're good at chess.

34:53.040 --> 34:55.320
No, stereotypes.

34:55.320 --> 34:56.880
Not all stereotypes are true.

34:58.240 --> 34:59.320
Well, I'm terrible at chess.

34:59.320 --> 35:04.320
So, but I think perhaps another type of intelligence

35:04.680 --> 35:09.000
that I have is this ability of sort of building models

35:09.000 --> 35:14.000
to the world from reasoning, obviously, but also data.

35:15.960 --> 35:18.920
And those models generally are more kind of analogical.

35:18.920 --> 35:23.920
So it's reasoning by simulation and by analogy

35:23.960 --> 35:26.920
where you use one model to apply to a new situation.

35:26.920 --> 35:28.480
Even though you've never seen that situation,

35:28.480 --> 35:31.600
you can sort of connect it to a situation

35:31.600 --> 35:33.520
you've encountered before.

35:34.040 --> 35:36.680
And your reasoning is more akin

35:36.680 --> 35:38.400
to some sort of internal simulation.

35:38.400 --> 35:41.120
So you're kind of simulating what's happening

35:41.120 --> 35:42.200
when you're building, I don't know,

35:42.200 --> 35:44.040
a box out of wood or something, right?

35:44.040 --> 35:46.400
You kind of imagine in advance,

35:46.400 --> 35:48.360
like what would be the result of cutting the wood

35:48.360 --> 35:49.600
in this particular way?

35:49.600 --> 35:51.840
Are you gonna use screws or nails or whatever?

35:52.840 --> 35:54.120
When you are interacting with someone,

35:54.120 --> 35:55.760
you also have a model of that person

35:55.760 --> 35:58.320
and sort of interact with that person.

35:59.520 --> 36:03.440
Having this model in mind to kind of tell the person

36:04.360 --> 36:05.280
what you think is useful to them.

36:05.280 --> 36:10.200
So I think this ability to construct models to the world

36:10.200 --> 36:13.880
is basically the essence of intelligence.

36:13.880 --> 36:18.240
And the ability to use it then to plan actions

36:18.240 --> 36:23.080
that will fulfill a particular criterion,

36:23.080 --> 36:25.440
of course, is necessary as well.

36:25.440 --> 36:27.760
So I'm gonna ask you a series of impossible questions

36:27.760 --> 36:30.160
as we keep asking, as I've been doing.

36:30.160 --> 36:33.440
So if that's the fundamental sort of dark matter

36:33.440 --> 36:36.600
of intelligence, this ability to form a background model,

36:36.600 --> 36:41.480
what's your intuition about how much knowledge is required?

36:41.480 --> 36:43.160
You know, I think dark matter,

36:43.160 --> 36:46.040
you can put a percentage on it

36:46.040 --> 36:50.080
of the composition of the universe

36:50.080 --> 36:51.480
and how much of it is dark matter,

36:51.480 --> 36:52.680
how much of it is dark energy.

36:52.680 --> 36:57.680
How much information do you think is required

36:57.920 --> 36:59.960
to be a house cat?

37:00.800 --> 37:02.920
So you have to be able to, when you see a box go in it,

37:02.920 --> 37:06.240
when you see a human compute the most evil action,

37:06.240 --> 37:09.600
if there's a thing that's near an edge, you knock it off.

37:09.600 --> 37:12.760
All of that, plus the extra stuff you mentioned,

37:12.760 --> 37:15.720
which is a great self-awareness of the physics

37:15.720 --> 37:18.760
of your own body and the world.

37:18.760 --> 37:21.560
How much knowledge is required, do you think, to solve it?

37:22.520 --> 37:25.640
I don't even know how to measure an answer to that question.

37:25.640 --> 37:27.600
I'm not sure how to measure it, but whatever it is,

37:27.600 --> 37:32.600
it fits in about 800,000 neurons, 800 million neurons.

37:33.920 --> 37:35.400
The representation does.

37:36.280 --> 37:38.520
Everything, all knowledge, everything, right?

37:40.320 --> 37:42.520
It's less than a billion, a dog is two billion,

37:42.520 --> 37:44.400
but a cat is less than one billion.

37:45.480 --> 37:48.120
And so multiply that by a thousand

37:48.120 --> 37:50.320
and you get the number of synapses.

37:50.320 --> 37:52.760
And I think almost all of it is learned

37:52.760 --> 37:55.920
through a sort of self-supervised learning.

37:55.920 --> 37:58.880
Although I think a tiny sliver is learned

37:58.880 --> 38:00.800
through reinforcement learning and certainly very little

38:00.800 --> 38:03.360
through classical supervised learning,

38:03.360 --> 38:05.200
although it's not even clear how supervised learning

38:05.200 --> 38:08.160
actually works in the biological world.

38:09.280 --> 38:12.880
So I think almost all of it is self-supervised learning,

38:12.880 --> 38:17.880
but it's driven by the sort of ingrained objective functions

38:18.200 --> 38:21.440
that a cat or a human have at the base of their brain,

38:21.440 --> 38:24.920
which kind of drives their behavior.

38:24.920 --> 38:28.600
So nature tells us, you're hungry.

38:29.520 --> 38:31.920
It doesn't tell us how to feed ourselves.

38:31.920 --> 38:33.520
That's something that the rest of our brain

38:33.520 --> 38:34.840
has to figure out, right?

38:35.800 --> 38:37.960
Well, it's interesting because there might be more

38:37.960 --> 38:39.720
like deeper objective functions

38:39.720 --> 38:41.320
underlying the whole thing.

38:41.320 --> 38:44.560
So hunger may be some kind of,

38:44.560 --> 38:46.160
now you go to like neurobiology,

38:46.160 --> 38:51.160
might be just the brain trying to maintain homeostasis.

38:52.160 --> 38:57.160
So hunger is just one of the human perceivable symptoms

38:57.760 --> 39:01.200
of the brain being unhappy with the way things are currently.

39:01.200 --> 39:03.000
It could be just like one really dumb

39:03.000 --> 39:04.680
objective function at the core.

39:04.680 --> 39:08.200
But that's how behavior is driven.

39:08.200 --> 39:10.960
The fact that the Orbital Ganglia

39:12.080 --> 39:14.840
drives us to do things that are different from,

39:14.840 --> 39:17.920
say, an orang-utong or certainly a cat,

39:18.200 --> 39:20.080
is what makes human nature

39:20.080 --> 39:23.280
versus orang-utong nature versus cat nature.

39:23.280 --> 39:27.640
So for example, our basal ganglia drives us

39:27.640 --> 39:32.200
to seek the company of other humans.

39:32.200 --> 39:34.520
And that's because nature has figured out

39:34.520 --> 39:36.120
that we need to be social animals

39:36.120 --> 39:37.520
for our species to survive.

39:37.520 --> 39:40.320
And it's true of many primates.

39:41.320 --> 39:42.600
It's not true of orang-utongs.

39:42.600 --> 39:44.920
Orang-utongs are solitary animals.

39:44.920 --> 39:46.920
They don't seek the company of others.

39:46.920 --> 39:48.160
In fact, they avoid them.

39:49.280 --> 39:51.040
In fact, they scream at them when they come too close

39:51.040 --> 39:52.720
because they're territorial.

39:52.720 --> 39:54.920
Because for their survival,

39:55.920 --> 39:58.280
evolution has figured out that's the best thing.

39:58.280 --> 40:00.040
I mean, they're occasionally social, of course,

40:00.040 --> 40:03.520
for reproduction and stuff like that.

40:03.520 --> 40:05.920
But they're mostly solitary.

40:05.920 --> 40:09.680
So all of those behaviors are not part of intelligence.

40:09.680 --> 40:11.080
People say, oh, you're never gonna have

40:11.080 --> 40:13.960
intelligent machines because human intelligence is social.

40:13.960 --> 40:16.840
But then you look at orang-utongs, you look at octopus.

40:16.840 --> 40:18.800
Octopus never know their parents.

40:18.800 --> 40:21.000
They barely interact with any other.

40:21.000 --> 40:23.880
And they get to be really smart in less than a year,

40:23.880 --> 40:24.920
in like half a year.

40:26.640 --> 40:27.640
In a year, they're adults.

40:27.640 --> 40:28.800
In two years, they're dead.

40:28.800 --> 40:32.840
So there are things that we think,

40:32.840 --> 40:35.760
as humans, are intimately linked with intelligence,

40:35.760 --> 40:38.840
like social interaction, like language.

40:40.720 --> 40:43.520
I think we give way too much importance to language

40:43.520 --> 40:46.760
as a substrate of intelligence as humans.

40:47.680 --> 40:48.840
Because we think our reasoning

40:48.840 --> 40:50.560
is so linked with language.

40:50.560 --> 40:54.280
So to solve the house cat intelligence problem,

40:54.280 --> 40:56.360
you think you could do it on a desert island.

40:56.360 --> 40:59.280
You could have a cat sitting there

41:01.680 --> 41:03.960
looking at the waves, at the ocean waves,

41:03.960 --> 41:06.560
and figure a lot of it out.

41:06.560 --> 41:09.600
It needs to have the right set of drives

41:10.920 --> 41:14.920
to get it to do the thing and learn the appropriate things.

41:17.680 --> 41:21.880
Baby humans are driven to learn to stand up and walk.

41:24.640 --> 41:26.040
This desire is hardwired.

41:26.040 --> 41:28.560
How to do it precisely is not, that's learned.

41:28.560 --> 41:31.800
But the desire to move around and stand up,

41:32.880 --> 41:35.960
that's sort of hardwired.

41:35.960 --> 41:38.960
It's very simple to hardwire this kind of stuff.

41:38.960 --> 41:40.720
Oh, like the desire to...

41:40.720 --> 41:42.800
Well, that's interesting.

41:42.800 --> 41:44.400
You're hardwired to wanna walk.

41:45.400 --> 41:50.200
There's gotta be a deeper need for walking.

41:50.200 --> 41:52.920
I think it was probably socially imposed by society

41:52.920 --> 41:55.360
that you need to walk all the other bipedal...

41:55.360 --> 41:57.360
No, like a lot of simple animals

41:57.360 --> 41:58.880
that would probably walk

41:58.880 --> 42:03.680
without ever watching any other members of the species.

42:03.680 --> 42:06.600
It seems like a scary thing to have to do

42:06.600 --> 42:09.080
because you suck at bipedal walking at first.

42:09.080 --> 42:13.600
It seems crawling is much safer, much more like,

42:13.800 --> 42:15.680
why are you in a hurry?

42:15.680 --> 42:18.800
Well, because you have this thing that drives you to do it,

42:20.360 --> 42:25.040
which is sort of part of the sort of human development.

42:25.040 --> 42:26.720
Is that understood actually what...

42:26.720 --> 42:28.240
Not entirely, no.

42:28.240 --> 42:29.760
What's the reason to get on two feet?

42:29.760 --> 42:30.840
It's really hard.

42:30.840 --> 42:32.800
Most animals don't get on two feet.

42:32.800 --> 42:34.240
Well, they get on four feet.

42:34.240 --> 42:36.800
Many mammals get on four feet very quickly.

42:36.800 --> 42:38.520
Some of them extremely quickly.

42:38.520 --> 42:42.640
But from the last time I've interacted with a table,

42:42.640 --> 42:44.960
that's much more stable than a thing on two legs.

42:44.960 --> 42:46.440
It's just a really hard problem.

42:46.440 --> 42:49.640
Yeah, I mean, birds have figured it out with two feet.

42:49.640 --> 42:52.000
Well, technically we can go into ontology.

42:52.000 --> 42:53.160
They have four.

42:53.160 --> 42:54.480
I guess they have two feet.

42:54.480 --> 42:56.400
They have two feet. Chickens.

42:56.400 --> 42:58.840
You know, dinosaurs have two feet, many of them.

42:58.840 --> 42:59.680
Allegedly.

43:01.560 --> 43:04.320
I'm just now learning that T-Rex was eating grass,

43:04.320 --> 43:05.400
not other animals.

43:05.400 --> 43:08.040
T-Rex might've been a friendly pet.

43:08.040 --> 43:09.280
What do you think about,

43:10.280 --> 43:13.440
I don't know if you looked at the test

43:13.440 --> 43:16.320
for general intelligence that Franois Chalet put together.

43:16.320 --> 43:17.160
I don't know if you got a chance

43:17.160 --> 43:19.600
to look at that kind of thing.

43:19.600 --> 43:21.920
What's your intuition about how to solve

43:21.920 --> 43:23.680
an IQ type of test?

43:23.680 --> 43:24.520
I don't know.

43:24.520 --> 43:26.080
I think it's so outside of my radar screen

43:26.080 --> 43:31.000
that it's not really relevant, I think, in the short term.

43:31.000 --> 43:33.880
I guess one way to ask, another way,

43:33.880 --> 43:37.240
perhaps more closer to your work,

43:37.240 --> 43:42.240
is how do you solve MNIST with very little example data?

43:42.720 --> 43:43.560
That's right.

43:43.560 --> 43:45.840
And the answer to this probably is self-supervised learning.

43:45.840 --> 43:47.280
Just learn to represent images,

43:47.280 --> 43:51.040
and then learning to recognize handwritten digits

43:51.040 --> 43:53.640
on top of this will only require a few samples.

43:53.640 --> 43:55.480
And we observe this in humans, right?

43:55.480 --> 43:58.680
You show a young child a picture book

43:58.680 --> 44:01.960
with a couple of pictures of an elephant, and that's it.

44:01.960 --> 44:03.920
The child knows what an elephant is.

44:03.920 --> 44:06.720
And we see this today with practical systems

44:06.720 --> 44:09.520
that we train image recognition systems

44:09.520 --> 44:13.680
with enormous amounts of images,

44:13.680 --> 44:15.720
either completely self-supervised

44:15.720 --> 44:16.960
or very weakly supervised.

44:16.960 --> 44:20.840
For example, you can train a neural net

44:20.840 --> 44:24.120
to predict whatever hashtag people type on Instagram, right?

44:24.120 --> 44:25.720
Then you can do this with billions of images,

44:25.720 --> 44:28.520
because there's billions per day that are showing up.

44:28.520 --> 44:30.640
So the amount of training data there

44:30.640 --> 44:32.280
is essentially unlimited.

44:32.280 --> 44:35.360
And then you take the output representation

44:35.360 --> 44:37.360
a couple layers down from the output

44:37.360 --> 44:39.440
of what the system learned,

44:39.440 --> 44:42.040
and feed this as input to a classifier

44:42.040 --> 44:43.800
for any object in the world that you want,

44:43.800 --> 44:45.000
and it works pretty well.

44:45.000 --> 44:47.640
So that's transfer learning, okay?

44:47.640 --> 44:50.160
Or weakly supervised transfer learning.

44:51.360 --> 44:53.520
People are making very, very fast progress

44:53.520 --> 44:55.320
using self-supervised learning

44:55.320 --> 44:57.440
for this kind of scenario as well.

44:58.640 --> 45:02.520
And my guess is that that's gonna be the future.

45:02.520 --> 45:03.680
For self-supervised learning,

45:03.680 --> 45:06.840
how much cleaning do you think is needed

45:06.840 --> 45:11.800
for filtering malicious signal,

45:11.800 --> 45:13.040
or what's a better term?

45:13.040 --> 45:15.760
But a lot of people use hashtags on Instagram

45:16.760 --> 45:20.080
to get good SEO

45:20.080 --> 45:23.320
that doesn't fully represent the contents of the image.

45:23.320 --> 45:24.560
They'll put a picture of a cat

45:24.560 --> 45:28.080
and hashtag it with science, awesome, fun,

45:28.080 --> 45:29.800
I don't know, all kinds,

45:29.800 --> 45:31.240
why would you put science?

45:31.240 --> 45:33.120
That's not very good SEO.

45:33.120 --> 45:35.040
The way my colleagues who worked on this project

45:35.040 --> 45:39.040
at Facebook, now Meta, Meta AI,

45:39.040 --> 45:41.640
a few years ago dealt with this is that

45:41.640 --> 45:43.840
they only selected something like 17,000 tags

45:43.840 --> 45:48.160
that correspond to kind of physical things or situations,

45:48.160 --> 45:50.400
like, you know, that has some visual content.

45:52.400 --> 45:55.880
So, you know, you wouldn't have like hash TBT

45:55.880 --> 45:57.200
or anything like that.

45:57.200 --> 46:00.920
Also, they keep a very select set of hashtags,

46:00.920 --> 46:01.760
is what you're saying?

46:01.760 --> 46:05.160
Yeah, but it's still on the order of, you know,

46:05.160 --> 46:07.960
10 to 20,000, so it's fairly large.

46:07.960 --> 46:11.280
Okay, can you tell me about data augmentation?

46:11.280 --> 46:13.120
What the heck is data augmentation?

46:13.120 --> 46:18.120
And how is it used, maybe contrast of learning for video?

46:19.120 --> 46:20.880
What are some cool ideas here?

46:20.880 --> 46:22.120
Right, so data augmentation,

46:22.120 --> 46:23.840
I mean, first, data augmentation, you know,

46:23.840 --> 46:26.120
is the idea of artificially increasing the size

46:26.120 --> 46:30.040
of your training set by distorting the images that you have

46:30.040 --> 46:32.400
in ways that don't change the nature of the image, right?

46:32.400 --> 46:34.040
So you take, you do MNIST,

46:34.040 --> 46:35.560
you can do data augmentation on MNIST,

46:35.560 --> 46:37.400
and people have done this since the 1990s, right?

46:37.400 --> 46:40.960
You take a MNIST digit and you shift it a little bit,

46:40.960 --> 46:45.960
or you change the size or rotate it, skew it, you know,

46:46.480 --> 46:47.320
et cetera.

46:47.320 --> 46:48.320
Add noise.

46:48.320 --> 46:50.880
Add noise, et cetera, and it works better.

46:50.880 --> 46:53.560
If you train a supervised classifier with augmented data,

46:53.560 --> 46:55.640
you're gonna get better results.

46:55.640 --> 46:58.680
Now, it's become really interesting

46:58.680 --> 47:00.440
over the last couple of years

47:00.440 --> 47:04.200
because a lot of self-supervised learning techniques

47:04.200 --> 47:08.040
to pre-train vision systems are based on data augmentation.

47:08.040 --> 47:12.040
And the basic techniques is originally inspired

47:12.040 --> 47:15.880
by techniques that I worked on in the early 90s

47:15.880 --> 47:17.760
and Jeff Hinton worked on also in the early 90s.

47:17.760 --> 47:20.080
They were sort of parallel work.

47:20.080 --> 47:21.680
I used to call this Siamese networks.

47:21.680 --> 47:25.000
So basically you take two identical copies

47:25.000 --> 47:27.800
of the same network, they share the same weights,

47:27.800 --> 47:31.800
and you show two different views of the same object.

47:31.800 --> 47:33.120
Either those two different views

47:33.120 --> 47:35.480
may have been obtained by data augmentation,

47:35.480 --> 47:37.720
or maybe it's two different views of the same scene

47:37.720 --> 47:39.400
from a camera that you moved,

47:39.400 --> 47:41.440
or at different times or something like that, right?

47:41.440 --> 47:44.480
Or two pictures of the same person, things like that.

47:44.480 --> 47:46.520
And then you train this neural net,

47:46.520 --> 47:48.480
those two identical copies of this neural net,

47:48.480 --> 47:51.560
to produce an output representation, a vector,

47:52.520 --> 47:56.600
in such a way that the representation for those two images

47:56.640 --> 47:58.960
are as close to each other as possible,

47:58.960 --> 48:00.920
as identical to each other as possible, right?

48:00.920 --> 48:04.720
Because you want the system to basically learn a function

48:04.720 --> 48:07.200
that will be invariant, that will not change,

48:07.200 --> 48:08.280
whose output will not change

48:08.280 --> 48:12.560
when you transform those inputs in those particular ways,

48:12.560 --> 48:14.160
right?

48:14.160 --> 48:15.760
So that's easy to do.

48:15.760 --> 48:17.800
What's complicated is how do you make sure

48:17.800 --> 48:19.600
that when you show two images that are different,

48:19.600 --> 48:22.040
the system will produce different things?

48:22.040 --> 48:26.280
Because if you don't have a specific provision for this,

48:26.280 --> 48:29.240
the system will just ignore the inputs when you train it.

48:29.240 --> 48:30.440
It will end up ignoring the input

48:30.440 --> 48:31.800
and just produce a constant vector

48:31.800 --> 48:33.720
that is the same for every input, right?

48:33.720 --> 48:35.280
That's called a collapse.

48:35.280 --> 48:36.760
Now, how do you avoid collapse?

48:36.760 --> 48:37.880
So there's two ideas.

48:38.880 --> 48:41.640
One idea that I proposed in the early 90s

48:41.640 --> 48:43.160
with my colleagues at Bell Labs,

48:43.160 --> 48:45.440
Jane Bromley and a couple other people,

48:46.360 --> 48:48.360
which we now call contrastive learning,

48:48.360 --> 48:50.080
which is to have negative examples, right?

48:50.080 --> 48:53.320
So you have pairs of images that you know are different,

48:54.480 --> 48:55.680
and you show them to the network,

48:55.680 --> 48:57.520
and those two copies,

48:57.520 --> 48:59.800
and then you push the two output vectors away

48:59.800 --> 49:01.120
from each other,

49:01.120 --> 49:02.240
and that will eventually guarantee

49:02.240 --> 49:04.920
that things that are semantically similar

49:04.920 --> 49:06.520
produce similar representations,

49:06.520 --> 49:07.360
and things that are different

49:07.360 --> 49:09.120
produce different representations.

49:10.320 --> 49:11.480
We actually came up with this idea

49:11.480 --> 49:14.520
for a project of doing signature verification.

49:14.520 --> 49:17.880
So we would collect signatures

49:17.880 --> 49:20.200
from multiple signatures on the same person,

49:20.200 --> 49:21.480
and then train a neural net

49:21.480 --> 49:23.320
to produce the same representation,

49:23.360 --> 49:25.760
and then, you know,

49:25.760 --> 49:28.600
force the system to produce different representation

49:28.600 --> 49:29.800
for different signatures.

49:31.040 --> 49:33.520
This was actually, the problem was proposed by people

49:33.520 --> 49:38.320
from what was a subsidiary of AT&T at the time called NCR,

49:38.320 --> 49:41.080
and they were interested in storing representation

49:41.080 --> 49:43.560
of the signature on the 80 bytes

49:43.560 --> 49:46.720
of the magnetic strip of a credit card.

49:46.720 --> 49:47.720
So we came up with this idea

49:47.720 --> 49:50.360
of having a neural net with 80 outputs,

49:50.360 --> 49:52.360
you know, that we would quantize on bytes

49:52.360 --> 49:53.920
so that we could encode the...

49:53.920 --> 49:55.520
And that encoding was then used to compare

49:55.520 --> 49:57.160
whether the signature matches or not.

49:57.160 --> 49:58.000
That's right.

49:58.000 --> 49:59.680
So then you would, you know, sign,

49:59.680 --> 50:00.720
it would run through the neural net,

50:00.720 --> 50:02.480
and then you would compare the output vector

50:02.480 --> 50:03.560
to whatever is stored on your card.

50:03.560 --> 50:04.720
Did it actually work?

50:04.720 --> 50:06.760
It worked, but they ended up not using it.

50:09.000 --> 50:10.200
Because nobody cares, actually.

50:10.200 --> 50:13.880
I mean, the American financial payment system

50:13.880 --> 50:16.880
is incredibly lax in that respect

50:16.880 --> 50:17.720
compared to Europe, for example.

50:17.720 --> 50:19.040
Oh, with the signatures?

50:19.040 --> 50:20.600
What's the purpose of signatures anyway?

50:20.600 --> 50:21.440
This is very difficult.

50:21.440 --> 50:23.360
Nobody looks at them, nobody cares.

50:23.360 --> 50:24.480
It's, yeah.

50:24.480 --> 50:25.320
Yeah, no.

50:25.320 --> 50:27.840
So that's contrastive learning, right?

50:27.840 --> 50:29.480
So you need positive and negative pairs.

50:29.480 --> 50:31.800
And the problem with that is that, you know,

50:31.800 --> 50:34.760
even though I had the original paper on this,

50:34.760 --> 50:36.800
I'm actually not very positive about it

50:36.800 --> 50:38.680
because it doesn't work in high dimension.

50:38.680 --> 50:41.040
If your representation is high dimensional,

50:41.040 --> 50:44.320
there's just too many ways for two things to be different.

50:44.320 --> 50:46.360
And so you would need lots and lots and lots

50:46.360 --> 50:48.280
of negative pairs.

50:48.280 --> 50:50.840
So there is a particular implementation of this,

50:50.840 --> 50:51.960
which is relatively recent,

50:51.960 --> 50:55.680
from actually the Google Toronto group,

50:55.680 --> 50:58.840
where, you know, Jeff Hinton is the senior member there,

50:58.840 --> 51:02.040
and it's called SimClear, S-I-M-C-L-R.

51:02.040 --> 51:03.760
And it, you know, basically a particular way

51:03.760 --> 51:06.800
of implementing this idea of contrastive learning,

51:06.800 --> 51:08.640
the particular objective function.

51:08.640 --> 51:13.200
Now, what I'm much more enthusiastic about these days

51:13.200 --> 51:14.640
is non-contrastive methods.

51:14.640 --> 51:19.640
So other ways to guarantee that the representations

51:20.120 --> 51:23.240
would be different for different inputs.

51:24.160 --> 51:26.160
And it's actually based on an idea

51:26.160 --> 51:29.520
that Jeff Hinton proposed in the early 90s

51:29.520 --> 51:31.960
with his student at the time, Sue Becker.

51:31.960 --> 51:32.800
And it's based on the idea

51:32.800 --> 51:34.280
of maximizing the mutual information

51:34.280 --> 51:36.160
between the outputs of the two systems.

51:36.160 --> 51:37.440
You only show positive pairs,

51:37.440 --> 51:39.120
you only show pairs of images that you know

51:39.120 --> 51:41.640
are somewhat similar,

51:41.640 --> 51:44.160
and you train the two networks to be informative,

51:45.120 --> 51:49.720
but also to be as informative of each other as possible.

51:49.720 --> 51:52.240
So basically one representation has to be predictable

51:52.240 --> 51:53.880
from the other, essentially.

51:55.400 --> 51:57.160
And, you know, he proposed that idea,

51:57.160 --> 52:00.200
had, you know, a couple of papers in the early 90s,

52:00.200 --> 52:03.080
and then nothing was done about it for decades.

52:03.080 --> 52:04.800
And I kind of revived this idea

52:04.800 --> 52:07.000
together with my post-docs at FAIR,

52:08.240 --> 52:09.640
particularly a post-doc called Stephane Denis,

52:09.640 --> 52:12.520
who is now a junior professor in Finland

52:12.760 --> 52:14.120
at the University of Aalto.

52:15.320 --> 52:17.280
We came up with something called,

52:17.280 --> 52:19.280
that we called Barlow Twins,

52:19.280 --> 52:21.600
and it's a particular way of maximizing

52:21.600 --> 52:25.240
the information content of a vector,

52:25.240 --> 52:28.080
you know, using some hypotheses.

52:28.960 --> 52:32.040
And we have kind of another version of it

52:32.040 --> 52:34.760
that's more recent now called Vicreg, V-I-C-R-E-G.

52:34.760 --> 52:37.880
That means variance, invariance, covariance, regularization.

52:37.880 --> 52:39.960
And it's the thing I'm the most excited about

52:39.960 --> 52:41.720
in machine learning in the last 15 years.

52:41.720 --> 52:44.360
I mean, I'm not, I'm really, really excited about this.

52:44.360 --> 52:47.520
What kind of data augmentation is useful

52:47.520 --> 52:50.240
for that non-contrastive learning method?

52:50.240 --> 52:52.640
Are we talking about, does that not matter that much?

52:52.640 --> 52:55.920
Or it seems like a very important part of the step.

52:55.920 --> 52:56.760
Yeah.

52:56.760 --> 52:58.120
How you generate the images that are similar,

52:58.120 --> 52:59.600
but sufficiently different.

52:59.600 --> 53:00.440
Yeah, that's right.

53:00.440 --> 53:02.400
It's an important step, and it's also an annoying step,

53:02.400 --> 53:03.760
because you need to have that knowledge

53:03.760 --> 53:06.800
of what data augmentation you can do

53:06.800 --> 53:10.400
that do not change the nature of the object.

53:10.400 --> 53:13.160
And so the standard scenario,

53:13.160 --> 53:15.400
which a lot of people working in this area are using,

53:15.400 --> 53:19.640
is you use the type of distortion.

53:19.640 --> 53:22.040
So basically you do a geometric distortion.

53:22.040 --> 53:24.240
So one basically just shifts the image a little bit.

53:24.240 --> 53:25.240
It's called cropping.

53:25.240 --> 53:27.760
Another one kind of changes the scale a little bit.

53:27.760 --> 53:29.160
Another one kind of rotates it.

53:29.160 --> 53:30.880
Another one changes the colors.

53:30.880 --> 53:32.960
You can do a shift in color balance

53:32.960 --> 53:34.080
or something like that.

53:34.960 --> 53:35.800
Saturation.

53:35.800 --> 53:37.120
Another one sort of blurs it.

53:37.120 --> 53:38.120
Another one adds noise.

53:38.120 --> 53:41.160
So you have like a catalog of kind of standard things,

53:41.160 --> 53:44.000
and people try to use the same ones for different algorithms

53:44.000 --> 53:45.960
so that they can compare.

53:45.960 --> 53:48.240
But some algorithms, some self-supervised algorithm

53:48.240 --> 53:50.680
actually can deal with much bigger,

53:50.680 --> 53:53.560
like more aggressive data augmentation, and some don't.

53:53.560 --> 53:56.400
So that kind of makes the whole thing difficult.

53:56.400 --> 53:58.800
But that's the kind of distortions we're talking about.

53:58.800 --> 54:03.600
And so you train with those distortions,

54:03.600 --> 54:07.360
and then you chop off the last layer,

54:07.360 --> 54:11.200
a couple layers of the network,

54:11.200 --> 54:13.600
and you use the representation as input to a classifier.

54:13.600 --> 54:17.680
You train the classifier on ImageNet, let's say,

54:17.680 --> 54:20.560
or whatever, and measure the performance.

54:20.560 --> 54:23.160
And interestingly enough,

54:23.160 --> 54:25.160
the methods that are really good at eliminating

54:25.160 --> 54:26.880
the information that is irrelevant,

54:26.880 --> 54:29.160
which is the distortions between those images,

54:30.080 --> 54:32.440
do a good job at eliminating it.

54:32.440 --> 54:34.120
And as a consequence,

54:34.120 --> 54:37.280
you cannot use the representations in those systems

54:37.280 --> 54:39.960
for things like object detection and localization,

54:39.960 --> 54:41.640
because that information is gone.

54:42.680 --> 54:44.800
So the type of data augmentation you need to do

54:44.800 --> 54:48.720
depends on the task you want eventually the system to solve.

54:48.720 --> 54:50.760
And the type of data augmentation,

54:50.760 --> 54:52.640
standard data augmentation that we use today,

54:52.640 --> 54:54.760
are only appropriate for object recognition

54:54.760 --> 54:56.080
or image classification.

54:56.080 --> 54:57.800
They're not appropriate for things like...

54:57.800 --> 55:00.840
Can you help me out understand why the localization's...

55:00.840 --> 55:03.800
So you're saying it's just not good at the negative,

55:03.800 --> 55:05.480
like at classifying the negative,

55:05.480 --> 55:07.960
so that's why it can't be used for the localization?

55:07.960 --> 55:10.400
No, it's just that you train the system,

55:10.400 --> 55:12.400
you give it an image,

55:12.400 --> 55:15.040
and then you give it the same image shifted and scaled,

55:15.040 --> 55:17.440
and you tell it that's the same image.

55:17.440 --> 55:19.200
So the system basically is trained

55:19.200 --> 55:22.080
to eliminate the information about position and size.

55:22.080 --> 55:26.240
So now you want to use that to figure out

55:26.240 --> 55:27.800
where an object is and what size it is.

55:27.800 --> 55:28.640
Like a bounding box,

55:28.760 --> 55:30.080
to be able to actually...

55:30.080 --> 55:34.160
Okay, it can still find the object in the image,

55:34.160 --> 55:36.000
it's just not very good at finding

55:36.000 --> 55:37.600
the exact boundaries of that object.

55:37.600 --> 55:39.000
Interesting.

55:39.000 --> 55:41.160
Interesting, which, you know,

55:41.160 --> 55:43.520
that's an interesting sort of philosophical question,

55:43.520 --> 55:46.800
how important is object localization anyway?

55:46.800 --> 55:51.280
We're like obsessed by measuring, like image segmentation,

55:51.280 --> 55:53.440
obsessed by measuring perfectly knowing

55:53.440 --> 55:56.800
the boundaries of objects when arguably

55:57.800 --> 56:01.760
that's not that essential to an understanding

56:01.760 --> 56:03.680
what are the contents of the scene.

56:03.680 --> 56:05.760
On the other hand, I think evolutionarily,

56:05.760 --> 56:08.080
the first vision systems in animals

56:08.080 --> 56:09.960
were basically all about localization,

56:09.960 --> 56:12.360
very little about recognition.

56:12.360 --> 56:15.200
And in the human brain, you have two separate pathways

56:15.200 --> 56:20.200
for recognizing the nature of a scene or an object

56:20.760 --> 56:22.200
and localizing objects.

56:22.200 --> 56:25.080
So you use the first pathway called a ventral pathway

56:25.240 --> 56:28.200
for telling what you're looking at.

56:29.160 --> 56:30.600
The other pathway, the dorsal pathway,

56:30.600 --> 56:34.160
is used for navigation, for grasping, for everything else.

56:34.160 --> 56:36.920
And basically a lot of the things you need for survival

56:36.920 --> 56:39.760
are localization and detection.

56:41.920 --> 56:45.120
Is similarity learning or contrastive learning,

56:45.120 --> 56:46.560
are these non-contrastive methods

56:46.560 --> 56:48.920
the same as understanding something?

56:48.920 --> 56:50.720
Just because you know a distorted cat

56:50.720 --> 56:52.640
is the same as a non-distorted cat,

56:52.640 --> 56:56.760
does that mean you understand what it means to be a cat?

56:56.760 --> 56:57.600
To some extent.

56:57.600 --> 57:00.120
I mean, it's a superficial understanding, obviously.

57:00.120 --> 57:02.360
But like, what is the ceiling of this method do you think?

57:02.360 --> 57:05.120
Is this just one trick on the path

57:05.120 --> 57:07.320
to doing self-supervised learning?

57:07.320 --> 57:10.040
Can we go really, really far?

57:10.040 --> 57:11.280
I think we can go really far.

57:11.280 --> 57:16.280
So if we figure out how to use techniques of that type,

57:16.400 --> 57:19.480
perhaps very different, but you know, the same nature

57:19.480 --> 57:22.480
to train a system from video

57:22.480 --> 57:24.240
to do video prediction essentially.

57:25.920 --> 57:29.120
I think we'll have a path towards,

57:30.480 --> 57:31.360
I wouldn't say unlimited,

57:31.360 --> 57:33.920
but a path towards some level of

57:35.760 --> 57:38.160
physical common sense in machines.

57:38.160 --> 57:39.880
And I also think that

57:41.480 --> 57:45.360
that ability to learn how the world works

57:45.360 --> 57:48.840
from a sort of high throughput channel like vision,

57:49.880 --> 57:53.560
is a necessary step towards

57:53.560 --> 57:55.560
sort of real artificial intelligence.

57:55.560 --> 57:58.120
In other words, I believe in grounded intelligence.

57:58.120 --> 57:59.960
I don't think we can train a machine

57:59.960 --> 58:02.240
to be intelligent purely from text.

58:02.240 --> 58:04.440
Because I think the amount of information

58:04.440 --> 58:07.280
about the world that's contained in text is tiny

58:07.280 --> 58:10.000
compared to what we need to know.

58:11.640 --> 58:13.240
So for example, let's,

58:13.240 --> 58:15.360
and you know, people have attempted to do this

58:15.360 --> 58:16.720
for 30 years, right?

58:16.720 --> 58:18.440
The psych project and things like that, right?

58:18.440 --> 58:20.640
Of basically kind of writing down all the facts

58:20.640 --> 58:22.600
that are known and hoping

58:22.600 --> 58:25.240
that some sort of common sense will emerge.

58:25.240 --> 58:27.200
I think it's basically hopeless.

58:27.200 --> 58:28.320
But let me take an example.

58:28.320 --> 58:29.680
You take an object.

58:29.680 --> 58:31.280
I describe a situation to you.

58:31.280 --> 58:33.560
I take an object, I put it on the table

58:33.560 --> 58:34.960
and I push the table.

58:34.960 --> 58:36.480
It's completely obvious to you

58:36.480 --> 58:39.240
that the object will be pushed with the table, right?

58:39.240 --> 58:40.640
Because it's sitting on it.

58:41.840 --> 58:43.440
There's no text in the world, I believe,

58:43.440 --> 58:45.040
that explains this.

58:45.040 --> 58:48.360
And so if you train a machine as powerful as it could be,

58:49.080 --> 58:53.960
your GPT-5000 or whatever it is,

58:53.960 --> 58:55.600
it's never gonna learn about this.

58:57.080 --> 59:01.080
That information is just not present in any text.

59:01.080 --> 59:03.320
Well, the question, like with the psych project,

59:03.320 --> 59:08.040
the dream, I think, is to have like 10 million,

59:08.040 --> 59:13.040
say, facts like that, that give you a head start,

59:13.320 --> 59:15.520
like a parent guiding you.

59:15.520 --> 59:17.600
Now, we humans don't need a parent to tell us

59:17.600 --> 59:19.520
that the table will move, sorry,

59:19.520 --> 59:21.720
the smartphone will move with the table.

59:21.720 --> 59:25.920
But we get a lot of guidance in other ways.

59:25.920 --> 59:28.440
So it's possible that we can give it a quick shortcut.

59:28.440 --> 59:29.480
And what about a cat?

59:29.480 --> 59:31.040
The cat knows that.

59:31.040 --> 59:33.400
No, but they evolved, so.

59:33.400 --> 59:35.440
No, they learned like us.

59:35.440 --> 59:37.320
The, sorry, the physics of stuff?

59:37.320 --> 59:38.720
Yeah.

59:38.720 --> 59:41.640
Well, yeah, so you're saying it's,

59:42.520 --> 59:45.080
so you're putting a lot of intelligence

59:45.080 --> 59:47.160
onto the nurture side, not the nature.

59:47.560 --> 59:51.920
We seem to have, you know, there's a very inefficient,

59:51.920 --> 59:54.640
arguably, process of evolution that got us

59:54.640 --> 59:56.960
from bacteria to who we are today.

59:57.800 --> 59:59.760
Started at the bottom, now we're here.

59:59.760 --> 01:00:04.760
So the question is how fundamental is that,

01:00:06.000 --> 01:00:08.560
the nature of the whole hardware?

01:00:08.560 --> 01:00:11.640
And then is there any way to shortcut it,

01:00:11.640 --> 01:00:12.480
if it's fundamental?

01:00:12.480 --> 01:00:14.240
If it's not, if it's most of intelligence,

01:00:14.240 --> 01:00:15.880
most of the cool stuff we've been talking about

01:00:15.880 --> 01:00:18.760
is mostly nurture, mostly trained.

01:00:18.760 --> 01:00:20.640
We figured it out by observing the world.

01:00:20.640 --> 01:00:24.760
We can form that big, beautiful, sexy background model

01:00:24.760 --> 01:00:27.200
that you're talking about just by sitting there.

01:00:28.840 --> 01:00:32.560
Then, okay, then you need to, then like maybe,

01:00:34.760 --> 01:00:37.800
it is all supervised learning all the way down.

01:00:37.800 --> 01:00:38.960
Self-supervised learning, say.

01:00:38.960 --> 01:00:41.320
Whatever it is that makes, you know,

01:00:41.320 --> 01:00:44.040
human intelligence different from other animals,

01:00:44.080 --> 01:00:46.320
which, you know, a lot of people think is language

01:00:46.320 --> 01:00:48.720
and logical reasoning and this kind of stuff.

01:00:48.720 --> 01:00:51.000
It cannot be that complicated because it only popped up

01:00:51.000 --> 01:00:52.840
in the last million years.

01:00:52.840 --> 01:00:54.280
Yeah, yeah, it's-

01:00:54.280 --> 01:00:59.280
And, you know, it only involves less than 1% of a genome,

01:00:59.640 --> 01:01:01.200
which is the difference between human genome

01:01:01.200 --> 01:01:03.360
and chimps or whatever.

01:01:03.360 --> 01:01:06.640
So it can't be that complicated.

01:01:06.640 --> 01:01:08.000
You know, it can't be that fundamental.

01:01:08.000 --> 01:01:10.840
I mean, most of the, so complicated stuff,

01:01:10.840 --> 01:01:12.480
already existing cats and dogs,

01:01:12.480 --> 01:01:15.800
and, you know, certainly primates, non-human primates.

01:01:17.080 --> 01:01:18.600
Yeah, that little thing with humans

01:01:18.600 --> 01:01:22.400
might be just something about social interaction

01:01:22.400 --> 01:01:25.040
and ability to maintain ideas across

01:01:25.040 --> 01:01:28.080
like a collective of people.

01:01:28.080 --> 01:01:30.800
It sounds very dramatic and very impressive,

01:01:30.800 --> 01:01:33.360
but it probably isn't, mechanistically speaking.

01:01:33.360 --> 01:01:34.640
It is, but we're not there yet.

01:01:34.640 --> 01:01:39.480
Like, you know, we have, I mean, this is number 634,

01:01:39.480 --> 01:01:42.400
you know, in the list of problems we have to solve.

01:01:43.400 --> 01:01:46.880
So basic physics of the world is number one.

01:01:46.880 --> 01:01:51.600
What do you, just a quick tangent on data augmentation.

01:01:51.600 --> 01:01:56.600
So a lot of it is hard-coded versus learned.

01:01:57.920 --> 01:02:00.960
Do you have any intuition that maybe

01:02:00.960 --> 01:02:03.600
there could be some weird data augmentation,

01:02:03.600 --> 01:02:06.200
like generative type of data augmentation,

01:02:06.200 --> 01:02:07.640
like doing something weird to images,

01:02:07.640 --> 01:02:12.640
which then improves the similarity learning process.

01:02:13.120 --> 01:02:16.280
So not just kind of dumb, simple distortions,

01:02:16.280 --> 01:02:18.120
but by you shaking your head,

01:02:18.120 --> 01:02:20.880
just saying that even simple distortions are enough.

01:02:20.880 --> 01:02:22.800
I think, no, I think data augmentation

01:02:22.800 --> 01:02:25.080
is a temporary necessary evil.

01:02:26.480 --> 01:02:28.880
So what people are working on now is two things.

01:02:28.880 --> 01:02:32.920
One is the type of self-supervised learning,

01:02:32.920 --> 01:02:34.720
like trying to translate the type

01:02:34.720 --> 01:02:37.040
of self-supervised learning people use in language,

01:02:37.040 --> 01:02:38.680
translating these two images,

01:02:38.680 --> 01:02:41.800
which is basically a denoising autoencoder method, right?

01:02:41.800 --> 01:02:46.800
So you take an image, you block, you mask some parts of it,

01:02:47.320 --> 01:02:49.520
and then you train some giant neural net

01:02:49.520 --> 01:02:52.680
to reconstruct the parts that are missing.

01:02:52.680 --> 01:02:56.200
And until very recently,

01:02:56.200 --> 01:02:59.160
there was no working methods for that.

01:02:59.160 --> 01:03:01.600
All the autoencoder type methods for images

01:03:01.600 --> 01:03:03.720
weren't producing very good representation.

01:03:03.720 --> 01:03:06.600
But there's a paper now coming out of the FAIR group

01:03:06.760 --> 01:03:09.000
in Menlo Park that actually works very well.

01:03:09.000 --> 01:03:12.160
So that doesn't require data augmentation,

01:03:12.160 --> 01:03:14.480
that requires only masking.

01:03:14.480 --> 01:03:15.320
Okay.

01:03:15.320 --> 01:03:18.680
Only masking for images, okay.

01:03:18.680 --> 01:03:20.320
Right, so you mask part of the image

01:03:20.320 --> 01:03:21.760
and you train a system,

01:03:21.760 --> 01:03:24.560
which in this case is a transformer

01:03:24.560 --> 01:03:28.400
because the transformer represents the image

01:03:28.400 --> 01:03:30.920
as non-overlapping patches,

01:03:30.920 --> 01:03:33.320
so it's easy to mask patches and things like that.

01:03:33.320 --> 01:03:35.680
Okay, but then my question transfers to that problem,

01:03:35.840 --> 01:03:40.080
masking, like why should the mask be a square or rectangle?

01:03:40.080 --> 01:03:41.600
So it doesn't matter.

01:03:41.600 --> 01:03:44.360
I think we're gonna come up probably in the future

01:03:44.360 --> 01:03:49.360
with sort of ways to mask that are kind of random,

01:03:50.480 --> 01:03:52.920
essentially, I mean, they are random already, but.

01:03:52.920 --> 01:03:55.880
No, no, but like something that's challenging,

01:03:56.800 --> 01:03:59.400
like optimally challenging.

01:03:59.400 --> 01:04:02.480
So like, I mean, maybe it's a metaphor that doesn't apply,

01:04:02.480 --> 01:04:07.480
but it seems like there's a data augmentation or masking,

01:04:07.840 --> 01:04:09.840
there's an interactive element with it.

01:04:09.840 --> 01:04:11.960
Like, you're almost like playing with an image.

01:04:11.960 --> 01:04:12.800
Yeah.

01:04:12.800 --> 01:04:14.720
And like, it's like the way we play with an image

01:04:14.720 --> 01:04:15.640
in our minds.

01:04:15.640 --> 01:04:16.680
No, but it's like dropout.

01:04:16.680 --> 01:04:18.480
It's like Boston machine training.

01:04:20.080 --> 01:04:23.200
Every time you see a percept,

01:04:23.200 --> 01:04:26.840
you also, you can perturb it in some way.

01:04:26.840 --> 01:04:31.520
And then the principle of the training procedure

01:04:31.560 --> 01:04:33.640
is to minimize the difference of the output

01:04:33.640 --> 01:04:36.960
or the representation between the clean version

01:04:36.960 --> 01:04:40.320
and the corrupted version, essentially, right?

01:04:40.320 --> 01:04:42.040
And you can do this in real time, right?

01:04:42.040 --> 01:04:44.280
So, you know, Boston machine work like this, right?

01:04:44.280 --> 01:04:47.440
You show a percept, you tell the machine

01:04:47.440 --> 01:04:49.880
that's a good combination of activities

01:04:49.880 --> 01:04:50.920
or your input neurons,

01:04:52.080 --> 01:04:57.080
and then you either let them go their merry way

01:04:57.600 --> 01:05:00.120
without clamping them to values,

01:05:00.120 --> 01:05:02.440
or you only do this with a subset.

01:05:02.440 --> 01:05:04.680
And what you're doing is you're training the system

01:05:04.680 --> 01:05:08.040
so that the stable state of the entire network

01:05:08.040 --> 01:05:10.000
is the same, regardless of whether it sees

01:05:10.000 --> 01:05:12.600
the entire input or whether it sees only part of it.

01:05:13.920 --> 01:05:15.440
You know, Denoising Autoencoder method

01:05:15.440 --> 01:05:16.840
is basically the same thing, right?

01:05:16.840 --> 01:05:19.600
You're training a system to reproduce the input,

01:05:19.600 --> 01:05:21.880
the complete inputs and filling the blanks,

01:05:21.880 --> 01:05:24.120
regardless of which parts are missing.

01:05:24.120 --> 01:05:26.280
And that's really the underlying principle.

01:05:26.280 --> 01:05:28.320
And you could imagine sort of even in the brain,

01:05:28.320 --> 01:05:30.720
some sort of neural principle where, you know,

01:05:30.720 --> 01:05:32.800
neurons kind of oscillate, right?

01:05:32.800 --> 01:05:34.640
So they take their activity

01:05:34.640 --> 01:05:36.840
and then temporarily they kind of shut off

01:05:36.840 --> 01:05:39.400
to, you know, force the rest of the system

01:05:39.400 --> 01:05:43.960
to basically reconstruct the input without their help,

01:05:43.960 --> 01:05:48.960
you know, and I mean, you can imagine, you know,

01:05:49.040 --> 01:05:51.040
more or less biologically possible process.

01:05:51.040 --> 01:05:51.880
Something like that.

01:05:51.880 --> 01:05:54.960
And I guess with this Denoising Autoencoder

01:05:55.520 --> 01:05:58.680
and masking and data augmentation,

01:05:58.680 --> 01:06:01.160
you don't have to worry about being super efficient.

01:06:01.160 --> 01:06:03.960
You can just do as much as you want

01:06:03.960 --> 01:06:06.160
and get better over time.

01:06:06.160 --> 01:06:07.080
Because I was thinking like,

01:06:07.080 --> 01:06:08.800
you might want to be clever

01:06:08.800 --> 01:06:12.000
about the way you do all of these procedures, you know,

01:06:12.000 --> 01:06:16.720
but that's only, it's somehow costly to do every iteration,

01:06:16.720 --> 01:06:17.960
but it's not really.

01:06:17.960 --> 01:06:20.280
Not really, maybe.

01:06:20.280 --> 01:06:21.480
And then there is, you know,

01:06:21.480 --> 01:06:24.160
data augmentation without explicit data augmentation.

01:06:24.160 --> 01:06:25.560
It's data augmentation by weighting,

01:06:25.560 --> 01:06:28.080
which is, you know, the sort of video prediction.

01:06:29.320 --> 01:06:31.480
You're observing a video clip,

01:06:31.480 --> 01:06:33.680
observing the, you know,

01:06:33.680 --> 01:06:36.400
the continuation of that video clip.

01:06:36.400 --> 01:06:38.040
You try to learn a representation

01:06:38.040 --> 01:06:40.240
using the joint embedding architectures

01:06:40.240 --> 01:06:43.280
in such a way that the representation of the future clip

01:06:43.280 --> 01:06:46.120
is easily predictable from the representation

01:06:46.120 --> 01:06:47.400
of the observed clip.

01:06:48.600 --> 01:06:51.840
Do you think YouTube has enough raw data

01:06:52.720 --> 01:06:56.400
from which to learn how to be a cat?

01:06:56.400 --> 01:06:57.760
I think so.

01:06:57.760 --> 01:07:01.200
So the amount of data is not the constraint.

01:07:01.200 --> 01:07:04.120
No, it would require some selection, I think.

01:07:04.120 --> 01:07:05.400
Some selection.

01:07:05.400 --> 01:07:07.080
Some selection of, you know,

01:07:07.080 --> 01:07:08.480
maybe the right type of data.

01:07:08.480 --> 01:07:11.440
You don't go down the rabbit hole of just cat videos.

01:07:11.440 --> 01:07:14.600
You might need to watch some lectures or something.

01:07:14.600 --> 01:07:15.720
No.

01:07:15.720 --> 01:07:17.480
How meta would that be

01:07:17.480 --> 01:07:21.360
if it like watches lectures about intelligence

01:07:21.400 --> 01:07:24.320
and then learns, watches your lectures at NYU

01:07:24.320 --> 01:07:26.280
and learns from that how to be intelligent?

01:07:26.280 --> 01:07:27.760
I don't think that would be enough.

01:07:27.760 --> 01:07:32.760
What's your, do you find multimodal learning interesting?

01:07:33.240 --> 01:07:35.040
We've been talking about visual language,

01:07:35.040 --> 01:07:36.440
like combining those together,

01:07:36.440 --> 01:07:38.120
maybe audio, all those kinds of things.

01:07:38.120 --> 01:07:40.360
There's a lot of things that I find interesting

01:07:40.360 --> 01:07:41.200
in the short term,

01:07:41.200 --> 01:07:44.080
but are not addressing the important problem

01:07:44.080 --> 01:07:46.640
that I think are really kind of the big challenges.

01:07:46.640 --> 01:07:48.920
So I think, you know, things like multitask learning,

01:07:48.920 --> 01:07:53.920
continual learning, you know, adversarial issues.

01:07:54.360 --> 01:07:57.000
I mean, those have, you know, great practical interests

01:07:57.000 --> 01:08:00.280
in the relatively short term, possibly,

01:08:00.280 --> 01:08:01.240
but I don't think they're fundamental.

01:08:01.240 --> 01:08:02.600
You know, active learning,

01:08:02.600 --> 01:08:04.360
even to some extent reinforcement learning.

01:08:04.360 --> 01:08:07.920
I think those things will become either obsolete

01:08:07.920 --> 01:08:12.920
or useless or easy once we figure out

01:08:13.040 --> 01:08:15.880
how to do self-supervised representation learning

01:08:15.880 --> 01:08:19.280
or learning predictive models.

01:08:19.280 --> 01:08:21.520
And so I think that's what, you know,

01:08:21.520 --> 01:08:24.400
the entire community should be focusing on.

01:08:24.400 --> 01:08:25.720
At least people are interested in sort of

01:08:25.720 --> 01:08:27.200
fundamental questions or, you know,

01:08:27.200 --> 01:08:29.480
really kind of pushing the envelope of AI

01:08:29.480 --> 01:08:31.440
towards the next stage.

01:08:31.440 --> 01:08:33.320
But of course, there's like a huge amount of, you know,

01:08:33.320 --> 01:08:35.800
very interesting work to do in sort of practical questions

01:08:35.800 --> 01:08:38.000
that have, you know, short-term impact.

01:08:38.000 --> 01:08:41.240
Well, you know, it's difficult to talk about

01:08:41.240 --> 01:08:44.240
the temporal scale because all of human civilization

01:08:44.240 --> 01:08:45.400
will eventually be destroyed

01:08:45.400 --> 01:08:48.560
because the sun will die out.

01:08:48.560 --> 01:08:50.320
And even if Elon Musk is successful

01:08:50.320 --> 01:08:54.320
in multi-planetary colonization across the galaxy,

01:08:54.320 --> 01:08:56.600
eventually the entirety of it

01:08:56.600 --> 01:08:58.960
will just become giant black holes.

01:08:58.960 --> 01:09:02.160
And that's gonna take a while though.

01:09:02.160 --> 01:09:04.840
So, but what I'm saying is then that logic

01:09:04.840 --> 01:09:07.400
can be used to say it's all meaningless.

01:09:07.400 --> 01:09:10.960
I'm saying all that to say that multitask learning

01:09:11.920 --> 01:09:16.200
might be, you're calling it practical or pragmatic

01:09:16.200 --> 01:09:18.360
or whatever, that might be the thing

01:09:18.360 --> 01:09:21.160
that achieves something very akin to intelligence

01:09:22.600 --> 01:09:26.920
while we're trying to solve the more general problem

01:09:26.920 --> 01:09:29.440
of self-supervised learning of background knowledge.

01:09:29.440 --> 01:09:30.680
So the reason I bring that up,

01:09:30.680 --> 01:09:33.080
maybe one way to ask that question,

01:09:33.080 --> 01:09:34.720
I've been very impressed by what

01:09:34.720 --> 01:09:36.480
Tesla autopilot team is doing.

01:09:36.480 --> 01:09:38.360
I don't know if you got any chance to glance

01:09:38.360 --> 01:09:42.160
at this particular one example of multitask learning

01:09:42.160 --> 01:09:45.040
where they're literally taking the problem,

01:09:45.040 --> 01:09:48.960
like, I don't know, Charles Darwin starts studying animals.

01:09:48.960 --> 01:09:52.120
They're studying the problem of driving and asking,

01:09:52.120 --> 01:09:55.080
okay, what are all the things you have to perceive?

01:09:55.080 --> 01:09:57.880
And the way they're solving it is one,

01:09:57.880 --> 01:10:00.480
there's an ontology where you're bringing that to the table.

01:10:00.480 --> 01:10:02.320
So you're formulating a bunch of different tasks.

01:10:02.320 --> 01:10:04.320
It's like over a hundred tasks or something like that

01:10:04.320 --> 01:10:05.840
that they're involved in driving.

01:10:05.840 --> 01:10:07.800
And then they're deploying it

01:10:07.800 --> 01:10:10.560
and then getting data back from people that run to trouble.

01:10:10.560 --> 01:10:12.760
And they're trying to figure out, do we add tasks?

01:10:12.760 --> 01:10:16.040
Do we, like, we focus on each individual task separately.

01:10:16.040 --> 01:10:18.360
In fact, half, so the, I would say,

01:10:18.360 --> 01:10:20.760
I'll classify Andrej Karpathy's talk in two ways.

01:10:20.760 --> 01:10:22.440
So one was about doors

01:10:22.440 --> 01:10:24.800
and the other one about how much ImageNet sucks.

01:10:24.800 --> 01:10:28.640
He kept going back and forth on those two topics,

01:10:28.640 --> 01:10:30.080
which ImageNet sucks,

01:10:30.080 --> 01:10:33.120
meaning you can't just use a single benchmark.

01:10:33.120 --> 01:10:35.600
There's so, like, you have to have

01:10:35.640 --> 01:10:37.960
like a giant suite of benchmarks

01:10:37.960 --> 01:10:40.040
to understand how well your system actually works.

01:10:40.040 --> 01:10:40.880
Oh, I agree with him.

01:10:40.880 --> 01:10:43.000
I mean, he's a very sensible guy.

01:10:43.960 --> 01:10:47.640
Now, okay, it's very clear that if you're faced

01:10:47.640 --> 01:10:49.720
with an engineering problem

01:10:49.720 --> 01:10:52.000
that you need to solve in a relatively short time,

01:10:52.000 --> 01:10:55.960
particularly if you have it almost breathing down your neck,

01:10:55.960 --> 01:10:58.720
you're going to have to take shortcuts, right?

01:10:58.720 --> 01:11:02.640
You might think about the fact that the right thing to do

01:11:02.640 --> 01:11:04.600
in the long-term solution involves

01:11:04.600 --> 01:11:06.640
some fancy self-supervised running,

01:11:06.640 --> 01:11:10.320
but you have almost breathing down your neck.

01:11:10.320 --> 01:11:13.640
And this involves human lives.

01:11:13.640 --> 01:11:17.400
And so you have to basically just do

01:11:17.400 --> 01:11:22.400
the systematic engineering and fine-tuning and refinements

01:11:23.360 --> 01:11:26.400
and trial and error and all that stuff.

01:11:26.400 --> 01:11:27.480
There's nothing wrong with that.

01:11:27.480 --> 01:11:28.680
That's called engineering.

01:11:28.680 --> 01:11:33.680
That's called putting technology out in the world.

01:11:35.880 --> 01:11:40.040
And you have to kind of ironclad it before you do this.

01:11:43.200 --> 01:11:46.280
So much for grand ideas and principles.

01:11:48.320 --> 01:11:52.320
But I'm placing myself sort of some upstream of this,

01:11:54.560 --> 01:11:55.800
quite a bit upstream of this.

01:11:55.800 --> 01:11:58.280
Your play don't think about platonic forms.

01:11:58.280 --> 01:11:59.120
You're-

01:11:59.120 --> 01:12:01.360
It's not platonic because eventually

01:12:01.360 --> 01:12:03.120
I want that stuff to get used,

01:12:03.120 --> 01:12:06.960
but it's okay if it takes five or 10 years

01:12:06.960 --> 01:12:09.360
for the community to realize this is the right thing to do.

01:12:09.360 --> 01:12:11.320
I've done this before.

01:12:11.320 --> 01:12:14.480
It's been the case before that I've made that case.

01:12:14.480 --> 01:12:17.800
I mean, if you look back in the mid-2000s, for example,

01:12:17.800 --> 01:12:19.040
and you ask yourself the question,

01:12:19.040 --> 01:12:22.120
okay, I want to recognize cars or faces or whatever.

01:12:23.120 --> 01:12:25.600
I can use convolutional nets

01:12:25.600 --> 01:12:27.400
or I can use sort of more conventional

01:12:28.400 --> 01:12:29.920
kind of computer vision techniques,

01:12:29.920 --> 01:12:32.640
using interest point detectors or sift,

01:12:32.640 --> 01:12:35.800
dense sift features and sticking an SVM on top.

01:12:35.800 --> 01:12:37.840
At that time, the data sets were so small

01:12:37.840 --> 01:12:41.200
that those methods that use more

01:12:41.200 --> 01:12:43.600
and engineering work better than components.

01:12:43.600 --> 01:12:45.600
There was just not enough data for components

01:12:45.600 --> 01:12:47.920
and components were a little slow

01:12:47.920 --> 01:12:50.880
with the kind of hardware that was available at the time.

01:12:50.880 --> 01:12:53.880
And there was a sea change when basically

01:12:53.880 --> 01:12:58.560
when data sets became bigger and GPUs became available.

01:12:58.560 --> 01:13:02.920
That's what the two of the main factors

01:13:02.920 --> 01:13:05.920
that basically made people change their mind.

01:13:07.840 --> 01:13:12.840
And you can look at the history of like all sub branches

01:13:13.360 --> 01:13:15.520
of AI or pattern recognition.

01:13:16.360 --> 01:13:19.760
And there's a similar trajectory followed by techniques

01:13:19.760 --> 01:13:23.560
where people start by engineering the hell out of it.

01:13:25.200 --> 01:13:29.160
You know, be it optical character recognition,

01:13:29.160 --> 01:13:31.760
speech recognition, computer vision,

01:13:31.760 --> 01:13:34.280
like image recognition in general,

01:13:34.280 --> 01:13:35.480
natural language understanding,

01:13:35.480 --> 01:13:38.000
like translation, things like that, right?

01:13:38.000 --> 01:13:40.000
You start to engineer the hell out of it.

01:13:41.040 --> 01:13:42.680
You start to acquire all the knowledge,

01:13:42.680 --> 01:13:44.800
the prior knowledge you know about image formation,

01:13:44.800 --> 01:13:46.600
about the shape of characters,

01:13:46.600 --> 01:13:49.600
about morphological operations,

01:13:49.600 --> 01:13:50.920
about like feature extraction,

01:13:50.920 --> 01:13:52.800
Fourier transforms, you know,

01:13:52.800 --> 01:13:54.480
vernicane moments, you know, whatever, right?

01:13:54.480 --> 01:13:56.320
People have come up with thousands of ways

01:13:56.320 --> 01:13:57.720
of representing images

01:13:57.720 --> 01:14:01.640
so that they could be easily classified afterwards.

01:14:01.640 --> 01:14:03.040
Same for speech recognition, right?

01:14:03.040 --> 01:14:05.040
There is, you know, it took decades for people

01:14:05.040 --> 01:14:06.960
to figure out a good front end

01:14:06.960 --> 01:14:09.720
to pre-process speech signals

01:14:09.720 --> 01:14:12.240
so that all the information about what is being said

01:14:12.240 --> 01:14:14.480
is preserved, but most of the information

01:14:14.480 --> 01:14:16.960
about the identity of the speaker is gone.

01:14:17.120 --> 01:14:21.920
You know, Kestrel coefficients or whatever, right?

01:14:21.920 --> 01:14:24.560
And same for text, right?

01:14:24.560 --> 01:14:27.480
You do name entity recognition and you parse

01:14:27.480 --> 01:14:32.480
and you do tagging of the parts of speech

01:14:32.920 --> 01:14:35.600
and you know, you do this sort of tree representation

01:14:35.600 --> 01:14:37.520
of clauses and all that stuff, right?

01:14:37.520 --> 01:14:39.240
Before you can do anything.

01:14:41.920 --> 01:14:44.640
So that's how it starts, right?

01:14:44.640 --> 01:14:46.320
Just engineer the hell out of it.

01:14:46.320 --> 01:14:49.040
And then you start having data

01:14:49.040 --> 01:14:51.280
and maybe you have more powerful computers,

01:14:51.280 --> 01:14:53.480
maybe you know something about statistical learning.

01:14:53.480 --> 01:14:54.680
So you start using machine learning

01:14:54.680 --> 01:14:55.960
and it's usually a small sliver

01:14:55.960 --> 01:14:57.920
on top of your kind of handcrafted system

01:14:57.920 --> 01:15:00.640
where, you know, you extract features by hand.

01:15:00.640 --> 01:15:02.640
Okay, and now, you know, nowadays,

01:15:02.640 --> 01:15:04.560
the standard way of doing this is that you train

01:15:04.560 --> 01:15:06.480
the entire thing end to end with a deep learning system

01:15:06.480 --> 01:15:07.800
and it learns its own features

01:15:07.800 --> 01:15:11.960
and, you know, speech recognition systems nowadays

01:15:11.960 --> 01:15:13.920
or OCR systems are completely end to end.

01:15:13.920 --> 01:15:16.400
It's, you know, it's some giant neural net

01:15:16.400 --> 01:15:18.920
that takes raw waveforms

01:15:18.920 --> 01:15:21.440
and produces a sequence of characters coming out.

01:15:21.440 --> 01:15:23.080
And it's just a huge neural net, right?

01:15:23.080 --> 01:15:25.000
There's no, you know, Markov model,

01:15:25.000 --> 01:15:27.400
there's no language model that is explicit

01:15:27.400 --> 01:15:29.560
other than, you know, something that's ingrained

01:15:29.560 --> 01:15:31.960
in the sort of neural language model, if you want.

01:15:31.960 --> 01:15:34.360
Same for translation, same for all kinds of stuff.

01:15:34.360 --> 01:15:37.400
So you see this continuous evolution

01:15:37.400 --> 01:15:41.360
from, you know, less and less hand crafting

01:15:41.360 --> 01:15:42.720
and more and more learning.

01:15:44.320 --> 01:15:49.320
And I think, I mean, it's true in biology as well.

01:15:50.680 --> 01:15:52.880
So, I mean, we might disagree about this,

01:15:52.880 --> 01:15:56.840
maybe not, in this one little piece at the end,

01:15:56.840 --> 01:15:58.400
you mentioned active learning.

01:15:59.480 --> 01:16:01.440
It feels like active learning,

01:16:01.440 --> 01:16:04.720
which is the selection of data and also the interactivity

01:16:04.720 --> 01:16:06.800
needs to be part of this giant neural network.

01:16:06.800 --> 01:16:08.360
You cannot just be an observer

01:16:08.360 --> 01:16:09.720
to do self-supervised learning.

01:16:09.720 --> 01:16:12.200
You have to, well, I don't,

01:16:12.200 --> 01:16:14.560
self-supervised learning is just a word,

01:16:14.560 --> 01:16:16.760
but I would, whatever this giant stack

01:16:16.760 --> 01:16:19.640
of a neural network that's automatically learning,

01:16:19.640 --> 01:16:24.640
it feels, my intuition is that you have to have a system,

01:16:26.520 --> 01:16:30.200
whether it's a physical robot or a digital robot

01:16:30.200 --> 01:16:32.360
that's interacting with the world

01:16:32.360 --> 01:16:35.960
and doing so in a flawed way and improving over time

01:16:36.880 --> 01:16:41.520
in order to form the self-supervised learning.

01:16:41.520 --> 01:16:44.960
Well, you can't just give it a giant sea of data.

01:16:44.960 --> 01:16:47.120
Okay, I agree and I disagree.

01:16:47.120 --> 01:16:52.000
I agree in the sense that I think, I agree in two ways.

01:16:52.000 --> 01:16:54.160
The first way I agree is that if you want

01:16:55.160 --> 01:16:57.480
and you certainly need a causal model of the world

01:16:57.480 --> 01:17:00.480
that allows you to predict the consequences of your actions,

01:17:00.480 --> 01:17:02.760
to train that model, you need to take actions, right?

01:17:02.760 --> 01:17:06.120
You need to be able to act in a world and see the effect

01:17:06.120 --> 01:17:08.520
for you to learn causal models of the world.

01:17:08.520 --> 01:17:11.560
So that's not obvious because you can observe others.

01:17:11.560 --> 01:17:12.400
You can observe others.

01:17:12.400 --> 01:17:14.720
And you can infer that they're similar to you

01:17:14.720 --> 01:17:16.000
and then you can learn from that.

01:17:16.000 --> 01:17:18.840
Yeah, but then you have to kind of hardware that part, right?

01:17:18.840 --> 01:17:20.640
And you know, mirror neurons and all that stuff, right?

01:17:20.640 --> 01:17:23.280
So, and it's not clear to me

01:17:23.280 --> 01:17:24.440
how you would do this in a machine.

01:17:24.440 --> 01:17:29.440
So I think the action part would be necessary

01:17:30.240 --> 01:17:32.640
for having causal models of the world.

01:17:33.520 --> 01:17:36.680
The second reason it may be necessary

01:17:36.680 --> 01:17:40.600
or at least more efficient is that active learning

01:17:40.600 --> 01:17:44.920
basically goes for the juggler of what you don't know, right?

01:17:44.920 --> 01:17:49.920
Is obvious areas of uncertainty about your world

01:17:50.360 --> 01:17:53.000
and about how the world behaves.

01:17:53.000 --> 01:17:56.280
And you can resolve this uncertainty

01:17:56.280 --> 01:17:59.040
by systematic exploration of that part

01:17:59.040 --> 01:18:00.320
that you don't know.

01:18:00.320 --> 01:18:01.760
And if you know that you don't know,

01:18:01.760 --> 01:18:03.080
then it makes you curious.

01:18:03.080 --> 01:18:05.640
You kind of look into situations that...

01:18:05.640 --> 01:18:09.280
And, you know, across the animal world,

01:18:09.280 --> 01:18:13.840
different species at different levels of curiosity, right?

01:18:13.840 --> 01:18:15.160
Depending on how they're built, right?

01:18:15.160 --> 01:18:18.800
So, you know, cats and rats are incredibly curious,

01:18:18.800 --> 01:18:20.680
dogs not so much, I mean less.

01:18:20.680 --> 01:18:22.160
Yeah, so it could be useful

01:18:22.160 --> 01:18:23.960
to have that kind of curiosity.

01:18:23.960 --> 01:18:24.800
So it'd be useful,

01:18:24.800 --> 01:18:27.040
but curiosity just makes the process faster.

01:18:27.040 --> 01:18:28.840
It doesn't make the process exist.

01:18:30.360 --> 01:18:35.360
So what process, what learning process is it that...

01:18:35.640 --> 01:18:38.680
Active learning makes more efficient.

01:18:38.680 --> 01:18:41.080
And I'm asking that first question, you know,

01:18:43.200 --> 01:18:44.800
you know, we haven't answered that question yet.

01:18:44.800 --> 01:18:46.720
So, you know, I'll worry about active learning

01:18:46.720 --> 01:18:48.120
once this question is...

01:18:48.120 --> 01:18:50.840
So it's the more fundamental question to ask.

01:18:50.840 --> 01:18:54.600
And if active learning or interaction

01:18:54.600 --> 01:18:57.080
increases the efficiency of the learning,

01:18:57.080 --> 01:19:00.240
see, sometimes it becomes very different

01:19:00.240 --> 01:19:04.800
if the increase is several orders of magnitude, right?

01:19:04.840 --> 01:19:05.680
That's true.

01:19:05.680 --> 01:19:08.120
But fundamentally, it's still the same thing

01:19:08.120 --> 01:19:11.200
in building up the intuition about how to,

01:19:11.200 --> 01:19:13.880
in a self-supervised way to construct background models,

01:19:13.880 --> 01:19:18.680
efficient or inefficient, is the core problem.

01:19:18.680 --> 01:19:20.840
What do you think about Yoshua Bengios

01:19:20.840 --> 01:19:22.920
talking about consciousness

01:19:22.920 --> 01:19:24.560
and all of these kinds of concepts?

01:19:24.560 --> 01:19:29.560
Okay, I don't know what consciousness is, but...

01:19:30.360 --> 01:19:32.000
It's a good opener.

01:19:32.000 --> 01:19:33.600
And to some extent, a lot of the things

01:19:33.600 --> 01:19:35.320
that are said about consciousness

01:19:35.320 --> 01:19:38.720
remind me of the questions people were asking themselves

01:19:38.720 --> 01:19:41.360
in the 18th century or 17th century

01:19:41.360 --> 01:19:45.080
when they discovered that, you know, how the eye works

01:19:45.080 --> 01:19:47.080
and the fact that the image at the back of the eye

01:19:47.080 --> 01:19:49.880
was upside down, right?

01:19:49.880 --> 01:19:52.360
Because you have a lens and so on your retina,

01:19:52.360 --> 01:19:54.760
the image that forms is an image of the world,

01:19:54.760 --> 01:19:55.600
but it's upside down.

01:19:55.600 --> 01:19:58.200
How is it that you see right side up?

01:19:58.200 --> 01:20:00.480
And, you know, with what we know today in science,

01:20:00.720 --> 01:20:03.840
we realize this question doesn't make any sense

01:20:03.840 --> 01:20:06.320
or is kind of ridiculous in some way, right?

01:20:06.320 --> 01:20:08.160
So I think a lot of what is said about consciousness

01:20:08.160 --> 01:20:09.000
is of that nature.

01:20:09.000 --> 01:20:10.920
Now, that said, there's a lot of really smart people

01:20:10.920 --> 01:20:13.800
for whom I have a lot of respect

01:20:13.800 --> 01:20:15.040
who are talking about this topic,

01:20:15.040 --> 01:20:16.520
people like David Chalmers,

01:20:16.520 --> 01:20:18.120
who is a colleague of mine at NYU.

01:20:19.680 --> 01:20:23.680
I have kind of an unorthodox folk

01:20:25.760 --> 01:20:29.160
speculative hypothesis about consciousness.

01:20:29.200 --> 01:20:32.040
So we're talking about the study of world model.

01:20:32.040 --> 01:20:35.520
And I think, you know, our entire prefrontal cortex

01:20:35.520 --> 01:20:39.320
basically is the engine for our world model.

01:20:40.800 --> 01:20:44.600
But when we are attending at a particular situation,

01:20:44.600 --> 01:20:46.080
we're focused on that situation.

01:20:46.080 --> 01:20:48.600
We basically cannot attend to anything else.

01:20:48.600 --> 01:20:53.600
And that seems to suggest that we basically have only one

01:20:55.720 --> 01:20:58.400
world model engine in our prefrontal cortex.

01:20:59.760 --> 01:21:02.600
That engine is configurable to the situation at hand.

01:21:02.600 --> 01:21:04.640
So we are building a box out of wood,

01:21:04.640 --> 01:21:09.280
or we are, you know, driving down the highway playing chess.

01:21:09.280 --> 01:21:12.840
We basically have a single model of the world

01:21:12.840 --> 01:21:15.360
that we're configuring to the situation at hand,

01:21:15.360 --> 01:21:18.000
which is why we can only attend to one task at a time.

01:21:19.200 --> 01:21:21.640
Now, if there is a task that we do repeatedly,

01:21:22.880 --> 01:21:25.960
it goes from the sort of deliberate reasoning

01:21:25.960 --> 01:21:27.440
using model of the world and prediction,

01:21:27.520 --> 01:21:29.320
and perhaps something like model predictive control,

01:21:29.320 --> 01:21:31.360
which I was talking about earlier,

01:21:31.360 --> 01:21:33.320
to something that is more subconscious,

01:21:33.320 --> 01:21:34.360
that becomes automatic.

01:21:34.360 --> 01:21:35.960
So I don't know if you've ever played

01:21:35.960 --> 01:21:37.920
against a chess grandmaster.

01:21:38.960 --> 01:21:42.960
You know, I get wiped out in, you know, 10 plies, right?

01:21:43.840 --> 01:21:47.040
And, you know, I have to think about my move for,

01:21:47.040 --> 01:21:48.680
you know, like 15 minutes.

01:21:50.120 --> 01:21:52.280
And the person in front of me, the grandmaster,

01:21:52.280 --> 01:21:55.200
you know, would just like react within seconds, right?

01:21:56.160 --> 01:21:58.560
You know, he doesn't need to think about it.

01:21:58.560 --> 01:21:59.960
That's become part of the subconscious

01:21:59.960 --> 01:22:01.880
because, you know, it's basically just

01:22:01.880 --> 01:22:03.600
pattern recognition at this point.

01:22:04.720 --> 01:22:07.680
Same, you know, the first few hours you drive a car,

01:22:07.680 --> 01:22:09.640
you're really attentive, you can't do anything else.

01:22:09.640 --> 01:22:13.200
And then after 20, 30 hours of practice, 50 hours,

01:22:13.200 --> 01:22:14.120
you know, it's subconscious.

01:22:14.120 --> 01:22:15.440
You can talk to the person next to you,

01:22:15.440 --> 01:22:17.080
you know, things like that, right?

01:22:17.080 --> 01:22:19.000
Unless the situation becomes unpredictable

01:22:19.000 --> 01:22:21.040
and then you have to stop talking.

01:22:21.040 --> 01:22:23.800
So that suggests you only have one model in your head.

01:22:24.680 --> 01:22:27.200
And it might suggest the idea

01:22:27.200 --> 01:22:28.960
that consciousness basically is the module

01:22:28.960 --> 01:22:31.720
that configures this world model of yours.

01:22:31.720 --> 01:22:35.280
You know, you need to have some sort of executive

01:22:35.280 --> 01:22:38.320
kind of overseer that configures your world model

01:22:38.320 --> 01:22:40.560
for the situation at hand.

01:22:40.560 --> 01:22:43.760
And that leads to kind of the really curious concept

01:22:43.760 --> 01:22:46.040
that consciousness is not a consequence

01:22:46.040 --> 01:22:47.640
of the power of our minds,

01:22:47.640 --> 01:22:49.960
but of the limitation of our brains.

01:22:49.960 --> 01:22:52.040
But because we have only one world model,

01:22:52.040 --> 01:22:53.680
we have to be conscious

01:22:54.560 --> 01:22:56.240
because if we had as many world models

01:22:56.240 --> 01:22:59.400
as there are situations we encounter,

01:22:59.400 --> 01:23:01.640
then we could do all of them simultaneously

01:23:01.640 --> 01:23:03.960
and we wouldn't need this sort of executive control

01:23:03.960 --> 01:23:05.440
that we call consciousness.

01:23:05.440 --> 01:23:06.280
Yeah, interesting.

01:23:06.280 --> 01:23:10.000
And somehow maybe that executive controller,

01:23:10.000 --> 01:23:12.000
I mean, the hard problem of consciousness,

01:23:12.000 --> 01:23:13.840
there's some kind of chemicals in biology

01:23:13.840 --> 01:23:15.920
that's creating a feeling,

01:23:15.920 --> 01:23:18.800
like it feels to experience some of these things.

01:23:18.800 --> 01:23:23.440
That's kind of like the hard question is,

01:23:23.440 --> 01:23:24.920
and why is that useful?

01:23:24.920 --> 01:23:26.200
Maybe the more pragmatic question,

01:23:26.200 --> 01:23:28.480
why is it useful to feel like

01:23:28.480 --> 01:23:31.240
this is really you experiencing this

01:23:31.240 --> 01:23:34.400
versus just like information being processed?

01:23:36.000 --> 01:23:39.040
It could be just a very nice side effect

01:23:39.040 --> 01:23:41.800
of the way we evolved.

01:23:41.800 --> 01:23:46.800
That's just very useful to feel a sense of ownership

01:23:48.640 --> 01:23:49.960
to the decisions you make,

01:23:49.960 --> 01:23:51.160
to the perceptions you make,

01:23:51.160 --> 01:23:53.200
to the model you're trying to maintain.

01:23:53.960 --> 01:23:56.280
Like you own this thing and it's the only one you got

01:23:56.280 --> 01:23:58.400
and if you lose it, it's gonna really suck.

01:23:58.400 --> 01:24:00.640
And so you should really send the brain

01:24:00.640 --> 01:24:02.280
some signals about it.

01:24:03.880 --> 01:24:06.880
What ideas do you believe might be true

01:24:06.880 --> 01:24:10.120
that most or at least many people disagree with you with?

01:24:11.320 --> 01:24:13.800
Let's say in the space of machine learning.

01:24:13.800 --> 01:24:14.960
Well, it depends who you talk about,

01:24:14.960 --> 01:24:19.960
but I think, so certainly there is a bunch of people

01:24:20.120 --> 01:24:21.120
who are nativists, right?

01:24:21.120 --> 01:24:22.800
Who think that a lot of the basic things

01:24:22.800 --> 01:24:25.400
about the world are kind of hardwired in our minds.

01:24:26.440 --> 01:24:28.920
Things like the world is three dimensional, for example,

01:24:28.920 --> 01:24:30.440
is that hardwired?

01:24:30.440 --> 01:24:32.680
Things like object permanence,

01:24:32.680 --> 01:24:35.840
is it something that we learn before the age

01:24:35.840 --> 01:24:39.240
of three months or so or are we born with it?

01:24:39.240 --> 01:24:42.440
And there are very wide disagreements

01:24:42.440 --> 01:24:46.600
among the cognitive scientists for this.

01:24:46.600 --> 01:24:49.040
I think those things are actually very simple to learn.

01:24:50.040 --> 01:24:54.120
Is it the case that the oriented edge detectors in V1

01:24:54.120 --> 01:24:56.040
are learned or are they hardwired?

01:24:56.040 --> 01:24:57.160
I think they are learned.

01:24:57.160 --> 01:24:58.480
They might be learned before birth

01:24:58.480 --> 01:25:00.520
because it's really easy to generate signals

01:25:00.520 --> 01:25:02.920
from the retina that actually will train edge detectors.

01:25:02.920 --> 01:25:06.640
So, and again, those are things that can be learned

01:25:06.640 --> 01:25:09.480
within minutes of opening your eyes, right?

01:25:09.480 --> 01:25:13.920
I mean, since the 1990s, we have algorithms

01:25:13.920 --> 01:25:15.360
that can learn oriented edge detectors

01:25:15.360 --> 01:25:17.720
completely unsupervised with the equivalent

01:25:17.840 --> 01:25:19.080
of a few minutes of real time.

01:25:19.080 --> 01:25:21.520
So, those things have to be learned.

01:25:22.680 --> 01:25:24.560
There's also those MIT experiments

01:25:24.560 --> 01:25:27.840
where you kind of plug the optical nerve

01:25:27.840 --> 01:25:30.280
on the auditory cortex of a baby ferret, right?

01:25:30.280 --> 01:25:31.280
And that auditory cortex

01:25:31.280 --> 01:25:33.400
becomes a visual cortex essentially.

01:25:33.400 --> 01:25:37.960
So, clearly there's learning taking place there.

01:25:37.960 --> 01:25:41.320
So, I think a lot of what people think are so basic

01:25:41.320 --> 01:25:43.160
that they need to be hardwired,

01:25:43.160 --> 01:25:44.440
I think a lot of those things are learned

01:25:44.440 --> 01:25:46.240
because they are easy to learn.

01:25:46.280 --> 01:25:50.000
So, you put a lot of value in the power of learning.

01:25:50.000 --> 01:25:53.360
What kind of things do you suspect might not be learned?

01:25:53.360 --> 01:25:56.080
Is there something that could not be learned?

01:25:56.080 --> 01:25:59.800
So, your intrinsic drives are not learned.

01:25:59.800 --> 01:26:03.480
There are the things that make humans human

01:26:03.480 --> 01:26:07.440
or make cats different from dogs, right?

01:26:07.440 --> 01:26:10.040
It's the basic drives that are kind of hardwired

01:26:10.040 --> 01:26:11.960
in our basal ganglia.

01:26:13.080 --> 01:26:14.040
I mean, there are people who are working

01:26:14.360 --> 01:26:16.360
on this kind of stuff that's called intrinsic motivation

01:26:16.360 --> 01:26:18.200
in the context of reinforcement learning.

01:26:18.200 --> 01:26:20.080
So, these are objective functions

01:26:20.080 --> 01:26:23.080
where the reward doesn't come from the external world.

01:26:23.080 --> 01:26:24.640
It's computed by your own brain.

01:26:24.640 --> 01:26:28.160
Your own brain computes whether you're happy or not, right?

01:26:28.160 --> 01:26:32.560
It measures your degree of comfort or in comfort.

01:26:33.480 --> 01:26:36.120
And because it's your brain computing this,

01:26:36.120 --> 01:26:37.800
presumably it knows also how to estimate

01:26:37.800 --> 01:26:38.800
gradients of this, right?

01:26:38.800 --> 01:26:43.200
So, it's easier to learn

01:26:43.200 --> 01:26:45.480
when your objective is intrinsic.

01:26:47.080 --> 01:26:48.760
So, that has to be hardwired.

01:26:50.080 --> 01:26:53.440
The critic that makes long-term prediction of the outcome,

01:26:53.440 --> 01:26:56.760
which is the eventual result of this, that's learned.

01:26:57.840 --> 01:26:59.080
And perception is learned

01:26:59.080 --> 01:27:01.240
and your model of the world is learned.

01:27:01.240 --> 01:27:04.280
But let me take an example of why the critic,

01:27:04.280 --> 01:27:06.840
I mean, an example of how the critic may be learned, right?

01:27:06.840 --> 01:27:11.240
If I come to you, I reach across the table

01:27:11.240 --> 01:27:13.360
and I pinch your arm, right?

01:27:13.360 --> 01:27:15.080
Complete surprise for you.

01:27:15.080 --> 01:27:16.320
You would not have expected this from me.

01:27:16.320 --> 01:27:18.120
I was expecting that the whole time, but yes, right?

01:27:18.120 --> 01:27:20.400
Let's say for the sake of the story, yes.

01:27:21.760 --> 01:27:25.000
Okay, your basal ganglia is gonna light up

01:27:25.000 --> 01:27:26.880
because it's gonna hurt, right?

01:27:28.480 --> 01:27:30.600
And now your model of the world includes the fact

01:27:30.600 --> 01:27:34.800
that I may pinch you if I approach my...

01:27:34.800 --> 01:27:36.200
Don't trust humans.

01:27:36.200 --> 01:27:37.880
Right, my hand to your arm.

01:27:37.880 --> 01:27:40.000
So, if I try again, you're gonna recoil

01:27:40.000 --> 01:27:44.920
and that's your critic, your predictor

01:27:44.920 --> 01:27:47.920
of your ultimate pain system

01:27:50.440 --> 01:27:52.320
that predicts that something bad is gonna happen

01:27:52.320 --> 01:27:53.760
and you recoil to avoid it.

01:27:53.760 --> 01:27:55.160
So, even that can be learned?

01:27:55.160 --> 01:27:56.600
That is learned, definitely.

01:27:56.600 --> 01:28:00.600
This is what allows you also to define some goals, right?

01:28:00.600 --> 01:28:04.440
So, the fact that you're a school child,

01:28:04.440 --> 01:28:06.680
you wake up in the morning and you go to school

01:28:06.680 --> 01:28:11.680
and it's not because you necessarily like waking up early

01:28:11.960 --> 01:28:13.760
and going to school, but you know that there is

01:28:13.760 --> 01:28:15.840
a long-term objective you're trying to optimize.

01:28:15.840 --> 01:28:18.120
So, Ernest Becker, I'm not sure if you're familiar

01:28:18.120 --> 01:28:20.080
with him, the philosopher, he wrote the book

01:28:20.080 --> 01:28:22.800
Denial of Death and his idea is that one of the core

01:28:22.800 --> 01:28:25.480
motivations of human beings is our terror of death,

01:28:25.480 --> 01:28:27.240
our fear of death.

01:28:27.240 --> 01:28:28.880
That's what makes us unique from cats.

01:28:28.880 --> 01:28:30.520
Cats are just surviving.

01:28:30.520 --> 01:28:35.520
They do not have a deep, like a cognizance,

01:28:36.720 --> 01:28:41.720
introspection that over the horizon is the end.

01:28:41.800 --> 01:28:44.400
And he says that, I mean, there's a terror management theory

01:28:44.400 --> 01:28:46.200
that's just all these psychological experiments

01:28:46.200 --> 01:28:51.200
that show basically this idea that all of human civilization,

01:28:53.160 --> 01:28:56.840
everything we create is kind of trying to forget

01:28:56.840 --> 01:29:00.640
if even for a brief moment that we're going to die.

01:29:00.640 --> 01:29:03.760
When do you think humans understand

01:29:03.760 --> 01:29:04.880
that they're going to die?

01:29:04.880 --> 01:29:07.960
Is it learned early on also, like?

01:29:09.080 --> 01:29:12.480
I don't know at what point, I mean, it's a question,

01:29:12.480 --> 01:29:16.480
like at what point do you realize that death really is?

01:29:16.480 --> 01:29:18.200
And I think most people don't actually realize

01:29:18.200 --> 01:29:19.240
what death is, right?

01:29:19.240 --> 01:29:21.000
I mean, most people believe that you go to heaven

01:29:21.000 --> 01:29:21.920
or something, right?

01:29:21.920 --> 01:29:25.600
So to push back on that, what Ernest Becker says

01:29:25.600 --> 01:29:29.320
and Sheldon Solomon, all of those folks,

01:29:29.320 --> 01:29:31.640
and I find those ideas a little bit compelling

01:29:31.640 --> 01:29:34.120
is that there is moments in life, early in life,

01:29:34.120 --> 01:29:36.560
a lot of this fun happens early in life,

01:29:36.560 --> 01:29:41.600
when you are, when you do deeply experience

01:29:41.600 --> 01:29:43.560
the terror of this realization,

01:29:43.560 --> 01:29:45.960
and all the things you think about about religion,

01:29:45.960 --> 01:29:48.400
all those kinds of things that we kind of think about

01:29:48.400 --> 01:29:50.640
more like teenage years and later,

01:29:50.640 --> 01:29:52.080
we're talking about way earlier.

01:29:52.080 --> 01:29:53.200
No, it was like seven or eight years,

01:29:53.200 --> 01:29:54.040
something like that, yeah.

01:29:54.040 --> 01:29:59.040
You realize, holy crap, this is like the mystery,

01:29:59.680 --> 01:30:03.200
the terror, like it's almost like you're a little prey,

01:30:03.200 --> 01:30:05.360
a little baby deer sitting in the darkness

01:30:05.360 --> 01:30:08.040
of the jungle, the woods, looking all around you,

01:30:08.040 --> 01:30:09.560
the darkness full of terror.

01:30:09.560 --> 01:30:12.120
I mean, that realization says, okay,

01:30:12.120 --> 01:30:14.480
I'm gonna go back in the comfort of my mind

01:30:14.480 --> 01:30:16.800
where there is a deep meaning,

01:30:16.800 --> 01:30:20.400
where there is maybe like pretend I'm immortal

01:30:20.400 --> 01:30:25.080
in however way, however kind of idea I can construct

01:30:25.080 --> 01:30:27.200
to help me understand that I'm immortal.

01:30:27.200 --> 01:30:28.680
Religion helps with that.

01:30:28.680 --> 01:30:31.440
You can delude yourself in all kinds of ways,

01:30:31.480 --> 01:30:34.280
like lose yourself in the busyness of each day,

01:30:34.280 --> 01:30:36.440
have little goals in mind, all those kinds of things

01:30:36.440 --> 01:30:38.120
to think that it's gonna go on forever.

01:30:38.120 --> 01:30:40.800
And you kind of know you're gonna die, yeah,

01:30:40.800 --> 01:30:41.840
and it's gonna be sad,

01:30:41.840 --> 01:30:45.200
but you don't really understand that you're going to die.

01:30:45.200 --> 01:30:46.480
And so that's their idea.

01:30:46.480 --> 01:30:49.960
And I find that compelling because it does seem

01:30:49.960 --> 01:30:52.840
to be a core unique aspect of human nature

01:30:52.840 --> 01:30:57.480
that we're able to really understand

01:30:57.480 --> 01:30:59.600
that this life is finite.

01:30:59.600 --> 01:31:01.200
That seems important.

01:31:01.200 --> 01:31:02.280
There's a bunch of different things there.

01:31:02.280 --> 01:31:04.280
So first of all, I don't think there is a qualitative

01:31:04.280 --> 01:31:07.520
difference between us and cats in the term.

01:31:07.520 --> 01:31:10.160
I think the difference is that we just have a better

01:31:10.160 --> 01:31:14.720
long-term ability to predict in the long-term.

01:31:14.720 --> 01:31:17.360
And so we have a better understanding of other world works.

01:31:17.360 --> 01:31:20.160
So we have better understanding of finiteness of life

01:31:20.160 --> 01:31:21.000
and things like that.

01:31:21.000 --> 01:31:23.520
So we have a better planning engine than cats?

01:31:23.520 --> 01:31:24.440
Yeah.

01:31:24.440 --> 01:31:25.280
Okay.

01:31:25.280 --> 01:31:28.800
But what's the motivation for planning that far?

01:31:28.800 --> 01:31:30.160
Well, I think it's just a side effect

01:31:30.200 --> 01:31:32.320
of the fact that we have just a better planning engine

01:31:32.320 --> 01:31:34.760
because it makes us, as I said,

01:31:34.760 --> 01:31:37.400
the essence of intelligence is the ability to predict.

01:31:37.400 --> 01:31:41.200
And so because we're smarter as a side effect,

01:31:41.200 --> 01:31:43.480
we also have this ability to kind of make predictions

01:31:43.480 --> 01:31:47.320
about our own future existence or lack thereof.

01:31:48.480 --> 01:31:50.520
You say religion helps with that.

01:31:50.520 --> 01:31:53.000
I think religion hurts actually.

01:31:53.000 --> 01:31:54.960
It makes people worry about like,

01:31:54.960 --> 01:31:57.480
what's gonna happen after their death, et cetera.

01:31:57.480 --> 01:32:00.800
If you believe that, you just don't exist after death.

01:32:00.800 --> 01:32:02.920
Like, it solves completely the problem, at least.

01:32:02.920 --> 01:32:04.960
You're saying if you don't believe in God,

01:32:04.960 --> 01:32:07.200
you don't worry about what happens after death?

01:32:07.200 --> 01:32:08.240
Yeah.

01:32:08.240 --> 01:32:09.080
I don't know.

01:32:09.080 --> 01:32:11.880
You only worry about this life

01:32:11.880 --> 01:32:14.240
because that's the only one you have.

01:32:14.240 --> 01:32:16.160
I think it's, well, I don't know.

01:32:16.160 --> 01:32:17.760
If I were to say what Ernest Becker says,

01:32:17.760 --> 01:32:22.160
and I have to say I agree with him more than not,

01:32:22.160 --> 01:32:26.160
is you do deeply worry.

01:32:26.200 --> 01:32:27.920
If you believe there's no God,

01:32:27.920 --> 01:32:32.040
there's still a deep worry of the mystery of it all.

01:32:32.040 --> 01:32:35.720
How does that make any sense that it just ends?

01:32:35.720 --> 01:32:39.760
I don't think we can truly understand that this right,

01:32:39.760 --> 01:32:43.040
I mean, so much of our life, the consciousness, the ego,

01:32:43.040 --> 01:32:46.120
is invested in this being.

01:32:47.600 --> 01:32:51.600
Science keeps bringing humanity down from its pedestal.

01:32:51.600 --> 01:32:54.760
And that's just another example of it.

01:32:54.760 --> 01:32:57.840
That's wonderful, but for us individual humans,

01:32:57.840 --> 01:33:00.280
we don't like to be brought down from a pedestal.

01:33:00.280 --> 01:33:01.720
I'm fine with it.

01:33:01.720 --> 01:33:04.160
But see, you're fine with it because, well,

01:33:04.160 --> 01:33:06.360
so what Ernest Becker would say is you're fine with it

01:33:06.360 --> 01:33:08.560
because that's just a more peaceful existence for you,

01:33:08.560 --> 01:33:09.560
but you're not really fine.

01:33:09.560 --> 01:33:10.840
You're hiding from it.

01:33:10.840 --> 01:33:12.760
In fact, some of the people that experience

01:33:12.760 --> 01:33:17.000
the deepest trauma earlier in life,

01:33:17.000 --> 01:33:19.600
they often, before they seek extensive therapy,

01:33:19.600 --> 01:33:21.080
will say, I'm fine.

01:33:21.080 --> 01:33:23.480
It's like when you talk to people who are truly angry,

01:33:23.520 --> 01:33:24.320
how are you doing?

01:33:24.320 --> 01:33:25.440
I'm fine.

01:33:25.440 --> 01:33:27.840
The question is, what's going on?

01:33:27.840 --> 01:33:29.200
I had a near death experience.

01:33:29.200 --> 01:33:33.640
I had a very bad motorbike accident when I was 17.

01:33:33.640 --> 01:33:36.960
So, but that didn't have any impact

01:33:36.960 --> 01:33:40.440
on my reflection on that topic.

01:33:40.440 --> 01:33:43.120
So I'm basically just playing a bit of a devil's advocate

01:33:43.120 --> 01:33:45.880
and pushing back on wondering,

01:33:45.880 --> 01:33:47.560
is it truly possible to accept death?

01:33:47.560 --> 01:33:49.720
And the flip side that's more interesting, I think,

01:33:49.720 --> 01:33:53.080
for AI and robotics is how important

01:33:53.680 --> 01:33:57.160
is it to have this as one of the suite of motivations,

01:33:57.160 --> 01:34:02.160
is to not just avoid falling off the roof

01:34:03.320 --> 01:34:04.200
or something like that,

01:34:04.200 --> 01:34:09.200
but ponder the end of the ride.

01:34:10.200 --> 01:34:14.840
If you listen to the Stoics, it's a great motivator.

01:34:14.840 --> 01:34:16.920
It adds a sense of urgency.

01:34:16.920 --> 01:34:21.440
So maybe to truly fear death or be cognizant of it

01:34:21.440 --> 01:34:26.440
might give a deeper meaning and urgency to the moment

01:34:26.480 --> 01:34:28.760
to live fully.

01:34:28.760 --> 01:34:32.200
Well, maybe I don't disagree with that.

01:34:32.200 --> 01:34:34.880
I mean, I think what motivates me here is

01:34:37.040 --> 01:34:38.960
knowing more about human nature.

01:34:38.960 --> 01:34:41.760
I mean, I think human nature and human intelligence

01:34:41.760 --> 01:34:42.600
is a big mystery.

01:34:42.600 --> 01:34:46.560
It's a scientific mystery in addition to

01:34:46.560 --> 01:34:48.600
philosophical and et cetera,

01:34:48.600 --> 01:34:50.720
but I'm a true believer in science.

01:34:50.720 --> 01:34:55.720
So, and I do have kind of a belief

01:34:56.200 --> 01:34:59.960
that for complex systems like the brain and the mind,

01:34:59.960 --> 01:35:04.480
the way to understand it is to try to reproduce it

01:35:04.480 --> 01:35:07.080
with the artifacts that you build

01:35:07.080 --> 01:35:08.920
because you know what's essential to it

01:35:08.920 --> 01:35:10.200
when you try to build it.

01:35:10.200 --> 01:35:12.680
The same way, I've used this analogy before with you,

01:35:12.680 --> 01:35:16.640
I believe, the same way we only started to understand

01:35:17.520 --> 01:35:19.320
aerodynamics when we started building airplanes

01:35:19.320 --> 01:35:22.400
and that helped us understand how birds fly.

01:35:22.400 --> 01:35:25.480
So I think there's kind of a similar process here

01:35:25.480 --> 01:35:29.680
where we don't have a full theory of intelligence,

01:35:29.680 --> 01:35:31.760
but building intelligent artifacts

01:35:31.760 --> 01:35:35.480
will help us perhaps develop some underlying theory

01:35:35.480 --> 01:35:39.400
that encompasses not just artificial implements,

01:35:39.400 --> 01:35:43.840
but also human and biological intelligence in general.

01:35:43.840 --> 01:35:46.080
So you're an interesting person to ask this question

01:35:46.080 --> 01:35:49.400
about sort of all kinds of different other

01:35:49.400 --> 01:35:53.120
intelligent entities or intelligences.

01:35:53.120 --> 01:35:56.280
What are your thoughts about kind of like the touring

01:35:56.280 --> 01:35:58.000
or the Chinese room question?

01:35:59.240 --> 01:36:02.920
If we create an AI system that exhibits

01:36:02.920 --> 01:36:06.380
a lot of properties of intelligence and consciousness,

01:36:07.520 --> 01:36:10.200
how comfortable are you thinking of that entity

01:36:10.200 --> 01:36:12.320
as intelligent or conscious?

01:36:12.320 --> 01:36:14.580
So you're trying to build now systems

01:36:14.580 --> 01:36:16.420
that have intelligence and there's metrics

01:36:16.420 --> 01:36:21.300
about their performance, but that metric is external.

01:36:22.740 --> 01:36:26.420
So are you okay calling a thing intelligent

01:36:26.420 --> 01:36:29.020
or are you going to be like most humans

01:36:29.020 --> 01:36:32.700
and be once again unhappy to be brought down

01:36:32.700 --> 01:36:34.940
from a pedestal of consciousness slash intelligence?

01:36:34.940 --> 01:36:39.500
No, I'll be very happy to understand

01:36:41.260 --> 01:36:43.700
more about human nature, human mind,

01:36:43.780 --> 01:36:46.700
and human intelligence through the construction

01:36:46.700 --> 01:36:50.580
of machines that have similar abilities.

01:36:50.580 --> 01:36:54.540
And if a consequence of this is to bring down humanity

01:36:54.540 --> 01:36:58.020
one notch down from it's already low pedestal,

01:36:58.020 --> 01:36:59.140
I'm just fine with it.

01:36:59.140 --> 01:37:01.340
That's just the reality of life.

01:37:01.340 --> 01:37:02.460
So I'm fine with that.

01:37:02.460 --> 01:37:05.820
Now you were asking me about things that opinions I have

01:37:05.820 --> 01:37:07.940
that a lot of people may disagree with.

01:37:07.940 --> 01:37:12.780
I think if we think about the design

01:37:12.980 --> 01:37:14.260
of autonomous intelligence system,

01:37:14.260 --> 01:37:18.700
so assuming that we are somewhat successful at some level

01:37:18.700 --> 01:37:20.460
of getting machines to learn models of the world,

01:37:20.460 --> 01:37:22.620
predictive models of the world,

01:37:22.620 --> 01:37:25.860
we build intrinsic motivation objective functions

01:37:25.860 --> 01:37:28.340
to drive the behavior of that system.

01:37:28.340 --> 01:37:30.100
The system also has perception modules

01:37:30.100 --> 01:37:32.820
that allows it to estimate the state of the world

01:37:32.820 --> 01:37:35.500
and then have some way of figuring out a sequence of actions

01:37:35.500 --> 01:37:37.820
that to optimize a particular objective.

01:37:39.300 --> 01:37:42.740
If it has a critic of the type that I was describing before,

01:37:43.700 --> 01:37:44.940
the thing that makes you recoil your arm

01:37:44.940 --> 01:37:46.500
the second time I try to pinch you,

01:37:48.900 --> 01:37:51.940
intelligent autonomous machine will have emotions.

01:37:51.940 --> 01:37:54.300
I think emotions are an integral part

01:37:54.300 --> 01:37:56.660
of autonomous intelligence.

01:37:56.660 --> 01:37:59.300
If you have an intelligent system

01:37:59.300 --> 01:38:03.420
that is driven by intrinsic motivation, by objectives,

01:38:04.300 --> 01:38:07.940
if it has a critic that allows it to predict in advance

01:38:07.940 --> 01:38:11.300
whether the outcome of a situation is gonna be good or bad,

01:38:11.300 --> 01:38:12.500
it's going to have emotions.

01:38:12.500 --> 01:38:15.620
It's going to have fear when it predicts

01:38:15.620 --> 01:38:18.180
that the outcome is gonna be bad

01:38:18.180 --> 01:38:20.740
and something to avoid is gonna have elation

01:38:20.740 --> 01:38:22.540
when it predicts it's gonna be good.

01:38:24.300 --> 01:38:29.300
If it has drives to relate with humans in some ways,

01:38:29.420 --> 01:38:34.420
the way humans have, it's gonna be social, right?

01:38:34.500 --> 01:38:37.420
And so it's gonna have emotions about attachment

01:38:37.420 --> 01:38:38.820
and things of that type.

01:38:39.820 --> 01:38:44.700
So I think the sort of sci-fi thing

01:38:44.700 --> 01:38:46.900
where you see commander data,

01:38:46.900 --> 01:38:50.100
like having an emotion chip that you can turn off, right?

01:38:50.100 --> 01:38:51.700
I think that's ridiculous.

01:38:51.700 --> 01:38:56.700
So I mean, here's the difficult philosophical social question.

01:38:57.820 --> 01:39:00.060
Do you think there will be a time

01:39:00.060 --> 01:39:03.140
like a civil rights movement for robots where,

01:39:04.100 --> 01:39:05.180
okay, forget the movement,

01:39:05.180 --> 01:39:07.860
but a discussion like the Supreme Court

01:39:09.700 --> 01:39:12.900
that particular kinds of robots,

01:39:12.900 --> 01:39:14.860
you know, particular kinds of systems

01:39:16.100 --> 01:39:18.340
deserve the same rights as humans

01:39:18.340 --> 01:39:21.660
because they can suffer just as humans can,

01:39:22.900 --> 01:39:24.740
all those kinds of things.

01:39:24.740 --> 01:39:27.340
Well, perhaps, perhaps not.

01:39:27.340 --> 01:39:29.620
Like imagine that humans were,

01:39:29.620 --> 01:39:33.740
that you could, you know, die and be restored.

01:39:33.780 --> 01:39:35.540
Like, you know, you could be sort of, you know,

01:39:35.540 --> 01:39:37.580
be 3D reprinted and, you know,

01:39:37.580 --> 01:39:40.780
your brain could be reconstructed in its finest details.

01:39:40.780 --> 01:39:43.180
Our ideas of rights will change in that case.

01:39:43.180 --> 01:39:46.900
If you can always just, there's always a backup.

01:39:46.900 --> 01:39:48.260
You could always restore.

01:39:48.260 --> 01:39:50.300
Maybe like the importance of murder

01:39:50.300 --> 01:39:52.020
will go down one notch.

01:39:52.020 --> 01:39:52.860
That's right.

01:39:52.860 --> 01:39:57.580
But also your, you know, desire to do dangerous things

01:39:57.580 --> 01:40:02.020
like, you know, skydiving or, you know,

01:40:03.020 --> 01:40:05.940
or, you know, race car driving, you know,

01:40:05.940 --> 01:40:07.340
car racing, all that kind of stuff, you know,

01:40:07.340 --> 01:40:09.260
would probably increase.

01:40:09.260 --> 01:40:10.940
Or, you know, airplane aerobatics

01:40:10.940 --> 01:40:12.260
or that kind of stuff, right?

01:40:12.260 --> 01:40:13.980
It would be fine to do a lot of those things

01:40:13.980 --> 01:40:16.580
or explore, you know, dangerous areas

01:40:16.580 --> 01:40:17.420
and things like that.

01:40:17.420 --> 01:40:19.020
It would kind of change your relationship.

01:40:19.020 --> 01:40:22.220
So now it's very likely that robots would be like that

01:40:22.220 --> 01:40:26.900
because, you know, they'll be based on perhaps technology

01:40:26.900 --> 01:40:29.980
that is somewhat similar to this technology

01:40:29.980 --> 01:40:31.620
and you can always have a backup.

01:40:32.260 --> 01:40:35.700
So it's possible, I don't know if you like video games,

01:40:35.700 --> 01:40:39.340
but there's a game called Diablo and...

01:40:39.340 --> 01:40:41.860
Oh, my sons are huge fans of this.

01:40:41.860 --> 01:40:42.700
Yes.

01:40:44.420 --> 01:40:47.060
In fact, they made a game that's inspired by it.

01:40:47.060 --> 01:40:47.900
Awesome.

01:40:47.900 --> 01:40:49.260
Like built a game?

01:40:49.260 --> 01:40:52.380
My three sons have a game design studio between them.

01:40:52.380 --> 01:40:53.220
Yeah. That's awesome.

01:40:53.220 --> 01:40:55.540
They came out with a game last year.

01:40:55.540 --> 01:40:58.180
No, this was last year, early last year, about a year ago.

01:40:58.180 --> 01:40:59.020
That's awesome.

01:40:59.020 --> 01:41:01.980
But in Diablo, there's something called hardcore mode

01:41:02.940 --> 01:41:05.460
which if you die, there's no, you're gone.

01:41:05.460 --> 01:41:06.300
Right.

01:41:06.300 --> 01:41:07.140
That's it.

01:41:07.140 --> 01:41:09.580
And so it's possible with AI systems

01:41:10.620 --> 01:41:13.260
for them to be able to operate successfully

01:41:13.260 --> 01:41:15.580
and for us to treat them in a certain way

01:41:15.580 --> 01:41:18.380
because they have to be integrated in human society,

01:41:18.380 --> 01:41:22.060
they have to be able to die, no copies allowed.

01:41:22.060 --> 01:41:23.860
In fact, copying is illegal.

01:41:23.860 --> 01:41:25.300
It's possible with humans as well,

01:41:25.300 --> 01:41:28.620
like cloning will be illegal, even when it's possible.

01:41:28.620 --> 01:41:29.980
But cloning is not copying, right?

01:41:29.980 --> 01:41:33.140
I mean, you don't reproduce the mind of the person

01:41:33.140 --> 01:41:33.980
and experience.

01:41:33.980 --> 01:41:34.820
Right.

01:41:34.820 --> 01:41:36.460
It's just a delayed twin, so.

01:41:36.460 --> 01:41:39.100
But then it's, well, we were talking about with computers

01:41:39.100 --> 01:41:40.860
that you will be able to copy,

01:41:40.860 --> 01:41:42.700
you will be able to perfectly save,

01:41:42.700 --> 01:41:46.660
pickle the mind state.

01:41:46.660 --> 01:41:49.700
And it's possible that that will be illegal

01:41:49.700 --> 01:41:52.340
because that goes against,

01:41:53.340 --> 01:41:56.020
that will destroy the motivation of the system.

01:41:56.020 --> 01:41:59.140
Okay, so let's say you have a domestic robot.

01:41:59.140 --> 01:42:01.380
Okay, sometime in the future.

01:42:01.380 --> 01:42:02.460
Yes.

01:42:02.460 --> 01:42:05.540
And the domestic robot comes to you

01:42:05.540 --> 01:42:07.380
kind of somewhat pre-trained,

01:42:07.380 --> 01:42:08.700
can do a bunch of things,

01:42:08.700 --> 01:42:10.580
but it has a particular personality

01:42:10.580 --> 01:42:12.300
that makes it slightly different from the other robots

01:42:12.300 --> 01:42:14.220
because that makes them more interesting.

01:42:14.220 --> 01:42:18.060
And then because it's lived with you for five years,

01:42:18.060 --> 01:42:21.900
you've grown some attachment to it and vice versa,

01:42:21.900 --> 01:42:24.380
and it's learned a lot about you.

01:42:24.380 --> 01:42:25.900
Or maybe it's not a household robot,

01:42:25.900 --> 01:42:28.420
maybe it's a virtual assistant

01:42:28.420 --> 01:42:31.500
that lives in your augmented reality glasses or whatever.

01:42:31.500 --> 01:42:32.620
Right?

01:42:32.620 --> 01:42:35.060
You know, the her movie type thing, right?

01:42:36.700 --> 01:42:39.620
And that system to some extent,

01:42:39.620 --> 01:42:42.540
the intelligence in that system

01:42:42.540 --> 01:42:45.780
is a bit like your child or maybe your PhD student

01:42:45.780 --> 01:42:48.020
in the sense that there's a lot of you

01:42:48.020 --> 01:42:50.260
in that machine now, right?

01:42:50.260 --> 01:42:53.500
And so if it were a living thing,

01:42:53.540 --> 01:42:56.580
you would do this for free if you want, right?

01:42:56.580 --> 01:42:58.420
If it's your child, your child can, you know,

01:42:58.420 --> 01:43:01.580
then live his or her own life.

01:43:01.580 --> 01:43:04.020
And, you know, the fact that they learn stuff from you

01:43:04.020 --> 01:43:06.540
doesn't mean that you have any ownership of it, right?

01:43:06.540 --> 01:43:09.380
But if it's a robot that you've trained,

01:43:09.380 --> 01:43:13.980
perhaps you have some intellectual property claim about-

01:43:13.980 --> 01:43:15.140
Intellectual property?

01:43:15.140 --> 01:43:18.140
Oh, I thought you meant like a permanent value

01:43:18.140 --> 01:43:20.180
in the sense that part of you is in-

01:43:20.180 --> 01:43:21.700
Well, there is permanent value, right?

01:43:21.700 --> 01:43:24.660
So you would lose a lot if that robot were to be destroyed

01:43:24.660 --> 01:43:26.660
and you had no backup, you would lose a lot, right?

01:43:26.660 --> 01:43:28.100
You would lose a lot of investment, you know?

01:43:28.100 --> 01:43:31.860
Kind of like, you know, a person dying, you know?

01:43:31.860 --> 01:43:34.300
That a friend of you is dying

01:43:34.300 --> 01:43:36.580
or a coworker or something like that.

01:43:38.460 --> 01:43:42.340
But also you have like intellectual property rights

01:43:42.340 --> 01:43:45.940
in the sense that that system is fine-tuned

01:43:45.940 --> 01:43:47.340
to your particular existence.

01:43:47.340 --> 01:43:49.860
So that's now a very unique instantiation

01:43:49.860 --> 01:43:52.860
of that original background model, whatever it was,

01:43:52.860 --> 01:43:54.300
that arrived.

01:43:54.300 --> 01:43:55.700
And then there are issues of privacy, right?

01:43:55.700 --> 01:43:59.740
Because now, imagine that that robot has its own

01:43:59.740 --> 01:44:02.860
kind of volition and decides to work for someone else

01:44:02.860 --> 01:44:06.020
or kind of, you know, thinks life with you

01:44:06.020 --> 01:44:07.900
is sort of untenable or whatever.

01:44:09.740 --> 01:44:12.780
Now, all the things that that system learned from you,

01:44:14.780 --> 01:44:16.860
you know, can you like, you know,

01:44:16.860 --> 01:44:18.140
delete all the personal information

01:44:18.140 --> 01:44:19.660
that that system knows about you?

01:44:20.620 --> 01:44:22.180
I mean, that would be kind of an ethical question.

01:44:22.180 --> 01:44:24.740
Like, you know, can you erase the mind

01:44:24.740 --> 01:44:29.740
of an intelligent robot to protect your privacy?

01:44:30.020 --> 01:44:31.540
You can't do this with humans.

01:44:31.540 --> 01:44:32.660
You can ask them to shut up,

01:44:32.660 --> 01:44:35.620
but that you don't have complete power over them.

01:44:35.620 --> 01:44:36.780
Can't erase humans.

01:44:36.780 --> 01:44:38.580
Yeah, it's the problem with the relationships.

01:44:38.580 --> 01:44:40.140
You know, if you break up,

01:44:40.140 --> 01:44:43.460
you can't erase the other human with robots.

01:44:43.460 --> 01:44:45.820
I think it will have to be the same thing with robots,

01:44:45.820 --> 01:44:50.820
that risk that there has to be some risk

01:44:52.420 --> 01:44:55.100
to our interactions to truly experience them deeply,

01:44:55.100 --> 01:44:56.140
it feels like.

01:44:56.140 --> 01:44:59.620
So you have to be able to lose your robot friend

01:44:59.620 --> 01:45:01.660
and that robot friend to go tweeting

01:45:01.660 --> 01:45:03.700
about how much of an asshole you were.

01:45:03.700 --> 01:45:07.100
But then are you allowed to, you know, murder the robot

01:45:07.100 --> 01:45:08.660
to protect your private information?

01:45:08.660 --> 01:45:09.500
Yeah, probably not.

01:45:09.500 --> 01:45:10.320
If the robot decides to leave.

01:45:10.320 --> 01:45:14.540
I have this intuition that for robots with certain,

01:45:14.540 --> 01:45:16.820
like it's almost like a regulation.

01:45:16.820 --> 01:45:19.220
If you declare your robot to be,

01:45:19.220 --> 01:45:20.980
let's call it sentient or something like that,

01:45:20.980 --> 01:45:24.180
like this robot is designed for human interaction,

01:45:24.180 --> 01:45:26.020
then you're not allowed to murder these robots.

01:45:26.020 --> 01:45:28.180
It's the same as murdering other humans.

01:45:28.180 --> 01:45:30.300
Well, but what about you do a backup of the robot

01:45:30.300 --> 01:45:32.580
that you preserve on a hard drive

01:45:32.580 --> 01:45:33.860
or the equivalent in the future.

01:45:33.860 --> 01:45:34.700
That might be illegal.

01:45:34.700 --> 01:45:38.060
It's like piracy is illegal.

01:45:38.060 --> 01:45:39.780
But it's your own robot, right?

01:45:39.780 --> 01:45:41.620
But you can't, you don't.

01:45:41.620 --> 01:45:43.140
But then you can wipe out.

01:45:44.620 --> 01:45:45.460
It's brain.

01:45:45.460 --> 01:45:47.420
So the, this robot doesn't know anything about you anymore,

01:45:47.420 --> 01:45:50.400
but you still have, technically it's still in existence

01:45:50.400 --> 01:45:51.660
because you backed it up.

01:45:51.660 --> 01:45:53.500
And then there'll be these great speeches

01:45:53.500 --> 01:45:55.420
at the Supreme Court by saying,

01:45:55.420 --> 01:45:57.820
oh, sure, you can erase the mind of the robot,

01:45:57.820 --> 01:46:00.020
just like you can erase the mind of a human.

01:46:00.020 --> 01:46:01.060
We both can suffer.

01:46:01.060 --> 01:46:03.340
There'll be some epic, like Obama type character

01:46:03.340 --> 01:46:06.620
with a speech that we, like the robots

01:46:06.620 --> 01:46:08.820
and the humans are the same.

01:46:08.820 --> 01:46:09.860
We can both suffer.

01:46:09.860 --> 01:46:11.340
We can both hope.

01:46:11.380 --> 01:46:14.900
We can both, all of those kinds of things,

01:46:14.900 --> 01:46:17.260
raise families, all that kind of stuff.

01:46:17.260 --> 01:46:20.140
It's interesting for these, just like you said,

01:46:20.140 --> 01:46:24.220
emotion seems to be a fascinatingly powerful aspect

01:46:24.220 --> 01:46:27.380
of human-to-human interaction and human-robot interaction.

01:46:27.380 --> 01:46:30.480
And if they're able to exhibit emotions,

01:46:30.480 --> 01:46:33.580
at the end of the day, that's probably going to

01:46:33.580 --> 01:46:37.140
have us deeply consider human rights,

01:46:37.140 --> 01:46:38.500
like what we value in humans,

01:46:38.500 --> 01:46:40.340
what we value in other animals.

01:46:40.420 --> 01:46:42.140
That's why robots and AI is great.

01:46:42.140 --> 01:46:44.300
It makes us ask really good questions.

01:46:44.300 --> 01:46:45.500
The hard questions, yeah.

01:46:45.500 --> 01:46:49.620
But you asked about the Chinese room type argument.

01:46:49.620 --> 01:46:51.540
Is it real, if it looks real?

01:46:51.540 --> 01:46:54.220
I think the Chinese room argument is a ridiculous one.

01:46:55.460 --> 01:46:57.660
So for people who don't know, Chinese room is,

01:46:59.380 --> 01:47:00.780
I don't even know how to formulate it well,

01:47:00.780 --> 01:47:04.660
but basically, you can mimic the behavior

01:47:04.660 --> 01:47:06.780
of an intelligent system by just following

01:47:06.780 --> 01:47:10.180
a giant algorithm codebook that tells you

01:47:10.180 --> 01:47:12.900
exactly how to respond in exactly each case,

01:47:12.900 --> 01:47:14.700
but is that really intelligent?

01:47:14.700 --> 01:47:16.580
It's like a giant lookup table.

01:47:16.580 --> 01:47:18.580
When this person says this, you answer this.

01:47:18.580 --> 01:47:21.020
When this person says this, you answer this.

01:47:21.020 --> 01:47:24.340
And if you understand how that works,

01:47:24.340 --> 01:47:27.380
you have this giant, nearly infinite lookup table.

01:47:27.380 --> 01:47:28.620
Is that really intelligence?

01:47:28.620 --> 01:47:31.300
Because intelligence seems to be a mechanism

01:47:31.300 --> 01:47:33.420
that's much more interesting and complex

01:47:33.420 --> 01:47:34.620
than this lookup table.

01:47:34.620 --> 01:47:35.460
I don't think so.

01:47:35.460 --> 01:47:38.940
So the real question comes down to,

01:47:38.940 --> 01:47:43.940
do you think you can mechanize intelligence in some way,

01:47:44.340 --> 01:47:47.580
even if that involves learning?

01:47:47.580 --> 01:47:49.300
And the answer is, of course, yes.

01:47:49.300 --> 01:47:50.620
There's no question.

01:47:50.620 --> 01:47:52.140
There's a second question then,

01:47:52.140 --> 01:47:56.540
which is assuming you can reproduce intelligence

01:47:56.540 --> 01:47:59.660
in sort of different hardware than biological hardware,

01:47:59.660 --> 01:48:00.500
like computers,

01:48:00.500 --> 01:48:05.500
can you match human intelligence

01:48:06.620 --> 01:48:11.620
in all the domains in which humans are intelligent?

01:48:12.940 --> 01:48:13.980
Is it possible, right?

01:48:13.980 --> 01:48:17.100
So that's the hypothesis of strong AI.

01:48:17.100 --> 01:48:20.740
The answer to this, in my opinion, is an unqualified yes.

01:48:20.740 --> 01:48:22.660
This will as well happen at some point.

01:48:22.660 --> 01:48:25.340
There's no question that machines at some point

01:48:25.340 --> 01:48:26.620
will become more intelligent than humans

01:48:26.620 --> 01:48:28.620
in all domains where humans are intelligent.

01:48:28.620 --> 01:48:30.220
This is not for tomorrow.

01:48:30.220 --> 01:48:32.260
It's gonna take a long time,

01:48:32.260 --> 01:48:37.260
regardless of what Elon and others have claimed or believed.

01:48:38.100 --> 01:48:42.100
This is a lot harder than many of those guys think it is.

01:48:43.460 --> 01:48:45.300
And many of those guys who thought it was simpler

01:48:45.300 --> 01:48:48.380
than that five years ago now think it's hard

01:48:48.380 --> 01:48:49.940
because it's been five years

01:48:49.940 --> 01:48:53.460
and they realize it's gonna take a lot longer.

01:48:53.460 --> 01:48:55.180
That includes a bunch of people at DeepMind, for example.

01:48:55.180 --> 01:48:56.220
But-

01:48:56.220 --> 01:48:57.060
Oh, interesting.

01:48:57.060 --> 01:48:59.340
I haven't actually touched base with the DeepMind folks,

01:48:59.340 --> 01:49:03.300
but some of it, Elon or Demisus Havas.

01:49:03.300 --> 01:49:05.820
I mean, sometimes in your role,

01:49:05.820 --> 01:49:08.780
you have to kind of create deadlines

01:49:08.780 --> 01:49:10.740
that are nearer than farther away

01:49:10.740 --> 01:49:12.820
to kind of create an urgency

01:49:12.820 --> 01:49:15.180
because you have to believe the impossible is possible

01:49:15.180 --> 01:49:16.180
in order to accomplish it.

01:49:16.180 --> 01:49:18.540
And there's, of course, a flip side to that coin,

01:49:18.540 --> 01:49:20.140
but it's a weird,

01:49:20.140 --> 01:49:22.420
you can't be too cynical if you wanna get something done.

01:49:22.420 --> 01:49:24.300
Absolutely, I agree with that.

01:49:24.300 --> 01:49:26.860
But I mean, you have to inspire people

01:49:26.860 --> 01:49:28.780
to work on sort of ambitious things.

01:49:31.340 --> 01:49:35.580
So it's certainly a lot harder than we believe,

01:49:35.580 --> 01:49:38.180
but there's no question in my mind that this will happen.

01:49:38.180 --> 01:49:40.260
And now people are kind of worried about

01:49:40.260 --> 01:49:42.460
what does that mean for humans?

01:49:42.460 --> 01:49:45.700
They are gonna be brought down from their pedestal

01:49:45.700 --> 01:49:47.940
a bunch of notches with that.

01:49:47.940 --> 01:49:51.700
And is that gonna be good or bad?

01:49:51.700 --> 01:49:53.460
I mean, it's just gonna give more power, right?

01:49:53.460 --> 01:49:56.220
It's an amplifier for human intelligence, really.

01:49:56.220 --> 01:49:59.740
So speaking of doing cool, ambitious things,

01:49:59.740 --> 01:50:02.940
FAIR, the Facebook AI research group,

01:50:02.940 --> 01:50:05.540
has recently celebrated its eighth birthday.

01:50:05.540 --> 01:50:08.660
Or maybe you can correct me on that.

01:50:08.660 --> 01:50:12.420
Looking back, what has been the successes, the failures,

01:50:12.420 --> 01:50:14.460
the lessons learned from the eight years of FAIR?

01:50:14.460 --> 01:50:16.060
And maybe you can also give context

01:50:16.060 --> 01:50:20.620
of where does the newly minted meta AI fit

01:50:20.620 --> 01:50:22.660
into how does it relate to FAIR?

01:50:22.660 --> 01:50:23.820
Right, so let me tell you a little bit

01:50:23.820 --> 01:50:25.580
about the organization of all this.

01:50:26.740 --> 01:50:30.060
Yeah, FAIR was created almost exactly eight years ago.

01:50:30.060 --> 01:50:31.260
It wasn't called FAIR yet.

01:50:31.260 --> 01:50:33.620
It took that name a few months later.

01:50:34.700 --> 01:50:37.780
And at the time I joined Facebook,

01:50:37.780 --> 01:50:39.500
there was a group called the AI group

01:50:39.500 --> 01:50:43.580
that had about 12 engineers and a few scientists,

01:50:43.580 --> 01:50:45.460
like 10 engineers and two scientists

01:50:45.460 --> 01:50:47.100
and something like that.

01:50:47.100 --> 01:50:49.980
I ran it for three and a half years as a director,

01:50:50.940 --> 01:50:52.380
hired the first few scientists

01:50:53.020 --> 01:50:55.340
and kind of set up the culture and organized it,

01:50:55.340 --> 01:50:58.140
explained to the Facebook leadership

01:50:58.140 --> 01:51:00.180
what fundamental research was about

01:51:00.180 --> 01:51:03.620
and how it can work within industry

01:51:03.620 --> 01:51:05.820
and how it needs to be open and everything.

01:51:07.220 --> 01:51:12.220
And I think it's been an unqualified success

01:51:12.300 --> 01:51:16.500
in the sense that FAIR has simultaneously produced

01:51:18.020 --> 01:51:20.940
top-level research and advanced the science

01:51:20.940 --> 01:51:23.460
and the technology, provided tools, open source tools

01:51:23.460 --> 01:51:25.060
like PyTorch and many others.

01:51:26.660 --> 01:51:29.860
But at the same time has had a direct

01:51:29.860 --> 01:51:34.660
or mostly indirect impact on Facebook at the time,

01:51:34.660 --> 01:51:38.580
now Meta, in the sense that a lot of systems

01:51:38.580 --> 01:51:43.580
that Meta is built around now are based

01:51:43.580 --> 01:51:48.380
on research projects that started at FAIR.

01:51:48.380 --> 01:51:50.100
So if you were to take out deep learning

01:51:50.100 --> 01:51:55.100
out of Facebook services now and Meta more generally,

01:51:55.140 --> 01:51:57.740
I mean, the company would literally crumble.

01:51:57.740 --> 01:52:01.500
I mean, it's completely built around AI these days.

01:52:01.500 --> 01:52:03.980
And it's really essential to the operations.

01:52:03.980 --> 01:52:06.620
So what happened after three and a half years

01:52:06.620 --> 01:52:10.220
is that I changed role, I became chief scientist.

01:52:10.220 --> 01:52:14.900
So I'm not doing day-to-day management of FAIR anymore.

01:52:14.900 --> 01:52:17.140
I'm more of a kind of, you know,

01:52:17.140 --> 01:52:18.860
think about strategy and things like that.

01:52:19.100 --> 01:52:21.420
And I carry my, I conduct my own research.

01:52:21.420 --> 01:52:23.300
I've, you know, my own kind of research group

01:52:23.300 --> 01:52:25.300
working on self-supervised learning and things like this,

01:52:25.300 --> 01:52:28.220
which I didn't have time to do when I was director.

01:52:28.220 --> 01:52:33.220
So now FAIR is run by Joel Pinot and Antoine Borde together

01:52:34.700 --> 01:52:36.300
because FAIR is kind of split in two now.

01:52:36.300 --> 01:52:37.820
There's something called FAIR Labs,

01:52:37.820 --> 01:52:40.900
which is sort of bottom-up, science-driven research

01:52:40.900 --> 01:52:43.420
and FAIR Excel, which is slightly more organized

01:52:43.420 --> 01:52:47.660
for bigger projects that require a little more kind of focus

01:52:47.660 --> 01:52:49.740
and more engineering support and things like that.

01:52:49.740 --> 01:52:52.860
So Joel needs FAIR Lab and Antoine Borde needs FAIR Excel.

01:52:52.860 --> 01:52:54.580
Where are they located?

01:52:54.580 --> 01:52:55.420
All over?

01:52:55.420 --> 01:52:56.620
It's delocalized all over.

01:52:57.940 --> 01:53:02.500
So there's no question that the leadership of the company

01:53:02.500 --> 01:53:06.540
believes that this was a very worthwhile investment.

01:53:06.540 --> 01:53:11.540
And what that means is that it's there for the long run.

01:53:12.820 --> 01:53:16.780
Right, so there is, if you want to talk in these terms,

01:53:16.780 --> 01:53:19.500
which I don't like, there's a business model, if you want,

01:53:19.500 --> 01:53:23.660
where FAIR, despite being a very fundamental research lab,

01:53:23.660 --> 01:53:25.260
brings a lot of value to the company,

01:53:25.260 --> 01:53:27.820
either mostly indirectly through other groups.

01:53:29.860 --> 01:53:31.540
Now what happened three and a half years ago

01:53:31.540 --> 01:53:34.580
when I stepped down was also the creation of Facebook AI,

01:53:34.580 --> 01:53:37.660
which was basically a larger organization

01:53:37.660 --> 01:53:41.700
that covers FAIR, so FAIR is included in it,

01:53:41.700 --> 01:53:43.860
but also has other organizations

01:53:43.860 --> 01:53:47.820
that are focused on applied research

01:53:47.820 --> 01:53:51.220
or advanced development of AI technology

01:53:51.220 --> 01:53:54.700
that is more focused on the products of the company.

01:53:54.700 --> 01:53:56.660
So less emphasis on fundamental research.

01:53:56.660 --> 01:53:58.220
Less fundamental, but it's still a research.

01:53:58.220 --> 01:53:59.740
I mean, there's a lot of papers coming out

01:53:59.740 --> 01:54:03.980
of those organizations and people are awesome

01:54:03.980 --> 01:54:06.380
and wonderful to interact with.

01:54:06.380 --> 01:54:11.380
But it serves as kind of a way to kind of scale up

01:54:12.340 --> 01:54:15.260
if you want sort of AI technology,

01:54:15.260 --> 01:54:18.980
which may be very experimental and sort of lab prototypes

01:54:18.980 --> 01:54:20.220
into things that are usable.

01:54:20.220 --> 01:54:22.620
So FAIR is a subset of meta-AI.

01:54:22.620 --> 01:54:26.100
If FAIR becomes like KFC, it'll just keep the F.

01:54:26.100 --> 01:54:28.220
Nobody cares what the F stands for.

01:54:28.220 --> 01:54:33.220
We'll know soon enough by probably by the end of 2021.

01:54:35.140 --> 01:54:37.980
I think this is not a giant change, FAIR.

01:54:37.980 --> 01:54:39.100
Well, FAIR doesn't sound too good,

01:54:39.540 --> 01:54:43.540
but the brand people are kind of deciding on this

01:54:43.540 --> 01:54:45.860
and they've been hesitating for a while now

01:54:45.860 --> 01:54:48.500
and they tell us they're gonna come up with an answer

01:54:48.500 --> 01:54:50.460
as to whether FAIR is gonna change name

01:54:50.460 --> 01:54:53.460
or whether we're gonna change just the meaning of the F.

01:54:53.460 --> 01:54:54.300
That's a good call.

01:54:54.300 --> 01:54:56.140
I would keep FAIR and change the meaning of the F.

01:54:56.140 --> 01:54:57.620
That would be my preference.

01:54:57.620 --> 01:55:00.140
I would turn the F into fundamental.

01:55:00.980 --> 01:55:01.820
Oh, that's good.

01:55:01.820 --> 01:55:02.660
Fundamental AI research.

01:55:02.660 --> 01:55:03.500
Oh, that's really good, yeah.

01:55:03.500 --> 01:55:04.340
Within meta-AI.

01:55:04.340 --> 01:55:06.700
So this would be sort of meta-FAIR,

01:55:06.700 --> 01:55:08.340
but people will call it FAIR, right?

01:55:08.340 --> 01:55:09.380
Yeah, exactly.

01:55:09.380 --> 01:55:10.220
I like it.

01:55:10.220 --> 01:55:15.220
And now meta-AI is part of the Reality Lab.

01:55:16.740 --> 01:55:21.740
So Meta now, the new Facebook is called Meta

01:55:21.780 --> 01:55:26.780
and it's kind of divided into Facebook, Instagram, WhatsApp,

01:55:30.420 --> 01:55:33.020
and Reality Lab.

01:55:33.020 --> 01:55:37.940
Reality Lab is about AR, VR, telepresence,

01:55:37.940 --> 01:55:40.540
communication, technology, and stuff like that.

01:55:40.540 --> 01:55:44.220
It's kind of the, you can think of it as the sort of

01:55:44.220 --> 01:55:47.900
a combination of sort of new products

01:55:47.900 --> 01:55:51.980
and technology part of Meta.

01:55:51.980 --> 01:55:54.260
Is that where the touch sensing for robots,

01:55:54.260 --> 01:55:56.060
I saw that you were posting about, that's-

01:55:56.060 --> 01:55:58.260
But touch sensing for robot is part of FAIR, actually.

01:55:58.260 --> 01:55:59.460
That's a FAIR project. Oh, it is?

01:55:59.460 --> 01:56:00.540
Okay, cool.

01:56:00.540 --> 01:56:03.060
This is also the, no, but there is the other way,

01:56:03.060 --> 01:56:05.700
the haptic glove, right?

01:56:05.700 --> 01:56:07.620
Yes, that's more Reality Lab.

01:56:07.900 --> 01:56:10.740
That's Reality Lab research.

01:56:10.740 --> 01:56:11.940
Reality Lab research.

01:56:11.940 --> 01:56:14.380
By the way, the touch sensors are super interesting.

01:56:14.380 --> 01:56:19.140
Like integrating that modality into the whole sensing suite

01:56:19.140 --> 01:56:20.100
is very interesting.

01:56:20.100 --> 01:56:23.700
So what do you think about the metaverse?

01:56:23.700 --> 01:56:27.820
What do you think about this whole kind of expansion

01:56:27.820 --> 01:56:30.900
of the view of the role of Facebook and Meta in the world?

01:56:30.900 --> 01:56:32.500
Well, metaverse really should be thought of

01:56:32.500 --> 01:56:35.340
as the next step in the internet, right?

01:56:35.340 --> 01:56:40.340
Sort of trying to make the experience more compelling

01:56:43.220 --> 01:56:48.020
of being connected either with other people or with content.

01:56:49.540 --> 01:56:54.020
And we are evolved and trying to evolve

01:56:54.020 --> 01:56:58.700
in 3D environments where we can see other people,

01:56:58.700 --> 01:57:01.060
we can talk to them when we're near them,

01:57:01.060 --> 01:57:04.340
or another viewer far away can hear us,

01:57:04.340 --> 01:57:05.180
things like that, right?

01:57:05.180 --> 01:57:08.700
So there's a lot of social conventions that exist

01:57:08.700 --> 01:57:10.820
in the real world that we can try to transpose.

01:57:10.820 --> 01:57:13.300
Now, what is going to be eventually the,

01:57:15.180 --> 01:57:16.300
how compelling is it going to be?

01:57:16.300 --> 01:57:18.780
Like, is it going to be the case

01:57:18.780 --> 01:57:21.340
that people are going to be willing to do this

01:57:21.340 --> 01:57:24.660
if they have to wear a huge pair of goggles all day?

01:57:24.660 --> 01:57:26.460
Maybe not.

01:57:26.460 --> 01:57:27.500
But then again, if the experience

01:57:27.500 --> 01:57:30.380
is sufficiently compelling, maybe so.

01:57:30.380 --> 01:57:32.220
Or if the device that you have to wear

01:57:32.220 --> 01:57:34.580
is just basically a pair of glasses,

01:57:34.580 --> 01:57:36.820
technology makes sufficient progress for that.

01:57:38.380 --> 01:57:41.580
AR is a much easier concept to grasp

01:57:41.580 --> 01:57:45.020
that you're going to have augmented reality glasses

01:57:45.020 --> 01:57:48.660
that basically contain some sort of virtual assistant

01:57:48.660 --> 01:57:50.260
that can help you in your daily lives.

01:57:50.260 --> 01:57:51.900
But at the same time with AR,

01:57:51.900 --> 01:57:53.460
you have to contend with reality.

01:57:53.460 --> 01:57:55.860
With VR, you can completely detach yourself from reality,

01:57:55.860 --> 01:57:57.180
so it gives you freedom.

01:57:57.180 --> 01:58:00.340
It might be easier to design worlds in VR.

01:58:00.340 --> 01:58:05.340
Yeah, but you can imagine the metaverse being a mix, right?

01:58:06.500 --> 01:58:09.260
Or like you can have objects that exist in the metaverse

01:58:09.260 --> 01:58:11.180
that pop up on top of the real world

01:58:11.180 --> 01:58:14.380
or only exist in virtual reality.

01:58:14.380 --> 01:58:17.060
Okay, let me ask the hard question.

01:58:17.060 --> 01:58:18.500
Oh, because all of this was easy.

01:58:18.500 --> 01:58:19.340
This was easy.

01:58:20.700 --> 01:58:24.260
The Facebook, now Meta, the social network

01:58:24.260 --> 01:58:28.260
has been painted by the media as net negative for society,

01:58:28.300 --> 01:58:30.860
even destructive and evil at times.

01:58:30.860 --> 01:58:34.100
You've pushed back against this, defending Facebook.

01:58:34.100 --> 01:58:36.580
Can you explain your defense?

01:58:36.580 --> 01:58:38.660
Yeah, so the description,

01:58:38.660 --> 01:58:42.660
the company that is being described in some media

01:58:43.980 --> 01:58:47.380
is not the company we know when we work inside.

01:58:47.380 --> 01:58:52.140
And it could be claimed that a lot of employees

01:58:52.140 --> 01:58:54.620
are uninformed about what really goes on in the company,

01:58:54.620 --> 01:58:56.540
but I'm a vice president.

01:58:56.980 --> 01:58:58.980
I have a pretty good vision of what goes on.

01:58:58.980 --> 01:59:00.220
I don't know everything, obviously.

01:59:00.220 --> 01:59:01.900
I'm not involved in everything,

01:59:01.900 --> 01:59:05.340
but certainly not in decision about content moderation

01:59:05.340 --> 01:59:06.180
or anything like this.

01:59:06.180 --> 01:59:10.180
But I have some decent vision of what goes on.

01:59:10.180 --> 01:59:13.700
And this evil that is being described, I just don't see it.

01:59:13.700 --> 01:59:18.220
And then I think there is an easy story to buy,

01:59:18.220 --> 01:59:21.780
which is that all the bad things in the world

01:59:21.780 --> 01:59:25.100
and the reason your friend believes crazy stuff

01:59:27.020 --> 01:59:32.020
is an easy scapegoat in social media in general,

01:59:32.820 --> 01:59:34.500
Facebook in particular.

01:59:34.500 --> 01:59:35.740
We have to look at the data.

01:59:35.740 --> 01:59:40.100
Is it the case that Facebook, for example,

01:59:40.100 --> 01:59:41.700
polarizes people politically?

01:59:42.740 --> 01:59:45.260
Are there academic studies that show this?

01:59:45.260 --> 01:59:50.260
Is it the case that teenagers think of themselves less

01:59:50.300 --> 01:59:52.180
if they use Instagram more?

01:59:52.220 --> 01:59:57.220
Is it the case that people get more riled up against

01:59:59.180 --> 02:00:02.740
opposite sides in a debate or political opinion

02:00:02.740 --> 02:00:05.740
if they are more on Facebook or if they are less?

02:00:05.740 --> 02:00:10.740
And study after study show that none of this is true.

02:00:10.940 --> 02:00:12.460
This is independent studies by academic.

02:00:12.460 --> 02:00:14.620
They're not funded by Facebook or Meta,

02:00:15.900 --> 02:00:18.300
studied by Stanford, by some of my colleagues at NYU,

02:00:18.300 --> 02:00:20.300
actually, with whom I have no connection.

02:00:21.180 --> 02:00:24.980
You know, there's a study recently, they paid people,

02:00:24.980 --> 02:00:29.900
I think it was in the former Yugoslavia,

02:00:29.900 --> 02:00:31.780
I'm not exactly sure in what part,

02:00:31.780 --> 02:00:34.300
but they paid people to not use Facebook for a while

02:00:34.300 --> 02:00:39.300
in the period before the anniversary

02:00:40.220 --> 02:00:43.500
of the Srebrenica massacres, right?

02:00:43.500 --> 02:00:47.780
So people get riled up, like, should we have a celebration?

02:00:47.780 --> 02:00:51.100
I mean, a memorial kind of celebration for it or not.

02:00:51.100 --> 02:00:52.540
So they paid a bunch of people

02:00:52.540 --> 02:00:54.940
to not use Facebook for a few weeks.

02:00:56.260 --> 02:01:01.260
It turns out that those people ended up being more polarized

02:01:01.660 --> 02:01:02.660
than they were at the beginning,

02:01:02.660 --> 02:01:05.300
and the people who were more on Facebook were less polarized.

02:01:06.660 --> 02:01:10.460
There's a study from Stanford of economists at Stanford

02:01:10.460 --> 02:01:12.660
that tried to identify the causes

02:01:12.660 --> 02:01:16.020
of increasing polarization in the US.

02:01:16.020 --> 02:01:17.820
And it's been going on for 40 years

02:01:17.820 --> 02:01:22.540
before, you know, Mark Zuckerberg was born continuously.

02:01:22.540 --> 02:01:25.620
And so if there is a cause,

02:01:25.620 --> 02:01:27.620
it's not Facebook or social media.

02:01:27.620 --> 02:01:30.060
So you could say social media just accelerated, but no,

02:01:30.060 --> 02:01:33.060
I mean, it's basically a continuous evolution

02:01:33.060 --> 02:01:35.820
by some measure of polarization in the US.

02:01:35.820 --> 02:01:37.660
And then you compare this with other countries

02:01:37.660 --> 02:01:41.460
like the West half of Germany,

02:01:41.460 --> 02:01:44.700
because you can go 40 years in the East side,

02:01:44.740 --> 02:01:47.380
or Denmark or other countries,

02:01:47.380 --> 02:01:49.500
and they use Facebook just as much,

02:01:49.500 --> 02:01:50.740
and they're not getting more polarized,

02:01:50.740 --> 02:01:52.060
they're getting less polarized.

02:01:52.060 --> 02:01:56.060
So if you want to look for a causal relationship there,

02:01:57.660 --> 02:01:59.860
you can find a scapegoat, but you can't find a cause.

02:01:59.860 --> 02:02:01.740
Now, if you want to fix the problem,

02:02:01.740 --> 02:02:03.180
you have to find the right cause.

02:02:03.180 --> 02:02:07.740
And what rise me up is that people now are accusing Facebook

02:02:07.740 --> 02:02:09.300
of bad deeds that are done by others,

02:02:09.300 --> 02:02:12.380
and those others, we're not doing anything about them.

02:02:12.380 --> 02:02:14.460
And by the way, those others include

02:02:15.220 --> 02:02:16.060
the owner of the Wall Street Journal

02:02:16.060 --> 02:02:17.660
in which all of those papers were published.

02:02:17.660 --> 02:02:20.020
So I should mention that I'm talking to Shrep,

02:02:20.020 --> 02:02:23.420
Mike Shrepfer on this podcast, and also Mark Zuckerberg,

02:02:23.420 --> 02:02:26.300
and probably these are conversations I can have with them,

02:02:26.300 --> 02:02:27.140
because it's very interesting.

02:02:27.140 --> 02:02:30.540
To me, even if Facebook has some measurable

02:02:30.540 --> 02:02:33.740
negative effect, you can't just consider that in isolation,

02:02:33.740 --> 02:02:35.900
you have to consider about all the positive ways

02:02:35.900 --> 02:02:36.740
it connects us.

02:02:36.740 --> 02:02:38.220
So like every technology-

02:02:38.220 --> 02:02:39.620
It connects people, there's no question.

02:02:39.620 --> 02:02:43.860
You can't just say like, there's an increase in division.

02:02:43.900 --> 02:02:46.100
Yes, probably Google search engine

02:02:46.100 --> 02:02:47.900
has created increase in division.

02:02:47.900 --> 02:02:49.900
We have to consider about how much information

02:02:49.900 --> 02:02:51.140
are brought to the world.

02:02:51.140 --> 02:02:53.700
Like, I'm sure Wikipedia created more division.

02:02:53.700 --> 02:02:55.340
If you just look at the division,

02:02:55.340 --> 02:02:57.700
we have to look at the full context of the world

02:02:57.700 --> 02:02:59.140
and they didn't make a better world.

02:02:59.140 --> 02:02:59.980
You have to-

02:02:59.980 --> 02:03:01.700
The printing press has created more division, right?

02:03:01.700 --> 02:03:02.540
Exactly.

02:03:02.540 --> 02:03:06.900
I mean, so when the printing press was invented,

02:03:06.900 --> 02:03:10.820
the first books that were printed were things like the Bible,

02:03:10.820 --> 02:03:13.820
and that allowed people to read the Bible by themselves,

02:03:14.820 --> 02:03:17.420
not get the message uniquely from priests in Europe.

02:03:17.420 --> 02:03:20.340
And that created the Protestant movement

02:03:20.340 --> 02:03:23.660
and 200 years of religious persecution and wars.

02:03:23.660 --> 02:03:26.180
So that's a bad side effect of the printing press.

02:03:26.180 --> 02:03:28.500
Social networks aren't being nearly as bad

02:03:28.500 --> 02:03:30.060
as the printing press, but nobody would say

02:03:30.060 --> 02:03:31.820
the printing press was a bad idea.

02:03:33.500 --> 02:03:35.100
Yeah, a lot of it is perception

02:03:35.100 --> 02:03:38.420
and there's a lot of different incentives operating here.

02:03:38.420 --> 02:03:42.060
Maybe a quick comment, since you're one of the top leaders

02:03:42.060 --> 02:03:46.780
at Facebook and at Meta, sorry, that's in the tech space.

02:03:46.780 --> 02:03:49.020
I'm sure Facebook involves a lot

02:03:49.020 --> 02:03:52.060
of incredible technological challenges

02:03:52.060 --> 02:03:52.940
that need to be solved.

02:03:52.940 --> 02:03:55.020
A lot of it probably is in the computer infrastructure,

02:03:55.020 --> 02:03:58.100
the hardware, I mean, it's just a huge amount.

02:03:58.940 --> 02:04:01.940
Maybe can you give me context about how much

02:04:01.940 --> 02:04:06.260
of Shrep's life is AI and how much of it is low-level compute?

02:04:06.260 --> 02:04:09.620
How much of it is flying all around doing business stuff

02:04:09.620 --> 02:04:12.020
and the same with Zuckerberg, Mark Zuckerberg?

02:04:12.020 --> 02:04:13.740
They really focus on AI.

02:04:13.740 --> 02:04:18.740
I mean, certainly in the run-up of the creation affair

02:04:19.500 --> 02:04:24.060
and for at least a year after that, if not more,

02:04:24.060 --> 02:04:26.700
Mark was very, very much focused on AI

02:04:26.700 --> 02:04:29.700
and was spending quite a lot of effort on it.

02:04:29.700 --> 02:04:30.780
And that's his style.

02:04:30.780 --> 02:04:32.060
When he gets interested in something,

02:04:32.060 --> 02:04:34.100
he reads everything about it.

02:04:34.100 --> 02:04:36.860
He read some of my papers, for example, before I joined.

02:04:39.620 --> 02:04:41.820
And so he learned a lot about it.

02:04:41.820 --> 02:04:43.740
That'd be great if he said he liked notes.

02:04:43.740 --> 02:04:44.580
Right.

02:04:46.460 --> 02:04:51.100
And Shrep was really into it also.

02:04:51.100 --> 02:04:52.820
I mean, Shrep is really kind of,

02:04:54.780 --> 02:04:57.940
has something I've tried to preserve also

02:04:57.940 --> 02:05:00.180
despite my not so young age,

02:05:00.180 --> 02:05:03.180
which is a sense of wonder about science and technology.

02:05:03.180 --> 02:05:05.220
And he certainly has that.

02:05:06.300 --> 02:05:07.420
He's also a wonderful person.

02:05:07.420 --> 02:05:10.380
I mean, in terms of like as a manager,

02:05:10.380 --> 02:05:13.380
like dealing with people and everything, Mark also actually.

02:05:14.580 --> 02:05:18.060
I mean, they're very human people.

02:05:18.060 --> 02:05:20.620
In the case of Mark, it's shockingly human

02:05:20.620 --> 02:05:23.220
given his trajectory.

02:05:25.500 --> 02:05:28.140
I mean, the personality of him that is painted in the press,

02:05:28.140 --> 02:05:29.660
it's just completely wrong.

02:05:29.660 --> 02:05:30.500
Yeah.

02:05:30.500 --> 02:05:32.020
But you have to know how to play the press.

02:05:32.020 --> 02:05:36.260
So that's, I put some of that responsibility on him too.

02:05:36.260 --> 02:05:41.260
You have to, it's like the director,

02:05:42.780 --> 02:05:44.340
the conductor of an orchestra,

02:05:44.340 --> 02:05:47.020
you have to play the press and the public

02:05:47.020 --> 02:05:48.060
in a certain kind of way

02:05:48.060 --> 02:05:49.780
where you convey your true self to them

02:05:49.780 --> 02:05:51.740
if there's a depth and kindness to it.

02:05:51.740 --> 02:05:54.660
And it's probably not the best at it, so yeah.

02:05:56.500 --> 02:05:57.340
You have to learn.

02:05:57.340 --> 02:06:00.460
And it's sad to see, I'll talk to him about it,

02:06:00.460 --> 02:06:04.100
but Shrep is slowly stepping down.

02:06:04.100 --> 02:06:07.540
It's always sad to see folks sort of be there

02:06:07.540 --> 02:06:10.620
for a long time and slowly, I guess time is sad.

02:06:10.620 --> 02:06:14.820
I think he's done the thing he set out to do

02:06:14.820 --> 02:06:19.820
and he's got family priorities and stuff like that.

02:06:21.460 --> 02:06:26.460
And I understand after 13 years or something.

02:06:27.940 --> 02:06:28.940
It's been a good run.

02:06:28.940 --> 02:06:32.140
Which in Silicon Valley is basically a lifetime.

02:06:32.140 --> 02:06:32.980
Yeah.

02:06:33.580 --> 02:06:34.980
It's dog years.

02:06:34.980 --> 02:06:37.620
So NeurIPS, the conference just wrapped up.

02:06:38.660 --> 02:06:40.580
Let me just go back to something else.

02:06:40.580 --> 02:06:42.500
You posted that a paper you co-authored

02:06:42.500 --> 02:06:44.460
was rejected from NeurIPS.

02:06:44.460 --> 02:06:47.140
As you said, proudly in quotes, rejected.

02:06:48.020 --> 02:06:48.940
Good joke.

02:06:48.940 --> 02:06:49.780
Yeah, I know.

02:06:51.300 --> 02:06:55.700
Can you describe this paper and what was the idea in it?

02:06:55.700 --> 02:06:58.460
And also maybe this is a good opportunity

02:06:58.460 --> 02:07:00.580
to ask what are the pros and cons,

02:07:00.580 --> 02:07:03.620
what works and what doesn't about the review process?

02:07:03.620 --> 02:07:04.980
Yeah, let me talk about the paper first.

02:07:04.980 --> 02:07:08.260
I'll talk about the review process afterwards.

02:07:09.220 --> 02:07:10.700
The paper is called VicReg.

02:07:10.700 --> 02:07:12.540
So this is, I mentioned that before,

02:07:12.540 --> 02:07:14.900
variance, invariance, covariance, regularization.

02:07:14.900 --> 02:07:18.260
And it's a technique, a non-contrastive learning technique

02:07:18.260 --> 02:07:21.300
for what I call joint embedding architecture.

02:07:21.300 --> 02:07:23.340
So Siamese nets are an example

02:07:23.340 --> 02:07:24.860
of joint embedding architecture.

02:07:24.860 --> 02:07:26.580
So joint embedding architecture is,

02:07:26.580 --> 02:07:28.220
let me back up a little bit, right?

02:07:28.220 --> 02:07:30.420
So if you want to do self-supervised learning,

02:07:30.420 --> 02:07:32.740
you can do it by prediction.

02:07:32.740 --> 02:07:35.940
So let's say you want to train a system to predict video,

02:07:35.940 --> 02:07:36.780
right?

02:07:36.780 --> 02:07:39.980
You show it a video clip and you train the system

02:07:39.980 --> 02:07:42.500
to predict the next, the continuation of that video clip.

02:07:42.500 --> 02:07:45.100
Now, because you need to handle uncertainty,

02:07:45.100 --> 02:07:48.900
because there are many continuations that are plausible,

02:07:48.900 --> 02:07:51.500
you need to handle this in some way.

02:07:51.500 --> 02:07:54.340
You need to have a way for the system

02:07:54.340 --> 02:07:57.340
to be able to produce multiple predictions.

02:07:57.340 --> 02:08:00.500
And the way, the only way I know to do this

02:08:00.500 --> 02:08:02.540
is through what's called a latent variable.

02:08:02.540 --> 02:08:06.300
So you have some sort of hidden vector of a variable

02:08:06.300 --> 02:08:09.700
that you can vary over a set or draw from a distribution.

02:08:09.700 --> 02:08:11.740
And as you vary this vector over a set,

02:08:11.740 --> 02:08:13.180
the output, the prediction varies

02:08:13.180 --> 02:08:15.780
over a set of plausible predictions, okay?

02:08:15.780 --> 02:08:18.940
So that's called, I call this a generative

02:08:18.940 --> 02:08:20.540
latent variable model.

02:08:20.740 --> 02:08:23.220
A generative latent variable model.

02:08:24.100 --> 02:08:24.940
Got it.

02:08:24.940 --> 02:08:27.020
Okay, now there is an alternative to this

02:08:27.020 --> 02:08:28.660
to handle uncertainty.

02:08:28.660 --> 02:08:31.140
And instead of directly predicting

02:08:31.140 --> 02:08:34.860
the next frames of the clip,

02:08:35.820 --> 02:08:39.620
you also run those through another neural net.

02:08:41.060 --> 02:08:42.460
So you now have two neural nets,

02:08:42.460 --> 02:08:47.460
one that looks at the initial segment of the video clip

02:08:48.460 --> 02:08:51.260
and another one that looks at the continuation

02:08:51.260 --> 02:08:52.420
during training, right?

02:08:53.580 --> 02:08:56.260
And what you're trying to do is learn a representation

02:08:57.700 --> 02:09:00.780
of those two video clips that is maximally informative

02:09:00.780 --> 02:09:03.460
about the video clips themselves,

02:09:03.460 --> 02:09:07.180
but is such that you can predict the representation

02:09:07.180 --> 02:09:09.460
of the second video clip from the representation

02:09:09.460 --> 02:09:12.340
of the first one, easily, okay?

02:09:12.340 --> 02:09:13.620
And you can sort of formalize this

02:09:13.620 --> 02:09:15.340
in terms of maximizing mutual information

02:09:15.340 --> 02:09:17.420
and some stuff like that, but it doesn't matter.

02:09:18.100 --> 02:09:21.100
What you want is informative representations

02:09:24.500 --> 02:09:27.460
of the two video clips that are mutually predictable.

02:09:28.460 --> 02:09:30.860
What that means is that there's a lot of details

02:09:30.860 --> 02:09:33.220
in the second video clips that are irrelevant.

02:09:36.460 --> 02:09:40.460
Let's say a video clip consists in a camera panning

02:09:40.460 --> 02:09:43.340
the scene, there's gonna be a piece of that room

02:09:43.340 --> 02:09:46.180
that is gonna be revealed and I can somewhat predict

02:09:46.220 --> 02:09:48.100
what that room is gonna look like,

02:09:48.100 --> 02:09:50.260
but I may not be able to predict the details

02:09:50.260 --> 02:09:52.340
of the texture of the ground

02:09:52.340 --> 02:09:54.540
and where the tiles are ending and stuff like that, right?

02:09:54.540 --> 02:09:56.380
So those are irrelevant details

02:09:56.380 --> 02:09:59.620
that perhaps my representation will eliminate.

02:09:59.620 --> 02:10:03.700
And so what I need is to train this second neural net

02:10:03.700 --> 02:10:08.700
in such a way that whenever the continuation video clip

02:10:09.500 --> 02:10:12.260
varies over all the plausible continuations,

02:10:13.620 --> 02:10:15.660
the representation doesn't change.

02:10:15.660 --> 02:10:16.500
Got it.

02:10:16.500 --> 02:10:18.540
So it's the, yeah, yeah, got it.

02:10:18.540 --> 02:10:20.900
Over the space of representations,

02:10:20.900 --> 02:10:21.900
doing the same kind of thing

02:10:21.900 --> 02:10:24.340
as you're doing with similarity learning.

02:10:24.340 --> 02:10:25.700
Right.

02:10:25.700 --> 02:10:28.900
So these are two ways to handle multimodality

02:10:28.900 --> 02:10:29.740
in a prediction, right?

02:10:29.740 --> 02:10:32.300
In the first way, you parameterize the prediction

02:10:32.300 --> 02:10:33.500
with a latent variable,

02:10:33.500 --> 02:10:35.820
but you predict pixels essentially, right?

02:10:35.820 --> 02:10:38.420
In the second one, you don't predict pixels,

02:10:38.420 --> 02:10:40.740
you predict an abstract representation of pixels

02:10:40.740 --> 02:10:43.500
and you guarantee that this abstract representation

02:10:43.500 --> 02:10:45.140
has as much information as possible

02:10:45.140 --> 02:10:47.940
about the input, but sort of drops all the stuff

02:10:47.940 --> 02:10:50.420
that you really can't predict essentially.

02:10:52.100 --> 02:10:53.900
I used to be a big fan of the first approach.

02:10:53.900 --> 02:10:55.860
And in fact, in this paper with Hichand Mishra,

02:10:55.860 --> 02:10:58.380
this blog post, the Dark Matter Intelligence,

02:10:58.380 --> 02:10:59.780
I was kind of advocating for this.

02:10:59.780 --> 02:11:01.580
And in the last year and a half,

02:11:01.580 --> 02:11:02.820
I've completely changed my mind.

02:11:02.820 --> 02:11:04.580
I'm now a big fan of the second one.

02:11:05.580 --> 02:11:10.060
And it's because of a small collection of algorithms

02:11:10.060 --> 02:11:13.260
that have been proposed over the last year and a half

02:11:13.260 --> 02:11:17.820
and also two years to do this, including Vicregg,

02:11:17.820 --> 02:11:19.620
its predecessor called Barlow-Twins,

02:11:19.620 --> 02:11:23.180
which I mentioned, a method from our friends

02:11:23.180 --> 02:11:28.180
at DeepMind called BYOL and there's a bunch of others now

02:11:28.540 --> 02:11:29.620
that kind of work similarly.

02:11:29.620 --> 02:11:32.620
So they're all based on this idea of joint embedding.

02:11:32.620 --> 02:11:34.700
Some of them have an explicit criterion

02:11:34.700 --> 02:11:36.700
that is an approximation of mutual information.

02:11:36.700 --> 02:11:39.460
Some others at BYOL work, but we don't really know why.

02:11:39.460 --> 02:11:41.260
And there's been like lots of theoretical papers

02:11:41.260 --> 02:11:42.420
about why BYOL works.

02:11:42.420 --> 02:11:44.580
No, it's not that because we take it out and it still works

02:11:44.580 --> 02:11:46.020
and blah, blah, blah.

02:11:46.020 --> 02:11:47.820
I mean, so there's like a big debate,

02:11:47.820 --> 02:11:51.540
but the important point is that we now have a collection

02:11:51.540 --> 02:11:53.700
of non-contrastive joint embedding methods,

02:11:53.700 --> 02:11:56.380
which I think is the best thing since sliced bread.

02:11:56.380 --> 02:11:59.740
So I'm super excited about this because I think

02:11:59.740 --> 02:12:02.700
it's our best shot for techniques that would allow us

02:12:02.700 --> 02:12:06.340
to kind of build predictive models.

02:12:06.340 --> 02:12:07.460
And at the same time,

02:12:07.460 --> 02:12:09.900
learn hierarchical representations of the world

02:12:09.900 --> 02:12:11.860
where what matters about the world is preserved

02:12:11.860 --> 02:12:14.460
and what is irrelevant is eliminated.

02:12:14.460 --> 02:12:17.060
And by the way, the representations of before and after

02:12:17.060 --> 02:12:20.580
is in the space in a sequence of images

02:12:20.580 --> 02:12:22.340
or is it for single images?

02:12:22.340 --> 02:12:24.460
It would be either for a single image for a sequence.

02:12:24.460 --> 02:12:25.700
It doesn't have to be images.

02:12:25.700 --> 02:12:26.740
This could be applied to text.

02:12:26.740 --> 02:12:28.580
This could be applied to just about any signal.

02:12:28.580 --> 02:12:32.980
I'm looking for methods that are generally applicable

02:12:32.980 --> 02:12:36.220
that are not specific to one particular modality.

02:12:36.220 --> 02:12:37.660
It could be audio or whatever.

02:12:37.660 --> 02:12:40.140
Got it, so what's the story behind this paper?

02:12:40.140 --> 02:12:43.500
This paper is describing one such method?

02:12:43.500 --> 02:12:44.540
This is Vickregg method.

02:12:44.540 --> 02:12:45.740
So this is co-authored.

02:12:45.740 --> 02:12:49.300
The first author is a student called Adrien Bard,

02:12:49.300 --> 02:12:52.740
who is a resident PhD student at Fair Paris.

02:12:52.740 --> 02:12:55.820
He was co-advised by me and Jean Ponce,

02:12:55.820 --> 02:12:58.220
who's a professor at Ecole Normale Superiore,

02:12:58.220 --> 02:13:00.660
also a research director at INRIA.

02:13:01.580 --> 02:13:03.620
So this is wonderful program in France

02:13:03.620 --> 02:13:06.660
where PhD students can basically do their PhD in industry.

02:13:06.660 --> 02:13:08.980
And that's kind of what's happening here.

02:13:10.380 --> 02:13:15.380
And this paper is a follow-up on this Barlow-Twin paper

02:13:15.420 --> 02:13:18.260
by my former post-doc, now Stephane Denis,

02:13:18.260 --> 02:13:21.460
with Li Jing and Eurij Bontar

02:13:21.460 --> 02:13:24.620
and a bunch of other people from Fair.

02:13:24.620 --> 02:13:27.740
And one of the main criticism from reviewers

02:13:27.740 --> 02:13:31.300
is that Vickregg is not different enough from Barlow-Twin.

02:13:31.300 --> 02:13:36.300
But my impression is that it's Barlow-Twin

02:13:37.740 --> 02:13:39.820
with a few bugs fixed essentially,

02:13:40.460 --> 02:13:42.820
and in the end, this is what people will use.

02:13:44.500 --> 02:13:49.020
But I'm used to stuff that I submit being rejected for.

02:13:49.020 --> 02:13:49.860
So it might be rejected

02:13:49.860 --> 02:13:51.300
and actually exceptionally well cited

02:13:51.300 --> 02:13:52.260
because people use it.

02:13:52.260 --> 02:13:54.340
Well, it's already cited a bunch of times.

02:13:54.340 --> 02:13:57.580
So the question is then to the deeper question

02:13:57.580 --> 02:14:00.220
about peer review and conferences.

02:14:00.220 --> 02:14:02.580
I mean, computer science is a field that's kind of unique

02:14:02.580 --> 02:14:04.940
that the conference is highly prized.

02:14:04.940 --> 02:14:06.060
That's one.

02:14:06.060 --> 02:14:09.100
And it's interesting because the peer review process there

02:14:09.100 --> 02:14:11.060
is similar, I suppose, to journals,

02:14:11.060 --> 02:14:13.620
but it's accelerated significantly.

02:14:13.620 --> 02:14:16.540
Well, not significantly, but it goes fast.

02:14:16.540 --> 02:14:19.780
And it's a nice way to get stuff out quickly,

02:14:19.780 --> 02:14:21.940
to peer review quickly, go to present it quickly

02:14:21.940 --> 02:14:22.780
to the community.

02:14:22.780 --> 02:14:25.700
So not quickly, but quicker.

02:14:25.700 --> 02:14:27.820
But nevertheless, it has many of the same flaws

02:14:27.820 --> 02:14:29.100
of peer review,

02:14:29.100 --> 02:14:31.500
because it's a limited number of people look at it.

02:14:31.500 --> 02:14:32.780
There's bias and the following,

02:14:32.780 --> 02:14:35.580
like that if you wanna do new ideas,

02:14:35.580 --> 02:14:37.060
you're gonna get pushback.

02:14:38.060 --> 02:14:39.620
There's self-interested people

02:14:39.620 --> 02:14:43.540
that kind of can infer who submitted it

02:14:43.540 --> 02:14:47.700
and kind of be cranky about it, all that kind of stuff.

02:14:47.700 --> 02:14:50.980
Yeah, I mean, there's a lot of social phenomenon there.

02:14:50.980 --> 02:14:52.140
There's one social phenomenon,

02:14:52.140 --> 02:14:54.180
which is that because the field

02:14:54.180 --> 02:14:56.780
has been growing exponentially,

02:14:56.780 --> 02:14:59.980
the vast majority of people in the field are extremely junior.

02:14:59.980 --> 02:15:00.820
Yeah.

02:15:00.820 --> 02:15:01.900
So as a consequence,

02:15:01.900 --> 02:15:04.900
and that's just a consequence of the field growing, right?

02:15:04.900 --> 02:15:07.900
So as the number of, as the size of the field

02:15:07.900 --> 02:15:08.980
kind of starts saturating,

02:15:08.980 --> 02:15:11.460
you will have less of that problem

02:15:11.460 --> 02:15:15.420
of reviewers being very inexperienced.

02:15:15.420 --> 02:15:20.180
A consequence of this is that young reviewers,

02:15:20.180 --> 02:15:21.660
I mean, there's a phenomenon,

02:15:21.660 --> 02:15:24.660
which is that reviewers try to make their life easy.

02:15:24.660 --> 02:15:27.500
And to make their life easy when reviewing a paper

02:15:27.500 --> 02:15:28.320
is very simple.

02:15:28.320 --> 02:15:29.980
You just have to find a flaw in the paper, right?

02:15:29.980 --> 02:15:34.540
So basically they see their task as finding flaws in papers.

02:15:34.900 --> 02:15:36.780
Most papers have flaws, even the good ones.

02:15:36.780 --> 02:15:38.180
Yeah.

02:15:38.180 --> 02:15:41.500
So it's easy to do that.

02:15:41.500 --> 02:15:44.860
Your job is easier as a reviewer

02:15:44.860 --> 02:15:46.460
if you just focus on this.

02:15:46.460 --> 02:15:49.700
But what's important is like,

02:15:49.700 --> 02:15:51.540
is there a new idea in that paper

02:15:51.540 --> 02:15:54.140
that is likely to influence?

02:15:54.140 --> 02:15:56.260
It doesn't matter if the experiments are not that great,

02:15:56.260 --> 02:16:01.260
if the protocol is so things like that.

02:16:01.420 --> 02:16:05.020
As long as there is a worthy idea in it

02:16:05.020 --> 02:16:08.060
that will influence the way people think about the problem,

02:16:09.180 --> 02:16:10.620
even if they make it better,

02:16:10.620 --> 02:16:15.420
eventually I think that's really what makes a paper useful.

02:16:15.420 --> 02:16:19.460
And so this combination of social phenomena

02:16:19.460 --> 02:16:24.460
creates a disease that has plagued other fields in the past

02:16:25.180 --> 02:16:26.620
like speech recognition,

02:16:26.700 --> 02:16:31.700
where basically people chase numbers on benchmarks.

02:16:31.700 --> 02:16:34.700
And it's much easier to get a paper accepted

02:16:34.700 --> 02:16:37.060
if it brings an incremental improvement

02:16:37.060 --> 02:16:42.060
on a sort of mainstream well-accepted method or problem.

02:16:43.820 --> 02:16:46.060
And those are to me boring papers.

02:16:46.060 --> 02:16:47.860
I mean, they're not useless, right?

02:16:47.860 --> 02:16:52.380
Because industry strives on those kinds of progress,

02:16:52.380 --> 02:16:54.060
but they're not the one that I'm interested in

02:16:54.060 --> 02:16:55.660
in terms of like new concepts and new ideas.

02:16:55.660 --> 02:16:59.340
So papers that are really trying to strike

02:16:59.340 --> 02:17:02.620
kind of new advances generally don't make it.

02:17:02.620 --> 02:17:04.260
Now, thankfully we have archive.

02:17:04.260 --> 02:17:05.340
Archive, exactly.

02:17:05.340 --> 02:17:08.700
And then there's open review type of situations where you,

02:17:08.700 --> 02:17:11.700
and then I mean, Twitter is a kind of open review.

02:17:11.700 --> 02:17:13.900
I'm a huge believer that review should be done

02:17:13.900 --> 02:17:15.740
by thousands of people, not two people.

02:17:15.740 --> 02:17:16.780
I agree.

02:17:16.780 --> 02:17:19.620
And so archive, like do you see a future

02:17:19.620 --> 02:17:21.260
where a lot of really strong papers,

02:17:21.260 --> 02:17:22.340
it's already the present,

02:17:22.500 --> 02:17:25.300
a growing future where it'll just be archive

02:17:26.260 --> 02:17:31.260
and you're presenting an ongoing continuous conference

02:17:31.260 --> 02:17:35.540
called Twitter slash the internet slash archive sanity.

02:17:35.540 --> 02:17:38.020
Andre just released a new version.

02:17:38.020 --> 02:17:43.020
So just not being so elitist about this particular gating.

02:17:43.420 --> 02:17:44.940
It's not a question of being elitist or not.

02:17:44.940 --> 02:17:49.940
It's a question of being basically recommendation

02:17:50.100 --> 02:17:53.380
and seal of approvals for people who don't see themselves

02:17:53.380 --> 02:17:55.980
as having the ability to do so by themselves.

02:17:55.980 --> 02:17:57.340
So it saves time.

02:17:57.340 --> 02:18:00.020
If you rely on other people's opinion

02:18:00.020 --> 02:18:03.740
and you trust those people or those groups

02:18:03.740 --> 02:18:08.740
to evaluate a paper for you, that saves you time.

02:18:09.940 --> 02:18:13.620
Cause you don't have to like scrutinize the paper as much,

02:18:13.620 --> 02:18:15.180
is brought to your attention.

02:18:15.180 --> 02:18:16.020
I mean, there's a whole idea

02:18:16.020 --> 02:18:18.740
of sort of collective recommender system.

02:18:18.740 --> 02:18:21.220
So I actually thought about this a lot,

02:18:22.340 --> 02:18:24.180
about 10, 15 years ago,

02:18:24.180 --> 02:18:27.060
cause there were discussions at NIPS

02:18:27.060 --> 02:18:31.260
and we're about to create iClear with Yosha Bengio.

02:18:31.260 --> 02:18:32.820
And so I wrote a document

02:18:34.020 --> 02:18:36.300
kind of describing a reviewing system,

02:18:36.300 --> 02:18:38.020
which basically was,

02:18:38.020 --> 02:18:39.700
you post your paper on some repository,

02:18:39.700 --> 02:18:42.540
let's say archive or now could be open review

02:18:42.540 --> 02:18:46.220
and then you can form a reviewing entity,

02:18:46.220 --> 02:18:49.620
which is equivalent to a reviewing board of a journal

02:18:49.620 --> 02:18:53.980
or program committee of a conference.

02:18:53.980 --> 02:18:55.580
You have to list the members.

02:18:55.580 --> 02:18:59.460
And then that group reviewing entity

02:18:59.460 --> 02:19:03.180
can choose to review a particular paper spontaneously

02:19:03.180 --> 02:19:04.020
or not.

02:19:04.020 --> 02:19:05.620
There is no exclusive relationship anymore

02:19:05.620 --> 02:19:09.220
between a paper and a venue or reviewing entity.

02:19:09.220 --> 02:19:11.220
Any reviewing entity can review any paper

02:19:12.740 --> 02:19:14.140
or may choose not to.

02:19:15.020 --> 02:19:16.660
And then, you know, give an evaluation.

02:19:16.660 --> 02:19:17.940
It's not published, not published.

02:19:17.940 --> 02:19:20.340
It's just an evaluation and a comment,

02:19:20.340 --> 02:19:21.740
which would be public,

02:19:21.740 --> 02:19:23.700
signed by the reviewing entity.

02:19:23.700 --> 02:19:25.900
And if it's signed by a reviewing entity,

02:19:25.900 --> 02:19:27.780
you know, it's one of the members of reviewing entity.

02:19:27.780 --> 02:19:30.700
So if the reviewing entity is, you know,

02:19:30.700 --> 02:19:33.740
Lex Fridman's preferred papers, right?

02:19:33.740 --> 02:19:35.660
You know, it's Lex Fridman writing the review.

02:19:35.660 --> 02:19:36.740
Yes.

02:19:36.740 --> 02:19:40.940
So for me, that's a beautiful system, I think,

02:19:40.940 --> 02:19:42.940
but what's in addition to that,

02:19:42.940 --> 02:19:45.820
it feels like there should be a reputation system

02:19:45.820 --> 02:19:47.540
for the reviewers.

02:19:47.540 --> 02:19:49.060
For the reviewing entities,

02:19:49.060 --> 02:19:50.300
not the reviewers individually.

02:19:50.300 --> 02:19:51.740
The reviewing entities, sure.

02:19:51.740 --> 02:19:53.940
But even within that, the reviewers too,

02:19:53.940 --> 02:19:57.180
because there's another thing here.

02:19:57.180 --> 02:19:59.380
It's not just the reputation.

02:19:59.380 --> 02:20:02.700
It's an incentive for an individual person to do great.

02:20:02.700 --> 02:20:05.100
Right now, in the academic setting,

02:20:05.100 --> 02:20:07.940
the incentive is kind of internal,

02:20:07.940 --> 02:20:09.300
just wanting to do a good job.

02:20:09.300 --> 02:20:11.300
But honestly, that's not a strong enough incentive

02:20:11.300 --> 02:20:13.740
to do a really good job in reading a paper

02:20:13.740 --> 02:20:15.940
and finding the beautiful amidst the mistakes

02:20:15.940 --> 02:20:17.820
and the flaws and all that kind of stuff.

02:20:17.820 --> 02:20:19.260
Like, if you're the person

02:20:19.260 --> 02:20:22.460
that first discovered a powerful paper

02:20:22.460 --> 02:20:25.140
and you get to be proud of that discovery,

02:20:25.140 --> 02:20:27.780
then that gives a huge incentive to you.

02:20:27.780 --> 02:20:29.380
That's a big part of my proposal, actually,

02:20:29.380 --> 02:20:31.300
where I describe that as, you know,

02:20:31.300 --> 02:20:33.740
if your evaluation of papers

02:20:33.740 --> 02:20:37.540
is predictive of future success, okay,

02:20:37.540 --> 02:20:40.940
then your reputation should go up as a reviewing entity.

02:20:42.540 --> 02:20:43.740
So, yeah, exactly.

02:20:43.740 --> 02:20:46.300
I mean, I even had a master's student

02:20:46.300 --> 02:20:49.580
who was a master's student in library science

02:20:49.580 --> 02:20:50.420
and computer science,

02:20:50.420 --> 02:20:52.500
actually kind of work out exactly

02:20:52.500 --> 02:20:55.180
how that should work with formulas and everything.

02:20:55.180 --> 02:20:56.820
So in terms of implementation,

02:20:56.820 --> 02:20:58.620
do you think that's something that's doable?

02:20:58.620 --> 02:20:59.740
I mean, I've been sort of, you know,

02:20:59.740 --> 02:21:02.060
talking about this to sort of various people,

02:21:02.060 --> 02:21:05.940
like, you know, Andrew McCallum, who started Open Review.

02:21:05.940 --> 02:21:07.780
And the reason why we picked Open Review

02:21:07.780 --> 02:21:09.100
for iClear initially,

02:21:09.100 --> 02:21:11.420
even though it was very early for them,

02:21:11.420 --> 02:21:14.300
is because my hope was that iClear,

02:21:14.300 --> 02:21:16.740
it was eventually going to kind of

02:21:16.740 --> 02:21:18.580
inaugurate this type of system.

02:21:18.580 --> 02:21:22.220
So iClear kept the idea of open reviews,

02:21:22.220 --> 02:21:23.780
so where the reviews are, you know,

02:21:23.780 --> 02:21:27.300
published with a paper, which I think is very useful.

02:21:27.300 --> 02:21:29.740
But in many ways, that's kind of reverted

02:21:29.740 --> 02:21:33.260
to kind of more of a conventional type conferences

02:21:33.260 --> 02:21:34.100
for everything else.

02:21:34.100 --> 02:21:37.780
And that, I mean, I don't run iClear,

02:21:37.780 --> 02:21:41.180
I'm just the president of the foundation,

02:21:41.180 --> 02:21:44.100
but, you know, people who run it

02:21:44.100 --> 02:21:45.620
should make decisions about how to run it.

02:21:45.620 --> 02:21:48.540
And I'm not going to tell them because they're volunteers

02:21:48.540 --> 02:21:50.340
and I'm really thankful that they do that.

02:21:50.340 --> 02:21:53.020
So, but I'm saddened by the fact

02:21:53.020 --> 02:21:57.060
that we're not being innovative enough.

02:21:57.060 --> 02:21:57.900
Yeah, me too.

02:21:57.900 --> 02:21:59.620
I hope that changes.

02:21:59.620 --> 02:22:02.060
Yeah, because the communication science broadly,

02:22:02.060 --> 02:22:04.220
the communication and computer science ideas

02:22:05.420 --> 02:22:08.340
is how you make those ideas have impact, I think.

02:22:08.340 --> 02:22:11.420
Yeah, and I think, you know, a lot of this is

02:22:11.420 --> 02:22:16.220
because people have in their mind kind of an objective,

02:22:16.220 --> 02:22:19.140
which is, you know, fairness for authors

02:22:19.140 --> 02:22:22.260
and the ability to count points, basically,

02:22:22.260 --> 02:22:24.860
and give credits accurately.

02:22:24.860 --> 02:22:28.860
But that comes at the expense of the progress of science.

02:22:28.860 --> 02:22:30.460
So to some extent, we're slowing down

02:22:30.460 --> 02:22:32.140
the progress of science.

02:22:32.140 --> 02:22:34.420
And are we actually achieving fairness?

02:22:34.420 --> 02:22:36.500
And we're not achieving fairness, you know,

02:22:36.500 --> 02:22:38.060
we still have biases, you know,

02:22:38.060 --> 02:22:39.780
we're doing, you know, a double blind review,

02:22:39.780 --> 02:22:44.340
but, you know, the biases are still there,

02:22:44.340 --> 02:22:46.700
there are different kinds of biases.

02:22:46.700 --> 02:22:49.340
You write that the phenomenon of emergence,

02:22:49.340 --> 02:22:51.660
collective behavior exhibited by a large collection

02:22:51.660 --> 02:22:54.220
of simple elements in interaction

02:22:54.220 --> 02:22:56.780
is one of the things that got you into neural nets

02:22:56.780 --> 02:22:57.740
in the first place.

02:22:57.740 --> 02:22:59.060
I love cellular automata,

02:22:59.060 --> 02:23:01.940
I love simple interacting elements

02:23:01.940 --> 02:23:04.020
and the things that emerge from them.

02:23:04.020 --> 02:23:07.260
Do you think we understand how complex systems

02:23:07.260 --> 02:23:09.580
can emerge from such simple components

02:23:09.580 --> 02:23:11.020
that interact simply?

02:23:11.020 --> 02:23:12.260
No, we don't.

02:23:12.260 --> 02:23:13.100
It's a big mystery.

02:23:13.100 --> 02:23:14.460
Also, it's a mystery for physicists,

02:23:14.460 --> 02:23:16.020
it's a mystery for biologists.

02:23:16.980 --> 02:23:22.060
You know, how is it that the universe around us

02:23:22.060 --> 02:23:25.140
seems to be increasing in complexity and not decreasing?

02:23:25.140 --> 02:23:28.220
I mean, that is a kind of curious property

02:23:28.220 --> 02:23:32.340
of physics that despite the second law of thermodynamics,

02:23:32.340 --> 02:23:35.940
we seem to be, you know, evolution and learning

02:23:35.940 --> 02:23:39.700
and et cetera, seems to be kind of, at least locally,

02:23:40.660 --> 02:23:43.980
to increase complexity, not decrease it.

02:23:43.980 --> 02:23:46.500
So perhaps the ultimate purpose of the universe

02:23:46.500 --> 02:23:49.060
is to just get more complex.

02:23:49.060 --> 02:23:54.060
Have these, I mean, small pockets of beautiful complexity.

02:23:55.140 --> 02:23:57.140
Does that, do cellular automata,

02:23:57.140 --> 02:23:59.700
do these kinds of emergence and complex systems

02:23:59.700 --> 02:24:04.140
give you some intuition or guide your understanding

02:24:04.140 --> 02:24:06.700
of machine learning systems and neural networks and so on?

02:24:06.700 --> 02:24:09.460
Or are these, for you right now, disparate concepts?

02:24:09.460 --> 02:24:10.900
Well, it got me into it.

02:24:10.900 --> 02:24:15.620
You know, I discovered the existence of the perceptron

02:24:15.620 --> 02:24:19.700
when I was a college student, you know, by reading a book.

02:24:19.700 --> 02:24:21.700
It was a debate between Chomsky and Piaget

02:24:21.700 --> 02:24:24.220
and Seymour Pepper from MIT,

02:24:24.220 --> 02:24:26.700
he was kind of singing the praise of the perceptron

02:24:27.220 --> 02:24:28.860
in that book and the first time I heard

02:24:28.860 --> 02:24:29.980
about the learning machine, right?

02:24:29.980 --> 02:24:31.380
So I started digging the literature

02:24:31.380 --> 02:24:34.780
and I found those books, which were basically

02:24:34.780 --> 02:24:38.740
transcription of, you know, workshops or conferences

02:24:38.740 --> 02:24:42.180
from the 50s and 60s about self-organizing systems.

02:24:42.180 --> 02:24:44.580
So there was a series of conferences

02:24:44.580 --> 02:24:48.180
on self-organizing systems and these books on this.

02:24:48.180 --> 02:24:50.220
Some of them are, you can actually get them

02:24:50.220 --> 02:24:53.300
at the Internet Archive, you know, the digital version.

02:24:54.140 --> 02:24:58.340
And there are like fascinating articles in there by,

02:24:58.340 --> 02:25:00.420
there's a guy whose name has been largely forgotten,

02:25:00.420 --> 02:25:04.540
Heinz von Frster, he's a German physicist

02:25:04.540 --> 02:25:06.260
who immigrated to the U.S.

02:25:06.260 --> 02:25:11.340
and worked on self-organizing systems in the 50s.

02:25:11.340 --> 02:25:12.900
And in the 60s, he created,

02:25:12.900 --> 02:25:14.500
at University of Illinois, Urbana-Champaign,

02:25:14.500 --> 02:25:18.940
he created the biological computer laboratory, BCL,

02:25:18.940 --> 02:25:21.660
which was, you know, all about neural nets.

02:25:21.700 --> 02:25:23.420
Unfortunately, that was kind of towards the end

02:25:23.420 --> 02:25:24.900
of the popularity of neural nets,

02:25:24.900 --> 02:25:27.740
so that lab never kind of strived very much.

02:25:27.740 --> 02:25:30.380
But he wrote a bunch of papers about self-organization

02:25:30.380 --> 02:25:33.500
and about the mystery of self-organization.

02:25:33.500 --> 02:25:35.700
An example he has is, you take,

02:25:35.700 --> 02:25:38.060
imagine you are in space, there's no gravity,

02:25:38.060 --> 02:25:42.220
you have a big box with magnets in it, okay?

02:25:42.220 --> 02:25:43.940
You know, kind of rectangular magnets

02:25:43.940 --> 02:25:46.900
with North Pole on one end, South Pole on the other end.

02:25:46.900 --> 02:25:48.500
You shake the box gently

02:25:48.500 --> 02:25:50.220
and the magnets will kind of stick to themselves

02:25:50.220 --> 02:25:53.740
and probably form a complex structure, you know,

02:25:53.740 --> 02:25:55.500
spontaneously, you know,

02:25:55.500 --> 02:25:57.100
that could be an example of self-organization.

02:25:57.100 --> 02:25:58.420
But, you know, you have lots of examples,

02:25:58.420 --> 02:26:00.380
neural nets are an example of self-organization

02:26:00.380 --> 02:26:03.060
to, you know, in many respects.

02:26:03.060 --> 02:26:05.980
And it's a bit of a mystery, you know,

02:26:05.980 --> 02:26:09.260
how, like what is possible with this?

02:26:09.260 --> 02:26:12.020
You know, pattern formation in physical systems,

02:26:12.020 --> 02:26:14.780
in chaotic system and things like that, you know,

02:26:14.780 --> 02:26:16.900
the emergence of life, you know, things like that.

02:26:16.900 --> 02:26:19.580
So, you know, how does that happen?

02:26:19.580 --> 02:26:22.540
It's a big puzzle for physicists as well.

02:26:22.540 --> 02:26:24.700
It feels like understanding this,

02:26:24.700 --> 02:26:29.700
the mathematics of emergence in some constrained situations

02:26:29.700 --> 02:26:32.100
might help us create intelligence.

02:26:32.100 --> 02:26:35.980
Like, help us add a little spice to the systems

02:26:35.980 --> 02:26:39.500
because you seem to be able to,

02:26:39.500 --> 02:26:41.900
in complex systems with emergence,

02:26:41.900 --> 02:26:44.580
to be able to get a lot from little.

02:26:44.580 --> 02:26:47.020
And so that seems like a shortcut

02:26:47.020 --> 02:26:49.100
to get big leaps in performance.

02:26:49.700 --> 02:26:51.100
But...

02:26:51.100 --> 02:26:54.980
But there's a missing concept that we don't have.

02:26:54.980 --> 02:26:55.820
Yeah.

02:26:55.820 --> 02:26:58.380
And it's something also I've been fascinated by

02:26:58.380 --> 02:27:00.700
since my undergrad days.

02:27:00.700 --> 02:27:03.860
And it's how you measure complexity, right?

02:27:03.860 --> 02:27:06.940
So we don't actually have good ways of measuring,

02:27:06.940 --> 02:27:09.820
or at least we don't have good ways of interpreting

02:27:09.820 --> 02:27:11.900
the measures that we have at our disposal.

02:27:11.900 --> 02:27:14.460
Like, how do you measure the complexity of something, right?

02:27:14.460 --> 02:27:15.660
So there's all those things, you know,

02:27:15.660 --> 02:27:17.860
like, you know, Kolmogorov, Chaiting, Solomonov,

02:27:17.860 --> 02:27:19.420
complexity of, you know,

02:27:19.420 --> 02:27:20.940
the length of the shortest program

02:27:20.940 --> 02:27:22.460
that would generate a bit string

02:27:22.460 --> 02:27:25.460
can be thought of as the complexity of that bit string, right?

02:27:26.860 --> 02:27:28.220
I've been fascinated by that concept.

02:27:28.220 --> 02:27:30.140
The problem with that is that

02:27:31.500 --> 02:27:33.900
that complexity is defined up to a constant,

02:27:33.900 --> 02:27:35.060
which can be very large.

02:27:35.900 --> 02:27:36.860
Right.

02:27:36.860 --> 02:27:39.660
There are similar concepts that are derived from, you know,

02:27:40.700 --> 02:27:43.340
you know, Bayesian probability theory,

02:27:43.340 --> 02:27:45.580
where, you know, the complexity of something

02:27:45.580 --> 02:27:49.460
is the negative log of its probability, essentially, right?

02:27:49.460 --> 02:27:52.260
And you have a complete equivalence between the two things.

02:27:52.260 --> 02:27:53.180
And there you would think, you know,

02:27:53.180 --> 02:27:56.220
the probability is something that's well defined mathematically,

02:27:56.220 --> 02:27:58.220
which means complexity is well-defined.

02:27:58.220 --> 02:27:59.060
But it's not true.

02:27:59.060 --> 02:28:02.660
You need to have a model of the distribution.

02:28:02.660 --> 02:28:03.780
You may need to have a prior,

02:28:03.780 --> 02:28:05.100
if you're doing Bayesian inference,

02:28:05.100 --> 02:28:07.940
and the prior plays the same role as the choice of the computer

02:28:07.940 --> 02:28:10.460
with which you measure Kolmogorov complexity.

02:28:10.460 --> 02:28:12.940
And so every measure of complexity we have

02:28:12.980 --> 02:28:14.500
has some arbitrary in acidity,

02:28:16.340 --> 02:28:17.740
you know, an additive constant,

02:28:17.740 --> 02:28:20.500
which can be arbitrarily large.

02:28:20.500 --> 02:28:24.260
And so, you know, how can we come up with a good theory

02:28:24.260 --> 02:28:25.540
of how things become more complex

02:28:25.540 --> 02:28:26.900
if we don't have a good measure of complexity?

02:28:26.900 --> 02:28:27.740
Yeah.

02:28:27.740 --> 02:28:28.980
Which we need for,

02:28:28.980 --> 02:28:32.980
one way that people study this in the space of biology,

02:28:32.980 --> 02:28:34.540
the people that study the origin of life,

02:28:34.540 --> 02:28:37.820
or try to recreate the life in the laboratory,

02:28:37.820 --> 02:28:39.860
and the more interesting one is the alien one,

02:28:39.860 --> 02:28:42.020
is when we go to other planets,

02:28:42.020 --> 02:28:44.700
how do we recognize that it's life?

02:28:44.700 --> 02:28:47.500
Because, you know, complexity, we associate complexity,

02:28:47.500 --> 02:28:49.860
maybe some level of mobility with life.

02:28:50.700 --> 02:28:55.700
You know, we have to be able to have concrete algorithms

02:28:55.780 --> 02:29:00.780
for measuring the level of complexity we see

02:29:00.860 --> 02:29:03.340
in order to know the difference between life and non-life.

02:29:03.340 --> 02:29:04.620
And the problem is that complexity

02:29:04.620 --> 02:29:06.060
is in the eye of the beholder.

02:29:06.060 --> 02:29:08.100
So let me give you an example.

02:29:08.100 --> 02:29:13.100
If I give you an image of the MNIST digits, right,

02:29:13.860 --> 02:29:16.020
and I flip through MNIST digits,

02:29:16.020 --> 02:29:18.700
there is obviously some structure to it

02:29:18.700 --> 02:29:21.060
because local structure, you know,

02:29:21.060 --> 02:29:22.780
neighboring pixels are correlated

02:29:23.780 --> 02:29:26.140
across the entire data set.

02:29:26.140 --> 02:29:30.980
I imagine that I apply a random permutation

02:29:30.980 --> 02:29:34.580
to all the pixels, a fixed random permutation.

02:29:34.580 --> 02:29:37.460
Now I show you those images, they will look,

02:29:37.500 --> 02:29:41.300
you know, really disorganized to you, more complex.

02:29:41.300 --> 02:29:43.540
In fact, they're not more complex in absolute terms.

02:29:43.540 --> 02:29:46.140
They're exactly the same as originally, right?

02:29:46.140 --> 02:29:47.620
And if you knew what the permutation was,

02:29:47.620 --> 02:29:50.060
you know, you could undo the permutation.

02:29:50.060 --> 02:29:52.940
Now, imagine I give you special glasses

02:29:52.940 --> 02:29:54.740
that undo that permutation.

02:29:54.740 --> 02:29:57.660
Now, all of a sudden, what looked complicated becomes simple.

02:29:57.660 --> 02:29:58.500
Right.

02:29:58.500 --> 02:30:02.220
So if you have two, if you have humans on one end,

02:30:02.220 --> 02:30:03.860
and then another race of aliens

02:30:03.860 --> 02:30:06.020
that sees the universe with permutation glasses.

02:30:06.020 --> 02:30:08.940
What we perceive as simple to them is hardly complicated.

02:30:08.940 --> 02:30:09.780
It's probably heat.

02:30:09.780 --> 02:30:10.940
Yeah, heat, yeah.

02:30:10.940 --> 02:30:13.420
Okay, and what they perceive as simple to us

02:30:13.420 --> 02:30:16.380
is random fluctuation, it's heat.

02:30:16.380 --> 02:30:17.220
Yeah.

02:30:18.060 --> 02:30:20.260
But truly in the eye of the beholder,

02:30:20.260 --> 02:30:22.380
it depends what kind of glasses you're wearing.

02:30:22.380 --> 02:30:23.220
Right.

02:30:23.220 --> 02:30:24.460
Depends what kind of algorithm you're running

02:30:24.460 --> 02:30:25.860
in your perception system.

02:30:25.860 --> 02:30:28.660
So I don't think we'll have a theory of intelligence,

02:30:28.660 --> 02:30:31.860
self-organization, evolution, things like this,

02:30:31.860 --> 02:30:35.180
until we have good handles on the data set.

02:30:35.180 --> 02:30:38.580
We have a good handle on a notion of complexity,

02:30:38.580 --> 02:30:40.900
which we know is in the eye of the beholder.

02:30:42.380 --> 02:30:45.100
Yeah, it's sad to think that we might not be able to detect

02:30:45.100 --> 02:30:47.660
or interact with alien species

02:30:47.660 --> 02:30:50.380
because we're wearing different glasses.

02:30:50.380 --> 02:30:51.540
Because their notion of locality

02:30:51.540 --> 02:30:52.500
might be different from ours.

02:30:52.500 --> 02:30:53.340
Yeah, exactly.

02:30:53.340 --> 02:30:55.300
This actually connects with fascinating questions

02:30:55.300 --> 02:30:58.220
in physics at the moment, like modern physics,

02:30:58.220 --> 02:31:00.580
quantum physics, like, you know, questions about like,

02:31:00.580 --> 02:31:02.620
you know, can we recover the information

02:31:02.620 --> 02:31:04.620
that's lost in a black hole and things like this, right?

02:31:05.580 --> 02:31:08.020
And that relies on notions of complexity,

02:31:09.420 --> 02:31:11.700
which, you know, I find this fascinating.

02:31:11.700 --> 02:31:13.420
Can you describe your personal quest

02:31:13.420 --> 02:31:18.420
to build an expressive electronic wind instrument, EWI?

02:31:19.820 --> 02:31:20.660
What is it?

02:31:20.660 --> 02:31:24.060
What does it take to build it?

02:31:24.060 --> 02:31:25.140
Well, I'm a tinkerer.

02:31:25.140 --> 02:31:26.820
I like building things.

02:31:26.820 --> 02:31:29.020
I like building things with combinations of electronics

02:31:29.020 --> 02:31:31.100
and, you know, mechanical stuff.

02:31:32.460 --> 02:31:34.180
You know, I have a bunch of different hobbies,

02:31:34.180 --> 02:31:38.020
but, you know, probably my first one was little,

02:31:38.020 --> 02:31:39.860
was building model airplanes and stuff like that.

02:31:39.860 --> 02:31:41.940
And I still do that to some extent,

02:31:41.940 --> 02:31:42.780
but also electronics.

02:31:42.780 --> 02:31:45.220
I taught myself electronics before I studied it.

02:31:46.260 --> 02:31:48.180
And the reason I taught myself electronics

02:31:48.180 --> 02:31:49.620
is because of music.

02:31:49.620 --> 02:31:53.220
My cousin was inspiring electronic musician

02:31:53.220 --> 02:31:55.060
and he had an analog synthesizer

02:31:55.060 --> 02:31:57.100
and I was basically modifying it for him

02:31:58.060 --> 02:31:59.940
and building sequencers and stuff like that, right?

02:31:59.940 --> 02:32:02.660
For him, I was in high school when I was doing this.

02:32:02.660 --> 02:32:06.060
How's the interest in like progressive rock, like 80s?

02:32:06.060 --> 02:32:07.980
Like what's the greatest band of all time,

02:32:07.980 --> 02:32:09.820
according to Yann LeCun?

02:32:09.820 --> 02:32:11.700
There's too many of them, but, you know,

02:32:11.700 --> 02:32:16.380
it's a combination of, you know,

02:32:16.380 --> 02:32:19.820
Mavishnu Orchestra, Weather Report,

02:32:19.820 --> 02:32:24.820
yes, Genesis, you know, pre-Peter Gabriel,

02:32:27.140 --> 02:32:29.100
Gentle Giant, you know, things like that.

02:32:29.100 --> 02:32:29.940
Great.

02:32:29.940 --> 02:32:32.260
Okay, so this level of electronics

02:32:32.860 --> 02:32:34.260
and this level of music combined together.

02:32:34.260 --> 02:32:36.380
Right, so I was actually trained to play

02:32:37.540 --> 02:32:39.500
Baroque and Renaissance music.

02:32:39.500 --> 02:32:43.300
And I played in an orchestra when I was in high school

02:32:43.300 --> 02:32:45.780
and first year of college.

02:32:45.780 --> 02:32:48.100
And I played the recorder, crumb horn,

02:32:48.100 --> 02:32:50.220
a little bit of oboe, you know, things like that.

02:32:50.220 --> 02:32:52.540
So I'm a wind instrument player.

02:32:52.540 --> 02:32:54.100
But I always wanted to play improvised music,

02:32:54.100 --> 02:32:56.340
even though I don't know anything about it.

02:32:56.340 --> 02:32:58.780
And the only way I figured, you know,

02:32:58.780 --> 02:33:01.100
short of like learning to play saxophone

02:33:01.100 --> 02:33:03.580
was to play electronic wind instruments.

02:33:03.580 --> 02:33:06.420
So they behave, the fingering is similar to a saxophone,

02:33:06.420 --> 02:33:09.140
but, you know, you have wide variety of sound

02:33:09.140 --> 02:33:11.060
because you control the synthesizer with it.

02:33:11.060 --> 02:33:13.180
So I had a bunch of those, you know,

02:33:13.180 --> 02:33:18.180
going back to the late 80s from either Yamaha or Akai.

02:33:18.900 --> 02:33:22.540
They're both kind of the main manufacturers of those.

02:33:22.540 --> 02:33:23.740
So they were classically, you know,

02:33:23.740 --> 02:33:25.620
going back several decades.

02:33:25.620 --> 02:33:27.700
But I've never been completely satisfied with them

02:33:27.700 --> 02:33:29.300
because of lack of expressivity.

02:33:31.140 --> 02:33:33.460
And, you know, those things, you know, are somewhat expressive.

02:33:33.460 --> 02:33:34.820
I mean, they measure the breath pressure,

02:33:34.820 --> 02:33:36.580
they measure the lip pressure,

02:33:36.580 --> 02:33:39.860
and, you know, you have various parameters.

02:33:39.860 --> 02:33:41.540
You can vary it with fingers,

02:33:41.540 --> 02:33:44.860
but they're not really as expressive

02:33:44.860 --> 02:33:47.100
as an acoustic instrument, right?

02:33:47.100 --> 02:33:49.460
You hear John Coltrane play two notes

02:33:49.460 --> 02:33:50.820
and you know it's John Coltrane.

02:33:50.820 --> 02:33:54.340
You know, it's got a unique sound or Miles Davis, right?

02:33:54.340 --> 02:33:57.580
You can hear it's Miles Davis playing the trumpet

02:33:57.580 --> 02:34:02.580
because the sound reflects their, you know, physiognomy,

02:34:04.340 --> 02:34:06.380
basically the shape of the vocal tract

02:34:08.180 --> 02:34:09.740
kind of shapes the sound.

02:34:09.740 --> 02:34:12.900
So how do you do this with an electronic instrument?

02:34:12.900 --> 02:34:16.180
And I was, many years ago, I met a guy called David Wessel.

02:34:16.180 --> 02:34:18.820
He was a professor at Berkeley

02:34:18.820 --> 02:34:21.980
and created the center for like, you know,

02:34:21.980 --> 02:34:23.540
music technology there.

02:34:23.540 --> 02:34:26.220
And he was interested in that question.

02:34:26.220 --> 02:34:28.660
And so I kept kind of thinking about this for many years.

02:34:28.660 --> 02:34:31.580
And finally, because of COVID, you know, I was at home.

02:34:31.580 --> 02:34:32.620
I was in my workshop.

02:34:32.620 --> 02:34:36.100
My workshop serves also as my kind of Zoom room

02:34:36.100 --> 02:34:37.620
and home office.

02:34:37.620 --> 02:34:38.820
This is in New Jersey?

02:34:38.820 --> 02:34:39.660
In New Jersey.

02:34:39.660 --> 02:34:43.620
And I started really being serious about, you know,

02:34:43.620 --> 02:34:45.860
building my own EOE instrument.

02:34:45.860 --> 02:34:48.220
What else is going on in that New Jersey workshop?

02:34:48.220 --> 02:34:50.940
Is there some crazy stuff you built,

02:34:50.940 --> 02:34:55.540
like just, or like left on the workshop floor, left behind?

02:34:55.540 --> 02:34:57.660
A lot of crazy stuff is, you know,

02:34:57.660 --> 02:35:01.740
electronics built with microcontrollers of various kinds

02:35:01.740 --> 02:35:04.940
and, you know, weird flying contraptions.

02:35:06.820 --> 02:35:08.780
So you still love flying.

02:35:08.780 --> 02:35:09.940
It's a family disease.

02:35:09.940 --> 02:35:13.580
My dad got me into it when I was a kid.

02:35:13.580 --> 02:35:16.900
And he was building model airplanes when he was a kid.

02:35:16.900 --> 02:35:19.860
And he was a mechanical engineer.

02:35:19.860 --> 02:35:21.260
He taught himself electronics also.

02:35:21.260 --> 02:35:24.140
So he built his early radio control systems

02:35:24.140 --> 02:35:26.460
in the late 60s, early 70s.

02:35:27.740 --> 02:35:29.820
And so that's what got me into, I mean,

02:35:29.820 --> 02:35:31.700
he got me into kind of, you know, engineering

02:35:31.700 --> 02:35:33.060
and science and technology.

02:35:33.060 --> 02:35:36.140
Do you also have an interest in appreciation of flight

02:35:36.140 --> 02:35:38.300
and other forms, like with drones, quadroptors,

02:35:38.300 --> 02:35:41.020
or do you, is it model airplane?

02:35:41.020 --> 02:35:45.220
You know, before drones were, you know,

02:35:45.220 --> 02:35:49.220
kind of a consumer product, you know,

02:35:49.220 --> 02:35:50.260
I built my own, you know,

02:35:50.260 --> 02:35:51.980
with also building a microcontroller

02:35:51.980 --> 02:35:56.260
with Javascoops and accelerometers for stabilization,

02:35:56.260 --> 02:35:57.740
writing the firmware for it, you know.

02:35:57.740 --> 02:35:59.180
And then when it became kind of a standard thing

02:35:59.180 --> 02:36:00.140
you could buy, it was boring.

02:36:00.140 --> 02:36:01.140
You know, I stopped doing it.

02:36:01.140 --> 02:36:02.420
It was not fun anymore.

02:36:03.540 --> 02:36:06.700
Yeah, you were doing it before it was cool.

02:36:06.700 --> 02:36:10.060
What advice would you give to a young person today

02:36:10.060 --> 02:36:13.820
in high school and college that dreams of doing

02:36:13.820 --> 02:36:15.980
something big like Yann LeCun,

02:36:15.980 --> 02:36:18.980
like let's talk in the space of intelligence,

02:36:18.980 --> 02:36:20.980
dreams of having a chance to solve

02:36:20.980 --> 02:36:23.980
some fundamental problem in space of intelligence,

02:36:23.980 --> 02:36:26.180
both for their career and just in life,

02:36:26.180 --> 02:36:30.700
being somebody who was a part of creating something special.

02:36:30.700 --> 02:36:35.420
So try to get interested by big questions,

02:36:35.420 --> 02:36:38.700
things like, you know, what is intelligence?

02:36:38.700 --> 02:36:40.460
What is the universe made of?

02:36:40.460 --> 02:36:41.700
What's life all about?

02:36:41.700 --> 02:36:42.540
Things like that.

02:36:45.060 --> 02:36:47.060
Like even like crazy big questions,

02:36:47.060 --> 02:36:50.340
like what's time, like nobody knows what time is.

02:36:51.980 --> 02:36:56.980
And then learn basic things, like basic methods,

02:36:59.900 --> 02:37:03.260
either from math, from physics or from engineering.

02:37:03.260 --> 02:37:05.620
Things that have a long shelf life.

02:37:05.620 --> 02:37:08.740
Like if you have a choice between like, you know,

02:37:08.740 --> 02:37:11.700
learning, you know, mobile programming on iPhone

02:37:12.620 --> 02:37:14.820
or quantum mechanics, take quantum mechanics.

02:37:16.900 --> 02:37:18.460
Because you're gonna learn things

02:37:18.460 --> 02:37:20.380
that you have no idea exist.

02:37:20.380 --> 02:37:25.340
And you may not, you may never be a quantum physicist,

02:37:25.340 --> 02:37:26.780
but you will learn about path integrals.

02:37:26.780 --> 02:37:29.140
And path integrals are used everywhere.

02:37:29.140 --> 02:37:31.100
It's the same formula that you use for, you know,

02:37:31.100 --> 02:37:33.300
Bayesian integration and stuff like that.

02:37:33.300 --> 02:37:37.700
So the ideas, the little ideas within quantum mechanics

02:37:37.700 --> 02:37:41.460
or within some of these kind of more solidified fields

02:37:41.460 --> 02:37:42.660
will have a longer shelf life.

02:37:42.660 --> 02:37:46.940
They will somehow use indirectly in your work.

02:37:46.940 --> 02:37:48.100
Learn classical mechanics,

02:37:48.100 --> 02:37:50.500
like you learn about Lagrangians, for example,

02:37:51.380 --> 02:37:55.180
which is like a hugely useful concept, you know,

02:37:55.180 --> 02:37:57.300
for all kinds of different things.

02:37:57.300 --> 02:38:01.700
Learn statistical physics because all the math

02:38:01.700 --> 02:38:04.380
that comes out of, you know, for machine learning

02:38:05.500 --> 02:38:07.300
basically comes out of what's figured out

02:38:07.300 --> 02:38:09.260
by statistical physicists in the, you know,

02:38:09.260 --> 02:38:10.980
late 19th, early 20th century, right?

02:38:10.980 --> 02:38:14.300
So, and for some of them actually more recently

02:38:14.300 --> 02:38:16.140
for people like Giorgio Parisi,

02:38:16.140 --> 02:38:19.060
who just got the Nobel Prize for the replica method,

02:38:19.060 --> 02:38:23.180
among other things, it's used for a lot of different things,

02:38:23.180 --> 02:38:25.580
you know, variational inference,

02:38:25.580 --> 02:38:27.660
that math comes from statistical physics.

02:38:28.580 --> 02:38:33.580
So a lot of those kind of, you know, basic courses,

02:38:33.940 --> 02:38:36.220
you know, if you do electrical engineering,

02:38:36.220 --> 02:38:37.620
you take signal processing,

02:38:37.620 --> 02:38:39.900
you'll learn about Fourier transforms.

02:38:39.900 --> 02:38:42.700
Again, something super useful is at the basis

02:38:42.700 --> 02:38:44.900
of things like graph neural nets,

02:38:44.900 --> 02:38:49.380
which is an entirely new sub area of, you know,

02:38:49.380 --> 02:38:50.660
AI machine learning, deep learning,

02:38:50.660 --> 02:38:52.140
which I think is super promising

02:38:52.140 --> 02:38:54.340
for all kinds of applications.

02:38:54.340 --> 02:38:55.180
Something very promising,

02:38:55.180 --> 02:38:56.660
if you're more interested in applications,

02:38:56.660 --> 02:38:58.780
is the applications of AI machine learning

02:38:58.780 --> 02:39:00.340
and deep learning to science

02:39:01.500 --> 02:39:04.540
or to science that can help solve

02:39:04.540 --> 02:39:05.860
big problems in the world.

02:39:05.860 --> 02:39:09.220
I have colleagues at Meta, at FAIR,

02:39:09.220 --> 02:39:11.220
who started this project called Open Catalyst,

02:39:11.220 --> 02:39:14.500
and it's an open project collaborative.

02:39:14.620 --> 02:39:16.700
And the idea is to use deep learning

02:39:16.700 --> 02:39:21.700
to help design new chemical compounds or materials

02:39:21.980 --> 02:39:23.820
that would facilitate the separation

02:39:23.820 --> 02:39:25.860
of hydrogen from oxygen.

02:39:25.860 --> 02:39:29.100
If you can efficiently separate oxygen from hydrogen

02:39:29.100 --> 02:39:33.540
with electricity, you solve climate change.

02:39:33.540 --> 02:39:34.500
It's as simple as that,

02:39:34.500 --> 02:39:37.660
because you cover, you know,

02:39:37.660 --> 02:39:39.820
some random desert with solar panels,

02:39:40.820 --> 02:39:43.500
and you have them work all day, produce hydrogen,

02:39:43.500 --> 02:39:45.420
and then you ship the hydrogen wherever it's needed.

02:39:45.420 --> 02:39:46.820
You don't need anything else.

02:39:48.580 --> 02:39:53.460
You know, you have controllable power

02:39:53.460 --> 02:39:55.660
that's, you know, can be transported anywhere.

02:39:55.660 --> 02:39:59.020
So if we have a large scale,

02:39:59.020 --> 02:40:02.140
efficient energy storage technology,

02:40:02.140 --> 02:40:06.660
like producing hydrogen, we solve climate change.

02:40:06.660 --> 02:40:08.540
Here's another way to solve climate change,

02:40:08.540 --> 02:40:10.460
is figuring out how to make fusion work.

02:40:10.460 --> 02:40:11.500
Now, the problem with fusion

02:40:11.500 --> 02:40:13.620
is that you make a super hot plasma,

02:40:13.620 --> 02:40:16.220
and the plasma is unstable, and you can't control it.

02:40:16.220 --> 02:40:17.940
Maybe with deep learning, you can find controllers

02:40:17.940 --> 02:40:19.100
that would stabilize plasma

02:40:19.100 --> 02:40:21.620
and make, you know, practical fusion reactors.

02:40:21.620 --> 02:40:23.060
I mean, that's very speculative,

02:40:23.060 --> 02:40:24.460
but, you know, it's worth trying,

02:40:24.460 --> 02:40:28.260
because, you know, the payoff is huge.

02:40:28.260 --> 02:40:29.860
There's a group at Google working on this,

02:40:29.860 --> 02:40:31.140
led by John Platt.

02:40:31.140 --> 02:40:33.900
So control, convert as many problems

02:40:33.900 --> 02:40:36.780
in science and physics and biology and chemistry

02:40:36.780 --> 02:40:39.740
into a learnable problem

02:40:39.740 --> 02:40:41.580
and see if a machine can learn it.

02:40:41.580 --> 02:40:43.940
Right, I mean, there's properties of, you know,

02:40:43.940 --> 02:40:46.340
complex materials that we don't understand

02:40:46.340 --> 02:40:48.580
from first principle, for example, right?

02:40:48.580 --> 02:40:53.100
So, you know, if we could design new, you know,

02:40:53.100 --> 02:40:56.460
new materials, we could make more efficient batteries.

02:40:56.460 --> 02:40:58.820
You know, we could make maybe faster electronics.

02:40:58.820 --> 02:41:00.060
We could, I mean, there's a lot of things

02:41:00.060 --> 02:41:02.820
we can imagine doing, or, you know,

02:41:02.820 --> 02:41:05.780
lighter materials for cars or airplanes

02:41:05.780 --> 02:41:07.660
or things like that, maybe better fuel cells.

02:41:07.660 --> 02:41:09.540
I mean, there's all kinds of stuff we can imagine.

02:41:09.540 --> 02:41:12.340
If we had good fuel cells, hydrogen fuel cells,

02:41:12.340 --> 02:41:13.660
we could use them to power airplanes

02:41:13.660 --> 02:41:16.140
and, you know, transportation wouldn't be,

02:41:16.140 --> 02:41:20.340
or cars, we wouldn't have a emission problem,

02:41:20.340 --> 02:41:24.620
CO2 emission problems for air transportation anymore.

02:41:24.620 --> 02:41:26.540
So there's a lot of those things,

02:41:26.540 --> 02:41:30.180
I think, where AI, you know, can be used.

02:41:30.180 --> 02:41:31.580
And this is not even talking about

02:41:31.580 --> 02:41:33.580
all the sort of medicine, biology,

02:41:33.580 --> 02:41:35.700
and everything like that, right?

02:41:35.700 --> 02:41:38.140
You know, like protein folding, you know,

02:41:38.140 --> 02:41:40.580
figuring out, like how can you design your proteins

02:41:40.580 --> 02:41:42.860
that it sticks to another protein at a particular site,

02:41:42.860 --> 02:41:45.220
because that's how you design drugs in the end.

02:41:46.300 --> 02:41:47.980
So, you know, deep learning would be useful, all of this.

02:41:47.980 --> 02:41:49.300
And those are kind of, you know,

02:41:49.300 --> 02:41:51.180
would be sort of enormous progress

02:41:51.180 --> 02:41:53.420
if we could use it for that.

02:41:53.420 --> 02:41:54.340
Here's an example.

02:41:54.340 --> 02:41:58.300
If you take, this is like from recent material physics,

02:41:58.300 --> 02:42:02.220
you take a monoatomic layer of graphene, right?

02:42:02.220 --> 02:42:04.940
So it's just carbon on an hexagonal mesh,

02:42:04.940 --> 02:42:09.140
and you make this single atom thick.

02:42:09.140 --> 02:42:10.380
You put another one on top,

02:42:10.380 --> 02:42:13.100
you twist them by some magic number of degrees,

02:42:13.100 --> 02:42:14.820
three degrees or something.

02:42:14.820 --> 02:42:16.780
It becomes superconductor.

02:42:16.780 --> 02:42:18.100
Nobody has any idea why.

02:42:18.100 --> 02:42:22.460
I want to know how that was discovered,

02:42:22.460 --> 02:42:23.900
but that's the kind of thing that machine learning

02:42:23.900 --> 02:42:25.820
can actually discover, these kinds of things.

02:42:25.820 --> 02:42:29.420
Maybe not, but there is a hint perhaps that

02:42:29.420 --> 02:42:31.740
with machine learning, we would train a system

02:42:31.740 --> 02:42:34.860
to basically be a phenomenological model

02:42:35.780 --> 02:42:37.260
of some complex emergent phenomenon,

02:42:37.260 --> 02:42:40.420
which, you know, superconductivity is one of those,

02:42:42.420 --> 02:42:44.780
where, you know, this collective phenomenon

02:42:44.780 --> 02:42:46.940
is too difficult to describe from first principles

02:42:46.940 --> 02:42:48.820
with the current, you know,

02:42:48.820 --> 02:42:51.940
the usual sort of reductionist type method.

02:42:51.940 --> 02:42:54.980
But we could have deep learning systems

02:42:54.980 --> 02:42:57.700
that predict the properties of a system

02:42:57.700 --> 02:42:59.220
from a description of it

02:42:59.220 --> 02:43:02.700
after being trained with sufficiently many samples.

02:43:03.660 --> 02:43:06.660
So this guy Pascal Foua at EPFL,

02:43:06.660 --> 02:43:09.780
he has a startup company that,

02:43:09.780 --> 02:43:13.900
where he basically trained a convolutional net essentially

02:43:13.900 --> 02:43:17.980
to predict the aerodynamic properties of solids.

02:43:17.980 --> 02:43:19.620
And you can generate as much data as you want

02:43:19.620 --> 02:43:21.900
by just running computational free dynamics, right?

02:43:21.900 --> 02:43:26.900
So you give like a wing airfoil

02:43:27.780 --> 02:43:29.780
or something shape of some kind,

02:43:29.780 --> 02:43:31.380
and you run computational free dynamics,

02:43:31.380 --> 02:43:35.140
you get as a result, the drag and, you know,

02:43:36.140 --> 02:43:37.500
lift and all that stuff, right?

02:43:37.500 --> 02:43:40.100
And you can generate lots of data,

02:43:40.100 --> 02:43:41.820
train a neural net to make those predictions.

02:43:41.820 --> 02:43:44.140
And now what you have is a differentiable model

02:43:44.140 --> 02:43:46.980
of, let's say, drag and lift

02:43:46.980 --> 02:43:48.700
as a function of the shape of that solid.

02:43:48.700 --> 02:43:49.940
And so you can do by gradient descent,

02:43:49.940 --> 02:43:51.500
you can optimize the shape

02:43:51.500 --> 02:43:53.300
so you get the properties you want.

02:43:54.900 --> 02:43:56.060
Yeah, that's incredible.

02:43:56.060 --> 02:43:56.900
That's incredible.

02:43:56.900 --> 02:43:58.300
And on top of all that,

02:43:58.300 --> 02:44:01.460
probably you should read a little bit of literature

02:44:01.460 --> 02:44:03.620
and a little bit of history

02:44:03.620 --> 02:44:06.620
for inspiration and for wisdom.

02:44:06.620 --> 02:44:08.780
Because after all, all of these technologies

02:44:08.780 --> 02:44:10.300
will have to work in the human world.

02:44:10.300 --> 02:44:11.140
Yes.

02:44:11.140 --> 02:44:12.660
And the human world is complicated.

02:44:12.660 --> 02:44:15.060
It is, certainly.

02:44:15.060 --> 02:44:18.420
Yeah, and this is an amazing conversation.

02:44:18.420 --> 02:44:20.420
I'm really honored that you would talk with me today.

02:44:20.420 --> 02:44:21.860
Thank you for all the amazing work

02:44:21.860 --> 02:44:23.820
you're doing at FAIR, at META.

02:44:23.820 --> 02:44:26.260
And thank you for being so passionate

02:44:26.260 --> 02:44:27.380
after all these years

02:44:27.380 --> 02:44:28.780
about everything that's going on.

02:44:28.780 --> 02:44:31.620
You're a beacon of hope for the machine learning community.

02:44:31.620 --> 02:44:32.700
And thank you so much

02:44:32.700 --> 02:44:34.460
for spending your valuable time with me today.

02:44:34.460 --> 02:44:35.300
That was awesome.

02:44:35.300 --> 02:44:36.260
Thanks for having me on.

02:44:36.260 --> 02:44:37.820
That was a pleasure.

02:44:38.820 --> 02:44:41.460
Thanks for listening to this conversation with Yann LeCun.

02:44:41.460 --> 02:44:42.820
To support this podcast,

02:44:42.820 --> 02:44:45.740
please check out our sponsors in the description.

02:44:45.740 --> 02:44:47.820
And now let me leave you with some words

02:44:47.820 --> 02:44:49.620
from Isaac Asimov.

02:44:50.620 --> 02:44:53.780
Your assumptions are your windows on the world.

02:44:53.780 --> 02:44:56.020
Scrub them off every once in a while,

02:44:56.060 --> 02:44:57.700
or the light won't come in.

02:44:58.820 --> 02:44:59.820
Thank you for listening,

02:44:59.820 --> 02:45:01.740
and hope to see you next time.


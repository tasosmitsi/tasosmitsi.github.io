WEBVTT

00:00.000 --> 00:04.100
A lot of people were saying like, oh, this whole idea of game theory, it's just nonsense.

00:04.100 --> 00:07.080
And if you really want to make money, you got to like look into the other person's eyes

00:07.080 --> 00:10.940
and read their soul and figure out what cards they have.

00:10.940 --> 00:15.760
But what happened was where we played our bot against four top heads up, no limit Holdem

00:15.760 --> 00:19.460
poker players, and the bot wasn't trying to adapt to them.

00:19.460 --> 00:20.780
It wasn't trying to exploit them.

00:20.780 --> 00:22.860
It wasn't trying to do these mind games.

00:22.860 --> 00:28.700
It was just trying to approximate the Nash equilibrium and it crushed them.

00:28.700 --> 00:33.700
The following is a conversation with No Brown, research scientist at FAIR, Facebook AI research

00:33.700 --> 00:35.720
group at Meta AI.

00:35.720 --> 00:41.540
He co-created the first AI system that achieved superhuman level performance in no limit Texas

00:41.540 --> 00:44.660
Holdem, both heads up and multiplayer.

00:44.660 --> 00:51.300
And now recently, he co-created an AI system that can strategically out negotiate humans

00:51.300 --> 00:56.520
using natural language in a popular board game called Diplomacy, which is a war game

00:56.520 --> 00:59.840
that emphasizes negotiation.

00:59.840 --> 01:01.640
This is the Lex Fridman podcast.

01:01.640 --> 01:05.000
To support it, please check out our sponsors in the description.

01:05.000 --> 01:09.560
And now, dear friends, here's Noem Brown.

01:09.560 --> 01:12.880
You've been a lead on three amazing AI projects.

01:12.880 --> 01:18.580
So we've got Labratas that solved or at least achieved human level performance on no limit

01:18.580 --> 01:22.260
Texas Holdem poker with two players heads up.

01:22.260 --> 01:28.460
We got Pluribus that solved no limit Texas Holdem poker with six players.

01:28.460 --> 01:33.980
And just now you have Cicero, these are all names of systems that solved or achieved human

01:33.980 --> 01:40.600
level performance on the game of Diplomacy, which for people who don't know is a popular

01:40.600 --> 01:42.240
strategy board game.

01:42.240 --> 01:51.140
It was loved by JFK, John F. Kennedy and Henry Kissinger and many other big famous people

01:51.280 --> 01:52.280
in the decade since.

01:52.280 --> 01:55.040
So let's talk about poker and diplomacy today.

01:55.040 --> 02:00.000
First poker, what is the game of no limit Texas Holdem and how is it different from

02:00.000 --> 02:01.000
chess?

02:01.000 --> 02:05.220
Well, no limit Texas Holdem poker is the most popular variant of poker in the world.

02:05.220 --> 02:08.480
So you go to a casino, you play sit down at the poker table.

02:08.480 --> 02:11.180
The game that you're playing is no limit Texas Holdem.

02:11.180 --> 02:14.920
If you watch movies about poker like Casino Royale or Rounders, the game that they're

02:14.920 --> 02:17.520
playing is no limit Texas Holdem poker.

02:17.520 --> 02:22.860
Now it's very different from limit Holdem in that you can bet any amount of chips that

02:22.860 --> 02:23.860
you want.

02:23.860 --> 02:26.140
And so the stakes escalate really quickly.

02:26.140 --> 02:28.800
You start out with like one or two dollars in the pot.

02:28.800 --> 02:32.180
And then by the end of the hand, you've got like a thousand dollars in there, maybe.

02:32.180 --> 02:36.580
So the option to increase the number very aggressively and very quickly is always there.

02:36.580 --> 02:37.580
Right.

02:37.580 --> 02:40.940
The no limit aspect is there's no limits to how much you can bet.

02:40.940 --> 02:43.800
In limit Holdem, there's like two dollars in the pot.

02:43.800 --> 02:45.620
You can only bet like two dollars.

02:45.720 --> 02:50.560
But if you got $10,000 in front of you, you're always welcome to put $10,000 into the pot.

02:50.560 --> 02:54.600
So I've got a chance to hang out with Phil Hellmuth, who plays all these different variants

02:54.600 --> 02:56.120
of poker.

02:56.120 --> 03:01.920
And correct me if I'm wrong, but it seems like no limit rewards crazy versus the other

03:01.920 --> 03:08.760
ones rewards more kind of calculated strategy or no, because you're sort of looking from

03:08.760 --> 03:14.920
an analytic perspective, is strategy also rewarded in no limit Texas Holdem?

03:14.920 --> 03:17.340
I think both variants reward strategy.

03:17.340 --> 03:23.900
But I think what's different about no limit Holdem is it's much easier to get jumpy.

03:23.900 --> 03:28.340
You go in there thinking you're going to lose, you're going to play for like a hundred dollars

03:28.340 --> 03:29.340
or something.

03:29.340 --> 03:31.020
And suddenly there's like a thousand dollars in the pot.

03:31.020 --> 03:32.340
A lot of people can't handle that.

03:32.340 --> 03:33.980
Can you define jumpy?

03:33.980 --> 03:38.140
When you're playing poker, you always want to choose the action that's going to maximize

03:38.140 --> 03:39.140
your expected value.

03:39.140 --> 03:41.340
It's kind of like with investing, right?

03:41.340 --> 03:47.280
If you're ever in a situation where the amount of money that's at stake is going to have

03:47.280 --> 03:51.600
a material impact on your life, then you're going to play in a more risk averse style.

03:51.600 --> 03:55.640
If somebody makes a huge bet, you're going to, if you're playing no limit Holdem and

03:55.640 --> 03:59.800
somebody makes a huge bet, there might come a point where you're like, this is too much

03:59.800 --> 04:00.920
money for me to handle.

04:00.920 --> 04:03.520
I can't risk this amount.

04:03.520 --> 04:05.640
And that's what throws a lot of people off.

04:05.640 --> 04:10.280
So that's the big difference, I think, between no limit and limit.

04:10.280 --> 04:14.340
What about on the action side when you're actually making that big bet?

04:14.340 --> 04:15.740
That's what I mean by crazy.

04:15.740 --> 04:23.340
I was trying to refer to the technical term of crazy, meaning use the big jump in the

04:23.340 --> 04:30.300
bet to completely throw off the other person in terms of their ability to reason optimally.

04:30.300 --> 04:31.300
I think that's right.

04:31.300 --> 04:37.100
I think one of the key strategies in poker is to put the other person into an uncomfortable

04:37.100 --> 04:38.140
position.

04:38.140 --> 04:40.980
And if you're doing that, then you're playing poker well.

04:40.980 --> 04:43.920
And there's a lot of opportunities to do that in no limit Holdem.

04:43.920 --> 04:50.080
You can have like $50 in there, you throw in a $1,000 bet and that's sometimes, if you

04:50.080 --> 04:53.280
do it right, it puts the other person in a really tough spot.

04:53.280 --> 04:56.460
Now it's also possible that you make huge mistakes that way.

04:56.460 --> 04:59.080
And so it's really easy to lose a lot of money in no limit Holdem if you don't know what

04:59.080 --> 05:00.080
you're doing.

05:00.080 --> 05:02.620
But there's a lot of upside potential too.

05:02.620 --> 05:06.540
So when you build systems, AI systems that play these games, we'll talk about poker,

05:06.540 --> 05:08.700
we'll talk about diplomacy.

05:08.700 --> 05:14.100
Are you drawn in in part by the beauty of the game itself, AI aside?

05:14.100 --> 05:20.180
Or is it to you primarily a fascinating problem set for the AI to solve?

05:20.180 --> 05:21.940
I'm drawn in by the beauty of the game.

05:21.940 --> 05:28.020
When I started playing poker when I was in high school, and the idea to me that there

05:28.020 --> 05:34.380
is an objectively correct way of playing poker, and if you could figure out what that is,

05:34.380 --> 05:41.260
when you're making unlimited money basically, that's a really fascinating concept to me.

05:41.260 --> 05:46.340
And so I was fascinated by the strategy of poker even when I was 16 years old.

05:46.340 --> 05:48.860
It wasn't until much later that I actually worked on poker AIs.

05:48.860 --> 05:54.660
So there was a sense that you can solve poker, in the way you can solve chess, for example,

05:54.660 --> 05:55.660
or checkers.

05:55.660 --> 05:57.580
I believe checkers got solved, right?

05:57.580 --> 05:59.780
Yeah, checkers are completely solved.

05:59.780 --> 06:00.780
Optimal strategy.

06:00.780 --> 06:01.780
It's impossible to beat the AI.

06:02.180 --> 06:05.820
And so in that same way, you could technically solve chess.

06:05.820 --> 06:07.620
You could solve chess, you could solve poker.

06:07.620 --> 06:08.860
You could solve poker.

06:08.860 --> 06:12.580
So this gets into the concept of a Nash Equilibrium.

06:12.580 --> 06:14.980
What is a Nash Equilibrium?

06:14.980 --> 06:21.660
So in any finite two-player zero-sum game, there is an optimal strategy that if you play

06:21.660 --> 06:26.660
it, you are guaranteed to not lose an expectation no matter what your opponent does.

06:26.660 --> 06:31.060
And this is kind of a radical concept to a lot of people, but it's true in chess, it's

06:31.060 --> 06:35.340
true in poker, it's true in any finite two-player zero-sum game.

06:35.340 --> 06:39.020
And to give some intuition for this, you can think of rock, paper, scissors.

06:39.020 --> 06:43.180
In rock, paper, scissors, if you randomly choose between throwing rock, paper, and scissors

06:43.180 --> 06:47.560
with equal probability, then no matter what your opponent does, you are not going to lose

06:47.560 --> 06:48.560
an expectation.

06:48.560 --> 06:51.180
You're not going to lose an expectation in the long run.

06:51.180 --> 06:53.100
Now the same is true for poker.

06:53.100 --> 06:58.340
There exists some strategy, some really complicated strategy, that if you play that, you are guaranteed

06:58.340 --> 07:00.420
to not lose money in the long run.

07:00.420 --> 07:02.220
And I should say, this is for two-player poker.

07:02.220 --> 07:03.220
Six-player poker is a different story.

07:03.220 --> 07:05.820
Yeah, it's a beautiful giant mess.

07:05.820 --> 07:10.780
When you say in expectation, you're guaranteed not to lose in expectation.

07:10.780 --> 07:12.820
What does in expectation mean?

07:12.820 --> 07:14.300
Poker is a very high-variance game.

07:14.300 --> 07:16.620
So you're going to have hands where you win, you're going to have hands where you lose.

07:16.620 --> 07:19.560
Even if you're playing the perfect strategy, you can't guarantee you're going to win every

07:19.560 --> 07:20.780
single hand.

07:20.780 --> 07:25.780
But if you play for long enough, then you are guaranteed to at least break even, and

07:25.780 --> 07:27.620
in practice, probably win.

07:27.620 --> 07:29.100
So that's an expectation.

07:29.100 --> 07:34.780
The size of your stack, generally speaking, now that doesn't include anything about the

07:34.780 --> 07:39.540
fact that you can go broke, it doesn't include any of those kinds of normal real-world limitations.

07:39.540 --> 07:42.780
You're talking in a theoretical world.

07:42.780 --> 07:44.780
What about the zero-sum aspect?

07:44.780 --> 07:46.100
How big of a constraint is that?

07:46.100 --> 07:48.540
How big of a constraint is finite?

07:48.540 --> 07:51.860
So finite's not a huge constraint.

07:51.860 --> 07:54.580
So I mean, most games that you play are finite in size.

07:54.580 --> 07:58.620
It's also true, actually, that there exists this perfect strategy in many infinite games

07:58.620 --> 07:59.620
as well.

07:59.620 --> 08:02.420
Technically, the game has to be compact.

08:02.420 --> 08:05.980
There are some edge cases where you don't have a Nash equilibrium in a two-player zero-sum

08:05.980 --> 08:06.980
game.

08:06.980 --> 08:09.940
So you can think of a game where you're like, if we're playing a game where whoever names

08:09.940 --> 08:13.700
the bigger number is the winner, there's no Nash equilibrium to that game.

08:13.700 --> 08:14.700
17.

08:14.700 --> 08:15.700
Yeah, exactly.

08:15.700 --> 08:16.700
18.

08:16.700 --> 08:17.700
You beat.

08:17.700 --> 08:18.700
You win again.

08:18.700 --> 08:19.700
You're good at this.

08:19.700 --> 08:20.700
I've played a lot of games.

08:20.700 --> 08:21.700
Okay.

08:21.700 --> 08:24.700
And then the zero-sum aspect.

08:24.700 --> 08:26.200
Zero-sum aspect.

08:26.200 --> 08:30.200
So there exists a Nash equilibrium in non-two-player zero-sum games as well.

08:30.200 --> 08:33.800
And by the way, just to clarify what I mean by two-player zero-sum, I mean there's two

08:33.800 --> 08:37.020
players and whatever one player wins, the other player loses.

08:37.020 --> 08:41.260
So if we're playing poker and I win $50, that means that you're losing $50.

08:41.260 --> 08:47.040
Now outside of two-player zero-sum games, there still exists Nash equilibria, but they're

08:47.040 --> 08:51.740
not as meaningful because you can think of a game like Risk.

08:51.740 --> 08:56.080
If everybody else on the board decides to team up against you and take you out, there's

08:56.080 --> 08:59.160
no perfect strategy you can play that's going to guarantee that you win there.

08:59.160 --> 09:00.700
There's just nothing you can do.

09:00.700 --> 09:05.320
So outside of two-player zero-sum games, there's no guarantee that you're going to win by playing

09:05.320 --> 09:06.320
a Nash equilibrium.

09:06.320 --> 09:07.320
19.

09:07.320 --> 09:13.180
Have you ever tried to model in the other aspects of the game, which is like the pleasure

09:13.180 --> 09:15.560
you draw from playing the game?

09:15.560 --> 09:23.640
And then if you're a professional poker player, if you're exciting, even if you lose the money

09:23.720 --> 09:27.440
you would get from the attention you get to the sponsor and all that kind of stuff, is

09:27.440 --> 09:32.640
that, that would be a fun thing to model, to model in, or is that, make it sort of super

09:32.640 --> 09:36.920
complex to include the human factor in this, in its full complexity?

09:36.920 --> 09:38.620
I think you bring up a couple of good points there.

09:38.620 --> 09:43.120
So I think a lot of professional poker players, I mean, they get a huge amount of money, not

09:43.120 --> 09:47.760
from actually playing poker, but from the sponsorships and having a personality that

09:47.760 --> 09:50.200
people want to tune in and watch.

09:50.200 --> 09:53.360
That's a big, that's a big way to make a name for yourself in poker.

09:53.360 --> 09:57.720
I just wonder from an AI perspective, if you create, and we'll talk about this more, maybe

09:57.720 --> 10:04.160
an AI system that also talks trash and all that kind of stuff, that, that becomes part

10:04.160 --> 10:05.320
of the function to maximize.

10:05.320 --> 10:08.800
So it's not just optimal poker play.

10:08.800 --> 10:10.360
Maybe sometimes you want to be chaotic.

10:10.360 --> 10:15.560
Maybe sometimes you want to be suboptimal and you lose the chaos.

10:15.560 --> 10:20.960
And maybe sometimes you want to be overly aggressive because people, the audience loves

10:20.960 --> 10:21.960
that.

10:21.960 --> 10:22.960
Yeah.

10:22.960 --> 10:25.480
I think, I think what you're getting at here is that there's a difference between making

10:25.480 --> 10:28.160
an AI that wins a game and an AI that's fun to play with.

10:28.160 --> 10:29.160
Yeah.

10:29.160 --> 10:30.160
Yeah.

10:30.160 --> 10:31.160
Or fun to watch.

10:31.160 --> 10:32.160
So those are all different things.

10:32.160 --> 10:33.160
Fun to play with and fun to watch.

10:33.160 --> 10:34.160
Yeah.

10:34.160 --> 10:38.600
And I think, you know, I've, I've heard talks from like game designers and they say like,

10:38.600 --> 10:42.960
you know, people that work on AI for actual recreational games that people play and they

10:42.960 --> 10:46.400
say, yeah, there's a big difference between trying to make an AI that actually wins.

10:46.400 --> 10:52.600
And you know, you look at a game like civilization, the way that the AIs play is not optimal for

10:52.600 --> 10:53.920
trying to win.

10:53.920 --> 10:54.920
They're playing a different game.

10:54.920 --> 10:55.920
They're trying to have personalities.

10:55.920 --> 10:59.240
They're trying to be fun and engaging.

10:59.240 --> 11:00.840
And that makes for a better game.

11:00.840 --> 11:01.840
Yeah.

11:01.840 --> 11:02.840
And we also talk about NPCs.

11:02.840 --> 11:06.120
I just talked to Todd Howard, who is the, the creator of Fallout and the Elder Scrolls

11:06.120 --> 11:12.900
series and Starfield, the new game coming out and the creator, what I think is the greatest

11:12.900 --> 11:17.280
game of all time, which is Skyrim and the NPCs there, the AI that governs that whole

11:17.280 --> 11:21.000
game is very interesting, but the NPCs also are super interesting.

11:21.400 --> 11:28.520
And considering what language models might do to NPCs in an open world RPG role playing

11:28.520 --> 11:30.800
game is super exciting.

11:30.800 --> 11:34.440
Yeah, honestly, I'm, I think this is like one of the first applications where we're

11:34.440 --> 11:39.800
going to see like real consumer interaction with large language models.

11:39.800 --> 11:42.760
I guess Elder Scrolls 6 is in development now.

11:42.760 --> 11:46.800
They're probably like pretty close to finishing it, but I would not be surprised at all if

11:46.800 --> 11:50.160
Elder Scrolls 7 was using large language models for their NPCs.

11:50.320 --> 11:51.320
They're not there.

11:51.320 --> 11:52.320
I mean, I'm not saying anything.

11:52.320 --> 11:53.320
I'm not saying anything.

11:53.320 --> 11:54.320
Okay.

11:54.320 --> 11:55.960
This is me speculating, not you.

11:55.960 --> 11:59.280
No, but there's, they're just releasing the Starfield game.

11:59.280 --> 12:00.600
They do one game at a time.

12:00.600 --> 12:01.600
Yeah.

12:01.600 --> 12:07.160
And so whatever it is, whenever the date is, I don't know what the date is, calm down.

12:07.160 --> 12:11.200
But it would be, I don't know, like 2024, 25, 26.

12:11.200 --> 12:14.240
So it's actually very possible they would include language models.

12:14.240 --> 12:21.240
I was listening to this talk by a gaming executive when I was in grad school.

12:21.240 --> 12:25.520
And one of the questions that a person in the audience asked is, why are all these games

12:25.520 --> 12:28.200
so focused on fighting and killing?

12:28.200 --> 12:32.880
And the person responded that it's just so much harder to make an AI that can talk with

12:32.880 --> 12:36.960
you and cooperate with you than it is to make an AI that can fight you.

12:36.960 --> 12:40.600
And I think once this technology develops further and you can have a, you can reach

12:40.640 --> 12:44.240
a point where like not every single line of dialogue has to be scripted.

12:44.240 --> 12:49.200
It unlocks a lot of potential for new kinds of games, like much more like positive interactions

12:49.200 --> 12:50.200
that are not so focused on fighting.

12:50.200 --> 12:51.200
And I'm really looking forward to that.

12:51.200 --> 12:52.200
It might not be positive.

12:52.200 --> 12:53.200
It might be just drama.

12:53.200 --> 12:58.000
So you'll be in like a Call of Duty game and instead of doing the shooting, you'll just

12:58.000 --> 13:03.800
be hanging out and like arguing with an AI about like, like passive aggressive.

13:03.800 --> 13:05.160
And then you won't be able to sleep that night.

13:05.240 --> 13:12.720
You have to return and continue the argument that you're emotionally hurt.

13:12.720 --> 13:15.680
I mean, yeah, I think that's actually an exciting world.

13:15.680 --> 13:19.760
Whatever is the drama, the chaos that we love, the push and pull of human connection, I think

13:19.760 --> 13:22.360
it's possible to do that in the video game world.

13:22.360 --> 13:26.400
And I think you could be messier and make more mistakes in a video game world, which

13:26.400 --> 13:29.160
is why it would be a nice place.

13:29.160 --> 13:34.880
And also it doesn't have a deep of a, as deep of a real psychological impact because inside

13:34.880 --> 13:39.440
video games, it's kind of understood that you're in a, not a real world.

13:39.440 --> 13:44.000
So whatever crazy stuff AI does, we have some flexibility to play.

13:44.000 --> 13:46.360
Just like with the game of diplomacy, it's a game.

13:46.360 --> 13:48.840
This is not real geopolitics, not real war.

13:48.840 --> 13:49.840
It's a, it's a game.

13:49.840 --> 13:53.840
So you could, you can have a little bit of fun, um, a little bit of chaos.

13:53.840 --> 13:54.840
Okay.

13:54.840 --> 13:55.840
Back to Nash Equilibrium.

13:55.840 --> 13:58.160
Uh, how do we find the Nash Equilibrium?

13:58.160 --> 13:59.520
All right.

13:59.520 --> 14:01.820
So there's different ways to find a Nash Equilibrium.

14:01.820 --> 14:06.860
So, um, the way that we do it is with this process called self play.

14:06.860 --> 14:12.180
Um, basically we have this algorithm that starts by playing totally randomly and it

14:12.180 --> 14:15.740
learns how to play the game by playing against itself.

14:15.740 --> 14:20.700
So, um, it will start playing the game totally randomly and then it, you know, if it's playing

14:20.700 --> 14:26.340
poker, it'll eventually like get to the end of the end of the game and make $50.

14:26.340 --> 14:30.840
And then it will like review all of the decisions that it made along the way and say, what would

14:30.840 --> 14:33.920
have happened if I had chosen this other action instead?

14:33.920 --> 14:37.600
You know, if I had raised here instead of called, um, what would the other player have

14:37.600 --> 14:38.600
done?

14:38.600 --> 14:41.760
And because it's playing against a copy of itself, it's able to do that counterfactual

14:41.760 --> 14:42.760
reasoning.

14:42.760 --> 14:46.520
So they can say, okay, well, if I took this action and the other person takes this action

14:46.520 --> 14:51.540
and then I take this action and eventually I make $150 instead of 50.

14:51.540 --> 14:56.520
And so it updates the regret value for that action.

14:56.520 --> 14:59.320
Regret is basically like, how much does it regret having not played that action in the

14:59.320 --> 15:00.880
past?

15:00.880 --> 15:04.960
And when it encounters that same situation again, it's going to pick actions that have

15:04.960 --> 15:07.800
higher regret with higher probability.

15:07.800 --> 15:11.000
Now it'll just keep simulating the games this way.

15:11.000 --> 15:14.280
It'll keep, um, you know, accumulating regrets for different situations.

15:14.280 --> 15:19.280
Um, and in the long run, if you pick actions that have higher regret with higher probability

15:19.280 --> 15:25.000
in the correct way, it's proven to converge to a Nash equilibrium.

15:25.000 --> 15:28.660
Even for super complex games, even for imperfect information games.

15:28.660 --> 15:29.660
It's true for all games.

15:29.660 --> 15:30.920
It's true for, it's true for chess.

15:30.920 --> 15:31.920
It's true for poker.

15:31.920 --> 15:33.600
It's particularly useful for poker.

15:33.600 --> 15:36.480
So this is the, the method of counterfactual regret minimization.

15:36.480 --> 15:38.000
This is counterfactual regret minimization.

15:38.000 --> 15:39.360
That doesn't have to do with self play.

15:39.360 --> 15:44.640
It has to do with just any, any, if you follow this kind of process, self play or not, you

15:44.640 --> 15:48.360
will be able to arrive at an optimal set of actions.

15:48.360 --> 15:51.560
So this counterfactual regret minimization is a kind of self play.

15:51.560 --> 15:55.680
It's a principled kind of self play that's proven to converge to Nash equilibria, even

15:55.680 --> 15:57.320
in imperfect information games.

15:57.320 --> 16:00.760
Now you can have other forms of self play and people use other forms of self play for

16:00.760 --> 16:04.880
perfect information games, um, where you have more flexibility.

16:04.880 --> 16:09.320
The algorithm doesn't have to be as theoretically sound, um, in order to converge to that class

16:09.320 --> 16:11.920
of games because there's, uh, it's a simpler setting.

16:11.920 --> 16:12.920
Sure.

16:12.920 --> 16:18.180
So I kind of, in my brain, the word self play has mapped in neural networks, but we're speaking

16:18.180 --> 16:20.120
something bigger than just neural networks.

16:20.120 --> 16:22.600
It could be anything.

16:22.600 --> 16:26.200
The self playing mechanism is just the mechanism of a system playing itself.

16:26.200 --> 16:27.200
Exactly.

16:27.200 --> 16:28.200
Yeah.

16:28.200 --> 16:29.200
Self play is not tied specifically to neural nets.

16:29.200 --> 16:32.120
It's, it's a kind of reinforcement learning basically.

16:32.120 --> 16:36.440
And I would also say this process of like trying to reason, Oh, what would the value

16:36.440 --> 16:39.340
have been if I had taken this other action instead?

16:39.340 --> 16:42.600
This is very similar to how humans learn to play a game like poker, right?

16:42.600 --> 16:46.280
Like you probably played poker before and with your friends, you probably ask like,

16:46.280 --> 16:48.480
Oh, what do you have called me if I raise there?

16:49.080 --> 16:52.480
You know, and that's, that's a person trying to do the same kind of like learning from

16:52.480 --> 16:54.680
a counterfactual that the AI is doing.

16:54.680 --> 16:55.680
Okay.

16:55.680 --> 16:59.520
And if you do that at scale, you're going to be able to learn an optimal policy.

16:59.520 --> 17:00.520
Yeah.

17:00.520 --> 17:05.000
Now where the neural nets come in, I said like, okay, if it's in that situation again,

17:05.000 --> 17:07.560
then it will choose the action that has, has high regret.

17:07.560 --> 17:10.440
Now the problem is that poker is such a huge game.

17:10.440 --> 17:13.840
You know, I think no limit Texas Holden, the version that we were playing has 10 to the

17:13.840 --> 17:18.200
161 different decision points, which is more than the number of atoms in the universe squared.

17:18.200 --> 17:19.200
That's heads up.

17:19.200 --> 17:20.200
That's heads up.

17:20.200 --> 17:21.200
Yeah.

17:21.200 --> 17:22.200
10 to the 161, you said?

17:22.200 --> 17:23.200
Yeah.

17:23.200 --> 17:24.480
I mean, it depends on the number of chips that you have, the stacks and everything, but like

17:24.480 --> 17:28.520
the version that we were playing was 10 to the 161, which I assume would be a somewhat

17:28.520 --> 17:34.480
simplified version anyway, because the, I bet there's some like step function you had

17:34.480 --> 17:35.480
for like bets.

17:35.480 --> 17:36.480
Oh, no, no, no.

17:36.480 --> 17:38.760
That's, that's, I'm saying like we played the, the full game.

17:38.760 --> 17:39.920
You can bet whatever amount you want.

17:39.920 --> 17:43.960
No, the bot maybe was constrained in like what it considered for bed sizes, but the,

17:43.960 --> 17:45.760
the person on the other side could bet whatever they wanted.

17:45.760 --> 17:46.760
Yeah.

17:46.960 --> 17:49.360
161 plus or minus 10 doesn't matter.

17:51.800 --> 17:56.160
And so the, the way neural nets help out here is, you know, you don't have to run into the

17:56.160 --> 17:58.240
same exact situation because that's never going to happen again.

17:58.280 --> 18:01.960
The odds of you running into the same exact situation are pretty slim, but if you run

18:01.960 --> 18:05.680
into a similar situation, then you can generalize from other states that you've been in that

18:05.680 --> 18:06.760
kind of look like that one.

18:07.040 --> 18:10.360
And you can say like, well, these other situations, I had high regret for this action.

18:10.360 --> 18:12.480
And so maybe I should play that action here as well.

18:12.640 --> 18:14.440
Which is the more complex game?

18:14.920 --> 18:18.400
Chess or poker or go or poker?

18:18.440 --> 18:18.760
Do you know?

18:18.760 --> 18:20.920
That is, that is a controversial question.

18:21.000 --> 18:21.320
Okay.

18:21.400 --> 18:22.240
Um, I'm going to,

18:22.240 --> 18:23.960
I was like somebody screaming on Reddit right now.

18:23.960 --> 18:25.600
It depends on which subreddit you're on.

18:25.640 --> 18:26.920
Is it chess or is it poker?

18:27.120 --> 18:29.280
I'm sure like David Silver is going to get really angry at me.

18:29.520 --> 18:31.680
Uh, I'll, I'll say, I'm going to say poker actually.

18:31.680 --> 18:35.080
And I think for a couple of reasons, um, they're not here to defend themselves.

18:36.080 --> 18:39.360
So first of all, you have the imperfect information aspect.

18:39.360 --> 18:44.280
And so it's, um, it, we can go into that, but like, once you

18:44.280 --> 18:48.520
introduce imperfect information, uh, things get much more complicated.

18:48.960 --> 18:54.280
So we should say, maybe you can describe what is seen to the players.

18:54.280 --> 18:57.600
What is not seen, uh, in the game of Texas Hold'em.

18:57.840 --> 18:58.040
Yeah.

18:58.040 --> 19:02.080
So Texas Hold'em, you get two cards facedown that only you see.

19:02.520 --> 19:04.440
Um, and so that's the hidden information in the game.

19:04.440 --> 19:07.240
The other players also all get two cards facedown that only they see.

19:07.880 --> 19:10.760
Um, and so you have to kind of, as you're playing reason about like,

19:10.760 --> 19:12.200
okay, what do they think I have?

19:12.520 --> 19:13.360
What do they have?

19:13.600 --> 19:14.440
What do they think?

19:14.440 --> 19:16.160
I think they have that kind of stuff.

19:16.160 --> 19:20.000
And, um, that's, that's kind of where bluffing comes into play, right?

19:20.000 --> 19:24.080
Because the fact that you can bluff, the fact that you can bet with a bad

19:24.080 --> 19:27.160
hand and still win is because they don't know what your cards are.

19:28.160 --> 19:30.760
And that's the, that's the key difference between a perfect information

19:30.760 --> 19:34.640
game like poker, uh, sorry, like chess and go, um, and imperfect

19:34.640 --> 19:35.680
information games like poker.

19:36.000 --> 19:38.240
This is what trash talk looks like.

19:39.000 --> 19:43.080
The implied statement is the game I solved is much tougher.

19:43.520 --> 19:44.480
Um, but yeah.

19:44.520 --> 19:47.600
So, uh, when you're playing, I'm just going to do random questions here.

19:47.600 --> 19:54.080
So when you're playing your opponent under imperfect information, is there

19:54.080 --> 19:57.960
some degree to which you're trying to estimate the range of hands that they have?

19:59.360 --> 20:00.760
Or is that not part of the algorithm?

20:00.760 --> 20:05.120
So what are the different approaches to the imperfect information game?

20:05.360 --> 20:08.560
So the key thing to understand about why imperfect information makes things

20:08.560 --> 20:13.280
difficult is that you have to worry not just about which actions to play, but

20:13.280 --> 20:15.080
the probability that you're going to play those actions.

20:15.800 --> 20:20.120
So you think about, um, rock paper, scissors, for example, rock paper,

20:20.120 --> 20:21.640
scissors is an imperfect information game.

20:22.320 --> 20:24.840
Um, because you don't know what I'm about to throw.

20:24.880 --> 20:26.360
I do, but yeah, usually not.

20:26.360 --> 20:26.880
Yeah.

20:27.080 --> 20:29.760
And so you can't just say like, Oh, I'm just going to throw a rock every single

20:29.760 --> 20:33.440
time because the other person's going to figure that out and notice a pattern

20:33.440 --> 20:34.640
and then suddenly you're going to start losing.

20:35.160 --> 20:37.640
And so you don't just have to figure out like which action to play.

20:37.640 --> 20:39.160
You have to figure out the probability that you play it.

20:39.880 --> 20:43.760
And really importantly, the value of an action depends on the

20:43.760 --> 20:44.840
probability that you're going to play it.

20:45.400 --> 20:49.240
So if you're playing rock every single time, that value is really low.

20:49.760 --> 20:53.920
But if you're never playing rock, you play rock like 1% of the time, then

20:53.920 --> 20:57.120
suddenly the, the other person's probably going to be throwing scissors.

20:57.520 --> 21:00.080
And when you throw a rock, the value of that action is going to be really high.

21:00.080 --> 21:01.120
Now you take that to poker.

21:01.120 --> 21:04.520
What that means is the value of bluffing.

21:04.520 --> 21:07.520
For example, if you're the kind of person that never bluffs and you have this

21:07.520 --> 21:11.480
reputation as somebody that never bluffs and suddenly you bluff, there's a really

21:11.480 --> 21:14.000
good chance that that bluff is going to work and you're going to make a lot of money.

21:14.640 --> 21:17.840
On the other hand, if you got a reputation, like if they seen you play for a long time

21:17.840 --> 21:20.480
and they see, Oh, you're the kind of person that's bluffing all the time.

21:21.200 --> 21:23.560
When you bluff, they're not going to buy it and they're going to call you down.

21:23.560 --> 21:24.360
You're going to lose a lot of money.

21:25.360 --> 21:28.320
And that is the kind of thing that you're going to lose a lot of money.

21:28.880 --> 21:34.640
And that finding that balance of how often you should be bluffing is, uh,

21:34.680 --> 21:36.400
the key challenge of a game of poker.

21:37.200 --> 21:40.000
And, um, you contrast that with a game like chess.

21:41.040 --> 21:44.400
It doesn't matter if you're opening with the queen's gambit 10% of the

21:44.400 --> 21:47.920
time or a hundred percent of the time, the value, the expected value is the same.

21:49.520 --> 21:55.280
So, um, so that's, that's why we need these algorithms that understand, not

21:55.280 --> 21:57.840
just we have to figure out what actions are good, but the probabilities we need

21:57.840 --> 21:59.360
to get the exact probabilities correct.

21:59.600 --> 22:03.040
And that's actually when we created the bot, Lebron, this Lebron, this means

22:03.040 --> 22:07.760
balanced because the algorithm that we designed was designed to find that right

22:07.760 --> 22:12.320
balance of how often it should play each action, the balance of how often in the

22:12.320 --> 22:17.880
key sort of branching is the bluff or not to bluff is that a, is that a good crude

22:17.880 --> 22:20.680
simplification of the major decision in poker?

22:21.040 --> 22:22.160
It's a good simplification.

22:22.160 --> 22:26.720
I think that's like the main tension, but it's, it's not just how often

22:26.720 --> 22:27.760
to bluff or not to bluff.

22:27.760 --> 22:29.760
It's like, how often should you bet in general?

22:29.760 --> 22:32.600
How often should you, what, what kind of bet should you make?

22:32.920 --> 22:36.800
Um, should you bet big or should you bet small when with which, with, with which

22:36.800 --> 22:37.240
hands?

22:37.760 --> 22:41.840
Uh, and so this is where the idea of a range comes from, because when you're

22:41.840 --> 22:45.640
bluffing with a particular hand in a particular spot, you don't want there to

22:45.640 --> 22:48.320
be a pattern for the other person to pick up on, you don't want them to figure

22:48.320 --> 22:51.520
out, Oh, whenever this person is in this spot, they're always bluffing.

22:51.800 --> 22:56.960
And so you have to reason about, okay, would I also bet with a good hand in

22:56.960 --> 22:59.320
this spot, you want to be unpredictable.

22:59.640 --> 23:03.960
So you have to think about what would I do if I had this different set of cards?

23:04.240 --> 23:09.240
Is there explicit estimation of like a theory of mind that the other person has

23:09.240 --> 23:12.960
about you, or is that just a emergent thing that happens?

23:14.320 --> 23:18.560
The way that the bots handle it, that are really successful, they have an

23:18.560 --> 23:19.600
explicit theory of mind.

23:19.600 --> 23:25.040
So they're explicitly reasoning about what are, what's the common knowledge

23:25.040 --> 23:25.360
belief?

23:25.360 --> 23:27.280
What does, what do you think I have?

23:27.280 --> 23:28.480
What do I think you have?

23:28.760 --> 23:29.440
What do you think?

23:29.440 --> 23:32.640
I think you have, um, it's explicitly reasoning about that.

23:32.640 --> 23:34.240
Is there multiple use there?

23:34.360 --> 23:39.440
So maybe that's jumping ahead to six players, but is there a stickiness to the

23:39.440 --> 23:42.040
person to, so it's an iterative game.

23:42.040 --> 23:43.320
You're playing the same person.

23:44.840 --> 23:47.440
There is, there's a stickiness to that, right?

23:47.440 --> 23:49.160
You're gathering information as you play.

23:49.720 --> 23:53.200
It's not every, every, um, every hand is a new hand.

23:53.320 --> 23:57.760
Is there, um, a continuation in terms of estimating what kind of

23:57.760 --> 23:58.720
player I'm facing here?

23:59.320 --> 24:00.000
That's a good question.

24:00.000 --> 24:04.880
So you could approach the game that way, the way that the bots do it, they don't

24:04.880 --> 24:06.400
end with the way that humans approach it.

24:06.400 --> 24:10.920
Also expert human players, the way they approach it is to basically assume that

24:10.920 --> 24:12.760
you know my strategy.

24:13.240 --> 24:18.080
So I'm going to try to pick a strategy where even if I were to play it for 10,000

24:18.080 --> 24:20.960
hands and you could figure out exactly what it was, you still wouldn't be able

24:20.960 --> 24:21.360
to beat it.

24:21.400 --> 24:24.080
Basically what that means is I'm trying to approximate the Nash equilibrium.

24:24.360 --> 24:27.000
I'm trying to be perfectly balanced because if, if I'm playing the Nash

24:27.000 --> 24:31.520
equilibrium, even if you know what my strategy is, like I said, I'm still

24:31.520 --> 24:32.600
unbeatable in expectation.

24:33.080 --> 24:35.360
So, so that's what, that's what the bot aims for.

24:35.720 --> 24:39.160
And that's actually what a lot of expert poker players aim for as well, to start

24:39.160 --> 24:40.960
by playing the Nash equilibrium.

24:41.160 --> 24:44.040
And then maybe if they spot weaknesses in the way you're playing, then they can

24:44.040 --> 24:45.760
deviate a little bit to take advantage of that.

24:46.680 --> 24:48.880
They aim to be unbeatable in expectation.

24:48.920 --> 24:49.360
Okay.

24:50.080 --> 24:53.520
So who's the greatest poker player of all time and why is it Phil Hellmuth?

24:53.600 --> 24:54.560
So this is for Phil.

24:55.520 --> 25:02.880
So he's known, um, at least in part for maybe playing suboptimally and he still

25:02.880 --> 25:03.840
wins a lot.

25:04.240 --> 25:05.360
It's a big chaotic.

25:05.440 --> 25:11.680
So maybe can you speak from an AI perspective about the genius of his madness

25:11.720 --> 25:13.120
or the madness of his genius?

25:13.120 --> 25:19.360
So playing suboptimally, playing chaotically, um, as a way to make it hard

25:19.360 --> 25:21.760
to pin down about what your strategy is.

25:22.400 --> 25:23.360
So, okay.

25:23.760 --> 25:27.360
The thing that I should explain, first of all, with like Nash equilibrium, it

25:27.360 --> 25:28.800
doesn't mean that it's predictable.

25:28.880 --> 25:31.040
The whole point of it is that you're trying to be unpredictable.

25:31.600 --> 25:35.840
Now I think when somebody like Phil Hellmuth might be really successful is

25:35.840 --> 25:41.360
not in being unpredictable, but in being able to, um, take advantage of the

25:42.320 --> 25:47.280
other player and figure out where they're being predictable or guiding the

25:47.280 --> 25:52.080
other player into thinking that you have certain weaknesses and then, and then

25:52.080 --> 25:53.920
understanding how they're going to change their behavior.

25:54.000 --> 25:58.080
They're going to deviate from a Nash equilibrium style of play to try to take

25:58.080 --> 26:00.480
advantage of those perceived weaknesses and then counter exploit them.

26:00.640 --> 26:02.240
So you kind of get into the mind games there.

26:02.640 --> 26:07.600
So you think about at least has a poker as a, as a dance between two agents, I

26:07.600 --> 26:08.720
guess, are you playing the cards?

26:08.720 --> 26:10.000
Are you playing the player?

26:10.320 --> 26:14.560
So this, this gets down to a big argument in the poker community and the

26:14.560 --> 26:15.360
academic community.

26:15.360 --> 26:19.520
For a long time, there was this debate of like what's called GTO game theory,

26:19.520 --> 26:21.840
optimal poker or exploitative play.

26:22.720 --> 26:27.360
And, um, up until about like 2017 when we did the Lebronis match, I think

26:27.360 --> 26:29.520
actually exploitative play had the advantage.

26:29.520 --> 26:32.720
A lot of people were saying like, oh, this whole idea of game theory, it's

26:32.720 --> 26:33.440
just nonsense.

26:33.440 --> 26:36.000
And if you really want to make money, you gotta like look into the other

26:36.000 --> 26:39.600
person's eyes and read their soul and figure out what cards they have.

26:40.160 --> 26:43.680
But what happened was people started adopting the game theory, optimal

26:43.680 --> 26:49.120
strategy, um, and they were making good money and they weren't trying to adapt

26:49.120 --> 26:50.000
so much to the other player.

26:50.000 --> 26:52.640
They were just trying to play the Nash equilibrium.

26:52.640 --> 26:55.680
And then what really solidified it, I think, was the Lebronis, the Lebronis

26:55.680 --> 27:00.240
match where we played our bot against four top heads up, no limit hold them

27:00.240 --> 27:00.960
poker players.

27:01.520 --> 27:03.920
And the bot wasn't trying to adapt to them.

27:03.920 --> 27:05.280
It wasn't trying to exploit them.

27:05.280 --> 27:07.280
It wasn't trying to do these mind games.

27:07.280 --> 27:09.680
It was just trying to approximate the Nash equilibrium.

27:09.680 --> 27:10.560
And it crushed them.

27:11.440 --> 27:17.200
I think, you know, it, we were playing for $50, $100 blinds.

27:17.200 --> 27:21.040
And over the course of about 120,000 hands, it made close to $2 million.

27:21.040 --> 27:24.720
$120,000 hands, 120,000 hands against humans.

27:24.720 --> 27:24.960
Yeah.

27:24.960 --> 27:26.480
And this was, this was fake money to be clear.

27:26.480 --> 27:27.520
So there was real money at stake.

27:27.520 --> 27:28.480
There was $200,000.

27:28.480 --> 27:32.880
First of all, all money is fake, but, um, that's a, that's, that's a different

27:32.880 --> 27:34.000
conversation.

27:34.000 --> 27:36.240
Um, we give it meaning.

27:36.240 --> 27:40.000
Uh, it's an, it's a, it's, it's a phenomena that gets meaning from our, uh,

27:40.000 --> 27:42.160
complex psychology as a human civilization.

27:42.800 --> 27:46.400
Um, it's emerging from the collective intelligence of the human species, but

27:46.400 --> 27:47.200
that's not what you mean.

27:47.200 --> 27:50.320
You mean like there's literally, you can't, you can't buy stuff with it.

27:50.320 --> 27:50.800
Okay.

27:50.800 --> 27:55.120
Can you actually step back and take me through that, um, competition?

27:55.760 --> 27:56.080
Yeah.

27:56.080 --> 27:56.480
Okay.

27:56.480 --> 28:00.720
So when I was in grad school, um, there was this thing called the annual

28:00.720 --> 28:04.480
computer poker competition where every year all the different research labs

28:04.480 --> 28:06.800
that were working on AI for poker would get together.

28:06.800 --> 28:07.600
They would make a bot.

28:07.600 --> 28:08.800
They would play them against each other.

28:09.600 --> 28:14.800
Uh, and we made a bot that actually won the, um, 2014 competition, the 2016

28:14.800 --> 28:15.440
competition.

28:16.000 --> 28:21.040
Uh, and so we decided we're going to take this bot build on it and play against

28:21.040 --> 28:24.160
real top professional heads up, no limit, Texas Hold'em poker players.

28:24.880 --> 28:30.960
So we invited four of the world's best, um, players in this specialty and we

28:31.040 --> 28:34.480
challenged them to 120,000 hands of poker over the course of 20 days.

28:35.600 --> 28:40.400
Um, and we had $200,000, $200,000 in prize money at stake where it would

28:40.400 --> 28:43.280
basically be divided among them, depending on how well they did relative

28:43.280 --> 28:43.840
to each other.

28:44.640 --> 28:46.800
So we wanted to have some incentive for them to play their best.

28:47.600 --> 28:52.400
Did you have a confidence 2014, 16 that this is even possible?

28:52.400 --> 28:53.840
How much doubt was there?

28:53.840 --> 28:57.760
So we did a competition actually in 2015 where we also played against

28:57.760 --> 29:01.200
professional poker players and the bot lost by, by a pretty sizable margin,

29:01.200 --> 29:02.000
actually.

29:02.000 --> 29:05.760
Now there were some big improvements from 2015 to 2017.

29:05.760 --> 29:07.520
And so can you speak to the improvements?

29:07.520 --> 29:08.800
Is it computational nature?

29:08.800 --> 29:10.800
Is it the algorithm, the methods?

29:10.800 --> 29:13.840
It was, it was really an algorithmic approach that was the difference.

29:13.840 --> 29:20.400
So 2015, it was much more focused on trying to come up with a strategy up

29:20.400 --> 29:24.400
front, like trying to solve the entire game of poker, like, and then just have

29:24.400 --> 29:27.360
a lookup table where you're saying like, oh, I'm in this situation.

29:27.360 --> 29:28.160
What's the strategy?

29:29.280 --> 29:32.720
The approach that we took in 2017 was much more search-based.

29:32.720 --> 29:38.000
It was trying to say, okay, well, let me in real time try to compute a much

29:38.000 --> 29:41.840
better strategy than what I had pre-computed by playing against myself

29:41.840 --> 29:42.560
during self-play.

29:42.560 --> 29:46.240
What is the search space for poker?

29:46.800 --> 29:48.000
What are you searching over?

29:49.600 --> 29:50.560
What's that look like?

29:50.560 --> 29:53.760
There's different actions like raising, calling.

29:53.760 --> 29:54.000
Yeah.

29:54.000 --> 29:54.960
What are the actions?

29:57.520 --> 29:58.960
Is it just a search over actions?

29:59.520 --> 30:04.800
So in a game like chess, the search is like, okay, I'm in this chess position

30:04.800 --> 30:08.240
and I can like, you know, move these different pieces and see where things end up.

30:08.240 --> 30:12.800
In poker, what you're searching over is the actions that you can take for your hand,

30:12.800 --> 30:16.080
the probabilities that you take those actions, and then also the probabilities

30:16.080 --> 30:18.400
that you take other actions with other hands that you might have.

30:20.000 --> 30:23.040
And that's kind of like hard to wrap your head around.

30:23.040 --> 30:28.400
Why are you searching over these other hands that you might have and trying to figure out

30:28.400 --> 30:29.360
what you would do with those hands?

30:30.640 --> 30:36.160
And the idea is, again, you want to always be balanced and unpredictable.

30:36.720 --> 30:41.200
And so if your search algorithm is saying, oh, I want to raise with this hand,

30:41.200 --> 30:44.640
well, in order to know whether that's a good action, let's say it's a bluff.

30:44.640 --> 30:48.160
Let's say you have a bad hand and you're saying, oh, I think I should be betting here

30:48.160 --> 30:50.160
with this really bad hand and bluffing.

30:50.160 --> 30:55.280
Well, that's only a good action if you're also betting with a strong hand.

30:55.280 --> 30:56.880
Otherwise, it's an obvious bluff.

30:56.880 --> 31:01.920
So if your action, in some sense, maximizes your unpredictability,

31:01.920 --> 31:05.360
so that action could be mapped by your opponent to a lot of different hands,

31:05.360 --> 31:07.120
then that's a good action.

31:07.120 --> 31:10.400
Basically, what you want to do is put your opponent into a tough spot.

31:10.960 --> 31:14.560
So you want them to always have some doubt like, should I call here?

31:14.560 --> 31:15.840
Should I fold here?

31:15.840 --> 31:20.320
And if you are raising in the appropriate balance between bluffs and good hands,

31:20.320 --> 31:21.680
then you're putting them into that tough spot.

31:21.680 --> 31:22.640
And so that's what we're trying to do.

31:22.640 --> 31:24.640
We're always trying to search for a strategy

31:24.640 --> 31:26.800
that would put the opponent into a difficult position.

31:26.800 --> 31:30.480
Can you give a metric that you're trying to maximize or minimize?

31:30.480 --> 31:33.200
Does this have to do with the regret thing we're talking about

31:33.200 --> 31:36.560
in terms of putting your opponent in a maximally tough spot?

31:37.200 --> 31:40.480
Yeah, ultimately, what you're trying to maximize is your expected winning.

31:40.480 --> 31:43.600
So your expected value, the amount of money that you're going to walk away from,

31:43.600 --> 31:46.720
assuming that your opponent was playing optimally in response.

31:47.360 --> 31:51.200
So you're going to assume that your opponent is also playing

31:52.480 --> 31:54.960
as well as possible a Nash equilibrium approach,

31:54.960 --> 31:58.000
because if they're not, then you're just going to make more money.

32:00.400 --> 32:06.160
By definition, the Nash equilibrium is the strategy that does the best in expectation.

32:06.160 --> 32:09.440
And so if you're deviating from that, then they're going to lose money.

32:09.440 --> 32:12.320
And since it's a two-player, zero-sum game, that means you're going to make money.

32:12.320 --> 32:15.440
So there's not an explicit objective function that

32:16.240 --> 32:18.560
maximizes the toughness of the spot they're put in.

32:20.240 --> 32:23.920
This is from a self-play reinforcement learning perspective.

32:23.920 --> 32:27.680
You're just trying to maximize winnings, and the rest is implicit.

32:27.680 --> 32:31.680
That's right, yeah. So what we're actually trying to maximize is the expected value,

32:31.680 --> 32:34.400
given that the opponent is playing optimally in response to us.

32:34.400 --> 32:38.960
Now, in practice, what that ends up looking like is it's putting the opponent into difficult

32:38.960 --> 32:41.760
situations where there's no obvious decision to be made.

32:41.760 --> 32:46.240
So the system doesn't know anything about the difficulty of the situation?

32:46.240 --> 32:47.680
Not at all. It doesn't care.

32:47.680 --> 32:51.600
In my head, it was getting excited whenever it's making the opponent sweat.

32:51.600 --> 32:54.240
Okay, so in 2015, you didn't do as well.

32:55.200 --> 32:59.680
So what's the journey from that to a system that in your mind could have a chance?

33:00.240 --> 33:06.640
So in 2015, we beat pretty badly, and we actually learned a lot from that competition.

33:06.640 --> 33:11.520
And in particular, what became clear to me is that the way the humans were approaching the game

33:11.520 --> 33:14.320
was very different from how the bot was approaching the game.

33:14.880 --> 33:19.280
The bot would not be doing search. It would just be trying to compute.

33:19.280 --> 33:22.960
It would do like months of self-play. It would just be playing against itself for months.

33:22.960 --> 33:25.440
But then when it's actually playing the game, it would just act instantly.

33:26.320 --> 33:29.920
And the humans, when they're in a tough spot, they would sit there and think

33:30.560 --> 33:34.720
for sometimes even like five minutes about whether they're going to call or fold a hand.

33:36.320 --> 33:41.440
And it became clear to me that there's a good chance that that's what's missing

33:41.440 --> 33:45.840
from our bot. So I actually did some initial experiments to try to figure out how much of

33:45.840 --> 33:48.880
a difference does this actually make. And the difference was huge.

33:48.880 --> 33:52.080
As a signal to the human player, how long you took to think?

33:52.080 --> 33:55.600
No, no, no. I'm not saying that there were any timing tells. I was saying when the human,

33:55.600 --> 33:59.120
like the bot would always act instantly. It wouldn't try to come up with a better

33:59.120 --> 34:04.240
strategy in real time over what it had pre-computed during training.

34:04.240 --> 34:07.440
Whereas the human, like they have all this intuition about how to play,

34:07.440 --> 34:13.360
but they're also in real time leveraging their ability to think, to search, to plan,

34:14.240 --> 34:17.280
and coming up with an even better strategy than what their intuition would say.

34:17.280 --> 34:21.520
So you're saying that you're doing, that's what you mean by you're doing search also.

34:22.880 --> 34:28.480
You have an intuition and search on top of that looking for a better solution.

34:28.480 --> 34:34.000
Yeah. That's what I mean by search. That instead of acting instantly, a neural net

34:34.800 --> 34:37.920
usually gives you a response in like a hundred milliseconds or something. It depends on the size

34:37.920 --> 34:43.760
of the net. But if you can leverage extra computational resources, you can possibly

34:43.760 --> 34:50.240
get a much better outcome. And we did some experiments in small scale versions of poker.

34:50.800 --> 34:57.600
And what we found was that if you do a little bit of search, even just a little bit,

34:58.240 --> 35:03.440
it was the equivalent of making your, you know, your pre-computed strategy,

35:03.440 --> 35:06.080
like you can kind of think of it as your neural net, a thousand times bigger,

35:06.720 --> 35:11.120
with just a little bit of search. And it just like blew away all of the research that we had

35:11.120 --> 35:16.160
been working on and trying to like scale up this like pre-computed solution. It was

35:16.960 --> 35:21.840
dwarfed by the benefit that we got from search. Can you just linger on what you mean by search

35:21.840 --> 35:29.280
here? You're searching over a space of actions for your hand and for other hands. How are you

35:29.280 --> 35:35.200
selecting the other hands to search over? Is it randomly? No, it's all the other hands that

35:35.200 --> 35:38.960
you could have. So when you're playing No Limits Texas Hold'em, you've got two face-down cards.

35:38.960 --> 35:44.640
And so that's 52, choose two, 1,326 different combinations. Now that's actually a little bit

35:44.640 --> 35:48.640
lower because there's face-up cards in the middle and so you can eliminate those as well. But you're

35:48.640 --> 35:52.240
looking at like around a thousand different possible hands that you can have. And so when

35:52.240 --> 35:57.120
we're doing, when the bot's doing search, it's thinking explicitly, there are these thousand

35:57.120 --> 35:59.760
different hands that I could have. There are these thousand different hands that you could have.

36:00.560 --> 36:04.720
Let me try to figure out what would be a better strategy than what I've pre-computed

36:04.720 --> 36:14.640
for these hands and your hands. Okay. So that search, how do you fuse that with what the neural

36:14.640 --> 36:21.200
net is telling you or what the train system is telling you? Yeah. So you kind of like,

36:21.920 --> 36:29.600
where the train system comes in is the value at the end. So there's, you only look so far ahead,

36:29.600 --> 36:33.360
you look like maybe one round ahead. So if you're on the flop, you're looking to the start of the

36:33.360 --> 36:42.000
turn. And at that point you can use the pre-computed solution to figure out what's the value here of

36:42.000 --> 36:49.280
this strategy. Is it of a single action essentially in that spot? You're getting a value or is it the

36:49.280 --> 36:55.040
value of the entire series of actions? Well, it's kind of both because you're trying to maximize the

36:55.040 --> 36:59.920
value for the hand that you have, but in the process, in order to maximize the value of the

36:59.920 --> 37:03.920
hand that you have, you have to figure out what would I be doing with all these other hands as

37:03.920 --> 37:11.680
well. Okay. But are you in the search or was going to the end of the game? In Labrador we did. So we

37:11.680 --> 37:16.880
only use search starting on the turn. And then we searched all the way to the end of the game.

37:16.880 --> 37:23.520
The turn, the river. Can we take it through the terminology? Yeah. There's four rounds of poker.

37:23.520 --> 37:28.640
So there's the pre-flop, the flop, the turn, and the river. And so we would start doing

37:28.640 --> 37:32.880
search halfway through the game. Now the first half of the game, that was all pre-computed.

37:32.880 --> 37:38.000
It would just act instantly. And then when it got to the halfway point, then it would always search

37:38.000 --> 37:41.280
to the end of the game. Now we later improved this so we don't have to search all the way to

37:41.280 --> 37:47.360
the end of the game. It would actually search just a few moves ahead. But that came later and that

37:47.360 --> 37:52.160
drastically reduced the amount of computational resources that we needed. But the moves, because

37:52.160 --> 37:55.920
you can keep betting on top of each other. That's what you mean by moves. So like that's where

37:56.480 --> 38:02.240
you don't just get one bet per turn or poker. You can have multiple arbitrary number of bets,

38:02.240 --> 38:06.160
right? Right. I'm trying to think like, I'm going to bet and then what are you going to do in

38:06.160 --> 38:10.000
response? Are you going to raise me? Are you going to call? And then if you raise, what should I do?

38:10.000 --> 38:14.640
So it's reasoning about that whole process up until the end of the game in the case of

38:14.640 --> 38:19.200
Lebronis. So for the Lebronis, what's the most number of re-raises have you ever seen?

38:21.120 --> 38:25.840
You probably cap out at like five or something because at that point you're basically all in.

38:26.560 --> 38:32.080
I mean, is there like interesting patterns like that that you've seen that the game does? Like

38:32.080 --> 38:37.280
you'll have like AlphaZero doing way more sacrifices than humans usually do. Is there

38:37.280 --> 38:42.000
something like the Lebronis was constantly re-raising or something like that that you've

38:42.000 --> 38:47.920
noticed? There was something really interesting that we observed with Lebronis. So humans,

38:47.920 --> 38:52.400
when they're playing poker, they usually size their bets relative to the size of the pot.

38:52.400 --> 38:57.440
So if the pot has a hundred dollars in there, maybe you bet like $75 or somewhere around that,

38:57.440 --> 39:03.360
somewhere between like 50 and a hundred dollars. And with Lebronis, we gave it the option to

39:03.360 --> 39:07.360
basically bet whatever it wanted. It was actually really easy for us to say like,

39:07.360 --> 39:10.880
oh, if you want, you can bet like 10 times the pot. And we didn't think it would actually do that.

39:10.880 --> 39:14.960
It was just like, why not give it the option? And then during the competition,

39:14.960 --> 39:18.640
it actually started doing this. And by the way, this is like a very last minute decision on our

39:18.640 --> 39:25.200
part to add this option. And so we did not think the bot would do this. And I was actually kind of

39:25.200 --> 39:28.800
worried when it did start to do this, like, oh, is this a problem? Like humans don't do this.

39:28.880 --> 39:33.840
Is it screwing up? But it would put the humans into really difficult spots when it would do that.

39:34.640 --> 39:40.000
Because you could imagine like you have the second best hand that's possible given the board

39:40.000 --> 39:43.280
and you're thinking like, oh, you're in a really great spot here. And suddenly the bot

39:43.280 --> 39:49.600
bets $20,000 into a thousand dollar pot. And it's basically saying like, I have the best hand

39:50.160 --> 39:55.520
or I'm bluffing. And you having the second best hand, like now you get a really tough choice to

39:55.520 --> 40:00.400
make. And so the humans would sometimes think like five or 10 minutes about like,

40:00.400 --> 40:05.840
what do you do? Should I call? Should I fold? And when I saw the humans like really struggling

40:05.840 --> 40:08.640
with that decision, like that's when I realized like, oh, actually this is maybe a good thing to

40:08.640 --> 40:14.400
do after all. And of course the system doesn't know that it's making, again, like we said,

40:14.400 --> 40:20.560
that it's putting them in a tough spot. It's just that's part of the optimal, the game theory

40:20.560 --> 40:24.640
optimal. Right. From the bot's perspective, it's just doing the thing that's going to make it the

40:24.640 --> 40:30.640
most money. And the fact that it's putting the humans in a difficult spot, like that's just a

40:30.640 --> 40:35.760
side effect of that. And this was, I think the one thing, I mean, there were a few things that

40:35.760 --> 40:40.240
the humans walked away from, but this was the number one thing that the humans walked away from

40:40.240 --> 40:45.040
the competition saying like, we need to start doing this. And now these over bets, what are

40:45.040 --> 40:49.360
called over bets have become really common in high level poker play. Have you ever talked to

40:49.920 --> 40:53.440
somebody like Danny on the ground about this? He seems to be a student of the game.

40:53.920 --> 40:58.960
I did actually have a conversation with Daniel DeGranue once. Yeah. I was visiting the Isle of

40:58.960 --> 41:05.840
Man to talk to poker stars about AI. And Daniel DeGranue was there when we had dinner together

41:05.840 --> 41:11.280
with some other people. And yeah, he was really interested in it. He mentioned that he was like,

41:11.280 --> 41:13.600
you know, excited about like learning from these AIs.

41:14.240 --> 41:16.160
So he wasn't scared. He was excited.

41:16.160 --> 41:21.280
He was excited. And he honestly, he wanted to play against the bot. He thought he had a decent

41:21.280 --> 41:28.240
chance of beating it. I think, you know, this was like several years ago when I think it was like

41:28.240 --> 41:33.920
not as clear to everybody that, you know, the AIs were taking over. I think now people recognize

41:33.920 --> 41:38.160
that like, if you're playing against a bot, there's like no chance that you have in a game

41:38.160 --> 41:45.680
like poker. So consistently the bots will win. The bots have heads up and in other variants too. So

41:46.640 --> 41:50.960
six player Texas Hold'em, no limit Texas Hold'em, the bots win.

41:51.600 --> 41:55.280
Yeah, that's the case. So I think there's some debate about like, is it true for every single

41:55.280 --> 42:00.720
variant of poker? I think for every single variant of poker, if somebody really put in the effort,

42:00.720 --> 42:06.640
they can make an AI that would beat all humans at it. We've focused on the most popular variants.

42:06.640 --> 42:12.800
So heads up, no limit Texas Hold'em. And then we followed that up with six player poker as well,

42:12.800 --> 42:17.840
where we managed to make a bot that beat expert human players. And I think even there now,

42:18.560 --> 42:22.880
it's pretty clear that humans don't stand a chance. See, I would love to hook up an AI system that

42:23.760 --> 42:30.160
looks at EEG, like how like actually tries to optimize the toughness of the spot it puts a

42:30.160 --> 42:36.160
human in. And I would love to see how different is that from the game theory optimal. So you try to

42:36.160 --> 42:42.640
maximize the heart rate of the human player, like the freaking out over a long period of time. I

42:42.640 --> 42:49.600
wonder if there's going to be different strategies that emerge that are close in terms of effectiveness,

42:49.600 --> 42:57.120
because something tells me you could still achieve superhuman level performance by just making people

42:57.120 --> 43:03.600
sweat. I feel like that there's a good chance that that is the case. Yeah, if you're able to see like

43:03.600 --> 43:09.520
that, it's like a decent proxy for score, right? And this is actually like the common poker wisdom

43:10.160 --> 43:14.160
where they're teaching players before there were bots, and they were trying to teach people how to

43:14.160 --> 43:18.400
play poker, they would say like, the key to the game is to put your opponent to difficult spots.

43:18.400 --> 43:23.200
It's a good estimate for if you're making the right decision. So what else can you say about

43:23.200 --> 43:30.160
the fundamental role of search in poker? And maybe if you can also related to chess and go

43:30.160 --> 43:35.360
in these games? What's the role of search to solve in these games?

43:35.680 --> 43:40.480
Yeah, I think a lot of people under this is true for the general public. And I think it's true for

43:40.480 --> 43:45.200
the AI community. A lot of people underestimate the importance of search for these kinds of game

43:45.200 --> 43:53.840
AI results. An example of this is TD Gammon that came out in 1992. This was the first real instance

43:53.840 --> 43:57.760
of a neural net being used in a game AI. It's a landmark achievement. It was actually the

43:57.760 --> 44:03.120
inspiration for AlphaZero. And it used search, it used two ply search to figure out its next move.

44:03.120 --> 44:09.920
You got Deep Blue there. He was very heavily focused on search, looking many, many moves ahead

44:09.920 --> 44:14.800
farther than any human could. And that was key for why it won. And then even with something like

44:14.800 --> 44:22.480
AlphaGo, I mean, AlphaGo is commonly hailed as a landmark achievement for neural nets, and it is.

44:22.480 --> 44:28.000
But there's also this huge component of search, Monte Carlo tree search to AlphaGo, that was key,

44:28.960 --> 44:32.960
that was key, absolutely essential for the AI to be able to beat top humans.

44:35.040 --> 44:39.760
I think a good example of this is you look at the latest versions of Alpha of AlphaGo,

44:39.760 --> 44:45.920
like it was called AlphaZero. And there's this metric called Elo rating, where you can

44:45.920 --> 44:51.840
compare different humans and you can compare bots to humans. Now, a top human player is around

44:51.840 --> 44:58.880
3,600 Elo, maybe a little bit higher now. AlphaZero, the strongest version, is around 5,200 Elo.

44:59.760 --> 45:05.200
But if you take out the search that's being done at test time, and by the way, what I mean by search

45:05.200 --> 45:11.360
is the planning ahead, the thinking of like, oh, if I place this stone here and then he does this,

45:11.360 --> 45:14.400
and then you look like five moves ahead and you see what the board state looks like.

45:15.680 --> 45:19.600
That's what I mean by search. If you take out the search that's done during the game,

45:19.600 --> 45:25.920
the Elo rating drops to around 3,000. So even today, what, seven years after AlphaGo,

45:26.880 --> 45:31.840
if you take out the Monte Carlo tree search that's being done when playing against the human,

45:32.480 --> 45:38.080
the bots are not superhuman. Nobody has made a raw neural net that is superhuman in Go.

45:39.120 --> 45:45.920
That's worth lingering on. That's quite profound. So without search, that just means looking at the

45:45.920 --> 45:52.800
next move and saying this is the best move. So having a function that estimates accurately

45:52.800 --> 45:56.880
what the best move is without search. That's right. Yeah. And all these bots,

45:56.880 --> 46:01.760
they have what's called a policy network where it will tell you this is what the neural net thinks

46:01.760 --> 46:09.520
is the next best move. And it's kind of like the intuition that a human has. The human looks at

46:09.520 --> 46:15.920
the board and any Go or chess master will be able to tell you like, oh, instantly here's what I think

46:15.920 --> 46:22.480
the right move is. And the bot is able to do the same thing. But just like how a human grandmaster

46:22.480 --> 46:26.800
can make a better decision if they have more time to think, when you add on this Monte Carlo tree

46:26.800 --> 46:32.800
search, the bot is able to make a better decision. Yeah. I mean, of course a human is doing something

46:32.800 --> 46:39.840
like search in their brain, but it's not, I hesitate to draw a hard line, but it's not like

46:39.840 --> 46:47.680
Monte Carlo tree search. It's more like sequential language model generation. So it's like a different,

46:47.680 --> 46:52.960
it's a, the neural network is doing the searching. And I wonder what the human brain is doing in terms

46:52.960 --> 46:57.360
of searching. Cause you're doing that like computation. A human is computing. They have

46:57.360 --> 47:03.440
intuition. They've got, they have a really strong ability to estimate, you know, amongst the top

47:03.440 --> 47:08.640
players of what is good and not position without calculating all the details, but they're still

47:08.640 --> 47:12.800
doing a search in their head, but it's a different kind of search. Have you ever thought about like,

47:12.800 --> 47:18.480
what is the difference between the human, the search that the human is performing versus what

47:19.520 --> 47:24.080
computers are doing? I have thought a lot about that and I think it's a really important question.

47:24.080 --> 47:29.600
So the AI in alpha and alphas in alpha go or any of these go AIs, they're all doing

47:29.600 --> 47:35.280
Monte Carlo tree search, which is a particular kind of search. And it, it's actually a symbolic

47:35.280 --> 47:41.040
tabular search. It uses the neural net to guide its search, but it isn't actually like full,

47:41.040 --> 47:47.520
full on neural net. Now that kind of search is very successful in these kinds of like perfect

47:47.520 --> 47:51.920
information board games like chess and go. But if you take it to a game like poker, for example,

47:51.920 --> 47:56.880
it doesn't work. It can't, it can't understand the concept of hidden information. It doesn't

47:56.880 --> 48:00.400
understand the balance that you have to strike between like the amount that you're raising

48:00.400 --> 48:05.520
versus the amount that you're calling. And in every one of these games, you see a different kind of

48:05.520 --> 48:10.880
search and the human brain is able to plan for all these different games in a very general way.

48:11.840 --> 48:15.200
Now, I think that's one thing that we're missing from AI today. And I think it's a really important

48:15.200 --> 48:22.480
missing piece, the ability to plan and reason more generally across a wide variety of different

48:22.480 --> 48:29.600
settings. In a way where the general reasoning makes you better each one of the games, not worse.

48:29.600 --> 48:33.600
Yeah. So you can kind of think of it as like neural nets today. They'll give you like

48:33.600 --> 48:39.040
transformers, for example, are super general, but you know, they'll give you, it'll output an answer

48:39.040 --> 48:42.400
in like a hundred milliseconds. And if you tell it like, oh, you've got five minutes to give you

48:42.400 --> 48:46.000
a decision, you know, feel free to take more time to make a better decision. It's not going to know

48:46.000 --> 48:50.800
what to do with that. But a human, if you're playing a game like chess, they're going to

48:50.800 --> 48:54.240
give you a very different answer, depending on if you say, oh, you've got a hundred milliseconds

48:54.240 --> 49:01.680
or you've got five minutes. Yeah. I mean, people have started using transformers language models

49:01.680 --> 49:08.240
in an iterative way that does improve the answer or like showing the work kind of idea. Yeah. They

49:08.240 --> 49:12.640
got this thing called chain of thought reasoning and that's, I think super promising, right? Yeah.

49:12.640 --> 49:17.280
I think, and I think it's a good step in the right direction. I would kind of like say it's

49:17.280 --> 49:22.080
similar to Monte Carlo rollouts in a game like chess. There's a kind of search that you can do

49:22.080 --> 49:27.040
where you're saying like, I'm going to roll out my intuition and see like, without really thinking,

49:27.040 --> 49:31.520
you know, what are the better decisions I can make farther down the path? What would I do if I just

49:31.520 --> 49:37.200
acted according to intuition for the next 10 moves? And that gets you an improvement, but I think

49:37.200 --> 49:43.360
that there's much, much richer kinds of planning that we could do. So when the broadest actually

49:43.360 --> 49:48.080
beat the poker players, what did that feel like? What was that? I mean, actually on that day,

49:48.880 --> 49:55.040
what were you feeling like? Were you nervous? I mean, poker was one of the games that you thought

49:55.040 --> 50:00.080
like is not going to be solved because it's the human factor. So at least in the narratives,

50:00.080 --> 50:04.400
we'll tell ourselves the human factor. So fundamental to the game of poker.

50:05.120 --> 50:11.360
Yeah. The Lebronis competition was super stressful for me. Also, I mean, I was working on this like

50:11.360 --> 50:15.920
basically continuously for a year leading up to the competition. I mean, for me, it became like

50:15.920 --> 50:19.680
very clear, like, okay, this is the search technique. This is the approach that we need.

50:19.680 --> 50:23.840
And then I spent a year working on this pretty much like nonstop. Oh, can we actually get into

50:23.840 --> 50:27.760
details? Like what programming languages is it written in? What's some interesting

50:28.640 --> 50:32.640
implementation details that are like fun slash painful?

50:33.200 --> 50:37.440
Yeah. So one of the interesting things about Lebronis is that we had no idea what the bar was

50:37.440 --> 50:42.240
to actually beat top humans. We could play against like our prior bots and that kind of gives us some

50:42.240 --> 50:46.560
sense of like, are we making progress? Are we going in the right direction? But we had no idea

50:46.560 --> 50:51.360
like what the bar actually was. And so we threw a huge amount of resources at trying to make the

50:51.360 --> 50:56.560
strongest bot possible. So we use C++. It was parallelized. We were using, I think, like a

50:56.560 --> 51:03.600
thousand CPUs, maybe more actually. And today that sounds like nothing, but for a grad student back

51:03.600 --> 51:08.560
in 2016, that was a huge amount of resources. Well, it's still a lot for even any grad student

51:08.560 --> 51:17.760
today. It's still tough to get, or even to allow yourself to think in terms of scale at CMU and MIT,

51:17.760 --> 51:22.640
anything like that. Yeah. And, you know, talking about terabytes of memory. So it was a very

51:22.640 --> 51:27.920
parallelized and it had to be very fast too, because the more games that you could simulate,

51:28.560 --> 51:32.560
the stronger the bot would be. So is there some like John Carmack style,

51:33.440 --> 51:38.400
like efficiencies you had to come up with, like an efficient way to represent the hand,

51:38.960 --> 51:42.880
all that kind of stuff? There are all sorts of optimizations that I had to make to try to get

51:42.880 --> 51:46.960
this thing to run as fast as possible. They were like, how do you minimize the latency? How do you

51:47.760 --> 51:51.840
package things together so that you minimize the amount of communication between the different nodes?

51:53.280 --> 51:58.240
How do you optimize the algorithms so that you can try to squeeze out more and more from the

51:58.240 --> 52:02.880
game that you're actually playing? All these kinds of different decisions that I had to make.

52:03.840 --> 52:10.480
Just a fun question. What ID did you use for C++ at the time?

52:10.480 --> 52:15.440
I think I used Visual Studio, actually. Okay. Is that still carried through to today?

52:15.440 --> 52:17.680
VS Code is what I use today. It seems like it's pretty popular.

52:17.680 --> 52:23.440
It's the community, basically conversion on. Okay, cool. So you got this super optimized

52:23.440 --> 52:30.480
C++ system and then you show up to the day of competition. Humans versus machine.

52:32.240 --> 52:37.760
How did it feel throughout the day? Super stressful. I mean, I thought going

52:37.760 --> 52:43.200
into it that we had like a 50-50 chance. Basically, I thought if they play in a totally

52:43.200 --> 52:48.400
normal style, I think we'll squeak out a win, but there's always a chance that they can find some

52:48.400 --> 52:53.600
weakness in the bot. And if they do, and we're playing like for 20 days, 120,000 hands of poker,

52:53.600 --> 52:58.160
they have a lot of time to find weaknesses in the system. And if they do, we're going to get

52:58.160 --> 53:03.040
crushed. And that's actually what happened in the previous competition. The humans, they started

53:03.040 --> 53:06.720
out, it wasn't like they were winning from the start, but then they found these weaknesses that

53:06.720 --> 53:11.840
they could take advantage of. And for the next 10 days, they were just crushing the bot, stealing

53:11.840 --> 53:16.720
money from it. What were the weaknesses they found? Like maybe overbetting was effective,

53:16.720 --> 53:21.920
that kind of stuff. So certain betting strategies worked. What they found is, yeah, overbetting,

53:21.920 --> 53:25.360
like betting certain amounts, the bot would have a lot of trouble dealing with those sizes.

53:25.360 --> 53:33.440
And then also when the bot got into really difficult all-in situations, it wasn't able to,

53:33.440 --> 53:39.040
because it wasn't doing search, it had to clump different hands together and it would treat them

53:39.040 --> 53:45.040
identically. And so it wouldn't be able to distinguish having a king high flush versus

53:45.040 --> 53:48.880
an ace high flush. And in some situations that really matters a lot. And so they could put the

53:48.880 --> 53:54.720
bot into those situations and then the bot would just bleed money. Clever humans. Yeah. Okay. So

53:54.720 --> 54:02.400
I didn't realize it was over 20 days. So what were the humans like over those 20 days?

54:02.960 --> 54:08.160
And what was the bot like? So we had set up the competition. Like I said, there was $200,000 in

54:08.160 --> 54:13.360
prize money and they would get paid a fraction of that depending on how well they did relative

54:13.360 --> 54:17.360
to each other. So I was kind of hoping that they wouldn't work together to try to find weaknesses

54:17.360 --> 54:22.560
in the bot, but they enter the competition with their number one objective being to beat the bot.

54:22.560 --> 54:26.320
And they didn't care about individual glory. They were like, we're all going to work as a team

54:26.320 --> 54:31.120
to try to take down this bot. And so they immediately started comparing notes. What they

54:31.120 --> 54:36.640
would do is they would coordinate looking at different parts of the strategy to try to

54:37.600 --> 54:43.600
find out weaknesses. And then at the end of the day, we actually sent them a log of all the hands

54:43.600 --> 54:49.200
that were played and what cards the bot had on each of those hands. Oh, wow. Yeah. That's gutsy.

54:49.200 --> 54:53.440
Yeah. It was honestly, I'm not sure why we did that in retrospect, but I mean, I'm glad we did

54:53.440 --> 54:58.160
it because we ended up winning anyway. But if you've ever played poker before, that is golden

54:58.160 --> 55:02.800
information. Usually when you play poker, you see about a third of the hands to show down.

55:03.600 --> 55:10.720
And to just hand them all the cards that the bot had on every single hand, that was just a gold

55:10.720 --> 55:15.120
mine for them. And so then they would review the hands and try to see like, okay, could they find

55:15.120 --> 55:20.400
patterns in the bot, the weaknesses? And then they would coordinate and study together and try to

55:20.400 --> 55:24.400
figure out, okay, now this person's going to explore this part of the strategy for weaknesses.

55:24.400 --> 55:26.560
This person is going to explore this part of the strategy for weaknesses.

55:26.640 --> 55:33.280
It's a kind of psychological warfare showing them the hands. I mean, I'm sure you didn't think of

55:33.280 --> 55:38.560
it that way, but doing that means you're confident in the possibility to win.

55:38.560 --> 55:44.000
Well, that's one way of putting it. I wasn't super confident. So going in, like I said,

55:44.000 --> 55:49.440
I think I had like 50-50 odds on us winning. Actually, when we announced the competition,

55:49.440 --> 55:54.400
the poker community decided to gamble on who would win. And their initial odds against us were like

55:54.400 --> 56:00.400
four to one. They were really convinced that the humans were going to pull out a win. The bot ended

56:00.400 --> 56:05.440
up winning for three days straight. And even then, after three days, the betting odds were still just

56:05.440 --> 56:15.280
50-50. And then at that point, it started to look like the humans were coming back. But poker is a

56:15.280 --> 56:20.960
very high variance game. And I think what happened is they thought that they spotted some weaknesses

56:20.960 --> 56:25.360
that weren't actually there. And then around day eight, it was just very clear that they were

56:25.360 --> 56:31.760
getting absolutely crushed. And from that point, I mean, for a while there, I was super stressed out

56:31.760 --> 56:36.080
thinking like, oh my God, the humans are coming back and they've found weaknesses and now we're

56:36.080 --> 56:39.920
just going to lose the whole thing. But no, it ended up going in the other direction and the bot

56:39.920 --> 56:46.320
ended up crushing them in the long run. How did it feel at the end? As a human being,

56:47.280 --> 56:52.960
as a person who loves, appreciates the beauty of the game of poker and as a person who appreciates

56:53.760 --> 56:58.400
the beauty of AI, did you feel a certain kind of way about it?

56:59.280 --> 57:04.800
I felt a lot of things, man. I mean, at that point in my life, I had spent five years working on this

57:04.800 --> 57:11.040
project and it was a huge sense of accomplishment. I mean, to spend five years working on something

57:11.040 --> 57:15.040
and finally see it succeed, yeah, I wouldn't trade that for anything in the world.

57:15.840 --> 57:23.280
Yeah, because that's a real benchmark. It's not like getting some percent accuracy on a data set.

57:23.280 --> 57:30.480
This is real world. It's just a game, but it's also a game that means a lot to a lot of people.

57:30.480 --> 57:35.360
And this is humans doing their best to beat the machine. So this is a real benchmark unlike

57:35.360 --> 57:40.640
anything else. Yeah. And I mean, this is what I had been dreaming about since I was 16 playing

57:40.640 --> 57:44.880
poker with my friends in high school, the idea that you could find a strategy,

57:46.800 --> 57:50.480
approximate the Nash equilibrium, be able to beat all the poker players in the world with it.

57:51.280 --> 57:57.520
So to actually see that come to fruition and be realized, that was, it's kind of magical.

57:58.320 --> 58:01.840
Yeah, especially money is on the line too. It's different than chess

58:02.800 --> 58:08.800
in that aspect. That's why you want to look at betting markets if you want to actually

58:08.800 --> 58:14.080
understand what people really think. And in the same sense, poker, it's really high stakes

58:14.080 --> 58:20.720
because it's money. And to solve that game, that's an amazing accomplishment. So the leap from that to

58:21.760 --> 58:28.400
multi-way six-player poker, how difficult is that jump? And what are some interesting

58:28.400 --> 58:34.880
differences between heads-up poker and multi-way poker? Yeah, so I mentioned Nash equilibrium in

58:34.880 --> 58:39.520
two-player zero-sum games. If you play that strategy, you are guaranteed to not lose an

58:39.520 --> 58:43.520
expectation no matter what your opponent does. Now, once you go to six-player poker, you're no

58:43.520 --> 58:47.200
longer playing a two-player zero-sum game. And so there was a lot of debate among the academic

58:47.200 --> 58:52.400
community and among the poker community about how well these techniques would extend beyond just

58:52.400 --> 59:00.880
two-player heads-up poker. Now, what I had come to realize is that the techniques actually I thought

59:00.880 --> 59:06.400
really would extend to six-player poker because even though in theory they don't give you these

59:06.400 --> 59:10.400
guarantees outside of two-player zero-sum games, in practice, it still gives you a really strong

59:10.400 --> 59:16.480
strategy. Now, there were a lot of complications that would come up with six-player poker besides

59:16.480 --> 59:20.880
like the game theoretic aspect. I mean, for one, the game is just exponentially larger.

59:22.480 --> 59:27.840
So the main thing that allowed us to go from two-player to six-player was the idea of depth

59:27.840 --> 59:33.040
limited search. So I said before, like, you know, we would do search, we would plan out,

59:33.040 --> 59:36.960
the bot would plan out, like, what it's going to do next and for the next several moves.

59:36.960 --> 59:41.200
And in Labrador's, that search was done extending all the way to the end of the game.

59:41.200 --> 59:50.000
So it would have to start from the turn onwards, like looking maybe 10 moves ahead. It would have

59:50.000 --> 59:55.040
to figure out what it was doing for all those moves. Now, when you get to six-player poker,

59:55.040 --> 59:58.800
it can't do that exhaustive search anymore because the game is just way too large.

01:00:00.000 --> 01:00:05.600
But by only having to look a few moves ahead and then stopping there and substituting a value

01:00:05.600 --> 01:00:10.720
estimate of, like, how good is that strategy at that point, then we're able to do a much more

01:00:10.720 --> 01:00:16.880
scalable form of search. Is there something cool we're looking at the paper right now?

01:00:16.880 --> 01:00:22.320
Is there something cool in the paper in terms of graphics? A game tree traversal via Monte Carlo?

01:00:22.320 --> 01:00:28.000
I think if you go down a bit... Figure one, an example of equilibrium

01:00:28.000 --> 01:00:34.000
selection problem. Ooh, so yeah, what do we know about equilibrium when there's multiple players?

01:00:34.720 --> 01:00:39.840
So when you go outside of two players, you're a sum. So a natural equilibrium is a set of strategies,

01:00:39.840 --> 01:00:44.640
like one strategy for each player, where no player has an incentive to switch to a different

01:00:44.640 --> 01:00:50.560
strategy. And so you can kind of think of it as, like, imagine you have a game where there's a

01:00:50.560 --> 01:00:56.000
ring. That's actually the visual here. You got a ring, and the object of the game is to be as far

01:00:56.000 --> 01:01:02.080
away from the other players as possible. A natural equilibrium is for all the players to be spaced

01:01:02.080 --> 01:01:06.400
equally apart around this ring. But there's infinitely many different natural equilibria,

01:01:06.400 --> 01:01:13.360
right? There's infinitely many ways to space four dots along a ring. And if every single player

01:01:13.360 --> 01:01:18.720
independently computes a natural equilibrium, then there's no guarantee that the joint strategy

01:01:18.720 --> 01:01:24.160
that they're all playing is going to be a Nash equilibrium. They're just going to be like random

01:01:24.160 --> 01:01:28.720
dots scattered along this ring rather than four coordinated dots being equally spaced apart.

01:01:28.720 --> 01:01:31.360
Is it possible to sort of optimally do this kind of selection,

01:01:32.800 --> 01:01:38.960
to do the selection of the equilibrium you're chasing? So is there, like, a meta problem to

01:01:38.960 --> 01:01:44.720
be solved here? So the meta problem is, in some sense, how do you understand the Nash

01:01:44.720 --> 01:01:51.440
equilibria that the other players are going to play? And even if you do that, again, there's

01:01:51.440 --> 01:01:58.880
no guarantee that you're going to win. So if you're playing risk, like I said, and all the

01:01:58.880 --> 01:02:02.720
other players decide to team up against you, you're going to lose. Nash equilibrium doesn't help you

01:02:02.720 --> 01:02:08.400
there. And so there is this big debate about whether Nash equilibrium and all these techniques

01:02:08.400 --> 01:02:13.680
that compute it are even useful once you go outside of two-player zero-sum games. Now,

01:02:13.680 --> 01:02:17.840
I think for many games, there is a valid criticism here. And I think when we talk about,

01:02:17.840 --> 01:02:23.920
when we go to something like diplomacy, we run into this issue that the approach of trying to

01:02:23.920 --> 01:02:29.600
approximate a Nash equilibrium doesn't really work anymore. But it turns out that in six-player poker,

01:02:30.400 --> 01:02:36.400
because six-player poker is such an adversarial game where none of the players really try to

01:02:36.400 --> 01:02:41.040
work with each other, the techniques that were used into player poker to try to approximate an

01:02:41.040 --> 01:02:45.200
equilibrium, those still end up working in practice in six-player poker as well.

01:02:45.200 --> 01:02:51.520
There's some deep way in which six-player poker is just a bunch of heads-up poker

01:02:51.520 --> 01:02:56.560
like games in one. It's like embedded in it. So the competitiveness

01:02:58.240 --> 01:03:01.520
is more fundamental to poker than the cooperation.

01:03:01.520 --> 01:03:05.440
Right. Yeah. Poker is just such an adversarial game. There's no real cooperation. In fact,

01:03:05.440 --> 01:03:08.960
you're not even allowed to cooperate in poker. It's considered collusion. It's against the rules.

01:03:11.200 --> 01:03:15.120
And so for that reason, the techniques end up working really well. And I think that's true more

01:03:15.120 --> 01:03:20.800
broadly in extremely adversarial games in general. But that's sort of in practice versus being able

01:03:20.800 --> 01:03:25.840
to prove something. That's right. Nobody has a proof that that's the case. And it could be that

01:03:25.840 --> 01:03:32.080
six-player poker belongs to some class of games where approximating a Nash equilibrium through

01:03:32.080 --> 01:03:38.960
self-play provably works well. And there are other classes of games beyond just two-player zero sum

01:03:38.960 --> 01:03:43.040
where this is proven to work well. So there are these kinds of games called potential games,

01:03:43.040 --> 01:03:48.880
which I won't go into. It's kind of like a complicated concept, but there are classes

01:03:48.880 --> 01:03:54.160
of games where this approach to approximating a Nash equilibrium is proven to work well.

01:03:54.720 --> 01:03:58.880
Now, six-player poker is not known to belong to one of those classes, but it is possible that there

01:03:58.880 --> 01:04:03.360
is some class of games where it either provably performs well or provably performs not that badly.

01:04:04.160 --> 01:04:10.160
So what are some interesting things about Pluribus that was able to achieve human-level

01:04:10.160 --> 01:04:14.960
performance on this or superhuman-level performance on the six-player version of poker?

01:04:16.000 --> 01:04:21.440
Personally, I think the most interesting thing about Pluribus is that it was so much cheaper

01:04:21.440 --> 01:04:26.880
than Lebronis. I mean, Lebronis, if you had to put a price tag on the computational resources

01:04:26.880 --> 01:04:32.240
that went into it, I would say the final training run took about $100,000. You go to Pluribus,

01:04:32.800 --> 01:04:37.200
the final training run would cost less than $150 on AWS.

01:04:37.840 --> 01:04:46.400
Is this normalized to computational inflation? Does this just have to do with the fact that

01:04:46.400 --> 01:04:48.240
Pluribus was trained a year later?

01:04:48.960 --> 01:04:54.240
No, no, no. First of all, yeah, computing resources are getting cheaper every day,

01:04:54.880 --> 01:04:59.120
but you're not going to see a thousand-fold decrease in the computational resources over

01:04:59.120 --> 01:05:04.720
two years, or even anywhere close to that. The real improvement was algorithmic improvements,

01:05:04.720 --> 01:05:07.440
and in particular, the ability to do depth-limited search.

01:05:08.800 --> 01:05:11.680
Does depth-limited search also work for Lebronis?

01:05:12.320 --> 01:05:19.200
Yep, yes. Where this depth-limited search came from is I developed this technique and ran it

01:05:19.200 --> 01:05:24.880
on two-player poker first. That reduced the computational resources needed to make an AI

01:05:24.880 --> 01:05:31.440
that was superhuman from $100,000 for Lebronis to something you could train on your laptop.

01:05:31.440 --> 01:05:35.680
What do you learn from that discovery?

01:05:36.800 --> 01:05:40.160
What I would take away from that is that algorithmic improvements really do matter.

01:05:40.160 --> 01:05:44.400
How would you describe the more general case of limited-depth search?

01:05:45.040 --> 01:05:50.800
So it's basically constraining the scale, temporal, or in some other way of the computation

01:05:50.800 --> 01:05:58.000
you're doing in some clever way. How else can you significantly constrain computation?

01:05:59.520 --> 01:06:04.000
Well, I think the idea is that we want to be able to leverage search as much as possible,

01:06:04.000 --> 01:06:07.760
and the way that we were doing it in Lebronis required us to search all the way to the end

01:06:07.760 --> 01:06:11.280
of the game. Now, if you're playing a game like chess, the idea that you're going to search always

01:06:11.280 --> 01:06:15.600
to the end of the game is kind of unimaginable. There's just so many situations where you just

01:06:15.600 --> 01:06:23.040
won't be able to use search in that case, or the cost would be prohibitive, and this technique

01:06:23.040 --> 01:06:29.360
allowed us to leverage search without having to pay such a huge computational cost for it

01:06:29.360 --> 01:06:31.600
and be able to apply it more broadly.

01:06:31.600 --> 01:06:37.680
So to what degree did you use neural nets for Lebronis and Pleuribus, and more generally,

01:06:37.680 --> 01:06:43.920
what role do neural nets have to play in superhuman-level performance in poker?

01:06:44.480 --> 01:06:50.640
So we actually did not use neural nets at all for Lebronis or Pleuribus, and a lot of people

01:06:50.640 --> 01:06:56.240
found this surprising back in 2017. I think they found it surprising today that we were able to do

01:06:56.240 --> 01:07:02.240
this without using any neural nets, and I think the reason for that, I mean, I think neural nets

01:07:02.240 --> 01:07:09.520
are incredibly powerful, and the techniques that are used today even for poker AIs do rely quite

01:07:09.520 --> 01:07:15.680
heavily on neural nets, but it wasn't the main challenge for poker. I think what neural nets

01:07:15.680 --> 01:07:21.280
are really good for, if you're in a situation where finding features for a value function

01:07:21.920 --> 01:07:25.600
is really difficult, then neural nets are really powerful, and this was the problem in Go.

01:07:26.320 --> 01:07:32.560
The problem in Go, or the final problem in Go at least, was that nobody had a good way of

01:07:32.560 --> 01:07:38.320
looking at a board and figuring out who was winning or losing, describing through a simple algorithm

01:07:38.320 --> 01:07:43.760
who was winning or losing. And so there, neural nets were super helpful because you could just

01:07:43.760 --> 01:07:48.000
feed in a ton of different board positions into this neural net, and it would be able to predict

01:07:48.000 --> 01:07:54.560
then who was winning or losing. But in poker, the features weren't the challenge. The challenge was,

01:07:54.560 --> 01:08:00.640
how do you design a scalable algorithm that would allow you to find this balanced strategy

01:08:01.600 --> 01:08:04.560
that would understand that you have to bluff with the right probability?

01:08:05.440 --> 01:08:09.680
So can that be somehow incorporated into the value function?

01:08:11.680 --> 01:08:14.160
The complexity of poker that you've described?

01:08:14.720 --> 01:08:19.120
Yeah. So the way the value functions work in poker, like the latest and greatest poker AIs,

01:08:19.120 --> 01:08:24.560
they do use neural nets for the value function. The way it's done is very different from how

01:08:24.560 --> 01:08:30.320
it's done in a game like chess or Go because in poker, you have to reason about beliefs.

01:08:31.040 --> 01:08:38.480
And so the value of a state depends on the beliefs that players have about what the different cards

01:08:38.480 --> 01:08:45.120
are. Like if you have pocket Aces, then whether that's a really, really good hand or just an okay

01:08:45.120 --> 01:08:51.280
hand depends on whether you know I have pocket Aces. If you know that I have pocket Aces, then

01:08:51.280 --> 01:08:55.600
if I bet, you're going to fold immediately. But if you think that I have a really bad hand,

01:08:55.600 --> 01:09:01.520
then I could bet with pocket Aces and make a ton of money. So the value function in poker these

01:09:01.520 --> 01:09:07.840
days takes the beliefs as an input, which is very different from how chess and Go AIs work.

01:09:09.040 --> 01:09:15.680
So as a person who appreciates the game, who do you think is the greatest poker player of all time?

01:09:17.360 --> 01:09:23.120
That's a tough question. Can AI help answer that question? Can you actually analyze the

01:09:23.120 --> 01:09:29.760
quality of play? So the chess engines can give estimates of the quality of play.

01:09:33.040 --> 01:09:38.560
I wonder is there an ELO rating type of system for poker? I suppose you could,

01:09:38.560 --> 01:09:45.200
but there's just not enough. You would have to play a lot of games, a very large number of games,

01:09:45.200 --> 01:09:50.000
more than you would in chess. The deterministic game makes it easier to estimate ELO, I think.

01:09:50.000 --> 01:09:55.120
I think it is much harder to estimate something like ELO rating in poker. I think it's doable.

01:09:55.120 --> 01:10:00.800
The problem is that the game is very high variance. So you could be profitable in poker

01:10:00.800 --> 01:10:05.040
for a year, and you could actually be a bad player just because the variance is so high.

01:10:05.040 --> 01:10:08.160
I mean, you've got top professional poker players that would lose for a year

01:10:08.880 --> 01:10:16.080
just because they're on a really bad streak. So for ELO, you have to have a nice clean way of saying

01:10:16.080 --> 01:10:22.640
if player A played player B and A beats B, that says something. That's a signal.

01:10:22.640 --> 01:10:26.880
In poker, that's a very noisy signal. It's a very noisy signal. Now, there is a signal there. So

01:10:26.880 --> 01:10:33.840
you could do this calculation. It would just be much harder. But the same way that AIs have now

01:10:33.840 --> 01:10:40.880
taken over chess and all the top professional chess players train with AIs, the same is true

01:10:40.880 --> 01:10:47.440
for poker. The game has become a very computational. People train with AIs to

01:10:47.440 --> 01:10:51.920
try to find out where they're making mistakes, try to learn from the AIs to improve their strategy.

01:10:55.920 --> 01:11:00.800
The game has been revolutionized in the past five years by the development of AI in this sport.

01:11:00.800 --> 01:11:05.360
The skill with which you avoided the question of the greatest of all time was impressive.

01:11:05.360 --> 01:11:11.520
My feeling is that it's a difficult question because just like in chess where you can't really

01:11:11.520 --> 01:11:18.960
compare Magnus Carlsen today to Garry Kasparov because the game has evolved so much, the poker

01:11:18.960 --> 01:11:26.560
players today are so far beyond the skills of people that were playing even 10 or 20 years ago.

01:11:27.360 --> 01:11:32.240
So you look at the kinds of all-stars that were on ESPN at the height of the poker boom,

01:11:33.120 --> 01:11:38.880
pretty much all those players are actually not that good at the game today, at least the strategy

01:11:38.880 --> 01:11:43.440
aspect. I mean, they might still be good at reading the player at the other side of the table and

01:11:43.440 --> 01:11:48.240
trying to figure out are they bluffing or not. But in terms of the actual computational strategy

01:11:48.240 --> 01:11:54.160
of the game, a lot of them have really struggled to keep up with that development. Now, so for

01:11:54.160 --> 01:11:58.960
that reason, I'll give an answer and I'm going to say Daniel Legranio who you actually had on

01:11:58.960 --> 01:12:04.080
the podcast recently, I saw, it was a great episode. He's going to love this so much and Phil's

01:12:04.080 --> 01:12:11.520
going to hate this so much. And I'm going to give him credit because he is one of the few old school

01:12:11.520 --> 01:12:17.760
really strong players that have kept up with the development of AI. So he's constantly studying

01:12:17.760 --> 01:12:22.880
the game theory optimal way of playing. Exactly, yeah. And I think a lot of the old school poker

01:12:22.880 --> 01:12:27.440
players have just kind of given up on that aspect and I got to give Daniel Legranio credit for

01:12:27.440 --> 01:12:32.000
keeping up with all the developments that are happening in the sport. Yeah, it's fascinating

01:12:32.000 --> 01:12:38.000
to watch. It's fascinating to watch where it's headed. Yeah, so there you go. Some love for Daniel.

01:12:39.120 --> 01:12:44.320
Quick pause. Bath and break? Yeah, let's do it. Let's go from poker to diplomacy.

01:12:45.360 --> 01:12:51.040
What is at a high level the game of diplomacy? Yeah, so I talked a lot about two players,

01:12:51.040 --> 01:12:56.800
zero sum games. And what's interesting about diplomacy is that it's very different from

01:12:56.800 --> 01:13:04.000
these adversarial games like chess, go poker, even StarCraft and Dota. Diplomacy has a much

01:13:04.000 --> 01:13:09.040
bigger cooperative element to it. It's a seven player game. It was actually created in the 50s

01:13:10.080 --> 01:13:15.760
and it takes place before World War I. It's like a map of Europe with seven great powers

01:13:16.720 --> 01:13:22.080
and they're all trying to form alliances with each other. There's a lot of negotiation going on.

01:13:22.640 --> 01:13:29.360
And so the whole focus of the game is on forming alliances with the other players to take on the

01:13:29.360 --> 01:13:35.760
other players. England, Germany, Russia, Turkey, Austria, Hungary, Italy and France. That's right.

01:13:36.720 --> 01:13:44.240
So the way the game works is on each turn you spend about five to 15 minutes talking to the

01:13:44.240 --> 01:13:49.600
other players in privates and you make all sorts of deals with them. You say like, hey, let's work

01:13:49.600 --> 01:13:54.560
together. Let's team up against this other player because the only way that you can make progress

01:13:54.560 --> 01:14:00.320
is by working with somebody else against the others. And then after that negotiation period

01:14:00.320 --> 01:14:06.800
is done, all the players simultaneously submit their moves and they're all executed at the same

01:14:06.800 --> 01:14:12.080
time. And so you can tell people like, hey, I'm going to support you this turn but then you don't

01:14:12.080 --> 01:14:16.320
follow through with it. And they're only going to figure that out once they see the moves being

01:14:16.320 --> 01:14:21.680
read off. How much of it is natural language, like written actual text, how much is like

01:14:22.720 --> 01:14:27.920
you're actually saying phrases that are structured? So there's different ways to play the game. You

01:14:27.920 --> 01:14:32.160
know, you can play it in person. And in that case, it's all natural language, free form

01:14:32.160 --> 01:14:35.360
communication. There's no constraints on the kinds of deals that you can make, the kinds of things

01:14:35.360 --> 01:14:41.040
that you can discuss. You can also play it online. So you can, you know, send long emails back and

01:14:41.040 --> 01:14:48.320
forth. You can play it like live online or over voice chats. But the focus, the important thing

01:14:48.320 --> 01:14:52.000
to understand is that this is unstructured communication. You can say whatever you want.

01:14:52.800 --> 01:14:58.000
You can make any sorts of deals that you want and everything is done privately. So it's not like

01:14:58.000 --> 01:15:02.480
you're all around the board together having a conversation. You're grabbing somebody going off

01:15:02.480 --> 01:15:07.120
into a corner and conspiring behind everybody else's back about what you're planning. And

01:15:07.840 --> 01:15:12.400
there's no limit in theory to the conversation you can have directly with one person.

01:15:12.400 --> 01:15:15.840
That's right. You can make all sorts of, you can talk about anything. You could say like,

01:15:15.840 --> 01:15:18.720
hey, let's have a long-term alliance against this guy. You can say like, hey,

01:15:18.720 --> 01:15:22.880
can you support me this turn in return? I'll do this other thing for you next turn.

01:15:22.880 --> 01:15:27.600
Or, you know, yeah, just you can talk about like what you talked about with somebody else

01:15:27.600 --> 01:15:32.320
and gossip about like what they're planning. The way that I would describe the game is that it's

01:15:32.320 --> 01:15:38.720
kind of like a mix between risk, poker, and the TV show Survivor. There's like this big element

01:15:38.720 --> 01:15:44.960
of like trying to, yeah, there's a big social element. And the best way that I would describe

01:15:44.960 --> 01:15:48.320
the game is that it's really a game about people rather than the pieces.

01:15:49.760 --> 01:15:58.000
So risk, because it is a map, it's kind of war game like. Poker, because there's a game theory

01:15:58.000 --> 01:16:03.120
component that's very kind of strategic. So you could convert it into an artificial intelligence

01:16:03.120 --> 01:16:07.680
problem and then survive it because of the social component. That's a strong social component.

01:16:07.680 --> 01:16:14.400
I saw that somebody said online that the internet version of the game has this quality of that it's

01:16:14.400 --> 01:16:21.200
easier to almost to do like role playing as opposed to being yourself. You can actually like be the

01:16:21.200 --> 01:16:27.120
like really imagine yourself as the leader of France or Russia and so on, like really pretend

01:16:27.120 --> 01:16:32.720
to be that person. It's actually fun to really lean into being that leader.

01:16:32.720 --> 01:16:36.880
Yeah. So some players do go this route where they just like kind of view it as a strategy game,

01:16:36.880 --> 01:16:40.880
but also a role playing game where they can like act out like, what would I be like if I was,

01:16:40.880 --> 01:16:43.600
you know, a leader of France in 1900?

01:16:43.600 --> 01:16:49.520
Forfeit right away. No, I'm just kidding. And they sometimes use like the old-timey language

01:16:50.080 --> 01:16:56.880
to like, or how they imagine the elites would talk at that time. Anyway, so what are the different

01:16:56.880 --> 01:16:58.720
turns of the game? Like what are the rounds?

01:16:59.520 --> 01:17:04.080
Yeah. So on every turn, you've got like a bunch of different units that you start out with. So

01:17:04.080 --> 01:17:09.600
you start out controlling like just a few units and the object of the game is to gain control of

01:17:09.600 --> 01:17:13.680
the majority of the map. If you're able to do that, then you've won the game. But like I said,

01:17:13.680 --> 01:17:17.600
the only way that you're able to do that is by working with other players. So on every turn,

01:17:17.600 --> 01:17:23.120
you can issue a move order. So for each of your units, you can move them to an adjacent territory

01:17:23.840 --> 01:17:30.800
or you can keep them where they are, or you can support a move or a hold of a different unit.

01:17:30.800 --> 01:17:34.160
So what are the territories? How is the map divided up?

01:17:34.160 --> 01:17:38.560
It's kind of like risk where the map is divided up into like 50 different territories.

01:17:40.080 --> 01:17:45.680
Now you can enter a territory if you're moving into that territory with more supports than the

01:17:45.680 --> 01:17:50.000
person that's in there or the person that's trying to move in there. So if you're moving in and

01:17:50.000 --> 01:17:54.320
there's somebody already there, then if neither of you have support, it's a one versus one and

01:17:54.320 --> 01:17:58.480
you'll bounce back and neither of you will make progress. If you have a unit that's supporting

01:17:58.480 --> 01:18:02.400
that move into the territory, then it's a two versus one and you'll kick them out and they'll

01:18:02.400 --> 01:18:04.800
have to retreat somewhere. What does support mean?

01:18:04.800 --> 01:18:09.120
Support is like, it's an action that you can issue in the game. So you can say this unit,

01:18:09.120 --> 01:18:12.560
you write down this unit is supporting this other unit into this territory.

01:18:13.360 --> 01:18:17.520
Are these units from opposing forces? They could be, they could be. And this is

01:18:17.520 --> 01:18:21.360
where the interesting aspect of the game comes in because you can support your own units into

01:18:21.360 --> 01:18:25.600
territory, but you can also support other people's units into territories. And so that's

01:18:25.600 --> 01:18:30.720
what the negotiations really revolve around. But you don't have to do the thing you say

01:18:30.720 --> 01:18:36.800
you're going to do. So you can say, I'm going to support you, but then backstab the person.

01:18:36.800 --> 01:18:40.800
Yeah, that's absolutely right. And that tension is core to the game?

01:18:40.800 --> 01:18:46.560
That tension is absolutely core to the game. The fact that you can make all sorts of promises,

01:18:46.560 --> 01:18:50.720
but you have to reason about the fact that like, hey, they might not trust you if you say you're

01:18:50.720 --> 01:18:55.440
going to do something or they might be lying to you when they say they're going to support you.

01:18:56.240 --> 01:19:01.200
So maybe just to jump back, what's the history of the game in general?

01:19:01.200 --> 01:19:06.160
Is it true that Henry Kissinger loved the game and JFK and all those? I've heard like a bunch

01:19:06.160 --> 01:19:10.240
of different people that, or is that just one of those things that the cool kids say they do,

01:19:10.240 --> 01:19:13.120
but they don't actually play? So the game was created in the 50s.

01:19:13.280 --> 01:19:16.400
Yeah. And from what I understand, it was

01:19:17.120 --> 01:19:20.960
JFK's, it was played in like the JFK White House, Henry Kissinger's favorite game.

01:19:20.960 --> 01:19:23.680
I don't know if it's true, but that's definitely what I've heard.

01:19:23.680 --> 01:19:28.960
It's interesting that they went with World War I when it was created after World War II.

01:19:29.520 --> 01:19:36.000
So the story that I've heard for the creation of the game is it was created by somebody that had

01:19:36.000 --> 01:19:42.720
looked at the history of the 20th century and they saw World War I as a failure of diplomacy.

01:19:43.200 --> 01:19:50.000
So they saw the fact that this war broke out as the diplomats of all these countries really

01:19:50.000 --> 01:19:54.080
failed to prevent a war and he wanted to create a game that would basically teach people about

01:19:54.080 --> 01:20:01.760
diplomacy. And it's really fascinating that in his ideal version of the game of diplomacy,

01:20:01.760 --> 01:20:05.760
nobody actually wins the game because the whole point is that if somebody is about to win,

01:20:05.760 --> 01:20:10.080
then the other players should be able to work together to stop that person from winning.

01:20:10.160 --> 01:20:15.200
And so the ideal version of the game is just one where nobody actually wins. And it kind of has a

01:20:15.200 --> 01:20:19.520
nice like wholesome take home message then that war is ultimately futile.

01:20:21.760 --> 01:20:27.680
And that futile optimal could be achieved through great diplomacy.

01:20:28.800 --> 01:20:35.520
So is there some asymmetry in terms of which is more powerful, Russia versus Germany versus

01:20:36.480 --> 01:20:42.000
France and so on? So I think the general consensus is that France is the strongest power in the game,

01:20:42.000 --> 01:20:46.640
but the beautiful thing about diplomacy is that it's self-balancing, right? So the fact that

01:20:46.640 --> 01:20:51.680
France has an inherited advantage from the beginning means that the other players are less

01:20:51.680 --> 01:20:56.960
likely to work with it. I saw that Russia has four units or four of something that the others have

01:20:56.960 --> 01:21:00.560
three of something. That's true. Yeah. So Russia starts off with four units while all the other

01:21:00.560 --> 01:21:05.040
players start with three, but Russia is also in a much more vulnerable position because they have

01:21:05.040 --> 01:21:11.200
to like, they have a lot more neighbors as well. Got it. Larger territory, more, yeah, right. More

01:21:11.200 --> 01:21:19.600
border to defend. Okay. What else is important to know about the rules? So how many rounds are

01:21:19.600 --> 01:21:25.840
there? Like is this iterative game? Is it finite? Do you just keep going indefinitely? Usually the

01:21:25.840 --> 01:21:33.360
game lasts, I would say about 15 or 20 turns. There's in theory no limit. It could last longer,

01:21:33.360 --> 01:21:36.400
but at some point, I mean, if you're playing a house game with friends, at some point you just

01:21:36.400 --> 01:21:39.680
get tired and you all agree like, okay, we're going to end the game here and call it a draw.

01:21:41.120 --> 01:21:44.880
If you're playing online, there's usually like set limits on when the game will actually end.

01:21:44.880 --> 01:21:51.440
And what's the end? What's the termination condition? Like does one country have to conquer

01:21:51.440 --> 01:21:56.160
everything else? So if somebody is able to actually gain control of a majority of the map,

01:21:56.160 --> 01:22:00.960
then they've won the game and that is a solo victory as it's called. Now that pretty rarely

01:22:00.960 --> 01:22:04.480
happens, especially with strong players, because like I said, the game is designed

01:22:04.480 --> 01:22:08.960
to incentivize the other players to put a stop to that and all work together to stop the superpower.

01:22:10.560 --> 01:22:16.800
Usually what ends up happening is that all the players agree to a draw and then the score,

01:22:16.800 --> 01:22:21.680
the win is divided among the remaining players. There's a lot of different scoring systems. The

01:22:21.680 --> 01:22:29.360
one that we used in our research basically gives a score relative to how much control you have of

01:22:29.360 --> 01:22:34.800
the map. So the more that you control, the higher your score. What's the history of using this game

01:22:35.440 --> 01:22:42.080
as a benchmark for AI research? Do people use it? Yeah. So people have been working on AI for

01:22:42.080 --> 01:22:47.680
diplomacy since about the eighties. There was some really exciting research back then, but

01:22:48.320 --> 01:22:52.080
the approach that was taken was very different from what we see today. I mean, the research

01:22:52.080 --> 01:22:56.800
in the eighties was a very rule-based approach, kind of a heuristic approach. It was very in line

01:22:56.800 --> 01:23:01.040
with the kind of research that was being done in the eighties, basically trying to encode human

01:23:01.040 --> 01:23:07.120
knowledge into the strategy of the AI. Sure. And it's understandable. I mean, the game is so

01:23:07.120 --> 01:23:13.520
incredibly different and so much more complicated than the kinds of games that people working on,

01:23:13.520 --> 01:23:22.160
like chess and go and poker, that it was honestly even hard to start making any progress in diplomacy.

01:23:22.160 --> 01:23:27.440
Can you just formulate what is the problem from an AI perspective and why is it hard?

01:23:27.440 --> 01:23:32.000
Why is it a challenging game to solve? So there's a lot of aspects in diplomacy that make it

01:23:32.000 --> 01:23:37.280
a huge challenge. First of all, you have the natural language component. And I think this

01:23:37.280 --> 01:23:43.760
really is what makes it arguably the most difficult game among the major benchmarks.

01:23:43.760 --> 01:23:51.440
The fact that you have to, it's not about moving pieces on the board. Your action space is basically

01:23:51.440 --> 01:23:55.600
all the different sentences that you could communicate to somebody else in this game.

01:23:58.000 --> 01:24:06.880
Can we just linger on that? So is part of it the ambiguity in the language? If it was very strict,

01:24:07.840 --> 01:24:11.440
if you narrowed the set of possible sentences you could do, would that simplify the game

01:24:11.440 --> 01:24:17.440
significantly? The real difficulty is the breadth of things that you can talk about.

01:24:18.400 --> 01:24:21.840
You can have natural language in other games, like Settlers of Catan, for example,

01:24:21.840 --> 01:24:26.000
you could have a natural language Settlers of Catan AI. But the things that you're going to

01:24:26.000 --> 01:24:29.760
talk about are basically like, am I trading you two sheep for a wood or three sheep for a wood?

01:24:31.120 --> 01:24:36.400
Whereas in a game like Diplomacy, the breadth of conversations that you're going to have are like,

01:24:36.400 --> 01:24:40.960
am I going to support you? Are you going to support me in return? Which units are going to do what?

01:24:41.600 --> 01:24:46.800
What did this other person promise you? They're lying because they told this other person that

01:24:46.960 --> 01:24:51.040
they're going to do this instead. If you help me out this turn, then in the future, I'll

01:24:51.600 --> 01:24:59.600
do these things that will help you out. The depth and breadth of these conversations is really

01:24:59.600 --> 01:25:05.520
complicated and it's all being done in natural language. Now you could approach it, and we

01:25:05.520 --> 01:25:12.960
actually consider doing this, having a simplified language to make this complexity smaller. But

01:25:12.960 --> 01:25:19.440
ultimately, we thought the most impactful way of doing this research would be to address the

01:25:19.440 --> 01:25:23.600
natural language component head on and just try to go for the full game up front.

01:25:24.160 --> 01:25:28.880
Just looking at sample games and what the conversations look like. Greetings England,

01:25:28.880 --> 01:25:34.080
this should prove to be a fun game since all the private press is going to be made public at the

01:25:34.080 --> 01:25:39.520
end. At the least, it will be interesting to see if the press changes because of that anyway.

01:25:40.240 --> 01:25:44.000
So there's like... Yeah, that's just kind of like the generic greetings at the beginning of the

01:25:44.000 --> 01:25:48.400
game. I think that the meat comes a little bit later when you're starting to talk about specific

01:25:48.400 --> 01:25:54.480
strategy and stuff. I agree. There are a lot of advantages to the two of us keeping in touch,

01:25:54.480 --> 01:26:00.720
and our nations make strong natural allies in the middle game. So that kind of stuff. Making

01:26:00.720 --> 01:26:04.880
friends, making enemies. Yeah. If you look at the next line, so the person's saying like,

01:26:04.880 --> 01:26:10.080
I've heard bits about a Lepanto and an octopus opening and basically telling Austria like,

01:26:10.080 --> 01:26:14.880
hey, just a heads up. I've heard these whispers about what might be going on behind your back.

01:26:14.880 --> 01:26:24.080
Yeah. So there's all kinds of complexities in the language of that, right? To interpret what

01:26:24.080 --> 01:26:28.080
the heck that means. It's hard for us humans, but for AI it's even harder because you have to

01:26:28.080 --> 01:26:33.520
understand at every level the semantics of that. Right. I mean, there's the complexity and

01:26:33.520 --> 01:26:37.280
understanding when somebody is saying this to me, what does that mean? And then there's also the

01:26:37.280 --> 01:26:41.920
complexity of like, should I be telling this person this? Like I've overheard these whispers.

01:26:41.920 --> 01:26:46.000
Should I be telling this person that like, hey, you might be getting attacked by this other power?

01:26:46.640 --> 01:26:53.200
Okay. So what, how are we supposed to think about? Okay. So that's the natural language.

01:26:54.080 --> 01:27:00.000
How do you even begin trying to solve this game? It seems like the towing test on steroids.

01:27:00.000 --> 01:27:03.040
Yeah. And I mean, there's the natural language aspect. And then even besides

01:27:03.040 --> 01:27:07.840
the natural language aspect, you also have the cooperative elements of the game. And I think

01:27:07.840 --> 01:27:13.200
this is actually something that I find really interesting. If you look at all the previous game

01:27:13.200 --> 01:27:18.160
AI breakthroughs, they've all happened in these purely adversarial games where you don't actually

01:27:18.160 --> 01:27:23.600
need to understand how humans play the game. It's all just AI versus AI, right? Like you look at

01:27:24.960 --> 01:27:32.240
checkers, chess, Go, poker, StarCraft, Dota 2. Like in some of those cases, they leveraged human data,

01:27:32.240 --> 01:27:37.760
but they never needed to. They were always just trying to have a scalable algorithm

01:27:38.480 --> 01:27:42.880
that then they could throw a lot of computational resources at, a lot of memory at,

01:27:42.880 --> 01:27:47.840
and then eventually it would converge to an approximation of a Nash equilibrium. This

01:27:48.720 --> 01:27:53.360
perfect strategy that in a two-player zero-sum game guarantees that they're going to be able

01:27:53.360 --> 01:27:58.240
to not lose to any opponent. So you can't leverage self-play to solve this game.

01:27:58.560 --> 01:28:02.560
You can leverage self-play, but it's no longer sufficient to beat humans.

01:28:02.560 --> 01:28:05.120
So how do you integrate the human into the loop of this?

01:28:05.120 --> 01:28:11.120
So what you have to do is incorporate human data. And to kind of give you some intuition for why

01:28:11.120 --> 01:28:15.920
this is the case. Imagine you're playing a negotiation game like diplomacy, but you're

01:28:15.920 --> 01:28:23.040
training completely from scratch without any human data. The AI is not going to suddenly figure out

01:28:23.040 --> 01:28:27.600
how to communicate in English. It's going to figure out some weird robot language that only

01:28:27.600 --> 01:28:32.320
it will understand. And then when you stick that in a game with six other humans, they're going to

01:28:32.880 --> 01:28:35.760
think this person's talking gibberish and they're just going to ally with each other and team up

01:28:35.760 --> 01:28:40.320
against the bot. Or not even team up against the bot, but just not work with the bot. And so in

01:28:40.320 --> 01:28:45.600
order to be able to play this game with humans, it has to understand the human way of playing the

01:28:45.600 --> 01:28:51.360
game, not this machine way of playing the game. Yeah. Yeah, that's fascinating. So right.

01:28:52.320 --> 01:28:59.280
That's a nuanced thing to understand because a chess playing program doesn't need to play

01:28:59.280 --> 01:29:04.480
like a human to beat a human. Exactly. But here you have to play like a human in order to beat them.

01:29:04.480 --> 01:29:08.160
Or at least you have to understand how humans play the game so that you can understand how to work

01:29:08.160 --> 01:29:13.040
with them. If they have certain expectations about what does it mean to be a good ally?

01:29:13.040 --> 01:29:17.360
What does it mean to have like a reciprocal relationship where we're working together?

01:29:17.360 --> 01:29:21.360
You have to abide by those conventions. And if you don't, they're just going to work with

01:29:21.360 --> 01:29:28.080
somebody else instead. Do you think of this as a clean, in some deep sense of the spirit of the

01:29:28.080 --> 01:29:34.160
Turing test as formulated by Alan Turing? Is it some sense this is what the Turing test actually

01:29:34.160 --> 01:29:42.800
looks like? So because of open ended natural language conversation seems like very difficult

01:29:42.880 --> 01:29:47.680
to evaluate. Like here at a high stakes where humans are trying to win a game,

01:29:47.680 --> 01:29:51.280
that seems like how you actually perform the Turing test.

01:29:51.840 --> 01:29:55.840
I think it's different from the Turing test. Like the way that the Turing test is formulated,

01:29:55.840 --> 01:30:00.800
it's about trying to distinguish a human from a machine and seeing, oh, could the machine

01:30:01.680 --> 01:30:07.200
successfully pass as a human in this adversarial setting where the player is trying to figure out

01:30:07.200 --> 01:30:11.840
whether it's a machine or a human. Whereas in diplomacy, it's not about trying to figure out

01:30:11.840 --> 01:30:16.960
whether this player is a human or a machine. It's ultimately about whether I can work with this

01:30:16.960 --> 01:30:21.280
player, regardless of whether they are a human or a machine. And can the machine

01:30:21.280 --> 01:30:27.280
do that better than a human can? Yeah, I'm going to think about that, but that just feels like

01:30:28.400 --> 01:30:33.120
the implied requirement for that is for the machine to be human-like.

01:30:34.320 --> 01:30:38.240
I think that's true, that if you're going to play in this human game,

01:30:39.120 --> 01:30:44.640
you have to somehow adapt to the human surroundings and the human play style.

01:30:44.640 --> 01:30:48.960
And to win, you have to adapt. So you can't, if you're the outsider,

01:30:50.000 --> 01:30:53.680
if you're not human-like, I feel like that's a losing strategy.

01:30:53.680 --> 01:30:55.760
I think that's correct. Yeah.

01:30:55.760 --> 01:31:01.760
Yeah. So, okay. What are the complexities here? What was your approach to it?

01:31:02.560 --> 01:31:06.560
Before I get to that, one thing I should explain, like why we decided to work on diplomacy.

01:31:07.280 --> 01:31:14.160
So basically what happened is in 2019, I was wrapping up the work on Six-Player Poker on

01:31:14.160 --> 01:31:20.320
Pluribus and was trying to think about what to work on next. And I had been seeing all these other

01:31:20.320 --> 01:31:25.840
breakthroughs happening in AI. I mean, like 2019, you have StarCraft, you have AlphaStar beating

01:31:25.840 --> 01:31:32.400
humans in StarCraft. You've got the Dota 2 stuff happening at OpenAI. You have GPT-2 or GPT-3,

01:31:32.400 --> 01:31:37.840
I think it was GPT-2 at the time. And it became clear that AI was progressing really,

01:31:37.840 --> 01:31:44.240
really rapidly. And people were throwing out these other games about what should be the next

01:31:44.240 --> 01:31:52.560
challenge for multi-agent AI. And I just felt like we had to aim bigger. If you look at a game like

01:31:52.560 --> 01:31:58.720
chess or a game like Go, they took decades for researchers to ultimately reach superhuman

01:31:58.720 --> 01:32:03.680
performance at. I mean, chess took 40 years of AI research. Go took another 20 years.

01:32:05.280 --> 01:32:11.200
And we thought that diplomacy would be this incredibly difficult challenge that could

01:32:11.200 --> 01:32:16.960
easily take a decade to make an AI that could play competently. But we felt like that was a

01:32:16.960 --> 01:32:22.960
goal worth aiming for. And so honestly, I was kind of reluctant to work on it at first because I

01:32:22.960 --> 01:32:27.760
thought it was too far out of the realm of possibility. But I was talking to a co-worker

01:32:27.760 --> 01:32:31.760
of mine, Adam Learer, and he was basically saying like, why not aim for it? We'll learn

01:32:31.760 --> 01:32:36.240
some interesting things along the way and maybe it'll be possible. And so we decided to go for it.

01:32:36.240 --> 01:32:43.440
And I think it was the right choice considering just how much progress there was in AI and that

01:32:43.440 --> 01:32:48.560
progress has continued in the years since. So winning in diplomacy, what does that

01:32:48.560 --> 01:32:56.560
really look like? It means talking to six other players, six other entities, agents,

01:32:57.280 --> 01:33:05.200
and convincing them of stuff that you want them to be convinced of. What exactly? I'm trying to get

01:33:05.920 --> 01:33:13.120
to deeply understand what the problem is. Ultimately, the problem is it's simple to

01:33:13.120 --> 01:33:19.040
quantify. You're going to play this game with humans and you want your score on average to be

01:33:19.760 --> 01:33:25.440
as high as possible. If you can say like, I am winning more than any human alive,

01:33:26.880 --> 01:33:32.320
you're a champion diplomacy player. Ultimately, we didn't reach that. We got to human level

01:33:32.320 --> 01:33:39.920
performance. We played about 40 games with real humans online. The bot came in second out of all

01:33:39.920 --> 01:33:46.720
players that played five or more games. So not like number one, but way, way higher than that.

01:33:46.720 --> 01:33:51.840
What was the expertise level? Are they beginners? Are they intermediate players, advanced players?

01:33:52.800 --> 01:33:58.000
That's a great question. I think this goes into how do you measure the performance in diplomacy?

01:33:58.000 --> 01:34:01.600
I would argue that when you're measuring performance in a game like this, you don't

01:34:01.600 --> 01:34:06.960
actually want to measure it in games with all expert players. It's like if you're developing

01:34:06.960 --> 01:34:12.320
a self-driving car, you don't want to measure that car on the road with a bunch of expert stunt

01:34:12.320 --> 01:34:19.360
drivers. You want to put it on a road of an actual American city and see is this car crashing less

01:34:19.360 --> 01:34:24.880
often than an expert driver would. So that's the metric that we've used. We're saying like,

01:34:24.880 --> 01:34:29.120
we're going to stick this game, we're going to stick this bot in games with a wide variety

01:34:29.120 --> 01:34:34.880
of skill levels. And then are we doing better than a strong or expert human player would in

01:34:34.880 --> 01:34:40.640
the same situation? That's quite brilliant because I played a lot of sports in my life, like tennis,

01:34:40.640 --> 01:34:48.160
judo, whatever. And it's somehow almost easier to go against experts almost always. I think they're

01:34:48.160 --> 01:34:54.480
more predictable in the quality of play. The space of strategies you're operating under is

01:34:54.480 --> 01:34:59.360
narrower against experts. It's more fun. It's really frustrating to go against beginners.

01:34:59.360 --> 01:35:05.040
Also, because beginners talk trash to you when they somehow do beat you. So that's a human thing

01:35:05.040 --> 01:35:10.560
that they had to be worried about that. But yeah, the variance in strategies is greater,

01:35:10.560 --> 01:35:13.360
especially with natural language. It's just all over the place then.

01:35:14.000 --> 01:35:19.520
Yeah. And honestly, when you look at what makes a good human diplomacy player,

01:35:20.400 --> 01:35:23.920
obviously they're able to handle themselves in games with other expert humans, but where they

01:35:23.920 --> 01:35:28.400
really shine is when they're playing with these weak players. And they know how to take advantage

01:35:29.040 --> 01:35:33.120
of the fact that they're a weak player, that they won't be able to pull off a stab as well,

01:35:33.120 --> 01:35:37.360
or that they have certain tendencies and they can take them under their wing and persuade them to

01:35:37.360 --> 01:35:42.400
do things that might not even be in their interest. The really good diplomacy players

01:35:42.400 --> 01:35:47.200
are able to take advantage of the fact that there are some weak players in the game.

01:35:47.200 --> 01:35:52.160
Okay. So if you have to incorporate human play data, how do you do that? How do you do that

01:35:52.160 --> 01:35:58.560
in order to train an AI system to play diplomacy? Yeah. So that's really the crux of the problem.

01:35:58.560 --> 01:36:04.960
How do we leverage the benefits of self-play that have been so successful in all these other

01:36:04.960 --> 01:36:12.160
previous games while keeping the strategy as human compatible as possible? And so what we did

01:36:12.160 --> 01:36:18.640
is we first trained a language model and then we made that language model controllable on a set of

01:36:19.600 --> 01:36:24.000
intents, what we call intents, which are basically like an action that we want to play

01:36:24.000 --> 01:36:28.800
and an action that we would like the other player to play. And so this gives us a way to generate

01:36:28.800 --> 01:36:33.680
dialogue that's not just trying to imitate the human style, whatever a human would say in the

01:36:33.680 --> 01:36:39.600
situation, but to actually give it an intent, a purpose in its communication. We can talk about

01:36:39.600 --> 01:36:45.600
a specific move or we can make a specific request and the determination of what that move is that

01:36:45.600 --> 01:36:52.080
we're discussing comes from a strategic reasoning model that uses reinforcement learning and

01:36:52.080 --> 01:37:01.280
planning. So the computing the intents for all the players, how's that done? Just as a starting

01:37:01.280 --> 01:37:05.760
point, is that with reinforcement learning or is that just optimal determining what the optimal

01:37:05.760 --> 01:37:12.320
is for intents? It's a combination of reinforcement learning and planning, actually very similar to

01:37:12.320 --> 01:37:19.520
how we approached poker and how people approached chess and Go as well. We're using self-play

01:37:20.480 --> 01:37:27.040
and search to try to figure out what is an optimal move for us and what is a desirable move that we

01:37:27.040 --> 01:37:32.880
would like this other player to play. Now, the difference between the way that we approached

01:37:32.880 --> 01:37:37.680
reinforcement learning and search in this game versus those previous games is that we have to

01:37:37.680 --> 01:37:42.800
keep it human compatible. We have to understand how the other person is likely to play rather

01:37:42.800 --> 01:37:47.840
than just assuming that they're going to play like a machine. And how language gets them to play

01:37:49.760 --> 01:37:53.840
in a way that maximizes the chance of following the intent you want them to follow.

01:37:53.840 --> 01:38:00.720
Okay, how do you do that? How do you connect language to intent? So the way that RL and

01:38:00.720 --> 01:38:06.560
planning is done is actually not using language. So we're coming up with this plan for the action

01:38:07.520 --> 01:38:10.560
that we're going to play and the other person's going to play and then we feed that action into

01:38:10.560 --> 01:38:15.040
the dialogue model that will then send a message according to those plans. So the language model

01:38:15.040 --> 01:38:26.000
there is mapping action to message one word at a time. Basically, one message at a time. So we'll

01:38:26.000 --> 01:38:29.600
feed into the dialogue model like here are the actions that you should be discussing. Here's the

01:38:32.560 --> 01:38:36.880
content of the message that we would like you to send and then it will actually generate a message

01:38:36.880 --> 01:38:41.120
that corresponds to that. Okay, does this actually work? It works surprisingly well.

01:38:41.120 --> 01:38:48.960
Okay. Oh man, the number of ways it probably goes horribly. I would have imagined it goes

01:38:48.960 --> 01:38:55.120
horribly wrong. So how the heck is it effective at all? I mean, there are a lot of ways that this

01:38:55.120 --> 01:39:01.040
could fail. So for example, I mean, you could have a situation where you're basically like,

01:39:02.480 --> 01:39:07.440
we don't tell the language model like here are the pieces of our action or the other person's

01:39:07.520 --> 01:39:11.360
action that you should be communicating. And so like, let's say you're about to attack somebody,

01:39:11.360 --> 01:39:15.360
you probably don't want to tell them that you're going to attack them, but there's nothing in the

01:39:15.360 --> 01:39:18.640
language. Like the language model is not very smart at the end of the day. So it doesn't really

01:39:18.640 --> 01:39:22.480
have a way of knowing like, well, what should I be talking about? Should I tell this person that I'm

01:39:22.480 --> 01:39:28.160
about to attack them or not? So we have to like develop a lot of other techniques that deal with

01:39:28.160 --> 01:39:33.600
that. Like one of the things we do, for example, is we try to calculate if I'm going to send this

01:39:33.600 --> 01:39:38.480
message, what would I expect the other person to do in response? So if it's a message like,

01:39:38.480 --> 01:39:43.600
hey, I'm going to attack you this turn, they're probably going to attack us or defend against

01:39:43.600 --> 01:39:50.400
that attack. And so we have a way of recognizing like, hey, sending this message is a negative

01:39:50.400 --> 01:39:57.040
expected value action and we should not send this message. So yes, for particular kinds of messages,

01:39:57.040 --> 01:40:03.120
you have like an extra function that estimates the value of that message.

01:40:03.120 --> 01:40:05.600
Yeah. So we have these kinds of filters that like-

01:40:05.600 --> 01:40:11.280
So it's a filter. So is that filter in your network or is it rule-based?

01:40:12.240 --> 01:40:16.720
That's a neural network. So it's a combination. It's a neural network, but it's also using

01:40:16.720 --> 01:40:23.200
planning. It's trying to compute like, what is the policy that the other players are going to

01:40:23.200 --> 01:40:28.080
play given that this message has been sent? And then is that better than not sending the

01:40:28.080 --> 01:40:32.880
message? I feel like that's how my brain works too. Like there's a language model that generates

01:40:32.880 --> 01:40:39.280
random crap and then there's these other neural nets that are essentially filters. At least that's

01:40:39.280 --> 01:40:46.320
when I tweet. Usually my process of tweeting, I'll think of something and it's hilarious to me. And

01:40:46.320 --> 01:40:52.320
then about five seconds later, the filter network comes in and says, no, no, that's not funny at all.

01:40:52.320 --> 01:40:56.720
I mean, there's something interesting to that kind of process. So you have a set of actions that

01:40:58.720 --> 01:41:03.840
you have an intent that you want to achieve, an intent that you want your opponent to achieve,

01:41:03.840 --> 01:41:12.880
then you generate messages and then you evaluated those messages will achieve the goal you want.

01:41:13.440 --> 01:41:17.840
Yeah. And we're filtering for several things. We're filtering like, is this a sensible message?

01:41:17.840 --> 01:41:23.760
So sometimes language models will generate messages that are just like totally nonsense and

01:41:23.760 --> 01:41:30.880
we try to filter those out. We also try to filter out messages that are basically lies. So diplomacy

01:41:30.880 --> 01:41:37.200
has this reputation as a game that's really about deception and lying, but we try to actually

01:41:37.200 --> 01:41:42.320
minimize the amount that the bot would lie. This was actually mostly a-

01:41:42.320 --> 01:41:44.160
Or are you? No, I'm just kidding.

01:41:44.640 --> 01:41:52.080
I mean, like part of the reason for this is that we actually found that lying would make the bot

01:41:52.080 --> 01:41:56.400
perform worse in the long run. It would end up with a lower score because once the bot lies,

01:41:57.680 --> 01:42:01.760
people would never trust it again. And trust is a huge aspect of the game of diplomacy.

01:42:01.760 --> 01:42:06.720
I'm taking notes here because I think this applies to life lessons too.

01:42:06.720 --> 01:42:08.400
Oh, I think it's a really, yeah, really strong-

01:42:08.400 --> 01:42:14.320
So like lying is a dangerous thing to do. Like you want to avoid obvious lying.

01:42:14.880 --> 01:42:19.600
Yeah. I mean, I think when people play diplomacy for the first time, they approach it as a game

01:42:19.600 --> 01:42:25.840
of deception and lying. And ultimately if you talk to top diplomacy players, what they'll tell you

01:42:25.840 --> 01:42:30.400
is that diplomacy is a game about trust and being able to build trust in an environment

01:42:30.400 --> 01:42:36.000
that encourages people to not trust anyone. So that's the ultimate tension in diplomacy.

01:42:36.000 --> 01:42:41.040
How can this AI reason about whether you are being honest in your communication

01:42:41.040 --> 01:42:45.360
and how can the AI persuade you that it is being honest when it is telling you that,

01:42:45.360 --> 01:42:47.120
hey, I'm actually going to support you this turn?

01:42:48.000 --> 01:42:52.080
Is there some sense, I don't know if you step back and think that this process

01:42:53.040 --> 01:43:01.120
will indirectly help us study human psychology. So like if trust is the ultimate goal,

01:43:01.280 --> 01:43:05.920
wouldn't that help us understand what are the fundamental aspects of forming trust

01:43:06.400 --> 01:43:10.800
between humans and between humans and AI? I mean, that's a really, really important question.

01:43:10.800 --> 01:43:16.800
That's much bigger than the strategy games is how can this fundamental to the human-robot

01:43:16.800 --> 01:43:21.760
interaction problem, how do we form trust between intelligent entities?

01:43:22.720 --> 01:43:28.240
So one of the things I'm really excited about with diplomacy, there's never really been

01:43:29.200 --> 01:43:35.600
a good domain to investigate these kinds of questions. And diplomacy gives us a domain

01:43:35.600 --> 01:43:40.640
where trust is really at the center of it. And it's not just like you've hired a bunch

01:43:40.640 --> 01:43:45.920
of mechanical turkers that are being paid and trying to get through the task as quickly

01:43:45.920 --> 01:43:49.600
as possible. You have these people that are really invested in the outcome of the game

01:43:49.600 --> 01:43:54.800
and they're really trying to do the best that they can. And so I'm really excited that we're

01:43:54.800 --> 01:44:00.160
able to, we actually have put together this, we're open sourcing all of our models, we're

01:44:00.160 --> 01:44:07.200
open sourcing all of the code and we're making the data that we've used available to researchers

01:44:08.240 --> 01:44:11.920
so that they can investigate these kinds of questions. So the data of the different,

01:44:11.920 --> 01:44:18.000
the human and the AI play of diplomacy and the models that you use for the generation

01:44:18.000 --> 01:44:23.520
of the messages and the filtering? Yeah, not just even the data of the AI playing with the humans,

01:44:23.600 --> 01:44:29.360
but all the training data that we had, that we use to train the AI to understand how humans play

01:44:29.360 --> 01:44:34.960
the game. We're setting up a system where researchers will be able to apply to be able

01:44:34.960 --> 01:44:38.720
to gain access to that data and be able to use it in their own research. We should say,

01:44:38.720 --> 01:44:44.240
what is the name of the system? We're calling the bot Cicero. Cicero. And what's the name,

01:44:44.240 --> 01:44:49.840
like you're open sourcing, what's the name of the repository and the project? Is it also just

01:44:49.840 --> 01:44:54.720
called Cicero, the big project, or are you still coming up with a name? The data set comes from

01:44:54.720 --> 01:45:00.000
this website, WebDiplomacy.net, is the site that's been online for like 20 years now and it's one of

01:45:00.000 --> 01:45:06.080
the main sites that people use to play diplomacy on it. We've got like 50,000 games of diplomacy

01:45:06.080 --> 01:45:12.400
with natural language communication, over 10 million messages. So it's a pretty massive

01:45:12.400 --> 01:45:17.680
data set that people can use to, we're hoping that the academic community, the research community

01:45:17.680 --> 01:45:21.120
is able to use it for all sorts of interesting research questions.

01:45:21.120 --> 01:45:24.240
So to you, from having studied this game, is this

01:45:25.280 --> 01:45:31.600
a sufficiently rich problem space to explore this kind of human AI interaction?

01:45:31.600 --> 01:45:37.280
Yeah, absolutely. And I think it's maybe the best data set that I can think of out there

01:45:37.280 --> 01:45:44.480
to investigate these kinds of questions of negotiation, trust, persuasion. I wouldn't

01:45:44.480 --> 01:45:49.520
say it's the best data set in the world for human AI interaction. That's a very broad field,

01:45:49.520 --> 01:45:54.560
but I think that it's definitely up there. If you're really interested in language models

01:45:54.560 --> 01:45:59.520
interacting with humans in a setting where their incentives are not fully aligned,

01:45:59.520 --> 01:46:02.080
this seems like an ideal data set for investigating that.

01:46:02.880 --> 01:46:09.920
So you have a paper with some impressive results and just an impressive paper they've taken this

01:46:09.920 --> 01:46:16.480
problem on. What's the most exciting thing to you in terms of the results from the paper?

01:46:17.760 --> 01:46:18.560
Well, I think there's-

01:46:18.560 --> 01:46:19.600
Ideas or results?

01:46:20.560 --> 01:46:25.920
Yeah, I think there's a few aspects of the results that I think are really exciting. So first of all,

01:46:25.920 --> 01:46:31.920
the fact that we were able to achieve such strong performance, I was surprised by and

01:46:31.920 --> 01:46:38.560
pleasantly surprised by. So we played 40 games of diplomacy with real humans and the bot placed

01:46:38.560 --> 01:46:43.680
second out of all players that have played five or more games. So it's about 80 players total,

01:46:44.800 --> 01:46:48.400
19 of whom played five or more games, and the bot was ranked second out of those players.

01:46:51.120 --> 01:46:56.560
The bot was really good in two dimensions. One, being able to establish strong connections with

01:46:56.560 --> 01:47:01.920
the other players on the board, being able to persuade them to work with it, being able to

01:47:01.920 --> 01:47:07.760
coordinate with them about how it's going to work with them, and then also the raw tactical and

01:47:07.760 --> 01:47:13.280
strategic aspects of the game, being able to understand what the other players are likely

01:47:13.280 --> 01:47:18.160
to do, being able to model their behavior and respond appropriately to that, the bot also

01:47:18.160 --> 01:47:23.760
really excelled at. What are some interesting things that the bot said? By the way, are you

01:47:23.760 --> 01:47:29.440
allowed to swear in the- Are there rules to what you're allowed to say and not in diplomacy?

01:47:29.440 --> 01:47:33.280
You can say whatever you want. I think the site will get very angry at you if you start

01:47:33.280 --> 01:47:39.280
threatening somebody. Or if you threaten somebody, you're supposed to do it politely.

01:47:39.280 --> 01:47:46.720
Yeah, politely. Keep it in character. We actually had a researcher watching the bot 24-7. Well,

01:47:46.720 --> 01:47:50.400
whenever we played a game, we had a bot watching it to make sure that it wouldn't go off the rails

01:47:50.400 --> 01:47:54.000
and start threatening somebody or something like that. I would just love it if the bot started

01:47:54.000 --> 01:48:00.960
mocking everybody. Some weird, quirky strategies would emerge. Have you seen anything interesting

01:48:01.520 --> 01:48:10.160
that's a weird behavior, either the filter or the language model that was weird to you?

01:48:11.200 --> 01:48:17.920
Yeah, there were definitely things that the bot would do that were not in line with how humans

01:48:17.920 --> 01:48:24.720
would approach the game in a good way. We've talked to some expert diplomacy players about

01:48:24.720 --> 01:48:28.960
these results. Their takeaway is that, well, maybe humans are approaching this the wrong way,

01:48:28.960 --> 01:48:30.720
and this is actually the right way to play the game.

01:48:33.200 --> 01:48:40.480
What's required to win? What does it mean to mess up or to exploit the suboptimal behavior of a

01:48:40.480 --> 01:48:47.440
player? Is there optimally rational behavior and irrational behavior that you need to

01:48:48.160 --> 01:48:53.120
estimate, that kind of stuff? What stands out to you? Is there a crack that you can exploit?

01:48:53.680 --> 01:48:59.840
Is there a weakness that you can exploit in the game that everybody's looking for?

01:49:01.280 --> 01:49:08.240
Well, I think you're asking two questions there. One, modeling the irrationality and the

01:49:08.240 --> 01:49:15.680
suboptimality of humans. In diplomacy, you can't treat all the other players like they're machines.

01:49:15.680 --> 01:49:20.800
If you do that, you're going to end up playing really poorly. We actually ran this experiment.

01:49:21.440 --> 01:49:27.360
We trained a bot in a two-player zero-sum version of diplomacy the same way that you might approach

01:49:27.360 --> 01:49:33.360
a game like Chess or Poker. The bot was superhuman. It would crush any competitor. Then we took that

01:49:33.360 --> 01:49:37.680
same training approach, and we trained a bot for the full seven-player version of the game

01:49:37.680 --> 01:49:42.160
through self-play without any human data. We stuck it in a game with six humans,

01:49:42.160 --> 01:49:46.240
and it got destroyed. Even in the version of the game where there's no explicit natural

01:49:46.240 --> 01:49:50.880
language communication, it still got destroyed because it just wouldn't be able to understand

01:49:50.880 --> 01:49:53.840
how the other players were approaching the game and be able to work with that.

01:49:55.040 --> 01:50:00.640
Can you just linger on that meaning like there's an individual personality to each player and then

01:50:00.640 --> 01:50:06.240
you're supposed to remember that? What do you mean it's not able to understand the players?

01:50:06.240 --> 01:50:12.400
Well, it would, for example, expect the human to support it in a certain way when the human would

01:50:12.400 --> 01:50:18.080
simply think like, no, I'm not supposed to support you here. It's kind of like if you develop a

01:50:18.080 --> 01:50:22.640
self-driving car and it's trained completely from scratch with other self-driving cars,

01:50:22.640 --> 01:50:26.400
it might learn to drive on the left side of the road. That's a totally reasonable thing to do if

01:50:26.400 --> 01:50:29.920
you're with these other self-driving cars that are also driving on the left side of the road.

01:50:29.920 --> 01:50:32.880
But if you put it in an American city, it's going to crash.

01:50:32.880 --> 01:50:37.520
But I guess the intuition I'm trying to build up is why does it then crush a human player heads up?

01:50:38.240 --> 01:50:40.480
Versus multiple.

01:50:40.480 --> 01:50:44.560
This is an aspect of two-player zero-sum versus games that involve cooperation.

01:50:45.440 --> 01:50:51.040
In a two-player zero-sum game, you can do self-play from scratch and you will arrive

01:50:51.040 --> 01:50:57.280
at the Nash equilibrium where you don't have to worry about the other player playing in a very

01:50:57.280 --> 01:51:02.240
human suboptimal style. That's just going to be the only way that deviating from a Nash equilibrium

01:51:04.160 --> 01:51:06.080
would change things is if it helped you.

01:51:06.560 --> 01:51:10.400
So what's the dynamic of cooperation that's effective in diplomacy?

01:51:11.120 --> 01:51:14.160
Do you always have to have one friend in the game?

01:51:14.800 --> 01:51:19.040
You always want to maximize your friends and minimize your enemies.

01:51:22.160 --> 01:51:31.280
Got it. And boy, the line comes into play there. So the more friends you have, the better.

01:51:32.160 --> 01:51:35.120
Yeah. I mean, I guess you have to attack somebody or else you're not going to make progress.

01:51:35.200 --> 01:51:41.440
Right. So that's the tension. But this is too real. This is too real. This is too close to

01:51:41.920 --> 01:51:47.840
geopolitics of actual military conflict in the world. Okay. That's fascinating. So that

01:51:47.840 --> 01:51:51.360
cooperation element is what makes the game really, really hard.

01:51:51.360 --> 01:51:56.560
Yeah. And to give you an example of how this suboptimality and irrationality comes into play,

01:51:57.280 --> 01:52:04.080
there's a really common situation in the game of diplomacy where one player starts to win

01:52:04.080 --> 01:52:07.920
and they're at the point where they're controlling about half the map. And the

01:52:07.920 --> 01:52:12.640
remaining players who have all been fighting each other the whole game all have to work together now

01:52:12.640 --> 01:52:18.080
to stop this other player from winning or else everybody's going to lose. And it's kind of like

01:52:18.080 --> 01:52:21.600
Game of Thrones. I don't know if you've seen the show where you've got the others coming from the

01:52:21.600 --> 01:52:26.240
north and all the people have to work out their differences and stop them from taking over.

01:52:28.560 --> 01:52:33.040
And the bot will do this. The bot will work with the other players to stop the superpower from

01:52:33.040 --> 01:52:38.480
winning. But if it's trained from scratch or it doesn't really have a good grounding in how

01:52:38.480 --> 01:52:44.240
humans approach it, it will also at the same time attack the other players with its extra units.

01:52:44.240 --> 01:52:47.440
So all the units that are not necessary to stop the superpower from winning,

01:52:47.440 --> 01:52:51.120
it will use those to grab as many centers as possible from the other players.

01:52:51.680 --> 01:52:57.120
And in totally rational play, the other players should just live with that. They have to

01:52:57.120 --> 01:53:03.280
understand like, hey, a score of one is better than a score of zero. So okay, he's grabbed my

01:53:03.280 --> 01:53:09.360
centers, but I'll just deal with it. But humans don't act that way, right? The human gets really

01:53:09.360 --> 01:53:15.760
angry at the bot and ends up throwing the game because I'm going to screw you over because you

01:53:15.760 --> 01:53:20.960
did something that's not fair to me. Got it. And are you supposed to model that? Is the bot

01:53:20.960 --> 01:53:27.040
supposed to model that kind of human frustration? Yeah, exactly. And so that is something that

01:53:27.520 --> 01:53:31.440
seems almost impossible to model purely from scratch without any human data. It's a very

01:53:31.440 --> 01:53:37.840
cultural thing. And so you need human data to be able to understand that, hey, that's how humans

01:53:37.840 --> 01:53:41.680
behave. And you have to work around that. It might be suboptimal, it might be irrational,

01:53:42.640 --> 01:53:48.800
but that's an aspect of humanity that you have to deal with. So how difficult is it to train on

01:53:48.800 --> 01:53:54.320
human data given that human data is very limited versus what a purely self-playing mechanism can

01:53:54.320 --> 01:53:58.080
generate? That's actually one of the major challenges that we faced in the research,

01:53:58.080 --> 01:54:02.400
that we had a good amount of human data. We had about 50,000 games. What we try to do is

01:54:03.280 --> 01:54:07.760
leverage as much self-play as possible while still leveraging the human data.

01:54:08.640 --> 01:54:15.600
So what we do is we do self-play, very similar to how it's been done in poker and Go, but we

01:54:15.600 --> 01:54:20.960
try to regularize the self-play towards the human data. Basically, the way to think about it is

01:54:20.960 --> 01:54:31.040
we penalize the bot for choosing actions that are very unlikely under the human data set.

01:54:31.040 --> 01:54:36.240
How do you know? Is there some kind of function that says, this is human-like and not?

01:54:36.800 --> 01:54:42.240
Yeah. So we train a bot through supervised learning to model the human play as much as

01:54:42.240 --> 01:54:49.760
possible. So we basically train a neural net on those 50,000 games, and that gives us a policy

01:54:49.760 --> 01:54:53.840
that resembles to some extent how humans actually play the game. Now, this isn't a perfect

01:54:53.840 --> 01:54:58.880
model of human play because we don't have unlimited data. We don't have unlimited neural net capacity,

01:54:59.600 --> 01:55:04.480
but it gives us some approximation. Is there some data on the internet that's useful besides

01:55:04.480 --> 01:55:09.760
just diplomacy? So on the language side of things, is there some, can you go to like Reddit

01:55:09.760 --> 01:55:16.640
and some sort of background model formulation that's useful for the game of diplomacy?

01:55:16.640 --> 01:55:20.160
That's useful for the game of diplomacy. Yeah, absolutely. And so for the language model,

01:55:20.160 --> 01:55:24.640
which is kind of like a separate question, we didn't use the language model during self-play

01:55:24.640 --> 01:55:32.560
training, but we pre-trained the language model on tons of internet data as much as possible,

01:55:32.560 --> 01:55:37.120
and then we fine-tuned it specifically on the diplomacy games. So we are able to leverage the

01:55:37.120 --> 01:55:44.160
wider data set in order to fill in some of the gaps in how communication happens more broadly

01:55:44.160 --> 01:55:49.600
besides just specifically in these diplomacy games. Okay, cool. So what are some interesting

01:55:49.600 --> 01:55:56.080
things that came to life from this work to you? What are some insights about

01:55:58.640 --> 01:56:03.520
games where natural language is involved and cooperation, deep cooperation is involved?

01:56:04.160 --> 01:56:09.760
Well, I think there's a few insights. So first of all, the fact that you can't rely

01:56:10.720 --> 01:56:14.880
purely or even largely on self-play, that you really have to have an understanding of how

01:56:14.880 --> 01:56:19.840
humans approach the game. I think that that's one of the major conclusions that I'm drawing from this

01:56:19.840 --> 01:56:25.600
work, and that is I think applicable more broadly to a lot of different games. So we've actually

01:56:25.600 --> 01:56:30.400
already taken the approaches that we've used in diplomacy and tried them on a cooperative card

01:56:30.400 --> 01:56:36.400
game called Hanabi, and we've had a lot of success in that game as well. On the language side,

01:56:37.360 --> 01:56:44.720
I think the fact that we were able to control the language model through this intense approach

01:56:45.280 --> 01:56:51.280
was very effective. And it allowed us, instead of just imitating how humans would communicate,

01:56:51.280 --> 01:56:58.480
we're able to go beyond that and able to feed into its superhuman strategies that it can then

01:57:00.000 --> 01:57:04.960
generate messages corresponding to. Is there something you could say about detecting whether

01:57:05.120 --> 01:57:13.920
a person or AI is lying or not? The bot doesn't explicitly try to calculate whether somebody is

01:57:13.920 --> 01:57:19.840
lying or not. But what it will do is try to predict what actions they're going to take

01:57:19.840 --> 01:57:25.360
given the messages that they've sent to us. So given our conversation, what do I think

01:57:25.360 --> 01:57:29.920
you're going to do? And implicitly, there is a calculation about whether you're lying to me

01:57:29.920 --> 01:57:35.520
in that. Based on your messages, if I think you're going to attack me this turn,

01:57:36.240 --> 01:57:41.040
even though your messages say that you're not, then essentially the bot is predicting that

01:57:41.040 --> 01:57:46.320
you're lying. But it doesn't view it as lying the same way that we would view it as lying.

01:57:47.120 --> 01:57:53.840
But you could probably reformulate with all the same data and make a classifier lying or not.

01:57:54.560 --> 01:57:58.160
Yeah, I think you could do that. That was not something that we were focused on,

01:57:58.160 --> 01:58:03.840
but I think that it is possible that if you came up with some measurements of what does it mean

01:58:03.840 --> 01:58:09.040
to tell a lie, because there's a spectrum, right? If you're withholding some information,

01:58:09.040 --> 01:58:14.320
is that a lie? If you're mostly telling the truth, but you forgot to mention this one action out of

01:58:14.320 --> 01:58:20.400
10, is that a lie? It's hard to draw the line. But if you're willing to do that, then you could

01:58:20.400 --> 01:58:27.520
possibly use it to. This feels like an argument inside a relationship now. What constitutes a lie?

01:58:29.120 --> 01:58:37.440
It depends what you mean by the definition of the word. It's fascinating because trust and lying

01:58:37.440 --> 01:58:43.360
is all intermixed into this, and it's language models that are becoming more and more sophisticated.

01:58:43.360 --> 01:58:51.840
It's just a fascinating space to explore. What do you see as the future of this work

01:58:53.600 --> 01:58:57.840
that is inspired by the breakthrough performance that you're getting here with diplomacy?

01:58:58.560 --> 01:59:03.840
I think there's a few different directions to take this work.

01:59:05.520 --> 01:59:10.880
I think really what it's showing us is the potential that language models have. I mean,

01:59:10.880 --> 01:59:15.360
I think a lot of people didn't think that this kind of result was possible even today, despite

01:59:15.360 --> 01:59:20.800
all the progress that's been made in language models. And so it shows us how we can leverage

01:59:20.800 --> 01:59:26.320
the power of things like self-play on top of language models to get increasingly better

01:59:26.320 --> 01:59:31.600
performance. And the ceiling is really much higher than what we have right now.

01:59:32.240 --> 01:59:39.280
Is this transferable somehow to chatbots for the more general task of dialogue?

01:59:41.360 --> 01:59:46.720
There is a kind of negotiation here, a dance between entities that are trying to cooperate

01:59:46.720 --> 01:59:53.360
and at the same time a little bit adversarial, which I think maps somewhat to the general

01:59:54.240 --> 02:00:00.720
you know, the entire process of Reddit or like internet communication. You're cooperating,

02:00:00.720 --> 02:00:05.840
you're adversarial, you're having debates, you're having camaraderie, all that kind of stuff.

02:00:06.560 --> 02:00:11.440
I think one of the things that's really useful about diplomacy is that we have a well-defined

02:00:11.440 --> 02:00:17.920
value function. There is a well-defined score that the bot is trying to optimize. And in a

02:00:17.920 --> 02:00:25.280
setting like a general chatbot setting, it would need that kind of objective in order to fully

02:00:25.280 --> 02:00:31.520
leverage the techniques that we've developed. What about what we talked about earlier with NPCs

02:00:31.520 --> 02:00:42.480
inside video games? How can it be used to create for Elder Scrolls 6 more compelling NPCs that you

02:00:42.480 --> 02:00:47.040
could talk to instead of instead of committing all kinds of violence with a sword and fighting

02:00:47.040 --> 02:00:52.000
dragons just sitting in a tavern and drink all day and talk to the chatbot? The way that we've

02:00:52.000 --> 02:00:57.920
approached AI and diplomacy is you condition the language on an intent. Now that intent in diplomacy

02:00:57.920 --> 02:01:04.240
is an action, but it doesn't have to be. And you can imagine, you know, you could have NPCs

02:01:04.240 --> 02:01:09.360
in video games or the metaverse or whatever where there's some intent or there's some objective

02:01:09.360 --> 02:01:14.880
that they're trying to maximize and you can specify what that is. And then the language

02:01:15.440 --> 02:01:20.640
can correspond to that intent. Now I'm not saying that this is happening imminently, but I'm saying

02:01:20.640 --> 02:01:26.080
that this is like a future application potentially of this direction of research. So what's the more

02:01:26.080 --> 02:01:31.280
general formulation of this? Making self-play be able to scale the way self-play does and still

02:01:31.280 --> 02:01:39.280
maintain human-like behavior? The way that we've approached self-play in diplomacy is like we're

02:01:39.280 --> 02:01:44.960
trying to come up with good intents to condition the language model on. And the space of intents

02:01:45.520 --> 02:01:50.240
is actions that can be played in the game. Now there is like the potential to have a broader

02:01:50.240 --> 02:01:58.400
set of intents. Things like, you know, long-term cooperation or long-term objectives or, you know,

02:01:58.400 --> 02:02:02.560
gossip about what another player was saying. These are things that we're currently not

02:02:02.560 --> 02:02:07.520
conditioning the language model on. And so it's not able to, we're not able to control it to say

02:02:07.520 --> 02:02:12.000
like, oh, you should be talking about this thing right now. But it's quite possible that you could

02:02:12.000 --> 02:02:16.880
expand the scope of intents to be able to allow it to talk about those things. Now in the process

02:02:16.880 --> 02:02:23.040
of doing that, the self-play would become much more complicated. And so that is a potential for

02:02:23.040 --> 02:02:28.960
future work. Okay, the increase in the number of intents. I still am not quite clear how you keep

02:02:29.600 --> 02:02:38.480
the self-play integrated into the human world. Yeah. I'm a little bit loose on understanding

02:02:38.480 --> 02:02:44.480
how you do that. So we train in neural nets to imitate the human data as closely as possible.

02:02:44.480 --> 02:02:48.320
And that's what we call the anchor policy. And now when we're doing self-play,

02:02:49.360 --> 02:02:54.400
the problem with the anchor policy is that it's not a perfect approximation of how humans actually

02:02:54.400 --> 02:02:59.040
play. Because we don't have infinite data, because we don't have an unlimited neural

02:02:59.040 --> 02:03:03.840
network capacity, it's actually a relatively suboptimal approximation of how humans actually

02:03:03.840 --> 02:03:12.240
play. And we can improve that approximation by adding planning and RL. And so what we do is

02:03:12.240 --> 02:03:18.400
we get a better approximation, a better model of human play by during the self-play process,

02:03:19.040 --> 02:03:25.760
we say you can deviate from this human anchor policy if there is an action that has,

02:03:25.760 --> 02:03:32.160
you know, particularly high expected value. But it would have to be a really high expected value

02:03:32.160 --> 02:03:38.800
in order to deviate from this human-like policy. So you basically say try to maximize your expected

02:03:38.800 --> 02:03:44.480
value while at the same time stay as close as possible to the human policy. And there is a

02:03:44.480 --> 02:03:49.680
parameter that controls the relative weighting of those competing objectives.

02:03:50.480 --> 02:03:57.360
So the question I have is how sophisticated can the anchor policy get to have a policy

02:03:57.360 --> 02:04:03.280
that approximates human behavior, right? So as you increase the number of intents,

02:04:03.280 --> 02:04:10.560
as you generalize the space in which this is applicable, and given that the human data is

02:04:10.560 --> 02:04:18.720
limited, try to anticipate a policy that works for in a much larger number of cases. How difficult

02:04:18.720 --> 02:04:23.840
is the process of forming a damn good anchor policy? Well, it really comes down to how much

02:04:23.840 --> 02:04:29.360
human data you have. So it's all about scaling the human data. I think the more human data you have,

02:04:29.360 --> 02:04:35.680
the better. And I think that that's going to be the major bottleneck in scaling to more complicated

02:04:36.400 --> 02:04:40.640
domains. But that said, there might be the potential, just like in the language model

02:04:40.640 --> 02:04:45.600
where we leveraged tons of data on the internet and then specialized it for diplomacy,

02:04:46.560 --> 02:04:50.640
there is the future potential that you can leverage huge amounts of data across the board

02:04:50.640 --> 02:04:55.040
and then specialize it in the data set that you have for diplomacy. And in that way,

02:04:55.040 --> 02:04:57.200
you're essentially augmenting the amount of data that you have.

02:04:57.200 --> 02:05:07.600
To what degree does this apply to the general, the real world diplomacy, the geopolitics?

02:05:08.320 --> 02:05:12.960
You know, there's a game theory has a history of being applied to understand

02:05:12.960 --> 02:05:17.680
and to give us hope about nuclear weapons, for example, the mutually assured destruction

02:05:17.680 --> 02:05:22.880
is a game theoretic concept that you can formulate. Some people say it's oversimplified,

02:05:22.880 --> 02:05:28.240
but nevertheless, here we are, and we somehow haven't blown ourselves up. Do you see a future

02:05:28.240 --> 02:05:37.360
where this kind of system can be used to help us make geopolitical decisions in the world?

02:05:38.800 --> 02:05:42.800
Well, like I said, the original motivation for the game of diplomacy was the failures

02:05:42.800 --> 02:05:49.440
of World War I, the diplomatic failures that led to war. And the real take-home message of diplomacy

02:05:49.440 --> 02:05:57.040
is that if people approach diplomacy the right way, then war is ultimately unsuccessful.

02:05:58.160 --> 02:06:02.400
The way that I see it, war is an inherently negative sum game, right? There's always a

02:06:02.400 --> 02:06:10.240
better outcome than war for all the parties involved. And my hope is that as AI progresses,

02:06:10.240 --> 02:06:16.880
then maybe this technology could be used to help people make better decisions across the board

02:06:16.880 --> 02:06:20.480
and hopefully avoid negative sum outcomes like war.

02:06:21.120 --> 02:06:28.000
Yeah. I just came back from Ukraine. I'm going back there. On deep personal levels,

02:06:28.800 --> 02:06:36.160
think a lot about how peace can be achieved. And I'm a big believer in conversation,

02:06:37.280 --> 02:06:41.200
leaders getting together and having conversations and trying to understand each other.

02:06:42.160 --> 02:06:47.360
Yeah, it's fascinating to think whether each one of those leaders can run a simulation ahead of

02:06:47.360 --> 02:06:54.080
time. Like if I'm an asshole, what are the possible consequences? If I'm nice, what are

02:06:54.080 --> 02:07:01.920
the possible consequences? My guess is that if the president of the United States got together with

02:07:03.360 --> 02:07:10.320
Vladimir Zelensky and Vladimir Putin, that there will be significant benefits to

02:07:11.760 --> 02:07:16.960
the president of the United States not having the ego of kind of playing down,

02:07:16.960 --> 02:07:23.360
of giving away a lot of chips for the future success of a world. So giving a lot of power

02:07:23.360 --> 02:07:29.520
to the two presidents of the competing nations to achieve peace. That's my guess, but it'd be nice

02:07:29.520 --> 02:07:35.040
to run a bunch of simulations, but then you have to have human data, right? Because the game with

02:07:35.040 --> 02:07:41.040
diplomacy is fundamentally different than geopolitics. You need data. I guess that's

02:07:41.040 --> 02:07:46.560
the question I have. How transferable is this to, I don't know, any kind of negotiation,

02:07:47.680 --> 02:07:55.840
to any kind of some local, I don't know, a bunch of lawyers arguing, like divorce lawyers. How

02:07:55.840 --> 02:07:59.920
transferable is this to all kinds of human negotiation? Well, I feel like this isn't a

02:07:59.920 --> 02:08:04.240
question that's unique to diplomacy. I mean, I think you look at RL breakthroughs, reinforcement

02:08:04.240 --> 02:08:09.040
learning breakthroughs in previous games as well, like AI for StarCraft, AI for Atari.

02:08:09.040 --> 02:08:13.920
You haven't really seen it deployed in the real world because you have these problems of it's

02:08:13.920 --> 02:08:21.520
really hard to collect a lot of data and you don't have a well-defined action space. You don't have

02:08:21.520 --> 02:08:26.560
a well-defined reward function. These are all things that you really need for reinforcement

02:08:26.560 --> 02:08:31.040
learning and planning to be really successful today. Now, there are some domains where you

02:08:31.040 --> 02:08:37.040
do have that. Code generation is one example. Theorem proving mathematics, that's another

02:08:37.040 --> 02:08:40.240
example where you have a well-defined action space, you have a well-defined reward function,

02:08:41.120 --> 02:08:46.800
and those are the kinds of domains where I can see RL in the short term being incredibly powerful.

02:08:47.840 --> 02:08:53.360
But yeah, I think that those are the barriers to deploying this at scale in the real world, but

02:08:54.080 --> 02:08:59.120
the hope is that in the long run we'll be able to get there. Yeah, but see, diplomacy feels like

02:08:59.120 --> 02:09:05.040
closer to the real world than does StarCraft. Because it's natural language. You're operating

02:09:05.040 --> 02:09:09.360
in a space of intents and in a space of natural language. That feels very close to the real world

02:09:09.360 --> 02:09:15.760
and it also feels like you could get data on that from the internet. Yeah, and that's why I do think

02:09:15.760 --> 02:09:21.360
that diplomacy is taking a big step closer to the real world than anything that's came before in

02:09:21.360 --> 02:09:27.840
terms of game AI breakthroughs. The fact that we're communicating in natural language,

02:09:27.840 --> 02:09:35.200
we're leveraging the fact that we have this general data set of dialogue and communication

02:09:35.200 --> 02:09:41.280
from a breadth of the internet. That is a big step in that direction. We're not 100% there,

02:09:41.280 --> 02:09:46.480
but we're getting closer at least. So if we actually return back to poker and chess,

02:09:47.120 --> 02:09:52.560
are some of the ideas that you're learning here with diplomacy, could you construct AI systems that

02:09:52.560 --> 02:10:01.360
play like humans? Like make for a fun opponent in a game of chess? Yeah, absolutely. We've already

02:10:01.360 --> 02:10:05.280
started looking into this direction a bit, so we tried to use the techniques that we've developed

02:10:05.280 --> 02:10:13.040
for diplomacy to make chess and go AIs. And what we found is that it led to much more human-like,

02:10:13.040 --> 02:10:21.440
strong chess and go players. The way that AIs like Stockfish today play is in a very inhuman style.

02:10:21.440 --> 02:10:26.880
It's very strong, but it's very different from how humans play. So we can take the techniques

02:10:26.880 --> 02:10:33.920
that we've developed for diplomacy, we do something similar in chess and go, and we end up with a bot

02:10:33.920 --> 02:10:42.880
that's both strong and human-like. To elaborate on this a bit, one way to approach making a human-like

02:10:42.880 --> 02:10:49.360
AI for chess is to collect a bunch of human games, like a bunch of human grand master games,

02:10:49.360 --> 02:10:54.080
and just to supervise learning on those games. But the problem is that if you do that, what you

02:10:54.080 --> 02:10:59.600
end up with is an AI that's substantially weaker than the human grand masters that you trained on.

02:10:59.600 --> 02:11:06.880
Because the neural net is not able to approximate the nuance of the strategy. This goes back to the

02:11:06.880 --> 02:11:11.440
planning thing that I mentioned, the search thing that I talked about before, that these human

02:11:11.440 --> 02:11:16.400
grand masters when they're playing, they're using search and they're using planning. And the neural

02:11:16.400 --> 02:11:20.720
net alone, unless you have a massive neural net that's like a thousand times bigger than what we

02:11:20.720 --> 02:11:28.480
have right now, it's not able to approximate those details very effectively. And on the other hand,

02:11:28.480 --> 02:11:34.480
you can leverage search and planning very heavily, but then what you end up with is an AI that plays

02:11:34.480 --> 02:11:39.920
in a very different style from how humans play the game. Now if you strike this intermediate balance

02:11:39.920 --> 02:11:45.040
by setting the regularization parameters correctly and say you can do planning but try to keep it

02:11:45.040 --> 02:11:51.360
close to the human policy, then you end up with an AI that plays in both a very human-like style

02:11:51.360 --> 02:11:58.080
and a very strong style. And you can actually even tune it to have a certain ELO rating.

02:11:58.080 --> 02:12:04.160
So you can say playing the style of like a 2,800 ELO human. I wonder if you could do specific type

02:12:04.160 --> 02:12:11.280
of humans or categories of humans, not just skill but style. Yeah, I think so. And so this is where

02:12:12.240 --> 02:12:16.080
the research gets interesting. One of the things that I was thinking about is,

02:12:16.640 --> 02:12:19.920
and this is actually already being done, there's a researcher at the University of Toronto that's

02:12:19.920 --> 02:12:26.000
working on this, is to make an AI that plays in the style of a particular player. Like Magnus

02:12:26.000 --> 02:12:30.160
Carlsen, for example, you can make an AI that plays like Magnus Carlsen. And then where I think

02:12:30.160 --> 02:12:34.240
this gets interesting is maybe you're up against Magnus Carlsen in the world championship or

02:12:34.240 --> 02:12:40.320
something. You can play against this Magnus Carlsen bot to prepare against the real Magnus Carlsen.

02:12:40.320 --> 02:12:45.840
And you can try to explore strategies that he might struggle with and try to figure out

02:12:45.840 --> 02:12:50.880
how to beat this player in particular. On the other hand, you can also have Magnus Carlsen

02:12:50.880 --> 02:12:55.360
working with this bot to try to figure out where he's weak and where he needs to improve his

02:12:55.360 --> 02:13:03.760
strategy. And so I can envision this future where data on specific chess and Go players becomes

02:13:03.760 --> 02:13:08.960
extremely valuable because you can use that data to create specific models of how these particular

02:13:08.960 --> 02:13:15.520
players play. So increasingly human-like behavior in bots, however, as you've mentioned, makes

02:13:15.520 --> 02:13:21.440
cheating, cheat detection much harder. It does. Yeah. The way that cheat detection works

02:13:22.080 --> 02:13:28.080
in a game like poker and a game like chess and Go, from what I understand, is trying to see like,

02:13:28.080 --> 02:13:35.200
is this person making moves that are very common among chess AIs or AIs in general,

02:13:36.160 --> 02:13:44.800
but very uncommon among top human players. And if you have the development of these AIs that play

02:13:44.800 --> 02:13:50.080
in a very strong style, but also very human-like style, then that poses serious challenges for

02:13:50.080 --> 02:13:55.520
cheat detection. And it makes you now ask yourself a hard question about what is the role of AI

02:13:55.520 --> 02:14:01.360
systems as they become more and more integrated in our society. And this kind of human AI

02:14:02.240 --> 02:14:10.560
integration has some deep ethical issues that we should be aware of. And also it's a kind of

02:14:10.560 --> 02:14:17.120
cybersecurity challenge, right? One of the assumptions we have when we play games is that

02:14:17.120 --> 02:14:24.960
there's a trust that it's only humans involved. And the better AI systems we create, which makes

02:14:24.960 --> 02:14:29.600
it super exciting, human-like AI systems with different styles of humans is really exciting,

02:14:29.600 --> 02:14:34.800
but then we have to have the defenses better and better and better. If we're to trust that we

02:14:35.760 --> 02:14:43.520
can enjoy human versus human game in a deeply fair way. It's fascinating. It's just, it's humbling.

02:14:44.240 --> 02:14:48.320
Yeah. I think there's a lot of like negative potential for this kind of technology, but

02:14:49.040 --> 02:14:53.840
at the same time, there's a lot of upside for it as well. So, for example, right now it's really

02:14:53.840 --> 02:14:59.280
hard to learn how to get better in games like chess and poker and Go, because the way that the

02:14:59.280 --> 02:15:03.920
AI plays is so foreign and incomprehensible. But if you have these AIs that are playing,

02:15:04.560 --> 02:15:09.760
you can say like, oh, I'm a 2000 Elo human. How do I get to 2200? Now you can have an AI

02:15:09.760 --> 02:15:14.080
that plays in the style of a 2200 Elo human and that will help you get better.

02:15:14.080 --> 02:15:19.440
Or you mentioned this problem of like, how do you know that you're actually playing with humans when

02:15:19.440 --> 02:15:24.640
you're playing like online and in video games? Well, now we have the potential of populating

02:15:24.720 --> 02:15:31.520
these virtual worlds with AI agents that are actually fun to play with. And you don't have to

02:15:32.080 --> 02:15:38.480
always be playing with other humans to have a fun time. So yeah, a lot of upside potential too.

02:15:38.480 --> 02:15:43.440
And I think with any sort of tool, there's the potential for a lot of greatness and a lot of

02:15:43.440 --> 02:15:48.080
downsides as well. So in the paper, they got a chance to look at, there's a section on

02:15:49.040 --> 02:15:53.440
ethical considerations. What's in that section? What are some ethical considerations here?

02:15:53.440 --> 02:15:56.720
Is it some of the stuff we already talked about? There's some things that we've already talked

02:15:56.720 --> 02:16:03.920
about. I think specific to diplomacy, there's also the challenge that the game is,

02:16:05.040 --> 02:16:11.840
there is a deception aspect to the game. And so developing language models that

02:16:11.840 --> 02:16:17.520
are capable of deception is I think a dicey issue and something that makes research on diplomacy

02:16:17.520 --> 02:16:25.520
particularly challenging. And so those kinds of issues of like, should we even be developing

02:16:25.520 --> 02:16:29.600
AIs that are capable of lying to people? That's something that we have to think carefully about.

02:16:30.240 --> 02:16:34.320
That's so cool. I mean, you have to do that kind of stuff in order to figure out where the ethical

02:16:34.320 --> 02:16:41.360
lines are. But I can see in the future it being illegal to have a consumer product that lies.

02:16:41.360 --> 02:16:44.000
Yeah. Yeah.

02:16:44.000 --> 02:16:48.240
Like your personal assistant AI system is not allowed, is always have to tell the truth.

02:16:48.240 --> 02:16:54.880
But if I ask it, do I look, did I get fatter over the past month? I sure as hell want that AI system

02:16:54.880 --> 02:17:02.160
to lie to me. So there's a trade-off between lying and being nice. We have to somehow find,

02:17:03.760 --> 02:17:07.600
what is the ethics in that? And we're back to discussions inside relationships. Anyway,

02:17:07.600 --> 02:17:09.680
what were you saying? Oh yeah, I was saying like, yeah,

02:17:09.680 --> 02:17:13.680
that's kind of going to the question of like, what is a lie? You know, is a white lie a bad lie?

02:17:13.680 --> 02:17:20.640
Is it an ethical lie? You know, those kinds of questions. Boy, we return time and time again to

02:17:20.640 --> 02:17:24.880
deep human questions as we design AI systems. That's exactly what they do. They put a mirror

02:17:24.880 --> 02:17:30.880
to humanity to help us understand ourselves. There's also the issue of like, you know,

02:17:30.880 --> 02:17:36.800
in these diplomacy experiments in order to do a fair comparison, you know, what we found is that

02:17:36.800 --> 02:17:43.120
there's an inherent anti-AI bias in these kinds of games. So we actually played a tournament in

02:17:43.120 --> 02:17:47.280
a non-language version of the game where, you know, we told the participants like, hey, in every

02:17:47.280 --> 02:17:52.880
single game, there's going to be an AI. And what we found is that the humans would spend basically

02:17:52.880 --> 02:17:55.680
the entire game, like trying to figure out who the bot was. And then as soon as they thought

02:17:55.680 --> 02:18:02.000
they figured it out, they would all team up and try to kill it. And, you know, overcoming that

02:18:02.000 --> 02:18:11.200
inherent anti-AI bias is a challenge. On the flip side, I think when robots become the enemy,

02:18:11.200 --> 02:18:17.520
that's when we get to heal our human divisions and then we can become one. As long as we have one

02:18:17.520 --> 02:18:22.800
enemy, it's that Reagan thing. When the aliens show up, that's when we put our side, our divisions

02:18:22.800 --> 02:18:27.840
will become one human species. We might have our differences, but we're at least all human.

02:18:27.840 --> 02:18:33.040
At least we all hate the robots. No, no, no, no. I think there will be actually in the future

02:18:33.040 --> 02:18:37.040
something like a civil rights movement for robots. I think that's the fascinating thing

02:18:37.040 --> 02:18:44.560
about AI systems is they ask, they force us to ask about ethical questions about what is sentience?

02:18:44.560 --> 02:18:49.120
What is, how do we feel about systems that are capable of suffering or capable of displaying

02:18:49.120 --> 02:18:54.880
suffering? And how do we design products that show emotion and not? How do we feel about that?

02:18:54.880 --> 02:19:01.360
Lying is another topic. Are we going to allow bots to lie and not? And where's the balance

02:19:01.360 --> 02:19:06.880
between being nice and telling the truth? I mean, these are all fascinating human questions.

02:19:06.880 --> 02:19:13.280
It's like so exciting to be in a century where we create systems that take these

02:19:13.840 --> 02:19:18.960
philosophical questions that have been asked for centuries and now we can engineer them inside

02:19:18.960 --> 02:19:25.680
systems where you really have to answer them because you'll have transformational impact on

02:19:25.680 --> 02:19:30.720
human society depending on what you design inside those systems. It's fascinating.

02:19:31.280 --> 02:19:35.840
And like you said, I feel like diplomacy is a step towards the direction of the real world,

02:19:35.840 --> 02:19:41.280
applying these RL methods towards the real world. From all the breakthrough performances

02:19:41.840 --> 02:19:46.640
in Go and Chess and StarCraft and Dota, this feels like the real world.

02:19:46.720 --> 02:19:52.480
Especially now my mind's been on war and military conflict. This feels like it can give us some

02:19:52.480 --> 02:19:59.360
deep insights about human behavior at the large geopolitical scale. What do you think

02:20:00.960 --> 02:20:10.800
is the breakthrough or the directions of work that will take us towards solving intelligence,

02:20:10.800 --> 02:20:17.680
towards creating AGI systems? You've been a part of creating, by the way, we should say a part of

02:20:18.240 --> 02:20:25.520
great teams that do this, of creating systems that achieve breakthrough performances on before

02:20:26.320 --> 02:20:31.680
thought unsolvable problems like poker, multiplayer poker, diplomacy.

02:20:33.120 --> 02:20:37.360
We're taking steps towards that direction. What do you think it takes to go all the way

02:20:37.360 --> 02:20:41.760
to create superhuman level intelligence? There's a lot of people trying to figure

02:20:41.760 --> 02:20:46.720
that out right now. I should say the amount of progress that's been made, especially in

02:20:46.720 --> 02:20:52.960
the past few years, is truly phenomenal. You look at where AI was 10 years ago and the idea that

02:20:52.960 --> 02:20:57.360
you can have AIs that can generate language and generate images the way they're doing today

02:20:57.360 --> 02:21:03.760
and able to play a game like diplomacy was just unthinkable even five years ago, let alone 10

02:21:03.760 --> 02:21:14.000
years ago. Now, there are aspects of AI that I think are still lacking. I think there's general

02:21:14.000 --> 02:21:19.200
agreements that one of the major issues with AI today is that it's very data inefficient.

02:21:19.760 --> 02:21:26.000
It requires a huge number of samples of training examples to be able to train. You look at an AI

02:21:26.000 --> 02:21:31.840
that plays Go and it needs millions of games of Go to learn how to play the game well,

02:21:31.840 --> 02:21:36.480
whereas a human can pick it up in like, I don't know, how many games does a human Go player,

02:21:36.480 --> 02:21:42.240
Go Grandmaster play in their lifetime? Probably in the thousands or tens of thousands, I guess.

02:21:45.360 --> 02:21:50.160
That's one issue, overcoming this challenge of data efficiency. This is particularly important if

02:21:50.160 --> 02:21:56.400
we want to deploy AI systems in real world settings where they're interacting with humans

02:21:56.400 --> 02:22:00.880
because, for example, with robotics, it's really hard to generate a huge number of samples.

02:22:01.840 --> 02:22:06.880
A different story when you're working in these totally virtual games where you can play a million

02:22:06.880 --> 02:22:11.120
games and it's no big deal. I was planning on just launching like a thousand of these robots in

02:22:11.120 --> 02:22:16.720
Austin. I don't think it's illegal for legged robots to roam the streets and just collect data.

02:22:16.720 --> 02:22:18.320
That's not a crazy idea. What's the worst that could happen?

02:22:19.520 --> 02:22:22.720
That's one way to overcome the data efficiency problem is scale it.

02:22:23.840 --> 02:22:29.760
I actually tried to see if there's a law against robots like legged robots just operating

02:22:30.720 --> 02:22:37.760
in the streets of a major city and I couldn't find any. I'll take it all the way to the

02:22:37.760 --> 02:22:46.080
Supreme Court. Robot rights. Anyway, sorry, you were saying. What are the ideas for becoming

02:22:46.080 --> 02:22:51.680
more data efficient? That's the trillion dollar question in AI today. If you can figure out how

02:22:51.680 --> 02:22:58.320
to make AI systems more data efficient, then that's a huge breakthrough. Nobody really knows

02:22:58.400 --> 02:23:06.000
right now. It could be just a gigantic background language model and then the training becomes

02:23:06.000 --> 02:23:14.400
prompting that model to essentially do a querying, a search into the space of the things it's learned

02:23:14.400 --> 02:23:19.040
to customize that to whatever problem you're trying to solve. Maybe if you form a large enough

02:23:19.040 --> 02:23:24.640
language model, you can go quite a long way. I think there's some truth to that. You look at

02:23:24.640 --> 02:23:29.520
the way humans approach a game like poker. They're not coming at it from scratch. They're

02:23:29.520 --> 02:23:35.280
coming at it with a huge amount of background knowledge about how humans work, how the world

02:23:35.280 --> 02:23:43.040
works, the idea of money. They're able to leverage that kind of information to pick up the game

02:23:43.040 --> 02:23:47.760
faster. It's not really a fair comparison to then compare it to an AI that's like learning from

02:23:47.760 --> 02:23:53.040
scratch. Maybe one of the ways that we address this sample complexity problem is by allowing

02:23:53.120 --> 02:23:56.160
AIs to leverage that general knowledge across a ton of different domains.

02:23:58.240 --> 02:24:04.160
Like I said, you did a lot of incredible work in the space of research and actually building

02:24:04.160 --> 02:24:10.800
systems. Let's start with beginners. What advice would you give to beginners interested in machine

02:24:10.800 --> 02:24:15.200
learning? They're at the very start of their journey. They're in high school and college

02:24:15.200 --> 02:24:19.280
thinking this seems like a fascinating world. What advice would you give them?

02:24:20.240 --> 02:24:27.520
I would say that there are a lot of people working on similar aspects of machine learning

02:24:27.520 --> 02:24:35.520
and to not be afraid to try something a bit different. My own path in AI is pretty atypical

02:24:35.520 --> 02:24:42.480
for a machine learning researcher today. I started out working on game theory and then shifting more

02:24:42.480 --> 02:24:46.480
towards reinforcement learning as time went on. That actually had a lot of benefits, I think,

02:24:46.480 --> 02:24:51.040
because it allowed me to look at these problems in a very different way from the way a lot of

02:24:51.760 --> 02:24:58.720
machine learning researchers view it. That comes with drawbacks in some respects. I think there's

02:24:58.720 --> 02:25:04.480
definitely aspects of machine learning where I'm weaker than most of the researchers out there,

02:25:04.480 --> 02:25:10.320
but I think that diversity of perspective, when I'm working with my teammates, there's something

02:25:10.320 --> 02:25:12.720
that I'm bringing to the table and there's something that they're bringing to the table

02:25:12.720 --> 02:25:15.120
and that kind of collaboration becomes very fruitful for that reason.

02:25:15.600 --> 02:25:21.920
So there could be problems like poker, like you've chosen diplomacy. There could be problems like

02:25:21.920 --> 02:25:26.000
that still out there that you can just tackle, even if it seems extremely difficult.

02:25:28.160 --> 02:25:34.800
I think that there's a lot of challenges left. I think having a diversity of viewpoints and

02:25:34.800 --> 02:25:40.000
backgrounds is really helpful for working together to figure out how to tackle those kinds of

02:25:40.000 --> 02:25:46.320
challenges. So as a beginner, I would say that's more for a grad student where they already built

02:25:46.320 --> 02:25:51.360
up a base. Like a complete beginner, what's a good journey? So for you, that was doing more on the

02:25:51.360 --> 02:25:56.880
math side of things, doing game theory, all that. So it's basically build up a foundation in

02:25:56.880 --> 02:26:02.400
something. So programming, mathematics, it could even be physics, but build that foundation.

02:26:03.280 --> 02:26:07.280
Yeah, I would say build a strong foundation in math and computer science and statistics

02:26:07.280 --> 02:26:12.160
and these kinds of areas. But don't be afraid to try something that's different and learn

02:26:12.160 --> 02:26:16.240
something that's different from the thing that everybody else is doing to get into machine

02:26:16.240 --> 02:26:25.040
learning. There's value in having a different background than everybody else. But certainly

02:26:25.040 --> 02:26:28.960
having a strong math background, especially in things like linear algebra and statistics

02:26:28.960 --> 02:26:34.720
and probability are incredibly helpful today for learning about and understanding machine learning.

02:26:35.280 --> 02:26:39.600
Do you think one day we'll be able to, since you're taking steps from poker to diplomacy,

02:26:39.600 --> 02:26:45.040
one day we'll be able to figure out how to live life optimally?

02:26:46.320 --> 02:26:51.040
Well, what is it like in poker and diplomacy, you need a value function. You need to have a

02:26:51.040 --> 02:26:56.960
reward system. And so what does it mean to live a life that's optimal? So, okay. So then you can

02:26:57.520 --> 02:27:03.200
exactly like lay down a reward function being like, I want to be rich or I want to be

02:27:05.040 --> 02:27:10.400
I want to be in a happy relationship. And then you'll say, well, do X.

02:27:11.600 --> 02:27:17.360
You know, there's a lot of talk today about in AI safety circles about like misspecification of

02:27:18.240 --> 02:27:23.520
reward functions. So you say like, okay, my objective is to be rich. And maybe the AI

02:27:23.520 --> 02:27:26.320
tells you like, okay, well, if you want to maximize the probability that you're rich,

02:27:26.320 --> 02:27:31.920
go rob a bank. And so you want to, is that really what you want? Is your objective really to be

02:27:31.920 --> 02:27:37.600
rich at all costs or is it more nuanced than that? So the unintended consequences. Yeah.

02:27:39.360 --> 02:27:47.920
Yeah. So maybe life is more about defining the reward function that minimizes the unintended

02:27:47.920 --> 02:27:54.480
consequences than it is about the actual policy that gets you to the reward function. Maybe life

02:27:54.480 --> 02:28:00.960
is just about constantly updating the reward function. I think one of the challenges in life

02:28:01.280 --> 02:28:06.480
is figuring out exactly what that reward function is. Sometimes it's pretty hard to specify the same

02:28:06.480 --> 02:28:10.960
way that, you know, trying to handcraft the optimal policy in a game like chess is really

02:28:10.960 --> 02:28:17.120
difficult. It's not so clear cut what the reward function is for life. I think one day AI will

02:28:17.120 --> 02:28:25.040
figure it out. And I wonder what that would be. Until then, I just really appreciate the kind of

02:28:25.040 --> 02:28:31.520
work you're doing. And it's really fascinating taking a leap into a more and more real world like

02:28:33.200 --> 02:28:40.160
problem space and just achieving incredible results by applying reinforcement learning.

02:28:40.160 --> 02:28:45.200
No, since I saw your work on poker, you've been a constant inspiration. It's an honor to get to

02:28:45.200 --> 02:28:50.160
finally talk to you. And this is really fun. Thanks for having me. Thanks for listening to

02:28:50.160 --> 02:28:54.560
this conversation with No Brown. To support this podcast, please check out our sponsors

02:28:54.560 --> 02:29:00.640
in the description. And now let me leave you with some words from Sun Tzu and the art of war.

02:29:01.520 --> 02:29:07.680
The whole secret lies in confusing the enemy so that he cannot fathom our real intent.

02:29:09.120 --> 02:29:12.240
Thank you for listening and hope to see you next time.


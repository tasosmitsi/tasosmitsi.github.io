WEBVTT

00:00.000 --> 00:03.480
The following is a conversation with Demis Chazabes,

00:03.480 --> 00:06.720
CEO and co-founder of DeepMind,

00:06.720 --> 00:08.600
a company that has published and built

00:08.600 --> 00:12.200
some of the most incredible artificial intelligence systems

00:12.200 --> 00:14.120
in the history of computing,

00:14.120 --> 00:18.040
including AlphaZero that learned all by itself

00:18.040 --> 00:21.000
to play the game of Go better than any human in the world,

00:21.000 --> 00:25.760
and AlphaFold2 that solved protein folding.

00:25.760 --> 00:28.800
Both tasks considered nearly impossible

00:28.800 --> 00:30.340
for a very long time.

00:31.240 --> 00:34.000
Demis is widely considered to be one of the most brilliant

00:34.000 --> 00:36.600
and impactful humans in the history

00:36.600 --> 00:38.160
of artificial intelligence

00:38.160 --> 00:41.280
and science and engineering in general.

00:41.280 --> 00:44.640
This was truly an honor and a pleasure for me

00:44.640 --> 00:47.320
to finally sit down with him for this conversation.

00:47.320 --> 00:50.580
And I'm sure we will talk many times again in the future.

00:51.520 --> 00:53.320
This is the LuxFeedman podcast.

00:53.320 --> 00:55.520
To support it, please check out our sponsors

00:55.520 --> 00:56.800
in the description.

00:56.800 --> 00:58.440
And now, dear friends,

00:58.440 --> 01:00.560
here's Demis Chazabes.

01:01.580 --> 01:04.040
Let's start with a bit of a personal question.

01:04.040 --> 01:07.840
Am I an AI program you wrote to interview people

01:07.840 --> 01:10.100
until I get good enough to interview you?

01:11.120 --> 01:13.120
Well, I'd be impressed if you were.

01:13.120 --> 01:14.880
I'd be impressed with myself if you were.

01:14.880 --> 01:16.520
I don't think we're quite up to that yet,

01:16.520 --> 01:18.800
but maybe you're from the future, Lex.

01:18.800 --> 01:20.400
If you did, would you tell me?

01:20.400 --> 01:23.080
Is that a good thing to tell a language model

01:23.080 --> 01:25.080
that's tasked with interviewing

01:25.120 --> 01:27.480
that it is, in fact, AI?

01:27.480 --> 01:29.720
Maybe we're in a kind of meta-turing test.

01:29.720 --> 01:32.480
Probably it would be a good idea not to tell you

01:32.480 --> 01:33.960
so it doesn't change your behavior, right?

01:33.960 --> 01:34.920
This is a kind of-

01:34.920 --> 01:37.140
Eisenberg uncertainty principle situation.

01:37.140 --> 01:39.120
If I told you, you'd behave differently.

01:39.120 --> 01:41.020
Maybe that's what's happening with us, of course.

01:41.020 --> 01:42.840
This is a benchmark from the future

01:42.840 --> 01:46.600
where they replay 2022 as a year

01:46.600 --> 01:49.480
before AIs were good enough yet,

01:49.480 --> 01:52.160
and now we want to see, is it gonna pass?

01:52.160 --> 01:53.000
Exactly.

01:53.960 --> 01:55.980
If I was such a program,

01:55.980 --> 01:57.960
would you be able to tell, do you think?

01:57.960 --> 01:59.960
So to the Turing test question,

01:59.960 --> 02:04.960
you've talked about the benchmark for solving intelligence.

02:05.840 --> 02:07.320
What would be the impressive thing?

02:07.320 --> 02:09.120
You've talked about winning a Nobel Prize

02:09.120 --> 02:11.440
and asked us to winning a Nobel Prize,

02:11.440 --> 02:14.400
but I still return to the Turing test as a compelling test.

02:14.400 --> 02:17.280
The spirit of the Turing test is a compelling test.

02:17.280 --> 02:18.520
Yeah, the Turing test, of course,

02:18.520 --> 02:20.200
it's been unbelievably influential,

02:20.200 --> 02:22.120
and Turing's one of my all-time heroes,

02:22.160 --> 02:23.800
but I think if you look back

02:23.800 --> 02:25.720
at the 1950 paper, his original paper,

02:25.720 --> 02:27.600
and read the original, you'll see,

02:27.600 --> 02:30.920
I don't think he meant it to be a rigorous formal test.

02:30.920 --> 02:32.920
I think it was more like a thought experiment,

02:32.920 --> 02:34.680
almost a bit of philosophy he was writing,

02:34.680 --> 02:36.480
if you look at the style of the paper,

02:36.480 --> 02:38.680
and you can see he didn't specify it very rigorously.

02:38.680 --> 02:41.880
So for example, he didn't specify the knowledge

02:41.880 --> 02:44.080
that the expert or judge would have,

02:45.480 --> 02:48.360
how much time would they have to investigate this?

02:48.360 --> 02:49.520
So these are important parameters

02:49.520 --> 02:53.240
if you were gonna make it a true sort of formal test.

02:54.360 --> 02:56.640
And by some measures,

02:56.640 --> 02:59.080
people claim the Turing test passed several,

02:59.080 --> 03:00.960
a decade ago, I remember someone claiming that

03:00.960 --> 03:06.040
with a kind of very bog standard normal logic model,

03:06.040 --> 03:08.480
because they pretended it was a kid.

03:08.480 --> 03:13.320
So the judges thought that the machine was a child.

03:13.320 --> 03:17.640
So that would be very different from an expert AI person

03:17.640 --> 03:19.880
interrogating machine and knowing how it was built

03:19.880 --> 03:20.720
and so on.

03:20.720 --> 03:24.320
So I think we should probably move away from that

03:24.320 --> 03:28.800
as a formal test and move more towards a general test

03:28.800 --> 03:32.040
where we test the AI capabilities on a range of tasks

03:32.040 --> 03:35.360
and see if it reaches human level or above performance

03:35.360 --> 03:39.200
on maybe thousands, perhaps even millions of tasks eventually

03:39.200 --> 03:41.920
and cover the entire sort of cognitive space.

03:41.920 --> 03:45.520
So I think for its time, it was an amazing thought experiment

03:45.520 --> 03:47.000
and also 1950s,

03:47.000 --> 03:49.440
obviously there's barely the dawn of the computer age.

03:49.440 --> 03:51.480
So of course he only thought about text

03:51.480 --> 03:54.600
and now we have a lot more different inputs.

03:54.600 --> 03:57.080
So yeah, maybe the better thing to test

03:57.080 --> 03:59.680
is the generalizability so across multiple tasks,

03:59.680 --> 04:04.560
but I think it's also possible as systems like God or Sho

04:04.560 --> 04:08.320
that eventually that might map right back to language.

04:08.320 --> 04:10.800
So you might be able to demonstrate your ability

04:10.800 --> 04:13.240
to generalize across tasks

04:13.240 --> 04:15.320
by then communicating your ability

04:15.320 --> 04:17.080
to generalize across tasks,

04:17.080 --> 04:19.200
which is kind of what we do through conversation anyway

04:19.200 --> 04:20.800
when we jump around.

04:20.800 --> 04:23.720
Ultimately what's in there in that conversation

04:23.720 --> 04:27.000
is not just you moving around knowledge,

04:27.000 --> 04:30.360
it's you moving around like these entirely different

04:30.360 --> 04:34.920
modalities of understanding that ultimately map

04:34.920 --> 04:38.920
to your ability to operate successfully

04:38.920 --> 04:42.600
in all of these domains, which you can think of as tasks.

04:42.600 --> 04:45.600
Yeah, I think certainly we as humans use language

04:45.600 --> 04:48.480
as our main generalization communication tool.

04:48.480 --> 04:51.320
So I think we end up thinking in language

04:51.320 --> 04:54.360
and expressing our solutions in language.

04:54.360 --> 04:58.040
So it's gonna be very powerful mode

04:58.040 --> 05:03.040
in which to explain the system to explain what it's doing,

05:03.040 --> 05:07.520
but I don't think it's the only modality that matters.

05:07.520 --> 05:10.000
So I think there's gonna be a lot of,

05:10.000 --> 05:13.640
there's a lot of different ways to express capabilities

05:13.640 --> 05:15.640
other than just language.

05:15.640 --> 05:19.080
Yeah, visual, robotics, body language.

05:21.200 --> 05:23.840
Yeah, actions, the interactive aspect of all that,

05:23.840 --> 05:24.840
that's all part of it.

05:24.840 --> 05:27.680
But what's interesting with Gata is that it's

05:27.680 --> 05:30.280
it's sort of pushing prediction to the maximum

05:30.280 --> 05:31.920
in terms of like, you know,

05:31.920 --> 05:34.480
mapping arbitrary sequences to other sequences

05:34.480 --> 05:36.480
and sort of just predicting what's gonna happen next.

05:36.560 --> 05:41.080
So prediction seems to be fundamental to intelligence.

05:41.080 --> 05:44.160
And what you're predicting doesn't so much matter.

05:44.160 --> 05:46.840
Yeah, it seems like you can generalize that quite well.

05:46.840 --> 05:49.640
So obviously language models predict the next word.

05:49.640 --> 05:53.880
Gata predicts potentially any action or any token.

05:53.880 --> 05:55.320
And it's just the beginning really.

05:55.320 --> 05:58.120
It's our most general agent one could call it so far,

05:58.120 --> 06:01.240
but you know, that itself can be scaled up massively more

06:01.240 --> 06:02.160
than we've done so far.

06:02.160 --> 06:04.280
And obviously we're in the middle of doing that.

06:04.280 --> 06:08.240
But the big part of solving AGI is creating benchmarks

06:08.240 --> 06:11.080
that help us get closer and closer,

06:11.080 --> 06:14.920
sort of creating benchmarks that test the generalizability.

06:14.920 --> 06:17.440
And it's just still interesting that this fella,

06:17.440 --> 06:20.520
Alan Turing, was one of the first

06:20.520 --> 06:22.600
and probably still one of the only people

06:22.600 --> 06:25.040
that was trying, maybe philosophically,

06:25.040 --> 06:26.840
but was trying to formulate a benchmark

06:26.840 --> 06:27.880
that could be followed.

06:27.880 --> 06:30.960
It is, even though it's fuzzy,

06:30.960 --> 06:32.560
it's still sufficiently rigorous

06:32.560 --> 06:33.960
to where you can run that test.

06:34.640 --> 06:36.640
And I still think something like the Turing test

06:36.640 --> 06:38.720
will, at the end of the day,

06:38.720 --> 06:42.560
be the thing that truly impresses other humans,

06:42.560 --> 06:46.400
so that you can have a close friend who's an AI system.

06:46.400 --> 06:48.320
And for that friend to be a good friend,

06:48.320 --> 06:53.160
they're going to have to be able to play StarCraft,

06:53.160 --> 06:56.600
and they're gonna have to do all of these tasks,

06:56.600 --> 06:59.600
get you a beer, so the robotics tasks,

07:00.440 --> 07:03.760
play games with you, use language, humor,

07:04.600 --> 07:05.440
all of those kinds of things.

07:05.440 --> 07:08.000
But that ultimately can boil down to language.

07:08.000 --> 07:11.200
It feels like, not in terms of the AI community,

07:11.200 --> 07:13.120
but in terms of the actual impact

07:13.120 --> 07:14.760
of general intelligence on the world,

07:14.760 --> 07:16.640
it feels like language will be the place

07:16.640 --> 07:18.480
where it truly shines.

07:18.480 --> 07:20.960
I think so, because it's such an important

07:20.960 --> 07:22.480
input-output for us.

07:22.480 --> 07:23.320
I think you're right.

07:23.320 --> 07:27.440
I think the Turing test, the philosophy behind it,

07:27.440 --> 07:32.280
which is the idea of, can a machine mimic the behaviors

07:32.280 --> 07:35.680
of a human, and I would say,

07:35.680 --> 07:38.720
wider than just language and text,

07:38.720 --> 07:41.320
then in terms of actions and everything else,

07:41.320 --> 07:42.680
creativity, all these things,

07:42.680 --> 07:45.320
then if it can sort of match or exceed

07:45.320 --> 07:47.360
human cognitive capabilities,

07:47.360 --> 07:49.880
then I think we have a true intelligence.

07:49.880 --> 07:51.680
So from that perspective, you're right.

07:51.680 --> 07:56.000
I think he did formulate the right kind of setup.

07:56.000 --> 07:59.280
I think there'll be a kind of humor

07:59.280 --> 08:00.920
in the AI systems of the future,

08:00.920 --> 08:02.640
looking back to this conversation,

08:03.520 --> 08:06.040
and thinking about the Turing test,

08:06.040 --> 08:09.360
and also thinking about, by that time,

08:09.360 --> 08:11.880
they would know which year they were finally able

08:11.880 --> 08:15.560
to sort of cross the threshold of human-level intelligence

08:15.560 --> 08:18.800
and think how funny it is that we humans

08:18.800 --> 08:22.400
were still confused about this whole problem

08:22.400 --> 08:24.760
that's been long ago been solved.

08:24.760 --> 08:27.240
Well, hello to the AI agents of the future.

08:27.240 --> 08:30.280
Anyway, so going back to your journey,

08:30.320 --> 08:33.800
when did you fall in love with programming first?

08:33.800 --> 08:35.960
Well, I was pretty young age, actually.

08:35.960 --> 08:40.840
So I started off, actually, games was my first love.

08:40.840 --> 08:43.680
So starting to play chess when I was around four years old,

08:43.680 --> 08:46.160
and then it was actually with winnings

08:46.160 --> 08:48.400
from a chess competition that I managed

08:48.400 --> 08:50.800
to buy my first chess computer when I was about eight years old.

08:50.800 --> 08:52.120
It was a ZX Spectrum,

08:52.120 --> 08:54.720
which was hugely popular in the UK at the time.

08:54.720 --> 08:56.520
And it was an amazing machine,

08:56.520 --> 08:59.320
because I think it trained a whole generation

08:59.320 --> 09:02.520
of programmers in the UK, because it was so accessible.

09:02.520 --> 09:05.120
You literally switched it on, and there was the basic prompt,

09:05.120 --> 09:06.680
and you could just get going.

09:06.680 --> 09:10.440
And my parents didn't really know anything about computers.

09:10.440 --> 09:12.600
But because it was my money from a chess competition,

09:12.600 --> 09:15.760
I could say I wanted to buy it.

09:15.760 --> 09:17.920
And then I just went to bookstores,

09:17.920 --> 09:19.840
got books on programming,

09:19.840 --> 09:23.520
and started typing in the programming code.

09:23.520 --> 09:26.480
And then, of course, once you start doing that,

09:26.480 --> 09:29.120
you start adjusting it and then making your own games.

09:29.160 --> 09:30.840
And that's when I fell in love with computers

09:30.840 --> 09:34.880
and realised that they were a very magical device.

09:34.880 --> 09:37.440
In a way, I wouldn't have been able to explain this at the time,

09:37.440 --> 09:40.960
but I felt that they were almost a magical extension of your mind.

09:40.960 --> 09:42.160
I always had this feeling,

09:42.160 --> 09:43.880
and I've always loved this about computers,

09:43.880 --> 09:47.480
that you can set them off doing something, some task for you.

09:47.480 --> 09:51.280
You can go to sleep, come back the next day, and it's solved.

09:51.280 --> 09:53.120
You know, that feels magical to me.

09:53.120 --> 09:55.280
So, I mean, all machines do that to some extent.

09:55.280 --> 09:57.640
They all enhance our natural capabilities.

09:57.640 --> 10:01.160
Obviously, cars make us allow us to move faster than we can run,

10:01.160 --> 10:04.520
but this was a machine to extend the mind.

10:04.520 --> 10:08.480
And then, of course, AI is the ultimate expression

10:08.480 --> 10:11.360
of what a machine may be able to do or learn.

10:11.360 --> 10:13.520
So, very naturally for me,

10:13.520 --> 10:15.960
that thought extended into AI quite quickly.

10:15.960 --> 10:18.560
Do you remember the programming language

10:18.560 --> 10:20.320
that was first started in?

10:20.320 --> 10:21.080
Yeah.

10:21.080 --> 10:22.920
Was it special to the machine or was it something general?

10:22.920 --> 10:25.840
No, I think it was just basic on the ZX Spectrum.

10:25.840 --> 10:27.480
I don't know what specific form it was.

10:27.480 --> 10:29.640
And then, later on, I got a Commodore Amiga,

10:29.640 --> 10:32.760
which was a fantastic machine.

10:32.760 --> 10:33.800
Now, you're just showing off.

10:33.800 --> 10:36.480
So, yeah, well, lots of my friends had Atari STs,

10:36.480 --> 10:37.800
and I managed to get Amigas.

10:37.800 --> 10:40.680
It was a bit more powerful, and that was incredible.

10:40.680 --> 10:46.200
And I used to do programming in Assembler and also Amos Basic,

10:46.200 --> 10:48.080
this specific form of Basic.

10:48.080 --> 10:51.000
It was incredible, actually, as well as all my coding skills.

10:51.000 --> 10:53.000
And when did you fall in love with AI?

10:53.000 --> 10:56.840
So, when did you first start to gain an understanding

10:56.840 --> 10:58.840
that you can not just write programs

10:58.840 --> 11:01.600
that do some mathematical operations for you

11:01.600 --> 11:02.880
while you sleep,

11:02.880 --> 11:08.800
but something that's akin to bringing an entity to life,

11:08.800 --> 11:12.760
sort of a thing that can figure out something more complicated

11:12.760 --> 11:15.920
than a simple mathematical operation?

11:15.920 --> 11:17.560
Yeah, so there was a few stages for me

11:17.560 --> 11:18.920
all while I was very young.

11:18.920 --> 11:22.480
So, first of all, as I was trying to improve at playing chess,

11:22.480 --> 11:24.680
I was captaining various England Junior Chess teams.

11:24.680 --> 11:27.440
And at the time, when I was about maybe 10, 11 years old,

11:27.440 --> 11:29.320
I was going to become a professional chess player.

11:29.320 --> 11:32.000
That was my first thought.

11:32.000 --> 11:35.440
So that dream was there to try to get to the highest level of chess.

11:35.440 --> 11:39.120
Yeah, so when I was about 12 years old,

11:39.120 --> 11:40.120
I got to master standard,

11:40.120 --> 11:42.680
and I was second highest rated player in the world to Judith Polger,

11:42.680 --> 11:45.720
who obviously ended up being an amazing chess player

11:45.720 --> 11:48.560
and a world women's champion.

11:48.560 --> 11:50.760
And when I was trying to improve at chess,

11:50.760 --> 11:52.720
what you do is you obviously, first of all,

11:52.760 --> 11:55.120
you're trying to improve your own thinking processes.

11:55.120 --> 11:58.080
So that leads you to thinking about thinking.

11:58.080 --> 12:00.400
How is your brain coming up with these ideas?

12:00.400 --> 12:01.920
Why is it making mistakes?

12:01.920 --> 12:04.560
How can you improve that thought process?

12:04.560 --> 12:07.520
But the second thing is that it was just the beginning.

12:07.520 --> 12:11.240
This was like in the early 80s, mid 80s of chess computers.

12:11.240 --> 12:12.760
If you remember, they were physical balls

12:12.760 --> 12:13.960
like the one we have in front of us,

12:13.960 --> 12:17.000
and you press down the squares.

12:17.000 --> 12:21.000
And I think Kasparov had a branded version of it that I got.

12:21.040 --> 12:24.720
And they're not as strong as they are today,

12:24.720 --> 12:26.400
but they were pretty strong,

12:26.400 --> 12:29.080
and you used to practice against them

12:29.080 --> 12:31.440
to try and improve your openings and other things.

12:31.440 --> 12:33.520
And so I remember, I think I probably got my first one,

12:33.520 --> 12:35.000
I was around 11 or 12.

12:35.000 --> 12:37.840
And I remember thinking, this is amazing.

12:37.840 --> 12:42.840
How has someone programmed this chess board to play chess?

12:42.960 --> 12:45.680
And it was very formative book I bought,

12:45.680 --> 12:47.800
which was called The Chess Computer Handbook

12:47.800 --> 12:50.760
by David Levy, it came out in 1984 or something.

12:51.480 --> 12:52.400
So I must've got it when I was about 11, 12.

12:52.400 --> 12:56.200
And it explained fully how these chess programs were made.

12:56.200 --> 12:59.000
And I remember my first AI program being

12:59.000 --> 13:01.400
programming my Amiga, it couldn't,

13:01.400 --> 13:02.960
it wasn't powerful enough to play chess.

13:02.960 --> 13:04.240
I couldn't write a whole chess program,

13:04.240 --> 13:06.720
but I wrote a program for it to play Othello,

13:06.720 --> 13:09.360
or reverse it sometimes called I think in the US.

13:09.360 --> 13:11.800
And so a slightly simpler game than chess,

13:11.800 --> 13:14.400
but I used all of the principles that chess programs had,

13:14.400 --> 13:16.040
Alpha, Beta, Search, all of that.

13:16.040 --> 13:17.440
And that was my first AI program.

13:17.440 --> 13:19.480
I remember that very well, I was around 12 years old.

13:19.480 --> 13:21.720
So that brought me into AI.

13:21.720 --> 13:24.160
And then the second part was later on,

13:24.160 --> 13:25.600
when I was around 16, 17,

13:25.600 --> 13:28.880
and I was writing games professionally, designing games,

13:28.880 --> 13:30.680
writing a game called Theme Park,

13:30.680 --> 13:34.080
which had AI as a core gameplay component

13:34.080 --> 13:35.720
as part of the simulation.

13:35.720 --> 13:38.520
And it sold millions of copies around the world

13:38.520 --> 13:41.000
and people loved the way that the AI,

13:41.000 --> 13:42.320
even though it was relatively simple

13:42.320 --> 13:44.480
by today's AI standards,

13:44.480 --> 13:47.760
was reacting to the way you as the player played it.

13:47.760 --> 13:49.280
So it was called a sandbox game.

13:50.120 --> 13:51.360
So it was one of the first types of games like that,

13:51.360 --> 13:52.720
along with SimCity,

13:52.720 --> 13:55.720
and it meant that every game you played was unique.

13:55.720 --> 13:58.880
Is there something you could say just on a small tangent

13:58.880 --> 14:02.200
about really impressive AI

14:02.200 --> 14:06.600
from a game design, human enjoyment perspective,

14:06.600 --> 14:09.720
really impressive AI that you've seen in games,

14:09.720 --> 14:12.520
and maybe what does it take to create AI system,

14:12.520 --> 14:14.280
and how hard of a problem is that?

14:14.280 --> 14:18.360
So a million questions that would just as a brief tangent.

14:18.360 --> 14:22.680
Well, look, I think games have been significant

14:22.680 --> 14:23.760
in my life for three reasons.

14:23.760 --> 14:26.120
So first of all, I was playing them

14:26.120 --> 14:28.840
and training myself on games when I was a kid.

14:28.840 --> 14:31.520
Then I went through a phase of designing games

14:31.520 --> 14:33.040
and writing AI for games.

14:33.040 --> 14:35.960
So all the games I professionally wrote

14:35.960 --> 14:37.720
had AI as a core component.

14:37.720 --> 14:40.120
And that was mostly in the 90s.

14:40.120 --> 14:43.040
And the reason I was doing that in games industry

14:43.040 --> 14:45.160
was at the time, the games industry,

14:45.160 --> 14:47.240
I think was the cutting edge of technology.

14:47.240 --> 14:49.880
So whether it was graphics with people like John Carmack

14:49.880 --> 14:53.120
and Quake and those kinds of things, or AI,

14:53.120 --> 14:56.200
I think actually all the action was going on in games.

14:56.200 --> 14:58.520
And we're still reaping the benefits of that

14:58.520 --> 15:01.560
even with things like GPUs, which I find ironic,

15:01.560 --> 15:03.760
was obviously invented for graphics, computer graphics,

15:03.760 --> 15:06.320
but then turns out to be amazingly useful for AI.

15:06.320 --> 15:08.440
It just turns out everything's a matrix multiplication

15:08.440 --> 15:11.080
appears in the whole world.

15:11.080 --> 15:15.840
So I think games at the time had the most cutting edge AI,

15:15.840 --> 15:19.840
and a lot of the games, I was involved in writing.

15:19.840 --> 15:21.320
So there was a game called Black and White,

15:21.320 --> 15:22.800
which was one game I was involved with

15:22.800 --> 15:24.040
in the early stages of,

15:24.040 --> 15:28.400
which I still think is the most impressive example

15:28.400 --> 15:30.600
of reinforcement learning in a computer game.

15:30.600 --> 15:34.640
So in that game, you trained a little pet animal and-

15:34.640 --> 15:35.480
It's a brilliant game.

15:35.480 --> 15:37.680
Yeah, and it sort of learned from how you were treating it.

15:37.680 --> 15:40.720
So if you treated it badly, then it became mean.

15:40.720 --> 15:42.960
And then it would be mean to your villagers

15:42.960 --> 15:45.800
and your population, the sort of the little tribe

15:46.800 --> 15:47.640
that you were running.

15:47.640 --> 15:49.400
But if you were kind to it, then it would be kind.

15:49.400 --> 15:51.080
And people were fascinated by how that was.

15:51.080 --> 15:54.160
And so was I, to be honest with the way it kind of developed.

15:54.160 --> 15:55.120
And-

15:55.120 --> 15:57.240
Especially the mapping to good and evil.

15:57.240 --> 15:58.080
Yeah.

15:58.080 --> 16:01.600
Made you realize, made me realize that you can sort of,

16:01.600 --> 16:03.680
in the way, in the choices you make,

16:03.680 --> 16:07.440
can define where you end up.

16:07.440 --> 16:12.440
And that means all of us are capable of the good and evil.

16:12.600 --> 16:15.240
It all matters in the different choices

16:15.240 --> 16:18.200
along the trajectory to those places that you make.

16:18.200 --> 16:19.040
It's fascinating.

16:19.040 --> 16:21.360
I mean, games can do that philosophically to you.

16:21.360 --> 16:22.200
And it's rare.

16:22.200 --> 16:23.040
It seems rare.

16:23.040 --> 16:24.640
Yeah, well, games are, I think, a unique medium

16:24.640 --> 16:26.560
because you as the player,

16:26.560 --> 16:30.560
you're not just passively consuming the entertainment, right?

16:30.560 --> 16:34.280
You're actually actively involved as an agent.

16:34.280 --> 16:36.160
So I think that's what makes it, in some ways,

16:36.160 --> 16:38.400
can be more visceral than other mediums

16:38.400 --> 16:39.960
like films and books.

16:40.040 --> 16:42.680
So the second, so that was designing AI in games.

16:42.680 --> 16:46.520
And then the third use we've used of AI

16:46.520 --> 16:48.480
is in DeepMind from the beginning,

16:48.480 --> 16:50.960
which is using games as a testing ground

16:50.960 --> 16:55.040
for proving out AI algorithms and developing AI algorithms.

16:55.040 --> 16:59.120
And that was a sort of a core component of our vision

16:59.120 --> 17:00.400
at the start of DeepMind,

17:00.400 --> 17:03.280
was that we would use games very heavily

17:03.280 --> 17:06.440
as our main testing ground, certainly to begin with,

17:06.440 --> 17:08.640
because it's super efficient to use games.

17:08.840 --> 17:11.560
And also, it's very easy to have metrics

17:11.560 --> 17:14.160
to see how well your systems are improving

17:14.160 --> 17:15.920
and what direction your ideas are going in

17:15.920 --> 17:18.440
and whether you're making incremental improvements.

17:18.440 --> 17:20.480
And because those games are often rooted

17:20.480 --> 17:23.440
in something that humans did for a long time beforehand,

17:23.440 --> 17:26.560
there's already a strong set of rules.

17:26.560 --> 17:28.320
Like it's already a damn good benchmark.

17:28.320 --> 17:30.280
Yes, it's really good for so many reasons

17:30.280 --> 17:32.880
because you've got clear measures

17:32.880 --> 17:35.600
of how good humans can be at these things.

17:35.600 --> 17:36.880
And in some cases like Go,

17:36.880 --> 17:39.760
we've been playing it for thousands of years.

17:39.760 --> 17:43.360
And often they have scores or at least win conditions.

17:43.360 --> 17:45.680
So it's very easy for reward learning systems

17:45.680 --> 17:46.520
to get a reward.

17:46.520 --> 17:49.360
It's very easy to specify what that reward is.

17:49.360 --> 17:54.360
And also at the end, it's easy to test externally

17:54.520 --> 17:56.960
how strong is your system by, of course,

17:56.960 --> 17:59.520
playing against the world's strongest players

17:59.520 --> 18:00.360
at those games.

18:00.360 --> 18:02.720
So it's so good for so many reasons.

18:02.720 --> 18:05.120
And it's also very efficient to run potentially

18:05.120 --> 18:08.280
millions of simulations in parallel on the cloud.

18:08.280 --> 18:12.840
So I think there's a huge reason why we were so successful

18:12.840 --> 18:14.760
back in starting out 2010,

18:14.760 --> 18:16.680
how come we were able to progress so quickly

18:16.680 --> 18:18.880
because we'd utilize games.

18:18.880 --> 18:21.320
And at the beginning of DeepMind,

18:21.320 --> 18:24.600
we also hired some amazing game engineers

18:24.600 --> 18:28.000
who I knew from my previous lives in the games industry.

18:28.000 --> 18:30.920
And that helped to bootstrap us very quickly.

18:30.920 --> 18:33.880
And plus it's somehow super compelling

18:33.920 --> 18:38.120
almost at a philosophical level of man versus machine

18:38.120 --> 18:41.240
over chess board or a go board.

18:41.240 --> 18:43.640
And especially given that the entire history of AI

18:43.640 --> 18:46.000
is defined by people saying it's gonna be impossible

18:46.000 --> 18:51.000
to make a machine that beats a human being in chess.

18:51.000 --> 18:54.640
And then once that happened, people were certain

18:54.640 --> 18:55.920
when I was coming up in AI,

18:55.920 --> 18:58.800
that go is not a game that can be solved

18:58.800 --> 19:01.280
because of the combinatorial complexity.

19:01.280 --> 19:06.280
It's just too, no matter how much Moore's law you have,

19:06.640 --> 19:08.560
compute is just never going to be able

19:08.560 --> 19:10.200
to crack the game of go.

19:10.200 --> 19:14.880
And so then there's something compelling about facing,

19:14.880 --> 19:18.120
sort of taking on the impossibility of that task

19:18.120 --> 19:23.120
from the AI researcher perspective, engineer perspective.

19:23.400 --> 19:27.040
And then as a human being, just observing this whole thing,

19:27.040 --> 19:31.480
your beliefs about what you thought was impossible

19:32.520 --> 19:37.520
being broken apart, it's humbling

19:37.760 --> 19:40.480
to realize we're not as smart as we thought.

19:40.480 --> 19:41.840
It's humbling to realize

19:41.840 --> 19:44.040
that the things we think are impossible now

19:44.040 --> 19:47.000
perhaps will be done in the future.

19:47.000 --> 19:50.800
There's something really powerful about a game,

19:50.800 --> 19:52.920
AI system beating a human being in a game

19:52.920 --> 19:55.680
that drives that message home

19:55.680 --> 19:58.000
for like millions, billions of people,

19:58.000 --> 19:59.320
especially in the case of go.

19:59.320 --> 20:00.520
Sure.

20:00.520 --> 20:01.640
Well, look, I think it's a,

20:01.640 --> 20:03.720
I mean, it has been a fascinating journey

20:03.720 --> 20:06.880
and especially as I think about it from,

20:06.880 --> 20:08.760
I can understand it from both sides,

20:08.760 --> 20:13.080
both as the AI creators of the AI,

20:13.080 --> 20:15.640
but also as a games player originally.

20:15.640 --> 20:20.360
So, it was a really, I mean, it was a fantastic,

20:20.360 --> 20:22.080
but also somewhat bittersweet moment,

20:22.080 --> 20:25.320
the alpha go match for me, seeing that.

20:25.840 --> 20:28.520
And being obviously heavily involved in that.

20:29.440 --> 20:32.480
But, as you say, chess has been the,

20:32.480 --> 20:34.360
I mean, Kasparov, I think rightly called it

20:34.360 --> 20:37.280
the drosophila of intelligence, right?

20:37.280 --> 20:39.520
So, it's sort of, I love that phrase

20:39.520 --> 20:43.760
and I think he's right because chess has been hand in hand

20:43.760 --> 20:47.440
with AI from the beginning of the whole field, right?

20:47.440 --> 20:50.520
So, I think every AI practitioner starting with Turing

20:50.520 --> 20:52.440
and Claude Shannon and all those,

20:52.440 --> 20:55.360
the sort of forefathers of the field,

20:56.280 --> 20:58.800
tried their hand at writing a chess program.

20:58.800 --> 21:01.120
I've got original edition of Claude Shannon's

21:01.120 --> 21:03.960
first chess program, I think it was 1949,

21:03.960 --> 21:06.720
the original sort of paper.

21:06.720 --> 21:09.720
And they all did that and Turing famously

21:09.720 --> 21:12.440
wrote a chess program that all the computers around then

21:12.440 --> 21:13.720
were obviously too slow to run it.

21:13.720 --> 21:16.000
So, he had to run, he had to be the computer, right?

21:16.000 --> 21:18.880
So, he literally, I think spent two or three days

21:18.880 --> 21:21.320
running his own program by hand with pencil and paper

21:21.320 --> 21:24.960
and playing a friend of his with his chess program.

21:24.960 --> 21:29.960
So, of course, Deep Blue was a huge moment beating Kasparov.

21:30.640 --> 21:31.880
But actually, when that happened,

21:31.880 --> 21:34.080
I remember that very, very vividly, of course,

21:34.080 --> 21:36.600
because it was chess and computers and AI,

21:36.600 --> 21:39.240
all the things I loved when I was at college at the time.

21:39.240 --> 21:40.800
But I remember coming away from that,

21:40.800 --> 21:43.080
being more impressed by Kasparov's mind

21:43.080 --> 21:44.480
than I was by Deep Blue.

21:44.480 --> 21:47.680
Because here was Kasparov with his human mind,

21:47.680 --> 21:49.400
not only could he play chess more or less

21:49.400 --> 21:53.160
to the same level as this brute of a calculation machine,

21:53.160 --> 21:55.160
but of course, Kasparov can do everything else

21:55.160 --> 21:57.480
humans can do, ride a bike, talk many languages,

21:57.480 --> 21:59.400
do politics, all the rest of the amazing things

21:59.400 --> 22:00.880
that Kasparov does.

22:00.880 --> 22:04.560
And so, with the same brain, and yet Deep Blue,

22:05.480 --> 22:07.040
brilliant as it was at chess,

22:07.040 --> 22:09.400
it'd been hand coded for chess

22:09.400 --> 22:13.160
and actually had distilled the knowledge

22:13.160 --> 22:16.440
of chess grand masters into a cool program,

22:16.440 --> 22:17.840
but it couldn't do anything else.

22:18.040 --> 22:20.120
It couldn't even play a strictly simpler game

22:20.120 --> 22:21.320
like tic-tac-toe.

22:21.320 --> 22:25.920
So, something to me was missing from intelligence

22:25.920 --> 22:28.560
from that system that we would regard as intelligence.

22:28.560 --> 22:30.920
And I think it was this idea of generality

22:30.920 --> 22:32.200
and also learning.

22:33.040 --> 22:36.160
So, and that's obviously what we tried to do with AlphaGo.

22:36.160 --> 22:38.640
Yeah, with AlphaGo and AlphaZero, MuZero,

22:38.640 --> 22:40.480
and then Gado and all the things

22:40.480 --> 22:43.240
that we'll get into some parts of,

22:43.240 --> 22:45.680
there's just a fascinating trajectory here.

22:45.680 --> 22:48.560
But let's just stick on chess briefly,

22:48.560 --> 22:50.200
on the human side of chess.

22:50.200 --> 22:53.440
You've proposed that from a game design perspective,

22:53.440 --> 22:56.480
the thing that makes chess compelling as a game

22:57.800 --> 22:59.600
is that there's a creative tension

22:59.600 --> 23:02.960
between a bishop and the knight.

23:02.960 --> 23:04.080
Can you explain this?

23:04.080 --> 23:05.640
First off, it's really interesting

23:05.640 --> 23:08.680
to think about what makes a game compelling,

23:08.680 --> 23:11.040
makes it stick across centuries.

23:12.040 --> 23:13.520
Yeah, I was sort of thinking about this,

23:13.520 --> 23:15.480
and actually a lot of even amazing chess players

23:16.280 --> 23:17.120
don't think about it necessarily

23:17.120 --> 23:18.320
from a games designer point of view.

23:18.320 --> 23:20.280
So, it's with my game design hat on

23:20.280 --> 23:21.280
that I was thinking about this.

23:21.280 --> 23:23.120
Why is chess so compelling?

23:23.120 --> 23:27.600
And I think a critical reason is the dynamicness

23:27.600 --> 23:30.040
of the different kind of chess positions you can have,

23:30.040 --> 23:32.240
whether they're closed or open and other things,

23:32.240 --> 23:33.560
comes from the bishop and the knight.

23:33.560 --> 23:36.520
So, if you think about how different

23:36.520 --> 23:39.280
the capabilities of the bishop and knight are

23:39.280 --> 23:40.920
in terms of the way they move,

23:40.920 --> 23:43.120
and then somehow chess has evolved

23:43.120 --> 23:46.120
to balance those two capabilities more or less equally.

23:46.120 --> 23:48.760
So, they're both roughly worth three points each.

23:48.760 --> 23:50.600
So, you think that dynamics is always there

23:50.600 --> 23:51.680
and then the rest of the rules

23:51.680 --> 23:53.800
are kind of trying to stabilize the game.

23:53.800 --> 23:55.120
Well, maybe, I mean, it's sort of,

23:55.120 --> 23:56.600
I don't know if chicken and egg situation

23:56.600 --> 23:57.720
probably both came together,

23:57.720 --> 24:00.560
but the fact that it's got to this beautiful equilibrium

24:00.560 --> 24:02.400
where you can have the bishop and knight,

24:02.400 --> 24:04.440
they're so different in power,

24:04.440 --> 24:06.960
but so equal in value across the set

24:06.960 --> 24:09.520
of the universe of all positions, right?

24:09.520 --> 24:11.600
Somehow they've been balanced by humanity

24:11.600 --> 24:13.520
over hundreds of years,

24:13.520 --> 24:16.920
I think gives the game the creative tension

24:16.920 --> 24:19.040
that you can swap the bishop and knights

24:19.040 --> 24:20.200
for a bishop for a knight,

24:20.200 --> 24:22.120
and they're more or less worth the same,

24:22.120 --> 24:24.080
but now you aim for a different type of position.

24:24.080 --> 24:26.080
If you have the knight, you want a closed position.

24:26.080 --> 24:28.200
If you have the bishop, you want an open position.

24:28.200 --> 24:29.240
So, I think that creates a lot

24:29.240 --> 24:30.960
of the creative tension in chess.

24:30.960 --> 24:34.080
So, some kind of controlled creative tension.

24:34.080 --> 24:36.000
From an AI perspective,

24:36.000 --> 24:38.880
do you think AI systems could eventually design games

24:38.880 --> 24:40.920
that are optimally compelling to humans?

24:41.600 --> 24:42.920
Well, that's an interesting question.

24:42.920 --> 24:45.960
Sometimes I get asked about AI and creativity,

24:45.960 --> 24:48.840
and the way I answered that is relevant to that question,

24:48.840 --> 24:51.200
which is that I think they're different levels

24:51.200 --> 24:52.880
of creativity, one could say.

24:52.880 --> 24:55.280
So, I think if we define creativity

24:55.280 --> 24:57.240
as coming up with something original, right?

24:57.240 --> 24:59.280
That's useful for a purpose,

24:59.280 --> 25:02.200
then I think the kind of lowest level of creativity

25:02.200 --> 25:03.680
is like an interpolation.

25:03.680 --> 25:06.240
So, an averaging of all the examples you see.

25:06.240 --> 25:07.920
So, maybe a very basic AI system

25:07.920 --> 25:09.000
could say you could have that.

25:09.000 --> 25:11.360
So, you show it millions of pictures of cats,

25:11.360 --> 25:13.880
and then you say, give me an average looking cat, right?

25:13.880 --> 25:15.440
Generate me an average looking cat.

25:15.440 --> 25:17.160
I would call that interpolation.

25:17.160 --> 25:18.680
Then there's extrapolation,

25:18.680 --> 25:20.400
which something like AlphaGo showed.

25:20.400 --> 25:24.280
So, AlphaGo played millions of games of Go against itself,

25:24.280 --> 25:26.560
and then it came up with brilliant new ideas

25:26.560 --> 25:28.200
like move 37 in game two,

25:28.200 --> 25:30.720
bringing motif strategies in Go

25:30.720 --> 25:32.800
that no humans had ever thought of,

25:32.800 --> 25:34.760
even though we've played it for thousands of years

25:34.760 --> 25:36.560
and professionally for hundreds of years.

25:36.560 --> 25:38.800
So, that I call that extrapolation.

25:38.800 --> 25:41.040
But then there's still a level above that,

25:41.920 --> 25:45.320
you could call out of the box thinking or true innovation,

25:45.320 --> 25:47.560
which is could you invent Go?

25:47.560 --> 25:48.480
Could you invent chess?

25:48.480 --> 25:50.280
And not just come up with a brilliant chess move

25:50.280 --> 25:51.360
or brilliant Go move,

25:51.360 --> 25:53.720
but can you actually invent chess

25:53.720 --> 25:55.920
or something as good as chess or Go?

25:55.920 --> 25:58.880
And I think one day AI could,

25:58.880 --> 26:02.280
but what's missing is how would you even specify that task

26:02.280 --> 26:04.480
to a program right now?

26:04.480 --> 26:05.440
And the way I would do it,

26:05.440 --> 26:09.920
if I was telling a human to do it or a human games designer

26:10.400 --> 26:11.480
is I would say something like Go,

26:11.480 --> 26:14.200
I would say come up with a game

26:14.200 --> 26:16.160
that only takes five minutes to learn,

26:16.160 --> 26:17.960
which Go does, because it's got simple rules,

26:17.960 --> 26:20.320
but many lifetimes to master, right?

26:20.320 --> 26:22.160
Or impossible to master in one lifetime

26:22.160 --> 26:24.000
because it's so deep and so complex.

26:24.960 --> 26:27.520
And then it's aesthetically beautiful.

26:27.520 --> 26:30.840
And also it can be completed in three or four hours

26:30.840 --> 26:35.840
of game play time, which is useful for us in a human day.

26:36.000 --> 26:38.200
And so you might specify these side

26:38.200 --> 26:40.240
of high-level concepts like that.

26:40.240 --> 26:43.480
And then with that and maybe a few other things

26:43.480 --> 26:48.320
one could imagine that Go satisfies those constraints.

26:48.320 --> 26:50.360
But the problem is that we're not able

26:50.360 --> 26:53.760
to specify abstract notions like that,

26:53.760 --> 26:57.520
high-level abstract notions like that yet to our AI systems.

26:57.520 --> 26:59.560
And I think there's still something missing there

26:59.560 --> 27:02.520
in terms of high-level concepts or abstractions

27:02.520 --> 27:03.760
that they truly understand

27:03.760 --> 27:07.240
and they're combinable and compositional.

27:07.240 --> 27:10.400
So for the moment, I think AI is capable

27:10.400 --> 27:12.440
of doing interpolation and extrapolation,

27:12.440 --> 27:14.160
but not true invention.

27:14.160 --> 27:16.160
So coming up with rule sets

27:17.280 --> 27:20.040
and optimizing with complicated objectives

27:20.040 --> 27:22.920
around those rule sets we can't currently do.

27:22.920 --> 27:26.080
But you could take a specific rule set

27:26.080 --> 27:28.960
and then run a kind of self-play experiment

27:28.960 --> 27:32.640
to see how long, just observe how an AI system

27:32.640 --> 27:34.400
from scratch learns.

27:34.400 --> 27:36.440
How long is that journey of learning?

27:36.600 --> 27:39.760
And maybe if it satisfies some of those other things

27:39.760 --> 27:42.360
you mentioned in terms of quickness to learn and so on,

27:42.360 --> 27:44.800
and you could see a long journey to master

27:44.800 --> 27:46.880
for even an AI system,

27:46.880 --> 27:49.960
then you could say that this is a promising game.

27:49.960 --> 27:52.360
But it would be nice to do almost like alpha codes

27:52.360 --> 27:54.000
or programming rules.

27:54.000 --> 27:59.000
So generating rules that automate even that part

27:59.040 --> 28:00.480
of the generation of rules.

28:00.480 --> 28:02.960
So I have thought about systems actually

28:02.960 --> 28:05.760
that I think would be amazing for a games designer

28:05.760 --> 28:09.240
if you could have a system that takes your game,

28:09.240 --> 28:12.000
plays it tens of millions of times, maybe overnight,

28:12.000 --> 28:13.880
and then self-balances the rules better.

28:13.880 --> 28:18.160
So it tweaks the rules and maybe the equations

28:18.160 --> 28:22.760
and the parameters so that the game is more balanced,

28:22.760 --> 28:26.320
the units in the game or some of the rules could be tweaked.

28:26.320 --> 28:28.360
So it's a bit of like giving a base set

28:28.360 --> 28:30.840
and then allowing Monte Carlo Tree Search

28:30.840 --> 28:33.400
or something like that to sort of explore it.

28:33.400 --> 28:37.080
And I think that would be super powerful tool actually

28:37.080 --> 28:39.720
for balancing, auto-balancing a game,

28:39.720 --> 28:42.120
which usually takes thousands of hours

28:42.120 --> 28:44.520
from hundreds of human games testers normally

28:44.520 --> 28:47.480
to balance a game like StarCraft,

28:47.480 --> 28:50.640
which is Blizzard are amazing at balancing their games,

28:50.640 --> 28:52.600
but it takes them years and years and years.

28:52.600 --> 28:54.120
So one could imagine at some point

28:54.120 --> 28:57.560
when this stuff becomes efficient enough to,

28:57.560 --> 28:59.560
you might better do that like overnight.

28:59.560 --> 29:02.760
Do you think a game that is optimal,

29:02.760 --> 29:05.000
designed by an AI system,

29:05.000 --> 29:08.320
would look very much like a planet Earth?

29:09.640 --> 29:10.760
Maybe, maybe.

29:10.760 --> 29:13.080
It's certainly the sort of game I would love to make is,

29:13.080 --> 29:16.040
and I've tried, in my games career,

29:16.040 --> 29:17.480
the games design career,

29:17.480 --> 29:20.320
my first big game was designing a theme park,

29:20.320 --> 29:21.440
an amusement park.

29:21.440 --> 29:23.720
Then with games like Republic,

29:23.720 --> 29:26.760
I tried to have games where we designed whole cities

29:26.760 --> 29:28.480
and allowed you to play in.

29:29.120 --> 29:30.320
And of course people like Will Wright

29:30.320 --> 29:32.640
have written games like SimEarth,

29:32.640 --> 29:34.320
trying to simulate the whole of Earth.

29:34.320 --> 29:36.200
Pretty tricky, but I think-

29:36.200 --> 29:37.600
SimEarth, I haven't actually played that one.

29:37.600 --> 29:38.440
So what is it?

29:38.440 --> 29:40.320
Does it incorporate of evolution or?

29:40.320 --> 29:44.240
Yeah, it has evolution and it sort of treats it

29:44.240 --> 29:47.280
as an entire biosphere, but from quite high level.

29:47.280 --> 29:48.120
So-

29:48.120 --> 29:50.320
It'd be nice to be able to sort of zoom in,

29:50.320 --> 29:51.320
zoom out and zoom in.

29:51.320 --> 29:52.760
Exactly, so obviously it couldn't do,

29:52.760 --> 29:53.600
that was in the night.

29:53.600 --> 29:54.960
I think he wrote that in the 90s,

29:54.960 --> 29:57.600
so it couldn't, it wasn't able to do that.

29:57.640 --> 29:59.280
But that would be obviously

29:59.280 --> 30:01.520
the ultimate sandbox game, of course.

30:01.520 --> 30:02.360
On that topic,

30:02.360 --> 30:04.840
do you think we're living in a simulation?

30:04.840 --> 30:06.200
Yes, well, so, okay.

30:06.200 --> 30:07.040
So I-

30:07.040 --> 30:07.880
We're gonna jump around

30:07.880 --> 30:09.360
from the absurdly philosophical

30:09.360 --> 30:10.800
to the technical. Sure, sure.

30:10.800 --> 30:11.960
Very, very happy to.

30:11.960 --> 30:13.880
So I think my answer to that question

30:13.880 --> 30:14.920
is a little bit complex

30:14.920 --> 30:17.680
because there is simulation theory,

30:17.680 --> 30:18.840
which obviously Nick Bostrom,

30:18.840 --> 30:20.640
I think famously first proposed.

30:21.720 --> 30:24.760
And I don't quite believe it in that sense.

30:24.760 --> 30:27.320
So in the sense that

30:27.320 --> 30:29.640
are we in some sort of computer game

30:29.640 --> 30:32.760
or have our descendants somehow recreated

30:33.720 --> 30:35.640
earth in the 21st century

30:35.640 --> 30:38.520
and for some kind of experimental reason.

30:38.520 --> 30:39.920
I think that,

30:39.920 --> 30:43.440
but I do think that we might be,

30:43.440 --> 30:45.640
that the best way to understand physics

30:45.640 --> 30:49.360
and the universe is from a computational perspective.

30:49.360 --> 30:52.480
So understanding it as an information universe

30:52.480 --> 30:54.200
and actually information being

30:54.200 --> 30:58.360
the most fundamental unit of reality

30:58.360 --> 30:59.960
rather than matter or energy.

30:59.960 --> 31:01.520
So physicists would say,

31:01.520 --> 31:03.800
matter or energy equals MC squared.

31:03.800 --> 31:05.480
These are the things that

31:05.480 --> 31:07.440
are the fundamentals of the universe.

31:07.440 --> 31:09.920
I'd actually say information,

31:09.920 --> 31:11.800
which of course itself can be,

31:11.800 --> 31:13.600
can specify energy or matter, right?

31:13.600 --> 31:15.520
Matter is actually just,

31:15.520 --> 31:16.920
we're just out the way our bodies

31:16.920 --> 31:18.760
and all the molecules in our body are arranged

31:18.760 --> 31:19.760
is information.

31:19.760 --> 31:21.440
So I think information

31:21.440 --> 31:24.960
may be the most fundamental way to describe the universe.

31:24.960 --> 31:26.640
And therefore you could say

31:26.640 --> 31:29.880
we're in some sort of simulation because of that.

31:29.880 --> 31:30.720
But I don't, I do,

31:30.720 --> 31:33.880
I'm not really a subscriber to the idea that,

31:33.880 --> 31:35.520
you know, these are sort of throw away

31:35.520 --> 31:36.920
billions of simulations around.

31:36.920 --> 31:39.400
I think this is actually very critical

31:39.400 --> 31:41.960
and possibly unique, this simulation.

31:41.960 --> 31:42.800
This particular one.

31:42.800 --> 31:43.640
Yes.

31:43.640 --> 31:48.640
And you just mean treating the universe as a computer

31:48.800 --> 31:52.280
that's processing and modifying information

31:52.280 --> 31:54.920
is a good way to solve the problems of physics,

31:54.920 --> 31:57.160
of chemistry, of biology,

31:57.160 --> 31:59.720
and perhaps of humanity and so on.

31:59.720 --> 32:02.280
Yes, I think understanding physics

32:02.280 --> 32:04.880
in terms of information theory

32:04.880 --> 32:07.920
might be the best way to really understand

32:07.920 --> 32:09.400
what's going on here.

32:09.400 --> 32:13.560
From our understanding of a universal Turing machine,

32:13.560 --> 32:15.320
from our understanding of a computer,

32:15.320 --> 32:17.440
do you think there's something outside

32:17.440 --> 32:19.440
of the capabilities of a computer

32:19.440 --> 32:21.000
that is present in our universe?

32:21.000 --> 32:23.560
You have a disagreement with Roger Penrose

32:23.560 --> 32:25.520
about the nature of consciousness.

32:25.520 --> 32:29.040
He thinks that consciousness is more than just a computation.

32:30.080 --> 32:32.680
Do you think all of it, the whole shebangs,

32:32.680 --> 32:34.000
can be a computation?

32:34.000 --> 32:35.760
Yeah, I've had many fascinating debates

32:35.760 --> 32:37.600
with Sir Roger Penrose.

32:37.600 --> 32:39.600
And obviously he's famously,

32:39.600 --> 32:41.400
and I read, you know, Emperors of the New Mind

32:41.400 --> 32:45.320
and his books, his classical books,

32:45.320 --> 32:46.680
and they were pretty influential

32:47.040 --> 32:47.880
in the 90s.

32:47.880 --> 32:50.880
And he believes that there's something more,

32:50.880 --> 32:52.920
something quantum that is needed

32:52.920 --> 32:55.760
to explain consciousness in the brain.

32:55.760 --> 32:58.240
I think about what we're doing actually at DeepMind

32:58.240 --> 32:59.840
and what my career is being,

32:59.840 --> 33:01.840
we're almost like Turing's champion.

33:01.840 --> 33:03.560
So we are pushing Turing machines

33:03.560 --> 33:05.920
or classical computation to the limits.

33:05.920 --> 33:09.360
What are the limits of what classical computing can do?

33:09.360 --> 33:11.680
Now, and at the same time,

33:11.680 --> 33:14.160
I've also studied neuroscience to see,

33:14.160 --> 33:15.440
and that's why I did my PhD in,

33:15.440 --> 33:17.640
was to see, also to look at, you know,

33:17.640 --> 33:19.160
is there anything quantum in the brain

33:19.160 --> 33:21.240
from a neuroscience or biological perspective?

33:21.240 --> 33:24.440
And so far, I think most neuroscientists

33:24.440 --> 33:26.360
and most mainstream biologists and neuroscientists

33:26.360 --> 33:29.400
would say there's no evidence of any quantum systems

33:29.400 --> 33:30.720
or effects in the brain.

33:30.720 --> 33:31.840
As far as we can see,

33:31.840 --> 33:35.880
it can be mostly explained by classical theories.

33:35.880 --> 33:39.280
So, and then so there's sort of the search

33:39.280 --> 33:40.600
from the biology side.

33:40.600 --> 33:42.120
And then at the same time,

33:42.120 --> 33:44.960
there's the raising of the water at the bar

33:44.960 --> 33:47.200
from what classical Turing machines can do

33:48.360 --> 33:51.680
and, you know, including our new AI systems.

33:51.680 --> 33:55.040
And as you alluded to earlier, you know,

33:55.040 --> 33:57.760
I think AI, especially in the last decade plus,

33:57.760 --> 34:02.080
has been a continual story now of surprising events

34:02.080 --> 34:05.160
and surprising successes knocking over one theory

34:05.160 --> 34:07.480
after another of what was thought to be impossible,

34:07.480 --> 34:10.080
you know, from go to protein folding and so on.

34:10.080 --> 34:14.760
And so I think I would be very hesitant

34:14.760 --> 34:19.520
to bet against how far the universal Turing machine

34:19.520 --> 34:23.400
and classical computation paradigm can go.

34:23.400 --> 34:27.280
And my betting would be that all of certainly

34:27.280 --> 34:30.680
what's going on in our brain can probably be mimicked

34:30.680 --> 34:34.720
or approximated on a classical machine,

34:34.720 --> 34:38.400
not requiring something metaphysical or quantum.

34:38.400 --> 34:41.720
And we'll get there with some of the work with AlphaFold,

34:41.720 --> 34:45.080
which I think begins the journey of modeling

34:45.080 --> 34:48.160
this beautiful and complex world of biology.

34:48.160 --> 34:50.480
So you think all the magic of the human mind comes

34:50.480 --> 34:54.280
from this, just a few pounds of mush,

34:54.280 --> 34:58.880
of biological computational mush that's akin

34:58.880 --> 35:02.400
to some of the neural networks, not directly,

35:02.400 --> 35:06.200
but in spirit that DeepMind has been working with.

35:06.200 --> 35:08.680
Well, look, I think it's, you say it's a few, you know,

35:08.680 --> 35:10.880
of course this is the, I think the biggest miracle

35:10.880 --> 35:14.200
of the universe is that it is just a few pounds of mush

35:14.200 --> 35:15.040
in our skulls.

35:15.040 --> 35:18.080
And yet it's also, our brains are the most complex objects

35:18.080 --> 35:20.240
in the, that we know of in the universe.

35:20.240 --> 35:22.360
So there's something profoundly beautiful

35:22.360 --> 35:23.920
and amazing about our brains.

35:23.920 --> 35:28.640
And I think that it's an incredibly,

35:28.640 --> 35:30.720
incredible, efficient machine.

35:30.720 --> 35:35.560
And it's, you know, phenomenon basically.

35:35.560 --> 35:37.480
And I think that building AI,

35:37.480 --> 35:38.920
one of the reasons I want to build AI,

35:38.920 --> 35:41.480
and I've always wanted to, is I think by building

35:41.480 --> 35:43.800
an intelligent artifact like AI,

35:43.800 --> 35:46.480
and then comparing it to the human mind,

35:46.480 --> 35:49.560
that will help us unlock the uniqueness

35:49.560 --> 35:50.960
and the true secrets of the mind

35:50.960 --> 35:53.480
that we've always wondered about since the dawn of history,

35:53.480 --> 35:58.480
like consciousness, dreaming, creativity, emotions.

35:59.160 --> 36:00.760
What are all these things, right?

36:00.760 --> 36:04.200
We've wondered about them since the dawn of humanity.

36:04.200 --> 36:06.360
And I think one of the reasons, and you know,

36:06.360 --> 36:08.280
I love philosophy and philosophy of mind,

36:08.760 --> 36:11.200
we found it difficult, is there haven't been the tools

36:11.200 --> 36:13.680
for us to really, other than introspection to,

36:13.680 --> 36:15.880
from very clever people in history,

36:15.880 --> 36:17.200
very clever philosophers,

36:17.200 --> 36:19.360
to really investigate this scientifically.

36:19.360 --> 36:21.720
But now, suddenly we have a plethora of tools.

36:21.720 --> 36:23.240
Firstly, we have all of the neuroscience tools,

36:23.240 --> 36:25.920
fMRI machines, single cell recording, all of this stuff.

36:25.920 --> 36:29.000
But we also have the ability, computers and AI,

36:29.000 --> 36:31.640
to build intelligent systems.

36:31.640 --> 36:34.720
So I think that, you know,

36:34.720 --> 36:37.320
I think it is amazing what the human mind does.

36:37.400 --> 36:41.160
And I'm kind of in awe of it, really.

36:41.160 --> 36:44.480
And I think it's amazing that, with our human minds,

36:44.480 --> 36:46.800
we're able to build things like computers

36:46.800 --> 36:48.280
and actually even, you know,

36:48.280 --> 36:49.920
think and investigate about these questions.

36:49.920 --> 36:52.760
I think that's also a testament to the human mind.

36:52.760 --> 36:56.240
Yeah, the universe built the human mind

36:56.240 --> 36:59.640
that now is building computers that help us understand

36:59.640 --> 37:01.520
both the universe and our own human mind.

37:01.520 --> 37:02.720
That's right, that's exactly it.

37:02.720 --> 37:03.960
I mean, I think that's one, you know,

37:03.960 --> 37:07.240
one could say we are, maybe we're the mechanism

37:08.160 --> 37:09.000
by which the universe is going to try

37:09.000 --> 37:09.840
and understand itself.

37:09.840 --> 37:10.680
Yeah.

37:10.680 --> 37:13.160
It's beautiful.

37:13.160 --> 37:16.960
So let's go to the basic building blocks of biology

37:16.960 --> 37:19.440
that I think is another angle

37:19.440 --> 37:21.440
at which you can start to understand the human mind,

37:21.440 --> 37:23.400
the human body, which is quite fascinating,

37:23.400 --> 37:26.640
which is from the basic building blocks,

37:26.640 --> 37:28.960
start to simulate, start to model

37:28.960 --> 37:30.480
how from those building blocks,

37:30.480 --> 37:33.080
you can construct bigger and bigger, more complex systems,

37:33.080 --> 37:35.840
maybe one day the entirety of the human biology.

37:35.840 --> 37:39.680
So here's another problem that thought

37:39.680 --> 37:42.720
to be impossible to solve, which is protein folding.

37:42.720 --> 37:47.720
And Alpha Fold, or specifically Alpha Fold 2

37:47.720 --> 37:50.320
did just that, it solved protein folding.

37:50.320 --> 37:53.400
I think it's one of the biggest breakthroughs,

37:53.400 --> 37:55.160
certainly in the history of structural biology,

37:55.160 --> 37:58.200
but in general and in science,

38:00.240 --> 38:04.840
maybe from a high level, what is it and how does it work?

38:04.920 --> 38:08.760
And then we can ask some fascinating questions after.

38:08.760 --> 38:10.040
Sure.

38:10.040 --> 38:12.960
So maybe to explain it to people not familiar

38:12.960 --> 38:15.840
with protein folding is, first of all, explain proteins,

38:15.840 --> 38:18.920
which is proteins are essential to all life.

38:18.920 --> 38:21.600
Every function in your body depends on proteins.

38:21.600 --> 38:24.000
Sometimes they're called the workhorses of biology.

38:24.000 --> 38:24.920
And if you look into them,

38:24.920 --> 38:26.720
and obviously as part of Alpha Fold,

38:26.720 --> 38:30.280
I've been researching proteins and structural biology

38:30.280 --> 38:31.840
for the last few years,

38:31.840 --> 38:34.800
they're amazing little bio nanomachines proteins.

38:34.800 --> 38:36.520
They're incredible if you actually watch little videos

38:36.520 --> 38:39.040
of how they work, animations of how they work.

38:39.040 --> 38:42.640
And proteins are specified by their genetic sequence

38:42.640 --> 38:44.320
called their amino acid sequence.

38:44.320 --> 38:47.080
So you can think of it as their genetic makeup.

38:47.080 --> 38:50.360
And then in the body, in nature,

38:50.360 --> 38:53.360
when they fold up into a 3D structure.

38:53.360 --> 38:55.320
So you can think of it as a string of beads,

38:55.320 --> 38:57.160
and then they fold up into a ball.

38:57.160 --> 38:59.080
Now, the key thing is you want to know

38:59.080 --> 39:01.120
what that 3D structure is,

39:01.120 --> 39:04.520
because the structure, the 3D structure of a protein

39:04.520 --> 39:06.760
is what helps to determine what does it do,

39:06.760 --> 39:08.600
the function it does in your body.

39:08.600 --> 39:12.320
And also, if you're interested in drugs or disease,

39:12.320 --> 39:13.960
you need to understand that 3D structure.

39:13.960 --> 39:15.840
Because if you want to target something

39:15.840 --> 39:18.640
with a drug compound about to block something

39:18.640 --> 39:21.120
the protein's doing, you need to understand

39:21.120 --> 39:23.440
where it's going to bind on the surface of the protein.

39:23.440 --> 39:24.960
So obviously, in order to do that,

39:24.960 --> 39:26.720
you need to understand the 3D structure.

39:26.720 --> 39:28.640
So the structure is mapped to the function.

39:28.640 --> 39:29.880
The structure is mapped to the function.

39:29.880 --> 39:32.520
And the structure is obviously somehow specified

39:32.520 --> 39:34.840
by the amino acid sequence.

39:34.840 --> 39:37.400
And that's, in essence, the protein folding problem is,

39:37.400 --> 39:39.600
can you just from the amino acid sequence,

39:39.600 --> 39:42.520
the one dimensional string of letters,

39:42.520 --> 39:45.560
can you immediately computationally predict

39:45.560 --> 39:47.120
the 3D structure?

39:47.120 --> 39:50.000
And this has been a grand challenge in biology

39:50.000 --> 39:51.520
for over 50 years.

39:51.520 --> 39:54.360
So I think it was first articulated by Christian Anfinsen,

39:54.360 --> 39:57.040
a Nobel Prize winner in 1972,

39:57.040 --> 39:59.240
as part of his Nobel Prize winning lecture.

39:59.280 --> 40:01.880
And he just speculated this should be possible

40:01.880 --> 40:05.000
to go from the amino acid sequence to the 3D structure.

40:05.000 --> 40:06.120
But he didn't say how.

40:06.120 --> 40:09.480
So it's being described to me as equivalent

40:09.480 --> 40:12.360
to Fermat's last theorem, but for biology.

40:12.360 --> 40:15.160
You should, as somebody that very well might win

40:15.160 --> 40:18.360
the Nobel Prize in the future, but outside of that,

40:18.360 --> 40:20.040
you should do more of that kind of thing.

40:20.040 --> 40:22.200
In the margins, just put random things

40:22.200 --> 40:24.480
that will take like 200 years to solve.

40:24.480 --> 40:26.000
Set people off for 200 years.

40:26.000 --> 40:27.120
It should be possible.

40:27.120 --> 40:27.960
Exactly.

40:27.960 --> 40:29.120
Just don't give any of these things.

40:29.120 --> 40:31.520
Exactly, I think everyone's exactly should be,

40:31.520 --> 40:33.560
I'll have to remember that for future.

40:33.560 --> 40:36.320
So yeah, so he set off with this one throw away remark,

40:36.320 --> 40:41.320
just like Fermat, he set off this whole 50 year field really

40:42.480 --> 40:44.440
of computational biology.

40:44.440 --> 40:46.280
And they got stuck.

40:46.280 --> 40:48.600
They hadn't really got very far with doing this.

40:48.600 --> 40:52.520
And until now, until AlphaFold came along,

40:52.520 --> 40:55.520
this is done experimentally, very painstakingly.

40:55.520 --> 40:56.600
So the rule of thumb is,

40:56.640 --> 40:58.680
you have to crystallize the protein,

40:58.680 --> 40:59.800
which is really difficult.

40:59.800 --> 41:03.040
Some proteins can't be crystallized like membrane proteins.

41:03.040 --> 41:05.920
And then you have to use very expensive electron microscopes

41:05.920 --> 41:08.200
or X-ray crystallography machines,

41:08.200 --> 41:10.680
really painstaking work to get the 3D structure

41:10.680 --> 41:12.400
and visualize the 3D structure.

41:12.400 --> 41:14.880
So the rule of thumb in experimental biology

41:14.880 --> 41:16.840
is that it takes one PhD student,

41:16.840 --> 41:19.400
their entire PhD to do one protein.

41:20.280 --> 41:23.440
And with AlphaFold 2, we're able to predict

41:23.440 --> 41:26.400
the 3D structure in a matter of seconds.

41:27.360 --> 41:30.240
Over Christmas, we did the whole human proteome

41:30.240 --> 41:33.280
or every protein in the human body, all 20,000 proteins.

41:33.280 --> 41:34.760
So the human proteome is like the equivalent

41:34.760 --> 41:37.560
of the human genome, but on protein space.

41:37.560 --> 41:40.240
And sort of revolutionize really

41:40.240 --> 41:43.280
what structural biologists can do,

41:43.280 --> 41:45.720
because now they don't have to worry

41:45.720 --> 41:47.960
about these painstaking experimentals,

41:47.960 --> 41:49.560
should they put all of that effort in or not,

41:49.560 --> 41:51.080
they can almost just look up the structure

41:51.080 --> 41:53.280
of their proteins like a Google search.

41:53.280 --> 41:56.880
And so there's a data set on which it's trained

41:56.880 --> 41:58.800
and how to map this amino acid sequence.

41:58.800 --> 42:00.800
First of all, it's incredible that a protein,

42:00.800 --> 42:02.200
this little chemical computer is able

42:02.200 --> 42:03.800
to do that computation itself

42:03.800 --> 42:07.800
in some kind of distributed way and do it very quickly.

42:07.800 --> 42:08.840
That's a weird thing.

42:08.840 --> 42:11.720
And they evolved that way, because in the beginning,

42:11.720 --> 42:13.160
I mean, that's a great invention,

42:13.160 --> 42:14.560
just the protein itself.

42:14.560 --> 42:15.560
Yes, I mean-

42:15.560 --> 42:18.920
And then there's, I think, probably a history of,

42:18.920 --> 42:22.720
like they evolved to have many of these proteins

42:23.560 --> 42:26.560
figure out how to be computers themselves

42:26.560 --> 42:28.520
in such a way that you can create structures

42:28.520 --> 42:30.520
that can interact in complexes with each other

42:30.520 --> 42:32.640
in order to form high level functions.

42:32.640 --> 42:35.480
I mean, it's a weird system that they figured it out.

42:35.480 --> 42:36.320
Well, for sure.

42:36.320 --> 42:38.960
I mean, maybe we should talk about the origins of life too,

42:38.960 --> 42:41.120
but proteins themselves, I think, are magical

42:41.120 --> 42:45.720
and incredible, as I said, little bio-nano machines.

42:45.720 --> 42:50.720
And actually, Leventhal, who is another scientist,

42:50.960 --> 42:55.080
a contemporary of Anfinsen, he coined this Leventhal,

42:55.080 --> 42:56.800
what became known as Leventhal's paradox,

42:56.800 --> 42:58.280
which is exactly what you're saying.

42:58.280 --> 43:01.560
He calculated roughly an average protein,

43:01.560 --> 43:04.920
which is maybe 2,000 amino acids bases long,

43:07.080 --> 43:11.480
can fold in maybe 10 to the power 300 different confirmations.

43:11.480 --> 43:13.320
So there's 10 to the power 300 different ways

43:13.320 --> 43:14.800
that protein could fold up.

43:14.800 --> 43:19.120
And yet somehow, in nature, physics solves this

43:19.120 --> 43:20.520
in a matter of milliseconds.

43:21.360 --> 43:23.080
So proteins fold up in your body,

43:23.080 --> 43:25.600
sometimes in fractions of a second.

43:25.600 --> 43:29.080
So physics is somehow solving that search problem.

43:29.080 --> 43:31.200
And just to be clear, in many of these cases,

43:31.200 --> 43:33.040
maybe you can correct me if I'm wrong,

43:33.040 --> 43:37.680
there's often a unique way for that sequence to form itself.

43:37.680 --> 43:41.240
So among that huge number of possibilities,

43:41.240 --> 43:43.560
it figures out a way how to stably,

43:45.320 --> 43:47.800
in some cases, there might be a dysfunction and so on,

43:47.800 --> 43:50.040
which leads to a lot of the disorders and stuff like that.

43:50.720 --> 43:52.760
Most of the time, it's a unique mapping,

43:52.760 --> 43:54.840
and that unique mapping is not obvious.

43:54.840 --> 43:55.960
No, exactly.

43:55.960 --> 43:57.160
Which is what the problem is.

43:57.160 --> 43:58.960
Exactly, so there's a unique mapping,

43:58.960 --> 44:01.880
usually if it's healthy.

44:01.880 --> 44:05.440
And as you say, in disease, so for example, Alzheimer's,

44:05.440 --> 44:09.040
one conjecture is that it's because of a misfolded protein,

44:09.040 --> 44:12.080
a protein that folds in the wrong way, amyloid beta protein.

44:12.080 --> 44:14.600
So, and then because it folds in the wrong way,

44:14.600 --> 44:17.640
it gets tangled up in your neurons.

44:17.640 --> 44:20.560
So it's super important to understand

44:20.560 --> 44:23.600
both healthy functioning and also disease,

44:23.600 --> 44:26.480
is to understand what these things are doing

44:26.480 --> 44:27.600
and how they're structuring.

44:27.600 --> 44:30.520
Of course, the next step is sometimes proteins change shape

44:30.520 --> 44:32.160
when they interact with something.

44:32.160 --> 44:35.920
So they're not just static necessarily in biology.

44:37.200 --> 44:39.760
Maybe you can give some interesting,

44:39.760 --> 44:41.360
sort of beautiful things to you

44:41.360 --> 44:44.160
about these early days of alpha fold,

44:44.160 --> 44:46.160
of solving this problem,

44:46.160 --> 44:51.160
because unlike games, this is real physical systems

44:51.320 --> 44:55.760
that are less amenable to self-play type of mechanisms.

44:55.760 --> 44:56.600
Sure.

44:56.600 --> 44:58.480
The size of the data set is smaller

44:58.480 --> 44:59.800
than you might otherwise like.

44:59.800 --> 45:01.800
So you have to be very clever about certain things.

45:01.800 --> 45:03.760
Is there something you could speak to?

45:04.840 --> 45:06.720
What was very hard to solve

45:06.720 --> 45:09.960
and what are some beautiful aspects about the solution?

45:09.960 --> 45:12.840
Yeah, I would say alpha fold is the most complex

45:12.840 --> 45:15.880
and also probably most meaningful system we've built so far.

45:16.480 --> 45:19.360
It's been an amazing time actually in the last two, three years

45:19.360 --> 45:20.520
to see that come through,

45:20.520 --> 45:23.200
because as we talked about earlier,

45:23.200 --> 45:25.480
games is what we started on,

45:25.480 --> 45:27.920
building things like AlphaGo and AlphaZero.

45:27.920 --> 45:30.400
But really the ultimate goal was to,

45:30.400 --> 45:31.520
not just to crack games,

45:31.520 --> 45:35.320
it was just to use them to bootstrap general learning systems

45:35.320 --> 45:37.440
we could then apply to real world challenges.

45:37.440 --> 45:40.640
Specifically, my passion is scientific challenges

45:40.640 --> 45:41.920
like protein folding.

45:41.920 --> 45:43.280
And then alpha fold, of course,

45:43.280 --> 45:45.360
is our first big proof point of that.

45:45.360 --> 45:49.040
And so in terms of the data

45:49.040 --> 45:51.400
and the amount of innovations that had to go into it,

45:51.400 --> 45:54.480
it was like more than 30 different component algorithms

45:54.480 --> 45:57.960
needed to be put together to crack the protein folding.

45:57.960 --> 45:59.240
I think some of the big innovations

45:59.240 --> 46:04.240
were that kind of building in some hard-coded constraints

46:04.240 --> 46:07.760
around physics and evolutionary biology

46:07.760 --> 46:10.440
to constrain sort of things like the bond angles

46:11.680 --> 46:14.240
in the protein and things like that.

46:15.760 --> 46:18.040
But not to impact the learning system.

46:18.040 --> 46:21.000
So still allowing the system to be able to learn

46:21.000 --> 46:25.560
the physics itself from the examples that we had.

46:25.560 --> 46:26.640
And the examples, as you say,

46:26.640 --> 46:28.840
there are only about 150,000 proteins,

46:28.840 --> 46:31.240
even after 40 years of experimental biology,

46:31.240 --> 46:33.880
only around 150,000 proteins have been,

46:33.880 --> 46:35.920
the structures have been found out or about.

46:35.920 --> 46:37.120
So that was our training set,

46:37.120 --> 46:41.120
which is much less than normally we would like to use.

46:41.120 --> 46:43.840
But using various tricks, things like self-distillation,

46:43.840 --> 46:48.240
so actually using alpha fold predictions,

46:48.240 --> 46:49.480
some of the best predictions

46:49.480 --> 46:51.000
that it thought was highly confident in,

46:51.000 --> 46:53.320
we put them back into the training set,

46:53.320 --> 46:55.440
to make the training set bigger.

46:55.440 --> 46:58.400
That was critical to alpha fold working.

46:58.400 --> 47:00.160
So there was actually a huge number

47:00.160 --> 47:02.720
of different innovations like that

47:02.720 --> 47:06.080
that were required to ultimately crack the problem.

47:06.080 --> 47:09.720
Alpha fold one, what it produced was a histogram.

47:09.720 --> 47:13.600
So a kind of a matrix of the pairwise distances

47:14.320 --> 47:17.880
between all of the molecules in the protein.

47:17.880 --> 47:20.440
And then there had to be a separate optimization process

47:20.440 --> 47:23.640
to create the 3D structure.

47:23.640 --> 47:25.120
And what we did for alpha fold two

47:25.120 --> 47:26.920
is make it truly end to end.

47:26.920 --> 47:31.720
So we went straight from the amino acid sequence of bases

47:31.720 --> 47:33.880
to the 3D structure directly

47:33.880 --> 47:36.080
without going through this intermediate step.

47:36.080 --> 47:39.040
And in machine learning, what we've always found is that

47:39.040 --> 47:42.160
the more end to end you can make it, the better the system.

47:42.200 --> 47:46.200
And it's probably because in the end,

47:46.200 --> 47:48.520
the system's better at learning what the constraints are

47:48.520 --> 47:51.920
than we are as the human designers of specifying it.

47:51.920 --> 47:54.040
So anytime you can let it flow end to end

47:54.040 --> 47:55.400
and actually just generate what it is

47:55.400 --> 47:58.440
you're really looking for, in this case, the 3D structure,

47:58.440 --> 48:00.560
you're better off than having this intermediate step,

48:00.560 --> 48:03.360
which you then have to handcraft the next step for.

48:03.360 --> 48:06.160
So it's better to let the gradients and the learning

48:06.160 --> 48:09.000
flow all the way through the system from the end point,

48:09.000 --> 48:10.880
the end output you want to the inputs.

48:10.880 --> 48:13.040
So that's a good way to start on a new problem,

48:13.040 --> 48:14.360
handcraft a bunch of stuff,

48:14.360 --> 48:16.680
add a bunch of manual constraints

48:16.680 --> 48:18.680
with a small end to end learning piece

48:18.680 --> 48:21.600
or a small learning piece and grow that learning piece

48:21.600 --> 48:22.880
until it consumes the whole thing.

48:22.880 --> 48:23.720
That's right.

48:23.720 --> 48:25.360
And so you can also see,

48:25.360 --> 48:27.000
this is a bit of a method we've developed

48:27.000 --> 48:29.680
over doing many sort of successful alpha,

48:29.680 --> 48:32.280
we call them alpha X projects, right?

48:32.280 --> 48:33.800
And the easiest way to see that

48:33.800 --> 48:36.760
is the evolution of alpha go to alpha zero.

48:36.760 --> 48:39.680
So alpha go was a learning system,

48:39.680 --> 48:42.320
but it was specifically trained to only play go, right?

48:42.320 --> 48:45.400
So, and what we wanted to do with first version of alpha go

48:45.400 --> 48:47.560
is just get to world champion performance

48:47.560 --> 48:49.240
no matter how we did it, right?

48:49.240 --> 48:51.440
And then, of course, alpha go zero,

48:51.440 --> 48:55.280
we remove the need to use human games as a starting point,

48:55.280 --> 48:56.120
right?

48:56.120 --> 48:58.000
So it could just play against itself

48:58.000 --> 49:00.320
from random starting point from the beginning.

49:00.320 --> 49:03.760
So that removed the need for human knowledge about go.

49:03.760 --> 49:06.000
And then finally, alpha zero then generalized it

49:06.000 --> 49:08.960
so that any things we had in there, the system,

49:08.960 --> 49:12.280
including things like symmetry of the go board were removed.

49:12.280 --> 49:15.640
So the alpha zero could play from scratch any two player game

49:15.640 --> 49:17.480
and then mu zero, which is the final,

49:17.480 --> 49:19.680
our latest version of that set of things

49:19.680 --> 49:20.720
was then extending it

49:20.720 --> 49:22.160
so that you didn't even have to give it

49:22.160 --> 49:23.240
the rules of the game.

49:23.240 --> 49:24.920
It would learn that for itself.

49:24.920 --> 49:26.680
So it could also deal with computer games

49:26.680 --> 49:27.800
as well as board games.

49:27.800 --> 49:29.560
So that line of alpha go, alpha go zero,

49:29.560 --> 49:31.880
alpha zero, mu zero,

49:31.880 --> 49:34.240
that's the full trajectory of what you can take

49:34.240 --> 49:39.240
from imitation learning to full self-supervised learning.

49:40.440 --> 49:41.640
Yeah, exactly.

49:41.640 --> 49:45.520
And learning the entire structure of the environment

49:45.520 --> 49:47.640
you put in from scratch, right?

49:47.640 --> 49:51.840
And bootstrapping it through self-play yourself.

49:51.840 --> 49:53.720
But the thing is, it would have been impossible, I think,

49:53.720 --> 49:57.400
or very hard for us to build alpha zero or mu zero first

49:57.400 --> 49:58.600
out of the box.

49:58.600 --> 49:59.640
Even psychologically,

49:59.640 --> 50:03.040
because you have to believe in yourself for a very long time.

50:03.240 --> 50:04.640
You're constantly dealing with doubt

50:04.640 --> 50:06.680
because a lot of people say that it's impossible.

50:06.680 --> 50:07.520
Exactly.

50:07.520 --> 50:08.640
So it was hard enough just to do go,

50:08.640 --> 50:09.480
as you were saying,

50:09.480 --> 50:10.920
everyone thought that was impossible

50:10.920 --> 50:12.760
or at least a decade away

50:12.760 --> 50:17.320
from when we did it back in 2015, 2016.

50:17.320 --> 50:20.960
And so, yes, it would have been psychologically

50:20.960 --> 50:22.040
probably very difficult,

50:22.040 --> 50:23.480
as well as the fact that, of course,

50:23.480 --> 50:26.400
we learned a lot by building alpha go first.

50:26.400 --> 50:28.520
Right, so I think this is why I call AI

50:28.520 --> 50:29.880
an engineering science.

50:29.880 --> 50:32.280
It's one of the most fascinating science disciplines,

50:32.280 --> 50:33.680
but it's also an engineering science

50:33.680 --> 50:36.600
in the sense that unlike natural sciences,

50:36.600 --> 50:39.440
the phenomenon you're studying doesn't exist out in nature.

50:39.440 --> 50:40.880
You have to build it first.

50:40.880 --> 50:42.480
So you have to build the artifact first

50:42.480 --> 50:46.480
and then you can study and pull it apart and how it works.

50:46.480 --> 50:50.000
This is tough to ask you this question

50:50.000 --> 50:51.480
because you probably will say it's everything,

50:51.480 --> 50:54.360
but let's try to think through this

50:54.360 --> 50:56.480
because you're in a very interesting position

50:56.480 --> 51:00.320
where DeepMind is a place of some of the most brilliant ideas

51:00.320 --> 51:01.760
in the history of AI,

51:01.760 --> 51:04.600
but it's also a place of brilliant engineering.

51:05.880 --> 51:08.040
So how much of solving intelligence,

51:08.040 --> 51:09.880
this big goal for DeepMind,

51:09.880 --> 51:12.120
how much of it is science?

51:12.120 --> 51:13.320
How much is engineering?

51:13.320 --> 51:14.720
So how much is the algorithms?

51:14.720 --> 51:16.160
How much is the data?

51:16.160 --> 51:19.840
How much is the hardware compute infrastructure?

51:19.840 --> 51:22.760
How much is it the software compute infrastructure?

51:23.960 --> 51:24.800
What else is there?

51:24.800 --> 51:27.200
How much is the human infrastructure?

51:27.200 --> 51:30.680
And like just the humans interacting in certain kinds of ways.

51:30.720 --> 51:31.720
That's the base of all those ideas.

51:31.720 --> 51:33.640
And how much is maybe like philosophy?

51:33.640 --> 51:35.120
How much, what's the key?

51:38.240 --> 51:40.720
If you were to sort of look back,

51:40.720 --> 51:43.240
like if we go forward 200 years and look back,

51:43.240 --> 51:46.360
what was the key thing that solved intelligence?

51:46.360 --> 51:48.240
Is it the ideas or the engineering?

51:48.240 --> 51:49.080
I think it's a combination.

51:49.080 --> 51:49.920
First of all, of course,

51:49.920 --> 51:51.400
it's a combination of all those things,

51:51.400 --> 51:54.800
but the ratios of them changed over time.

51:54.800 --> 51:57.520
So even in the last 12 years,

51:57.520 --> 51:59.440
so we started DeepMind in 2010,

51:59.440 --> 52:00.720
which is hard to imagine now,

52:00.720 --> 52:03.400
because 2010, it's only 12 short years ago,

52:03.400 --> 52:05.600
but nobody was talking about AI.

52:05.600 --> 52:07.600
I don't know if you remember back to your MIT days,

52:07.600 --> 52:08.440
no one was talking about it.

52:08.440 --> 52:11.080
I did a postdoc at MIT back around then,

52:11.080 --> 52:12.880
and it was sort of thought of as a,

52:12.880 --> 52:14.200
well, look, we know AI doesn't work.

52:14.200 --> 52:17.040
We tried this hard in the 90s at places like MIT,

52:17.040 --> 52:19.880
mostly using logic systems and old fashioned,

52:19.880 --> 52:22.600
sort of good old fashioned AI, we would call it now.

52:22.600 --> 52:25.320
People like Minsky and Patrick Winston,

52:25.320 --> 52:26.720
and you know all these characters, right?

52:26.720 --> 52:28.280
And I used to debate a few of them,

52:28.280 --> 52:30.080
and they used to think I was mad thinking about

52:30.080 --> 52:32.320
that some new advance could be done with learning systems.

52:32.320 --> 52:34.720
And I was actually pleased to hear that,

52:34.720 --> 52:36.920
because at least you know you're on a unique track

52:36.920 --> 52:37.840
at that point, right?

52:37.840 --> 52:41.840
Even if all of your professors are telling you you're mad.

52:41.840 --> 52:43.840
And of course, in industry,

52:43.840 --> 52:47.680
we couldn't get, it was difficult to get two cents together,

52:47.680 --> 52:48.920
which is hard to imagine now as well,

52:48.920 --> 52:51.520
given that it's the biggest sort of buzzword in VCs

52:51.520 --> 52:54.680
and fundraising's easy and all these kinds of things today.

52:54.680 --> 52:57.680
So back in 2010, it was very difficult.

52:58.280 --> 52:59.400
The reason we started then,

52:59.400 --> 53:01.200
and Shane and I used to discuss,

53:02.520 --> 53:04.960
what were the sort of founding tenets of DeepMind?

53:04.960 --> 53:06.160
And it was various things.

53:06.160 --> 53:08.720
One was algorithmic advances.

53:08.720 --> 53:10.760
So deep learning, Jeff Hinton and Co.

53:10.760 --> 53:13.160
had just sort of invented that in academia,

53:13.160 --> 53:15.240
but no one in industry knew about it.

53:15.240 --> 53:16.680
We love reinforcement learning,

53:16.680 --> 53:18.280
we thought that could be scaled up.

53:18.280 --> 53:20.200
But also understanding about the human brain

53:20.200 --> 53:23.960
had advanced quite a lot in the decade prior

53:23.960 --> 53:25.480
with fMRI machines and other things.

53:25.480 --> 53:28.880
So we could get some good hints about architectures

53:28.880 --> 53:32.520
and algorithms and sort of representations maybe

53:32.520 --> 53:33.440
that the brain uses.

53:33.440 --> 53:36.920
So at a systems level, not at a implementation level.

53:37.800 --> 53:41.080
And then the other big things were compute and GPUs.

53:41.080 --> 53:44.160
So we could see a compute was gonna be really useful

53:44.160 --> 53:47.000
and it got to a place where it become commoditized,

53:47.000 --> 53:48.600
mostly through the games industry,

53:48.600 --> 53:50.800
and that could be taken advantage of.

53:50.800 --> 53:52.840
And then the final thing was also mathematical

53:52.840 --> 53:55.000
and theoretical definitions of intelligence.

53:55.000 --> 53:57.600
So things like AIXI, AIXC,

53:57.600 --> 54:00.240
which Shane worked on with his supervisor, Marcus Hutter,

54:00.240 --> 54:03.400
which is this sort of theoretical proof really

54:03.400 --> 54:05.360
of universal intelligence,

54:05.360 --> 54:08.600
which is actually reinforcement learning system in the limit.

54:08.600 --> 54:10.680
I mean, it seems infinite compute and infinite memory

54:10.680 --> 54:12.960
in the way like a Turing machine proves.

54:12.960 --> 54:15.880
But I was also waiting to see something like that too,

54:15.880 --> 54:19.520
to like Turing machines and computation theory

54:19.520 --> 54:21.600
that people like Turing and Shannon came up with

54:21.600 --> 54:23.720
underpins modern computer science.

54:25.000 --> 54:26.480
I was waiting for a theory like that

54:26.480 --> 54:28.960
to sort of underpin AGI research.

54:28.960 --> 54:31.040
So when I met Shane and saw he was working

54:31.040 --> 54:32.280
on something like that,

54:32.280 --> 54:34.640
that to me was a sort of final piece of the jigsaw.

54:34.640 --> 54:36.520
So in the early days,

54:36.520 --> 54:39.600
I would say that ideas were the most important.

54:40.560 --> 54:42.520
For us, it was deep reinforcement learning,

54:42.520 --> 54:44.720
scaling up deep learning.

54:44.720 --> 54:46.320
Of course, we've seen transformers.

54:46.320 --> 54:48.200
So huge leaps, I would say,

54:48.200 --> 54:51.600
three or four, if you think from 2010 till now,

54:51.600 --> 54:53.760
huge evolutions, things like AlphaGo.

54:54.680 --> 54:58.480
And maybe there's a few more still needed.

54:58.480 --> 55:01.720
But as we get closer to AI, AGI,

55:02.600 --> 55:06.040
I think engineering becomes more and more important and data

55:06.040 --> 55:09.880
because scale and of course the recent results of GPT-3

55:09.880 --> 55:11.720
and all the big language models and large models,

55:11.720 --> 55:13.400
including our ones,

55:13.400 --> 55:16.600
has shown that scale and large models

55:16.600 --> 55:18.680
are clearly gonna be a necessary,

55:18.680 --> 55:22.560
but perhaps not sufficient part of an AGI solution.

55:22.560 --> 55:25.080
And throughout that, like you said,

55:25.080 --> 55:27.280
and I'd like to give you a big thank you.

55:27.280 --> 55:30.000
You're one of the pioneers in this is

55:30.000 --> 55:32.720
sticking by ideas like reinforcement learning,

55:32.720 --> 55:34.160
that this can actually work,

55:35.160 --> 55:39.040
given actually limited success in the past.

55:39.040 --> 55:42.040
And also, which we still don't know,

55:42.040 --> 55:47.040
but proudly having the best researchers in the world

55:47.320 --> 55:49.920
and talking about solving intelligence.

55:49.920 --> 55:51.480
So talking about whatever you call it,

55:51.480 --> 55:53.800
AGI or something like this,

55:53.800 --> 55:55.320
that's speaking of MIT,

55:55.320 --> 55:57.880
that's just something you wouldn't bring up.

55:57.880 --> 55:58.720
No.

56:00.040 --> 56:04.160
Maybe you did in like 40, 50 years ago,

56:04.160 --> 56:05.480
but that was,

56:07.440 --> 56:11.480
AI was a place where you do tinkering, very small scale,

56:11.480 --> 56:13.160
not very ambitious projects.

56:13.160 --> 56:16.760
And maybe the biggest ambitious projects

56:16.760 --> 56:18.040
were in the space of robotics

56:18.040 --> 56:19.760
and doing like the DARPA challenge.

56:19.760 --> 56:21.840
But the task of solving intelligence

56:21.840 --> 56:23.760
and believing you can,

56:23.760 --> 56:24.960
that's really, really powerful.

56:24.960 --> 56:28.280
So in order for engineering to do its work,

56:28.280 --> 56:31.320
to have great engineers build great systems,

56:31.320 --> 56:32.720
you have to have that belief

56:32.720 --> 56:34.320
that threads throughout the whole thing

56:34.320 --> 56:35.440
that you can actually solve

56:35.440 --> 56:37.040
some of these impossible challenges.

56:37.040 --> 56:37.880
Yeah, that's right.

56:37.880 --> 56:41.120
And back in 2010, our mission statement

56:41.120 --> 56:42.680
and still is today,

56:42.680 --> 56:45.960
it used to be solving step one, solve intelligence.

56:45.960 --> 56:47.880
Step two, use it to solve everything else.

56:47.880 --> 56:51.160
So if you can imagine pitching that to a VC in 2010,

56:51.160 --> 56:52.720
the kind of looks we got,

56:52.720 --> 56:55.920
we managed to find a few kooky people to back us,

56:55.920 --> 56:57.720
but it was tricky.

56:57.720 --> 57:00.200
And it got to the point where we wouldn't mention it

57:00.200 --> 57:01.600
to any of our professors

57:01.600 --> 57:03.160
because they would just eye roll

57:03.160 --> 57:05.840
and think we committed career suicide.

57:05.840 --> 57:10.080
And so there's a lot of things that we had to do,

57:10.080 --> 57:11.600
but we always believed it.

57:11.600 --> 57:13.280
And one reason, by the way,

57:13.280 --> 57:16.200
one reason I've always believed in reinforcement learning

57:16.200 --> 57:19.160
is that if you look at neuroscience,

57:19.160 --> 57:22.720
that is the way that the primate brain learns.

57:22.720 --> 57:24.880
One of the main mechanisms is the dopamine system

57:24.880 --> 57:26.440
implements some form of TD learning

57:26.440 --> 57:28.680
as very famous result in the late 90s,

57:29.680 --> 57:31.320
where they saw this in monkeys

57:31.320 --> 57:34.520
and as a propagating prediction error.

57:34.520 --> 57:36.800
So again, in the limit,

57:36.800 --> 57:38.800
this is what I think you can use neuroscience for

57:38.800 --> 57:41.160
is in any mathematics,

57:41.160 --> 57:43.160
when you're doing something as ambitious

57:43.160 --> 57:44.560
as trying to solve intelligence

57:44.560 --> 57:46.480
and it's blue sky research,

57:46.480 --> 57:47.760
no one knows how to do it,

57:47.760 --> 57:50.160
you need to use any evidence

57:50.160 --> 57:52.120
or any source of information you can

57:52.120 --> 57:54.280
to help guide you in the right direction

57:54.280 --> 57:56.680
or give you confidence you're going in the right direction.

57:56.680 --> 57:59.840
So that was one reason we pushed so hard on that.

57:59.840 --> 58:01.840
And just going back to your earlier question

58:01.840 --> 58:03.160
about organization,

58:03.160 --> 58:05.360
the other big thing that I think we innovated with

58:05.360 --> 58:10.320
at DeepMind to encourage invention and innovation

58:10.320 --> 58:12.920
was the multidisciplinary organization we built

58:12.920 --> 58:14.160
and we still have today.

58:14.680 --> 58:16.680
DeepMind originally was a confluence

58:16.680 --> 58:19.400
of the most cutting edge knowledge in neuroscience

58:19.400 --> 58:22.840
with machine learning, engineering and mathematics

58:22.840 --> 58:24.400
and gaming.

58:24.400 --> 58:26.760
And then since then, we've built that out even further.

58:26.760 --> 58:30.280
So we have philosophers here and by ethicists,

58:30.280 --> 58:33.160
but also other types of scientists, physicists and so on.

58:33.160 --> 58:35.160
And that's what brings together,

58:35.160 --> 58:38.760
I tried to build a sort of new type of Bell Labs,

58:38.760 --> 58:43.760
but in its golden era and a new expression of that

58:43.800 --> 58:48.440
to try and foster this incredible sort of innovation machine.

58:48.440 --> 58:50.560
So talking about the humans in the machine,

58:50.560 --> 58:53.040
DeepMind itself is a learning machine

58:53.040 --> 58:55.560
with a lots of amazing human minds in it

58:55.560 --> 58:58.840
coming together to try and build these learning systems.

59:00.320 --> 59:04.920
If we return to the big ambitious dream of AlphaFold

59:04.920 --> 59:08.320
that may be the early steps on a very long journey

59:08.320 --> 59:10.720
in biology,

59:11.400 --> 59:14.200
do you think the same kind of approach

59:14.200 --> 59:16.400
can use to predict the structure and function

59:16.400 --> 59:18.720
of more complex biological systems?

59:18.720 --> 59:21.480
So multi-protein interaction,

59:21.480 --> 59:24.400
and then, I mean, you can go up from there,

59:24.400 --> 59:26.920
just simulating bigger and bigger systems

59:26.920 --> 59:29.560
that eventually simulate something like the human brain

59:29.560 --> 59:32.560
or the human body, just the big mush,

59:32.560 --> 59:36.440
the mess of the beautiful, resilient mess of biology.

59:36.440 --> 59:39.600
Do you see that as a long-term vision?

59:39.600 --> 59:42.560
I do, and I think if you think about

59:42.560 --> 59:45.640
what are the top things I wanted to apply AI to

59:45.640 --> 59:47.680
once we had powerful enough systems,

59:47.680 --> 59:52.200
biology and curing diseases and understanding biology

59:52.200 --> 59:54.120
was right up there, top of my list.

59:54.120 --> 59:56.760
That's one of the reasons I personally pushed that myself

59:56.760 --> 59:58.080
and with AlphaFold.

59:58.080 --> 01:00:01.200
But I think AlphaFold, amazing as it is,

01:00:01.200 --> 01:00:03.000
is just the beginning,

01:00:03.000 --> 01:00:07.160
and I hope it's evidence of what could be done

01:00:07.160 --> 01:00:08.800
with computational methods.

01:00:08.840 --> 01:00:12.200
So AlphaFold solved this huge problem

01:00:12.200 --> 01:00:15.240
of the structure of proteins, but biology's dynamic.

01:00:15.240 --> 01:00:16.920
So really, what I imagine from here,

01:00:16.920 --> 01:00:18.640
and we're working on all these things now,

01:00:18.640 --> 01:00:21.480
is protein-protein interaction,

01:00:21.480 --> 01:00:25.400
protein-ligand binding, so reacting with molecules.

01:00:25.400 --> 01:00:27.640
Then you wanna build up to pathways,

01:00:27.640 --> 01:00:30.000
and then eventually a virtual cell.

01:00:30.000 --> 01:00:32.680
That's my dream, maybe in the next 10 years.

01:00:32.680 --> 01:00:34.560
And I've been talking actually to a lot of biologists,

01:00:34.560 --> 01:00:36.800
friends of mine, Paul Nurse, who runs the Crick Institute,

01:00:36.800 --> 01:00:39.120
amazing biologists, Nobel Prize-winning biologists.

01:00:39.120 --> 01:00:42.120
We've been discussing for 20 years now virtual cells.

01:00:42.120 --> 01:00:44.760
Could you build a virtual simulation of a cell?

01:00:44.760 --> 01:00:46.280
And if you could, that would be incredible

01:00:46.280 --> 01:00:48.120
for biology and disease discovery,

01:00:48.120 --> 01:00:50.720
because you could do loads of experiments on the virtual cell

01:00:50.720 --> 01:00:53.960
and then only at the last stage validate it in the wet lab.

01:00:53.960 --> 01:00:56.440
So you could, in terms of the search space

01:00:56.440 --> 01:00:58.080
of discovering new drugs,

01:00:58.080 --> 01:01:03.080
it takes 10 years roughly to go from identifying a target

01:01:03.400 --> 01:01:06.520
to having a drug candidate.

01:01:06.520 --> 01:01:09.360
Maybe that could be shortened to by an order of magnitude

01:01:09.360 --> 01:01:13.160
with if you could do most of that work in silico.

01:01:13.160 --> 01:01:15.760
So in order to get to a virtual cell,

01:01:15.760 --> 01:01:18.360
we have to build up understanding

01:01:18.360 --> 01:01:20.760
of different parts of biology and the interactions.

01:01:20.760 --> 01:01:24.560
And so every few years we talk about this,

01:01:24.560 --> 01:01:25.640
I talked about this with Paul.

01:01:25.640 --> 01:01:27.840
And then finally, last year after AlphaFold,

01:01:27.840 --> 01:01:30.600
I said, now's the time we can finally go for it.

01:01:30.600 --> 01:01:32.360
And AlphaFold was the first proof point

01:01:32.360 --> 01:01:33.800
that this might be possible.

01:01:33.800 --> 01:01:34.840
And he's very exciting.

01:01:34.840 --> 01:01:37.280
We have some collaborations with his lab.

01:01:37.280 --> 01:01:39.200
They're just across the road, actually, from our search.

01:01:39.200 --> 01:01:41.000
It's a wonderful being here in King's Cross

01:01:41.000 --> 01:01:42.920
with the Crick Institute across the road.

01:01:42.920 --> 01:01:46.000
And I think the next steps,

01:01:46.000 --> 01:01:48.680
I think there's gonna be some amazing advances in biology

01:01:48.680 --> 01:01:51.000
built on top of things like AlphaFold.

01:01:51.000 --> 01:01:53.160
We're already seeing that with the community doing that

01:01:53.160 --> 01:01:56.040
after we've open-sourced it and released it.

01:01:56.040 --> 01:02:00.160
And I often say that I think,

01:02:00.160 --> 01:02:02.320
if you think of mathematics

01:02:02.320 --> 01:02:05.000
as the perfect description language for physics,

01:02:05.000 --> 01:02:06.840
I think AI might end up being

01:02:06.840 --> 01:02:09.200
the perfect description language for biology

01:02:09.200 --> 01:02:11.680
because biology is so messy.

01:02:11.680 --> 01:02:15.240
It's so emergent, so dynamic and complex.

01:02:15.240 --> 01:02:16.840
I think I find it very hard to believe

01:02:16.840 --> 01:02:18.520
we'll ever get to something as elegant

01:02:18.520 --> 01:02:21.680
as Newton's laws of motions to describe a cell.

01:02:21.680 --> 01:02:23.520
It's just too complicated.

01:02:23.520 --> 01:02:26.120
So I think AI is the right tool for this.

01:02:26.120 --> 01:02:29.400
You have to start at the basic building blocks

01:02:29.400 --> 01:02:31.640
and use AI to run the simulation

01:02:31.640 --> 01:02:32.800
for all those building blocks.

01:02:32.800 --> 01:02:35.960
So have a very strong way to do prediction

01:02:35.960 --> 01:02:37.760
of what, given these building blocks,

01:02:37.760 --> 01:02:40.840
what kind of biology, how the function

01:02:40.840 --> 01:02:43.600
and the evolution of that biological system.

01:02:43.600 --> 01:02:45.240
It's almost like a cellular automata.

01:02:45.240 --> 01:02:46.080
You have to run it.

01:02:46.080 --> 01:02:47.840
You can't analyze it from a high level.

01:02:47.840 --> 01:02:49.800
You have to take the basic ingredients,

01:02:49.800 --> 01:02:51.920
figure out the rules and let it run.

01:02:51.920 --> 01:02:52.760
But in this case,

01:02:52.760 --> 01:02:54.640
the rules are very difficult to figure out.

01:02:54.640 --> 01:02:56.160
You have to learn them.

01:02:56.160 --> 01:02:57.000
That's exactly it.

01:02:57.000 --> 01:03:00.760
So the biology is too complicated to figure out the rules.

01:03:00.760 --> 01:03:03.560
It's too emergent, too dynamic,

01:03:03.560 --> 01:03:05.040
say compared to a physics system

01:03:05.040 --> 01:03:07.000
like the motion of a planet.

01:03:07.000 --> 01:03:09.160
And so you have to learn the rules.

01:03:09.160 --> 01:03:11.880
And that's exactly the type of systems that we're building.

01:03:11.880 --> 01:03:14.760
So you mentioned you've open-sourced Alpha Fold

01:03:14.760 --> 01:03:16.600
and even the data involved.

01:03:16.600 --> 01:03:20.040
To me personally, also really happy

01:03:20.040 --> 01:03:23.520
and a big thank you for open-sourcing with JoCo,

01:03:23.520 --> 01:03:24.840
the physics simulation engine

01:03:24.840 --> 01:03:29.080
that's often used for robotics research and so on.

01:03:29.080 --> 01:03:31.120
So I think that's a pretty gangster move.

01:03:31.120 --> 01:03:32.520
So what's the...

01:03:35.160 --> 01:03:39.080
Very few companies or people do that kind of thing.

01:03:39.080 --> 01:03:41.200
What's the philosophy behind that?

01:03:41.200 --> 01:03:42.920
You know, it's a case-by-case basis.

01:03:42.920 --> 01:03:44.040
And in both those cases,

01:03:44.040 --> 01:03:47.360
we felt that was the maximum benefit to humanity to do that.

01:03:47.360 --> 01:03:49.480
And the scientific community,

01:03:49.480 --> 01:03:53.000
in one case the robotics physics community with JoCo.

01:03:53.000 --> 01:03:55.600
So we purchased it for open-source.

01:03:55.600 --> 01:03:57.640
Yes, we purchased it for the express principle

01:03:57.640 --> 01:03:58.560
to open-source it.

01:03:58.560 --> 01:04:02.440
So, you know, I hope people appreciate that.

01:04:02.440 --> 01:04:04.040
It's great to hear that you do.

01:04:04.040 --> 01:04:05.800
And then the second thing was...

01:04:05.800 --> 01:04:07.960
And mostly we did it because the person building it

01:04:07.960 --> 01:04:11.920
was not able to cope with supporting it anymore

01:04:11.920 --> 01:04:13.600
because it got too big for him.

01:04:13.600 --> 01:04:16.720
He's an amazing professor who built it in the first place.

01:04:16.720 --> 01:04:18.200
So we helped him out with that.

01:04:18.200 --> 01:04:20.520
And then with Alpha Fold is even bigger, I would say.

01:04:20.520 --> 01:04:21.960
And I think in that case,

01:04:21.960 --> 01:04:25.520
we decided that there were so many downstream applications

01:04:25.560 --> 01:04:29.400
of Alpha Fold that we couldn't possibly even imagine

01:04:29.400 --> 01:04:30.480
what they all were.

01:04:30.480 --> 01:04:34.360
So the best way to accelerate drug discovery

01:04:34.360 --> 01:04:36.400
and also fundamental research

01:04:36.400 --> 01:04:39.560
would be to give all that data away

01:04:39.560 --> 01:04:43.240
and the system itself.

01:04:43.240 --> 01:04:45.280
You know, it's been so gratifying to see

01:04:45.280 --> 01:04:47.040
what people have done that within just one year,

01:04:47.040 --> 01:04:49.240
which is a short amount of time in science.

01:04:49.240 --> 01:04:54.160
And it's been used by over 500,000 researchers have used it.

01:04:54.160 --> 01:04:56.560
We think that's almost every biologist in the world.

01:04:56.560 --> 01:04:58.840
I think there's roughly 500,000 biologists in the world,

01:04:58.840 --> 01:05:00.000
professional biologists,

01:05:00.000 --> 01:05:03.320
have used it to look at their proteins of interest.

01:05:04.480 --> 01:05:06.520
We've seen amazing fundamental research done.

01:05:06.520 --> 01:05:09.040
So a couple of weeks ago, front cover,

01:05:09.040 --> 01:05:10.840
there was a whole special issue of science,

01:05:10.840 --> 01:05:12.040
including the front cover,

01:05:12.040 --> 01:05:14.000
which had the nuclear pore complex on it,

01:05:14.000 --> 01:05:15.800
which is one of the biggest proteins in the body.

01:05:15.800 --> 01:05:18.960
The nuclear pore complex is a protein that governs

01:05:18.960 --> 01:05:21.680
all the nutrients going in and out of your cell nucleus.

01:05:21.680 --> 01:05:24.760
So they're like little whole gateways that open and close

01:05:24.760 --> 01:05:27.320
to let things go in and out of your cell nucleus.

01:05:27.320 --> 01:05:29.400
So they're really important, but they're huge

01:05:29.400 --> 01:05:31.680
because they're massive donut ring shaped things.

01:05:31.680 --> 01:05:33.440
And they've been looking to try and figure out

01:05:33.440 --> 01:05:35.000
that structure for decades.

01:05:35.000 --> 01:05:37.160
And they have lots of experimental data,

01:05:37.160 --> 01:05:39.600
but it's too low resolution, there's bits missing.

01:05:39.600 --> 01:05:43.080
And they were able to, like a giant Lego jigsaw puzzle,

01:05:43.080 --> 01:05:46.200
use alpha fold predictions plus experimental data

01:05:46.200 --> 01:05:49.760
and combined those two independent sources of information,

01:05:49.760 --> 01:05:51.240
actually four different groups around the world

01:05:51.240 --> 01:05:54.600
were able to put it together more or less simultaneously

01:05:54.600 --> 01:05:56.280
using alpha fold predictions.

01:05:56.280 --> 01:05:57.720
So that's been amazing to see.

01:05:57.720 --> 01:05:59.400
And pretty much every pharma company,

01:05:59.400 --> 01:06:01.440
every drug company executive I've spoken to

01:06:01.440 --> 01:06:03.760
has said that their teams are using alpha fold

01:06:03.760 --> 01:06:08.080
to accelerate whatever drugs they're trying to discover.

01:06:08.080 --> 01:06:11.440
So I think the knock-on effect has been enormous

01:06:11.440 --> 01:06:15.280
in terms of the impact that alpha fold has made.

01:06:15.280 --> 01:06:17.840
And it's probably bringing in, it's creating biologists,

01:06:17.840 --> 01:06:20.800
it's bringing more people into the field,

01:06:20.800 --> 01:06:21.840
both on the excitement

01:06:21.840 --> 01:06:24.560
and both on the technical skills involved.

01:06:24.560 --> 01:06:28.800
And it's almost like a gateway drug to biology.

01:06:28.800 --> 01:06:29.640
Yes, it is.

01:06:29.640 --> 01:06:32.680
And to get more computational people involved too, hopefully.

01:06:32.680 --> 01:06:35.200
And I think for us, the next stage,

01:06:35.200 --> 01:06:37.800
but as I said, in future we have to have other considerations

01:06:37.800 --> 01:06:39.640
too, we're building on top of alpha fold

01:06:39.640 --> 01:06:41.240
and these other ideas I discussed with you

01:06:41.240 --> 01:06:42.800
about protein-protein interactions

01:06:42.800 --> 01:06:44.840
and genomics and other things.

01:06:44.840 --> 01:06:46.240
And not everything will be open source.

01:06:46.240 --> 01:06:48.040
Some of it we'll do commercially

01:06:48.040 --> 01:06:49.040
because that will be the best way

01:06:49.040 --> 01:06:51.720
to actually get the most resources and impact behind it.

01:06:51.720 --> 01:06:53.520
In other ways, some other projects

01:06:53.520 --> 01:06:55.320
we'll do non-profit style.

01:06:55.320 --> 01:06:58.560
And also we have to consider for future things as well,

01:06:58.560 --> 01:07:01.640
safety and ethics as well, like synthetic biology,

01:07:01.640 --> 01:07:03.640
there is dual use.

01:07:03.640 --> 01:07:05.120
And we have to think about that as well.

01:07:05.120 --> 01:07:08.640
With alpha fold, we consulted with 30 different bioethicists

01:07:08.640 --> 01:07:10.280
and other people expert in this field

01:07:10.280 --> 01:07:13.320
to make sure it was safe before we released it.

01:07:13.320 --> 01:07:15.320
So there'll be other considerations in future,

01:07:15.320 --> 01:07:17.160
but for right now, I think alpha fold

01:07:17.320 --> 01:07:20.880
is a kind of a gift from us to the scientific community.

01:07:20.880 --> 01:07:24.240
So I'm pretty sure that something like alpha fold

01:07:25.640 --> 01:07:29.160
would be part of Nobel prizes in the future,

01:07:29.160 --> 01:07:32.560
but us humans, of course, are horrible with credit assignment.

01:07:32.560 --> 01:07:34.560
So we'll of course give it to the humans.

01:07:35.640 --> 01:07:39.440
Do you think there will be a day when AI system

01:07:40.800 --> 01:07:45.160
can't be denied that it earned that Nobel prize?

01:07:45.160 --> 01:07:47.440
Do you think we will see that in 21st century?

01:07:47.440 --> 01:07:50.240
It depends what type of AIs we end up building,

01:07:50.240 --> 01:07:55.240
whether they're goal seeking agents who specifies the goals,

01:07:55.960 --> 01:07:57.840
who comes up with the hypotheses,

01:07:57.840 --> 01:08:00.840
who determines which problems to tackle.

01:08:00.840 --> 01:08:03.080
And tweets about it, announcement of the results.

01:08:03.080 --> 01:08:04.200
Yes, it's announcement of the results,

01:08:04.200 --> 01:08:05.480
exactly as part of it.

01:08:06.400 --> 01:08:10.960
So I think right now, of course it's amazing human ingenuity

01:08:10.960 --> 01:08:12.200
that's behind these systems.

01:08:12.200 --> 01:08:15.120
And then the system, in my opinion, is just a tool,

01:08:16.120 --> 01:08:18.400
which would be a bit like saying with Galileo

01:08:18.400 --> 01:08:20.640
and his telescope, the ingenuity,

01:08:20.640 --> 01:08:22.400
the credit should go to the telescope.

01:08:22.400 --> 01:08:24.840
I mean, it's clearly Galileo building the tool

01:08:24.840 --> 01:08:26.400
which he then uses.

01:08:26.400 --> 01:08:28.600
So I still see that in the same way today,

01:08:28.600 --> 01:08:31.320
even though these tools learn for themselves.

01:08:31.320 --> 01:08:34.280
I think of things like alpha fold

01:08:34.280 --> 01:08:37.000
and the things we're building as the ultimate tools

01:08:37.000 --> 01:08:39.720
for science and for acquiring new knowledge

01:08:39.720 --> 01:08:42.400
to help us as scientists acquire new knowledge.

01:08:42.440 --> 01:08:43.240
So I think there will come a point

01:08:43.240 --> 01:08:46.400
where an AI system may solve

01:08:46.400 --> 01:08:48.840
or come up with something like general relativity

01:08:48.840 --> 01:08:52.080
of its own bat, not just by averaging everything

01:08:52.080 --> 01:08:55.280
on the internet or averaging everything on PubMed,

01:08:55.280 --> 01:08:56.360
although that would be interesting to see

01:08:56.360 --> 01:08:58.560
what that would come up with.

01:08:58.560 --> 01:09:00.440
So that to me is a bit like our earlier debate

01:09:00.440 --> 01:09:03.280
about creativity, inventing Go

01:09:03.280 --> 01:09:06.320
rather than just coming up with a good Go move.

01:09:06.320 --> 01:09:11.320
And so I think solving, I think if we wanted to give it

01:09:11.320 --> 01:09:13.560
the credit of like a Nobel type of thing,

01:09:13.560 --> 01:09:15.840
then it would need to invent Go

01:09:15.840 --> 01:09:19.320
and sort of invent that new conjecture out of the blue

01:09:19.320 --> 01:09:22.760
rather than being specified by the human scientists

01:09:22.760 --> 01:09:23.640
or the human creators.

01:09:23.640 --> 01:09:26.320
So I think right now it's definitely just a tool.

01:09:26.320 --> 01:09:27.920
Although it is interesting how far you get

01:09:27.920 --> 01:09:30.000
by averaging everything on the internet, like you said,

01:09:30.000 --> 01:09:33.200
because a lot of people do see science

01:09:33.200 --> 01:09:35.680
as you're always standing on the shoulders of giants.

01:09:35.680 --> 01:09:40.080
And the question is, how much are you really reaching

01:09:40.080 --> 01:09:42.040
up above the shoulders of giants?

01:09:42.040 --> 01:09:46.280
Maybe it's just a simulating different kinds of results

01:09:46.280 --> 01:09:49.400
of the past with ultimately this new perspective

01:09:49.400 --> 01:09:51.160
that gives you this breakthrough idea.

01:09:51.160 --> 01:09:54.880
But that idea may not be novel in the way

01:09:54.880 --> 01:09:56.760
that we can't be already discovered on the internet.

01:09:56.760 --> 01:10:00.080
Maybe the Nobel prizes of the next 100 years

01:10:00.080 --> 01:10:03.080
are already all there on the internet to be discovered.

01:10:03.080 --> 01:10:04.600
They could be, they could be.

01:10:04.600 --> 01:10:08.600
I mean, I think this is one of the big mysteries,

01:10:08.600 --> 01:10:11.760
I think, is that, first of all,

01:10:11.760 --> 01:10:13.800
I believe a lot of the big new breakthroughs

01:10:13.800 --> 01:10:15.320
that are gonna come in the next few decades

01:10:15.320 --> 01:10:17.440
and even in the last decade are gonna come

01:10:17.440 --> 01:10:20.240
at the intersection between different subject areas

01:10:20.240 --> 01:10:23.520
where there'll be some new connection that's found

01:10:23.520 --> 01:10:26.240
between what seemingly were disparate areas.

01:10:26.240 --> 01:10:28.880
And one can even think of DeepMind, as I said earlier,

01:10:28.880 --> 01:10:31.760
as a sort of interdisciplinary between neuroscience ideas

01:10:31.760 --> 01:10:35.080
and AI engineering ideas originally.

01:10:35.080 --> 01:10:38.000
And so I think there's that.

01:10:38.000 --> 01:10:40.440
And then one of the things we can't imagine today is,

01:10:40.440 --> 01:10:41.760
and one of the reasons I think people,

01:10:41.760 --> 01:10:44.480
we were so surprised by how well large models worked

01:10:44.480 --> 01:10:47.960
is that actually it's very hard for our human minds,

01:10:47.960 --> 01:10:49.480
our limited human minds to understand

01:10:49.480 --> 01:10:52.080
what it would be like to read the whole internet, right?

01:10:52.080 --> 01:10:53.560
I think we can do a thought experiment,

01:10:53.560 --> 01:10:54.720
and I used to do this of like,

01:10:54.720 --> 01:10:57.640
well, what if I read the whole of Wikipedia?

01:10:57.640 --> 01:10:58.480
What would I know?

01:10:58.480 --> 01:11:00.520
And I think our minds can just about comprehend

01:11:00.520 --> 01:11:01.960
maybe what that would be like,

01:11:01.960 --> 01:11:04.480
but the whole internet is beyond comprehension.

01:11:04.480 --> 01:11:07.480
So I think we just don't understand what it would be like

01:11:07.480 --> 01:11:10.360
to be able to hold all of that in mind potentially, right?

01:11:10.360 --> 01:11:12.960
And then active at once,

01:11:12.960 --> 01:11:14.560
and then maybe what are the connections

01:11:14.560 --> 01:11:15.800
that are available there?

01:11:15.800 --> 01:11:17.520
So I think no doubt there are huge things

01:11:17.520 --> 01:11:19.320
to be discovered just like that,

01:11:19.320 --> 01:11:22.320
but I do think there is this other type of creativity,

01:11:22.320 --> 01:11:25.440
of true spark, of new knowledge, new idea,

01:11:25.440 --> 01:11:26.680
never thought before about,

01:11:26.680 --> 01:11:29.320
can't be averaged from things that are known,

01:11:29.320 --> 01:11:32.000
that really, of course, everything come,

01:11:32.000 --> 01:11:33.680
nobody creates in a vacuum,

01:11:33.680 --> 01:11:35.400
so there must be clues somewhere,

01:11:35.400 --> 01:11:38.280
but just a unique way of putting those things together.

01:11:38.280 --> 01:11:40.480
I think some of the greatest scientists in history

01:11:40.480 --> 01:11:42.240
have displayed that, I would say,

01:11:42.240 --> 01:11:43.960
although it's very hard to know,

01:11:43.960 --> 01:11:45.120
going back to their time,

01:11:45.120 --> 01:11:48.160
what was exactly known when they came up with those things.

01:11:48.160 --> 01:11:51.280
Although, you're making me really think

01:11:51.280 --> 01:11:53.280
because just the thought experiment

01:11:53.280 --> 01:11:57.360
of deeply knowing a hundred Wikipedia pages,

01:11:57.360 --> 01:11:59.200
I don't think I can,

01:11:59.200 --> 01:12:03.400
I've been really impressed by Wikipedia for technical topics.

01:12:03.400 --> 01:12:07.040
So if you know a hundred pages or a thousand pages,

01:12:07.040 --> 01:12:10.080
I don't think who can truly comprehend

01:12:10.080 --> 01:12:13.200
what kind of intelligence that is.

01:12:13.200 --> 01:12:14.760
That's a pretty powerful intelligence.

01:12:14.760 --> 01:12:16.120
If you know how to use that

01:12:16.120 --> 01:12:18.360
and integrate that information correctly,

01:12:18.360 --> 01:12:20.040
I think you can go really far.

01:12:20.040 --> 01:12:23.720
You can probably construct thought experiments based on that,

01:12:23.720 --> 01:12:25.880
like simulate different ideas.

01:12:25.880 --> 01:12:27.280
So if this is true,

01:12:27.280 --> 01:12:28.880
let me run this thought experiment

01:12:28.880 --> 01:12:30.200
that maybe this is true.

01:12:30.200 --> 01:12:31.400
It's not really invention,

01:12:31.400 --> 01:12:34.800
it's like just taking literally the knowledge

01:12:34.800 --> 01:12:36.080
and using it to construct

01:12:36.080 --> 01:12:37.920
the very basic simulation of the world.

01:12:37.920 --> 01:12:40.120
I mean, some argue it's romantic in part,

01:12:40.120 --> 01:12:42.440
but Einstein would do the same kind of things

01:12:42.440 --> 01:12:43.720
with a thought experiment.

01:12:43.720 --> 01:12:46.320
Yeah, one could imagine doing that systematically

01:12:46.320 --> 01:12:48.440
across millions of Wikipedia pages,

01:12:48.440 --> 01:12:50.400
plus PubMed, all these things.

01:12:50.400 --> 01:12:54.040
I think there are many, many things to be discovered like that

01:12:54.040 --> 01:12:55.520
that are hugely useful.

01:12:55.520 --> 01:12:56.360
You could imagine,

01:12:56.360 --> 01:12:58.520
and I want us to do some of these things in material science

01:12:58.520 --> 01:13:00.040
like room temperature superconductors

01:13:00.040 --> 01:13:01.560
or something on my list one day

01:13:01.560 --> 01:13:05.000
that I'd like to have an AI system to help build,

01:13:05.000 --> 01:13:06.640
better optimized batteries,

01:13:06.640 --> 01:13:09.000
all of these sort of mechanical things.

01:13:09.000 --> 01:13:11.600
I think a systematic sort of search

01:13:11.600 --> 01:13:14.360
could be guided by a model,

01:13:14.360 --> 01:13:17.120
could be extremely powerful.

01:13:17.120 --> 01:13:18.200
So speaking of which,

01:13:18.200 --> 01:13:20.160
you have a paper on nuclear fusion,

01:13:21.320 --> 01:13:23.120
magnetic control of talk about plasmas

01:13:23.120 --> 01:13:24.720
through deep reinforcement learning.

01:13:24.720 --> 01:13:29.720
So you're seeking to solve nuclear fusion with deep RL.

01:13:29.800 --> 01:13:31.800
So it's doing control of high temperature plasmas.

01:13:31.800 --> 01:13:33.480
Can you explain this work

01:13:33.480 --> 01:13:37.240
and can AI eventually solve nuclear fusion?

01:13:37.240 --> 01:13:40.160
It's been very fun last year or two and very productive

01:13:40.160 --> 01:13:43.320
because we've been ticking off a lot of my dream projects,

01:13:43.320 --> 01:13:45.600
if you like, of things that I've collected over the years

01:13:45.600 --> 01:13:48.160
of areas of science that I would like to,

01:13:48.160 --> 01:13:49.640
I think could be very transformative

01:13:49.640 --> 01:13:53.600
if we helped accelerate and really interesting problems,

01:13:53.600 --> 01:13:55.680
scientific challenges in and of themselves.

01:13:55.680 --> 01:13:57.000
So this is energy.

01:13:57.000 --> 01:13:58.480
So energy, yes, exactly.

01:13:58.480 --> 01:13:59.960
So energy and climate.

01:13:59.960 --> 01:14:01.760
So we talked about disease and biology

01:14:01.760 --> 01:14:04.520
as being one of the biggest places I think AI can help with.

01:14:04.520 --> 01:14:07.120
I think energy and climate is another one.

01:14:07.120 --> 01:14:09.240
So maybe they would be my top two.

01:14:09.240 --> 01:14:12.480
And fusion is one area I think AI can help with.

01:14:12.480 --> 01:14:15.320
Now, fusion has many challenges,

01:14:15.320 --> 01:14:17.200
mostly physics and material science

01:14:17.200 --> 01:14:18.560
and engineering challenges as well

01:14:18.560 --> 01:14:20.480
to build these massive fusion reactors

01:14:20.480 --> 01:14:21.840
and contain the plasma.

01:14:21.840 --> 01:14:25.120
And what we try to do whenever we go into a new field

01:14:25.120 --> 01:14:27.720
to apply our systems is we look for,

01:14:27.720 --> 01:14:29.200
we talk to domain experts.

01:14:29.200 --> 01:14:30.640
We try and find the best people in the world

01:14:30.640 --> 01:14:31.640
to collaborate with.

01:14:32.960 --> 01:14:34.080
In this case, in fusion,

01:14:34.080 --> 01:14:36.360
we collaborated with EPFL in Switzerland,

01:14:36.360 --> 01:14:38.240
the Swiss Technical Institute, who are amazing.

01:14:38.240 --> 01:14:39.640
They have a test reactor.

01:14:39.640 --> 01:14:41.320
They were willing to let us use,

01:14:41.320 --> 01:14:43.360
which I double-checked with the team

01:14:43.360 --> 01:14:46.080
we were gonna use carefully and safely.

01:14:46.080 --> 01:14:47.720
I was impressed they managed to persuade them

01:14:47.720 --> 01:14:49.120
to let us use it.

01:14:49.120 --> 01:14:53.400
And it's an amazing test reactor they have there.

01:14:53.400 --> 01:14:56.960
And they try all sorts of pretty crazy experiments on it.

01:14:56.960 --> 01:14:59.680
And what we tend to look at is,

01:14:59.680 --> 01:15:01.720
if we go into a new domain like fusion,

01:15:01.720 --> 01:15:04.120
what are all the bottleneck problems?

01:15:04.120 --> 01:15:05.920
Like thinking from first principles,

01:15:05.920 --> 01:15:06.960
what are all the bottleneck problems

01:15:06.960 --> 01:15:09.280
that are still stopping fusion working today?

01:15:09.280 --> 01:15:12.080
And then we get a fusion expert to tell us,

01:15:12.080 --> 01:15:13.760
and then we look at those bottlenecks

01:15:13.760 --> 01:15:14.600
and we look at the ones,

01:15:14.600 --> 01:15:18.920
which ones are amenable to our AI methods today, right?

01:15:18.920 --> 01:15:22.200
And would be interesting from a research perspective

01:15:22.200 --> 01:15:24.400
from our point of view, from an AI point of view.

01:15:24.400 --> 01:15:26.760
And that would address one of their bottlenecks.

01:15:27.520 --> 01:15:29.680
And in this case, plasma control was perfect.

01:15:29.680 --> 01:15:32.480
So the plasma, it's a million degrees Celsius,

01:15:32.480 --> 01:15:35.040
something like that, it's hotter than the sun.

01:15:35.040 --> 01:15:37.640
And there's obviously no material that can contain it.

01:15:37.640 --> 01:15:39.440
So they have to be containing these magnetic,

01:15:39.440 --> 01:15:42.520
very powerful superconducting magnetic fields.

01:15:42.520 --> 01:15:43.360
But the problem is,

01:15:43.360 --> 01:15:45.600
plasma is pretty unstable as you imagine.

01:15:45.600 --> 01:15:47.280
You're kind of holding a mini sun,

01:15:47.280 --> 01:15:49.320
mini star in a reactor.

01:15:49.320 --> 01:15:52.560
So you kind of want to predict ahead of time

01:15:52.560 --> 01:15:54.080
what the plasma is gonna do

01:15:54.080 --> 01:15:56.280
so you can move the magnetic field

01:15:56.280 --> 01:15:59.640
within a few milliseconds to basically contain

01:15:59.640 --> 01:16:01.000
what it's gonna do next.

01:16:01.000 --> 01:16:03.200
So it seems like a perfect problem if you think of it

01:16:03.200 --> 01:16:06.320
for like a reinforcement learning prediction problem.

01:16:06.320 --> 01:16:08.160
So you got a controller,

01:16:08.160 --> 01:16:09.760
you're gonna move the magnetic field.

01:16:09.760 --> 01:16:11.440
And until we came along,

01:16:11.440 --> 01:16:14.520
they were doing it with traditional operational

01:16:14.520 --> 01:16:16.760
research type of controllers,

01:16:16.760 --> 01:16:18.360
which are kind of handcrafted.

01:16:18.360 --> 01:16:19.200
And the problem is, of course,

01:16:19.200 --> 01:16:20.520
they can't react in the moment

01:16:20.520 --> 01:16:21.880
to something the plasma is doing.

01:16:21.880 --> 01:16:23.080
They have to be hard coded.

01:16:23.080 --> 01:16:26.080
And again, knowing that that's normally our go-to solution

01:16:26.080 --> 01:16:27.960
is we would like to learn that instead.

01:16:27.960 --> 01:16:30.320
And they also had a simulator of these plasma.

01:16:30.320 --> 01:16:31.520
So there were lots of criteria

01:16:31.520 --> 01:16:34.800
that matched what we like to use.

01:16:34.800 --> 01:16:38.440
So can AI eventually solve nuclear fusion?

01:16:38.440 --> 01:16:39.800
Well, so with this problem,

01:16:39.800 --> 01:16:42.040
and we published it in the Nature paper last year,

01:16:42.040 --> 01:16:43.840
we held the fusion,

01:16:43.840 --> 01:16:46.200
that we held the plasma in a specific shapes.

01:16:46.200 --> 01:16:48.360
So actually it's almost like carving the plasma

01:16:48.360 --> 01:16:51.040
into different shapes and hold it there

01:16:51.040 --> 01:16:52.880
for a record amount of time.

01:16:53.720 --> 01:16:57.640
So that's one of the problems of fusion sort of solved.

01:16:57.640 --> 01:16:59.880
So have a controller that's able to,

01:16:59.880 --> 01:17:01.520
no matter the shape.

01:17:01.520 --> 01:17:02.360
Contain it.

01:17:02.360 --> 01:17:04.200
Yeah, contain it and hold it in structure.

01:17:04.200 --> 01:17:05.800
And there's different shapes that are better

01:17:05.800 --> 01:17:10.120
for the energy productions called droplets and so on.

01:17:10.120 --> 01:17:11.920
So that was huge.

01:17:11.920 --> 01:17:14.480
And now we're talking to lots of fusion startups

01:17:14.480 --> 01:17:17.440
to see what's the next problem we can tackle

01:17:17.440 --> 01:17:19.400
in the fusion area.

01:17:19.400 --> 01:17:22.320
So another fascinating place,

01:17:22.320 --> 01:17:23.160
in a paper title,

01:17:23.160 --> 01:17:25.160
pushing the frontiers of density functionals

01:17:25.160 --> 01:17:27.560
by solving the fractional electron problem.

01:17:27.560 --> 01:17:30.920
So you're taking on modeling and simulating

01:17:30.920 --> 01:17:33.440
the quantum mechanical behavior of electrons.

01:17:36.080 --> 01:17:39.280
Can you explain this work and can AI model

01:17:39.280 --> 01:17:41.600
and simulate arbitrary quantum mechanical systems

01:17:41.600 --> 01:17:42.440
in the future?

01:17:42.440 --> 01:17:44.320
Yeah, so this is another problem I've had my eye on

01:17:44.320 --> 01:17:47.240
for a decade or more,

01:17:47.240 --> 01:17:51.280
which is sort of simulating the properties of electrons.

01:17:51.280 --> 01:17:52.520
If you can do that,

01:17:52.520 --> 01:17:56.280
you can basically describe how elements and materials

01:17:56.280 --> 01:17:58.080
and substances work.

01:17:58.080 --> 01:18:00.080
So it's kind of like fundamental

01:18:00.080 --> 01:18:02.880
if you wanna advance material science.

01:18:02.880 --> 01:18:05.280
And we have Schrodinger's equation

01:18:05.280 --> 01:18:06.520
and then we have approximations

01:18:06.520 --> 01:18:08.440
to that density functional theory.

01:18:08.440 --> 01:18:10.600
These things are famous.

01:18:10.600 --> 01:18:13.240
And people try and write approximations

01:18:13.240 --> 01:18:15.840
to these functionals

01:18:15.840 --> 01:18:18.320
and kind of come up with descriptions

01:18:18.320 --> 01:18:19.920
of the electron clouds,

01:18:19.920 --> 01:18:20.760
where they're gonna go,

01:18:20.760 --> 01:18:22.160
how they're gonna interact

01:18:22.160 --> 01:18:24.280
when you put two elements together.

01:18:24.280 --> 01:18:26.840
And what we try to do is learn a simulation,

01:18:27.760 --> 01:18:30.600
learn a functional that will describe more chemistry,

01:18:30.600 --> 01:18:31.800
types of chemistry.

01:18:31.800 --> 01:18:35.600
So until now, you can run expensive simulations,

01:18:35.600 --> 01:18:38.800
but then you can only simulate very small molecules,

01:18:38.800 --> 01:18:40.200
very simple molecules.

01:18:40.200 --> 01:18:43.120
We would like to simulate large materials.

01:18:43.120 --> 01:18:45.840
And so today there's no way of doing that.

01:18:45.840 --> 01:18:48.600
And we're building up towards building functionals

01:18:48.600 --> 01:18:51.240
that approximate Schrodinger's equation

01:18:51.240 --> 01:18:54.040
and then allow you to describe

01:18:54.040 --> 01:18:55.600
what the electrons are doing.

01:18:55.600 --> 01:18:57.480
And all material sort of science

01:18:57.480 --> 01:18:59.920
and material properties are governed by the electrons

01:18:59.920 --> 01:19:01.360
and how they interact.

01:19:01.360 --> 01:19:05.840
So have a good summarization of the simulation

01:19:05.840 --> 01:19:07.080
through the functional,

01:19:08.720 --> 01:19:11.360
but one that is still close

01:19:11.360 --> 01:19:13.240
to what the actual simulation would come out with.

01:19:13.240 --> 01:19:16.720
So how difficult is that task?

01:19:16.720 --> 01:19:17.760
What's involved in that task?

01:19:17.760 --> 01:19:20.720
Is it running those complicated simulations

01:19:20.720 --> 01:19:23.280
and learning the task of mapping

01:19:23.280 --> 01:19:24.560
from the initial conditions

01:19:24.560 --> 01:19:26.400
and the parameters of the simulation,

01:19:26.400 --> 01:19:27.720
learning what the functional would be?

01:19:27.720 --> 01:19:29.440
Yeah, so it's pretty tricky.

01:19:29.440 --> 01:19:31.320
And we've done it with,

01:19:31.320 --> 01:19:35.000
the nice thing is we can run a lot of the simulations,

01:19:35.000 --> 01:19:39.080
the molecular dynamic simulations on our compute clusters.

01:19:39.080 --> 01:19:40.840
And so that generates a lot of data.

01:19:40.840 --> 01:19:42.800
So in this case, the data is generated.

01:19:42.800 --> 01:19:45.000
So we like those sort of systems,

01:19:45.000 --> 01:19:45.880
that's why we use games,

01:19:45.960 --> 01:19:48.080
simulator generated data.

01:19:48.080 --> 01:19:51.120
And we can kind of create as much of it as we want really.

01:19:51.120 --> 01:19:53.240
And just let's leave some,

01:19:53.240 --> 01:19:55.240
if any computers are free in the cloud,

01:19:55.240 --> 01:19:57.640
we just run some of these calculations, right?

01:19:57.640 --> 01:19:59.360
Compute cluster calculation.

01:19:59.360 --> 01:20:01.040
I like how the free compute time

01:20:01.040 --> 01:20:02.280
is used up on quantum mechanics.

01:20:02.280 --> 01:20:03.480
Yeah, quantum mechanics, exactly.

01:20:03.480 --> 01:20:06.240
Simulations and protein simulations and other things.

01:20:06.240 --> 01:20:09.840
And so when you're not searching on YouTube

01:20:09.840 --> 01:20:11.320
for video, cat videos,

01:20:11.320 --> 01:20:13.960
we're using those computers usefully in quantum chemistry.

01:20:14.000 --> 01:20:14.920
That's the idea.

01:20:14.920 --> 01:20:17.000
And putting them to good use.

01:20:17.000 --> 01:20:20.840
And then all of that computational data that's generated,

01:20:20.840 --> 01:20:23.480
we can then try and learn the functionals from that,

01:20:23.480 --> 01:20:25.680
which of course are way more efficient

01:20:25.680 --> 01:20:27.080
once we learn the functional

01:20:27.080 --> 01:20:30.600
than running those simulations would be.

01:20:30.600 --> 01:20:34.160
Do you think one day AI may allow us to do something like,

01:20:34.160 --> 01:20:36.400
basically crack open physics,

01:20:36.400 --> 01:20:39.520
so do something like travel faster than the speed of light?

01:20:39.520 --> 01:20:41.640
My ultimate aim has always been with AI

01:20:41.640 --> 01:20:45.560
is the reason I am personally working on AI

01:20:45.560 --> 01:20:46.400
for my whole life,

01:20:46.400 --> 01:20:50.360
it was to build a tool to help us understand the universe.

01:20:50.360 --> 01:20:53.800
So I wanted to, and that means physics really,

01:20:53.800 --> 01:20:54.920
and the nature of reality.

01:20:54.920 --> 01:20:58.000
So I don't think we have systems

01:20:58.000 --> 01:20:59.400
that are capable of doing that yet,

01:20:59.400 --> 01:21:01.040
but when we get towards AGI,

01:21:01.040 --> 01:21:02.920
I think that's one of the first things

01:21:02.920 --> 01:21:05.320
I think we should apply AGI to.

01:21:05.320 --> 01:21:07.160
I would like to test the limits of physics

01:21:07.160 --> 01:21:08.600
and our knowledge of physics.

01:21:08.600 --> 01:21:10.080
There's so many things we don't know.

01:21:10.080 --> 01:21:12.360
This is one thing I find fascinating about science

01:21:12.360 --> 01:21:15.120
and is a huge proponent of the scientific method

01:21:15.120 --> 01:21:17.960
as being one of the greatest ideas humanity's ever had

01:21:17.960 --> 01:21:20.480
and allowed us to progress with our knowledge.

01:21:20.480 --> 01:21:22.040
I think as a true scientist,

01:21:22.040 --> 01:21:25.240
I think what you find is the more you find out,

01:21:25.240 --> 01:21:27.080
the more you realize we don't know.

01:21:27.080 --> 01:21:29.920
And I always think that it's surprising

01:21:29.920 --> 01:21:31.920
that more people aren't troubled.

01:21:31.920 --> 01:21:34.040
Every night I think about all these things

01:21:34.040 --> 01:21:35.280
we interact with all the time

01:21:35.280 --> 01:21:36.920
that we have no idea how they work.

01:21:36.920 --> 01:21:40.960
Time, consciousness, gravity, life.

01:21:40.960 --> 01:21:41.800
We can't, I mean,

01:21:41.800 --> 01:21:43.880
these are all the fundamental things of nature.

01:21:43.880 --> 01:21:45.040
I think the way we-

01:21:45.040 --> 01:21:47.360
We don't really know what they are.

01:21:47.360 --> 01:21:51.560
To live life, we pin certain assumptions on them

01:21:51.560 --> 01:21:55.280
and kind of treat our assumptions as if they're a fact.

01:21:55.280 --> 01:21:56.640
That allows us to sort of-

01:21:56.640 --> 01:21:57.640
Box them off somehow.

01:21:57.640 --> 01:21:59.040
Yeah, box them off somehow.

01:21:59.040 --> 01:22:02.360
Well, the reality is when you think of time,

01:22:02.360 --> 01:22:03.600
you should remind yourself,

01:22:03.600 --> 01:22:06.800
you should take it off the shelf

01:22:07.640 --> 01:22:09.080
and realize like, no, we have a bunch of assumptions.

01:22:09.080 --> 01:22:10.160
There's still a lot of,

01:22:10.160 --> 01:22:11.600
there's even not a lot of debate.

01:22:11.600 --> 01:22:15.600
There's a lot of uncertainty about exactly what is time.

01:22:15.600 --> 01:22:18.080
Is there an era of time?

01:22:18.080 --> 01:22:19.520
There's a lot of fundamental questions

01:22:19.520 --> 01:22:21.240
that you can't just make assumptions about.

01:22:21.240 --> 01:22:26.240
And maybe AI allows you to not put anything on the shelf,

01:22:28.200 --> 01:22:30.280
not make any hard assumptions

01:22:30.280 --> 01:22:32.120
and really open it up and see what-

01:22:32.120 --> 01:22:34.720
Exactly, I think we should be truly open-minded about that.

01:22:34.720 --> 01:22:39.120
And exactly that, not be dogmatic to a particular theory.

01:22:40.160 --> 01:22:42.880
It'll also allow us to build better tools,

01:22:42.880 --> 01:22:45.440
experimental tools eventually,

01:22:45.440 --> 01:22:47.320
that can then test certain theories

01:22:47.320 --> 01:22:49.080
that may not be testable today

01:22:49.080 --> 01:22:52.280
about things about like what we spoke about at the beginning

01:22:52.280 --> 01:22:54.440
about the computational nature of the universe.

01:22:54.440 --> 01:22:56.280
How one might, if that was true,

01:22:56.280 --> 01:22:58.400
how one might go about testing that, right?

01:22:58.400 --> 01:23:00.360
And how much, you know,

01:23:00.360 --> 01:23:01.840
there are people who've conjectured people

01:23:01.880 --> 01:23:04.800
like Scott Aronson and others about, you know,

01:23:04.800 --> 01:23:07.880
how much information can a specific plank unit

01:23:07.880 --> 01:23:10.160
of space and time contain, right?

01:23:10.160 --> 01:23:13.000
So one might be able to think about testing those ideas

01:23:13.000 --> 01:23:16.440
if you had AI helping you build

01:23:16.440 --> 01:23:20.520
some new exquisite experimental tools.

01:23:20.520 --> 01:23:21.960
This is what I imagine that, you know,

01:23:21.960 --> 01:23:24.160
many decades from now we'll be able to do.

01:23:24.160 --> 01:23:26.840
And what kind of questions can be answered

01:23:26.840 --> 01:23:29.760
to running a simulation of them?

01:23:29.880 --> 01:23:31.840
So that there's a bunch of physics simulations

01:23:31.840 --> 01:23:33.680
you can imagine that could be run

01:23:33.680 --> 01:23:36.720
in some kind of efficient way,

01:23:36.720 --> 01:23:39.560
much like you're doing in the quantum simulation work.

01:23:41.240 --> 01:23:43.280
And perhaps even the origin of life,

01:23:43.280 --> 01:23:46.040
figuring out how going even back

01:23:46.040 --> 01:23:48.560
before the work of AlphaFold begins,

01:23:48.560 --> 01:23:53.560
of how this whole thing emerges from a rock,

01:23:54.080 --> 01:23:55.040
from a static thing.

01:23:55.040 --> 01:23:57.880
What do you think AI will allow us to,

01:23:57.880 --> 01:23:59.720
is that something you have your eye on?

01:24:00.680 --> 01:24:02.360
It's trying to understand the origin of life.

01:24:02.360 --> 01:24:04.600
First of all, yourself, what do you think,

01:24:06.320 --> 01:24:08.800
how the heck did life originate on Earth?

01:24:08.800 --> 01:24:11.160
Yeah, well, maybe I'll come to that in a second,

01:24:11.160 --> 01:24:13.840
but I think the ultimate use of AI

01:24:13.840 --> 01:24:18.120
is to kind of use it to accelerate science to the maximum.

01:24:18.120 --> 01:24:22.640
So I think of it a little bit like the tree of all knowledge.

01:24:22.640 --> 01:24:23.880
If you imagine that's all the knowledge

01:24:23.880 --> 01:24:25.840
there is in the universe to attain.

01:24:25.840 --> 01:24:29.320
And we sort of barely scratched the surface of that so far

01:24:29.840 --> 01:24:32.000
even though we've done pretty well

01:24:32.000 --> 01:24:34.360
since the enlightenment as humanity.

01:24:34.360 --> 01:24:36.880
And I think AI will turbocharge all of that

01:24:36.880 --> 01:24:38.640
like we've seen with AlphaFold.

01:24:38.640 --> 01:24:41.440
And I want to explore as much of that tree of knowledge

01:24:41.440 --> 01:24:42.960
as it's possible to do.

01:24:42.960 --> 01:24:46.440
And I think that involves AI helping us

01:24:46.440 --> 01:24:49.720
with understanding or finding patterns,

01:24:49.720 --> 01:24:52.240
but also potentially designing and building new tools,

01:24:52.240 --> 01:24:53.640
experimental tools.

01:24:53.640 --> 01:24:57.560
So I think that's all and also running simulations

01:24:57.560 --> 01:24:58.920
and learning simulations.

01:24:59.560 --> 01:25:04.560
We're sort of doing it at a baby steps level here,

01:25:05.000 --> 01:25:08.560
but I can imagine that in the decades to come

01:25:08.560 --> 01:25:12.960
as what's the full flourishing of that line of thinking.

01:25:12.960 --> 01:25:15.160
It's gonna be truly incredible, I would say.

01:25:15.160 --> 01:25:17.360
If I visualize this tree of knowledge,

01:25:17.360 --> 01:25:20.840
something tells me that that tree of knowledge for humans

01:25:20.840 --> 01:25:22.000
is much smaller.

01:25:22.920 --> 01:25:25.080
In the set of all possible trees of knowledge,

01:25:25.080 --> 01:25:30.080
it's actually quite small given our cognitive limitations,

01:25:31.480 --> 01:25:33.680
limited cognitive capabilities

01:25:33.680 --> 01:25:35.720
that even with the tools we build,

01:25:35.720 --> 01:25:38.080
we still won't be able to understand a lot of things.

01:25:38.080 --> 01:25:41.160
And that's perhaps what non-human systems

01:25:41.160 --> 01:25:44.920
might be able to reach farther, not just as tools,

01:25:44.920 --> 01:25:47.200
but in themselves understanding something

01:25:47.200 --> 01:25:48.480
that they can bring back.

01:25:48.480 --> 01:25:50.160
Yeah, it could well be.

01:25:50.160 --> 01:25:51.800
So, I mean, there's so many things

01:25:51.800 --> 01:25:55.000
that are sort of encapsulated in what you just said there.

01:25:55.960 --> 01:25:58.640
So, I think first of all, there's two different things there.

01:25:58.640 --> 01:26:00.600
It's like, what do we understand today?

01:26:00.600 --> 01:26:02.720
What could the human mind understand?

01:26:02.720 --> 01:26:06.440
And what is the totality of what is there to be understood?

01:26:06.440 --> 01:26:10.760
And so, you can think of them as three larger and larger trees

01:26:10.760 --> 01:26:12.920
or exploring more branches of that tree.

01:26:12.920 --> 01:26:16.000
And I think with AI, we're gonna explore that whole lot.

01:26:16.000 --> 01:26:19.160
Now, the question is, if you think about

01:26:19.160 --> 01:26:21.880
what is the totality of what could be understood,

01:26:22.720 --> 01:26:24.840
there may be some fundamental physics reasons

01:26:24.840 --> 01:26:26.320
why certain things can't be understood,

01:26:26.320 --> 01:26:29.040
like what's outside a simulation or outside the universe.

01:26:29.040 --> 01:26:32.360
Maybe it's not understandable from within the universe.

01:26:32.360 --> 01:26:34.880
So, there may be some hard constraints like that.

01:26:34.880 --> 01:26:36.040
Could be smaller constraints,

01:26:36.040 --> 01:26:40.560
like we think of space-time as fundamental.

01:26:40.560 --> 01:26:42.920
Our human brains are really used to this idea

01:26:42.920 --> 01:26:46.080
of a three-dimensional world with time, maybe.

01:26:46.080 --> 01:26:47.800
But our tools could go beyond that.

01:26:47.800 --> 01:26:49.800
They wouldn't have that limitation necessarily.

01:26:49.800 --> 01:26:51.800
They could think in 11 dimensions, 12 dimensions,

01:26:51.800 --> 01:26:52.960
whatever is needed.

01:26:52.960 --> 01:26:55.680
But we could still maybe understand that

01:26:55.680 --> 01:26:56.760
in several different ways.

01:26:56.760 --> 01:26:59.080
The example I always give is

01:26:59.080 --> 01:27:01.440
when I play Gary Kusparoff at Speed Chess,

01:27:01.440 --> 01:27:04.160
or we've talked about chess and these kinds of things,

01:27:05.800 --> 01:27:07.560
if you're reasonably good at chess,

01:27:07.560 --> 01:27:11.240
you can't come up with the move Gary comes up with

01:27:11.240 --> 01:27:13.360
in his move, but he can explain it to you.

01:27:13.360 --> 01:27:14.200
And you can understand.

01:27:14.200 --> 01:27:16.520
And you can understand post hoc the reasoning.

01:27:16.520 --> 01:27:17.360
Yeah.

01:27:17.360 --> 01:27:19.440
So, I think there's an even further level of like,

01:27:19.440 --> 01:27:21.680
well, maybe you couldn't have invented that thing,

01:27:21.680 --> 01:27:24.320
but going back to using language again,

01:27:24.320 --> 01:27:27.080
perhaps you can understand and appreciate that.

01:27:27.080 --> 01:27:30.160
Same way that you can appreciate Vivaldi or Mozart

01:27:30.160 --> 01:27:31.160
or something without,

01:27:31.160 --> 01:27:32.720
you can appreciate the beauty of that

01:27:32.720 --> 01:27:35.840
without being able to construct it yourself, right?

01:27:35.840 --> 01:27:37.440
Invent the music yourself.

01:27:37.440 --> 01:27:39.320
So, I think we see this in all forms of life.

01:27:39.320 --> 01:27:42.480
So, it'll be that times a million,

01:27:42.480 --> 01:27:45.840
but you can imagine also one sign of intelligence

01:27:45.840 --> 01:27:49.520
is the ability to explain things clearly and simply, right?

01:27:49.520 --> 01:27:50.360
People like Richard Fine,

01:27:50.400 --> 01:27:52.400
another one of my all time heroes used to say that, right?

01:27:52.400 --> 01:27:55.640
If you can't, if you can explain it something simply,

01:27:55.640 --> 01:27:58.640
then that's the best sign, a complex topic simply,

01:27:58.640 --> 01:28:00.680
then that's one of the best signs of you understanding it.

01:28:00.680 --> 01:28:01.520
Yeah.

01:28:01.520 --> 01:28:04.560
I can see myself talking trash in the AI system in that way.

01:28:04.560 --> 01:28:05.680
Yes.

01:28:05.680 --> 01:28:07.800
It gets frustrated how dumb I am

01:28:07.800 --> 01:28:09.880
in trying to explain something to me.

01:28:09.880 --> 01:28:11.600
I was like, well, that means you're not intelligent

01:28:11.600 --> 01:28:12.720
because if you were intelligent,

01:28:12.720 --> 01:28:14.400
you'd be able to explain it simply.

01:28:14.400 --> 01:28:16.720
Yeah, of course, there's also the other option,

01:28:16.720 --> 01:28:19.560
of course, we could enhance ourselves and with our devices.

01:28:19.800 --> 01:28:23.120
We are already sort of symbiotic with our compute devices,

01:28:23.120 --> 01:28:24.600
right, with our phones and other things,

01:28:24.600 --> 01:28:27.120
and there's stuff like Neuralink and Exceptra

01:28:27.120 --> 01:28:30.000
that could advance that further.

01:28:30.000 --> 01:28:33.880
So, I think there's lots of really amazing possibilities

01:28:33.880 --> 01:28:35.360
that I could foresee from here.

01:28:35.360 --> 01:28:37.040
Well, let me ask you some wild questions.

01:28:37.040 --> 01:28:39.920
So, out there looking for friends,

01:28:39.920 --> 01:28:40.920
do you think there's a lot

01:28:40.920 --> 01:28:43.120
of alien civilizations out there?

01:28:43.120 --> 01:28:44.960
So, I guess this also goes back

01:28:44.960 --> 01:28:46.680
to your origin of life question too,

01:28:46.680 --> 01:28:48.280
because I think that that's key.

01:28:49.280 --> 01:28:51.360
My personal opinion, looking at all this,

01:28:51.360 --> 01:28:53.680
and it's one of my hobbies, physics, I guess,

01:28:53.680 --> 01:28:56.880
so it's something I think about a lot

01:28:56.880 --> 01:29:00.800
and talk to a lot of experts on and read a lot of books on.

01:29:00.800 --> 01:29:05.280
And I think my feeling currently is that we are alone.

01:29:05.280 --> 01:29:07.160
I think that's the most likely scenario,

01:29:07.160 --> 01:29:08.760
given what evidence we have.

01:29:08.760 --> 01:29:13.760
So, and the reasoning is I think that we've tried,

01:29:14.280 --> 01:29:16.080
since things like SETI program,

01:29:16.560 --> 01:29:19.960
I guess since the dawning of the space age,

01:29:19.960 --> 01:29:22.400
we've had telescopes, open radio telescopes

01:29:22.400 --> 01:29:23.400
and other things.

01:29:23.400 --> 01:29:27.360
And if you think about and try to detect signals,

01:29:27.360 --> 01:29:30.200
now, if you think about the evolution of humans on earth,

01:29:30.200 --> 01:29:34.040
we could have easily been a million years ahead

01:29:34.040 --> 01:29:36.960
of our time now or million years behind, easily,

01:29:36.960 --> 01:29:40.360
with just some slightly different quirk thing happening

01:29:40.360 --> 01:29:42.480
hundreds of thousands years ago.

01:29:42.480 --> 01:29:43.760
Things could have been slightly different.

01:29:43.840 --> 01:29:46.320
If the meteor would hit the dinosaurs a million years earlier,

01:29:46.320 --> 01:29:48.200
maybe things would have evolved.

01:29:48.200 --> 01:29:51.040
We'd be a million years ahead of where we are now.

01:29:51.040 --> 01:29:54.200
So what that means is if you imagine where humanity will be

01:29:54.200 --> 01:29:56.840
in a few hundred years, let alone a million years,

01:29:56.840 --> 01:30:01.320
especially if we hopefully solve things like climate change

01:30:01.320 --> 01:30:03.880
and other things and we continue to flourish

01:30:03.880 --> 01:30:07.240
and we build things like AI and we do space traveling

01:30:07.240 --> 01:30:10.920
and all of the stuff that humans have dreamed of forever,

01:30:10.920 --> 01:30:12.720
and sci-fi is talked about forever.

01:30:14.400 --> 01:30:16.800
We will be spreading across the stars, right?

01:30:16.800 --> 01:30:19.320
And von Neumann famously calculated,

01:30:19.320 --> 01:30:20.840
it would only take about a million years

01:30:20.840 --> 01:30:23.560
if you send out von Neumann probes to the nearest,

01:30:23.560 --> 01:30:26.240
the nearest other solar systems.

01:30:26.240 --> 01:30:29.120
And then all they did was build two more versions

01:30:29.120 --> 01:30:30.480
of themselves and set those two out

01:30:30.480 --> 01:30:32.720
to the next nearest systems.

01:30:32.720 --> 01:30:34.480
Within a million years, I think you would have one

01:30:34.480 --> 01:30:36.960
of these probes in every system in the galaxy.

01:30:36.960 --> 01:30:40.080
So it's not actually in cosmological time.

01:30:40.080 --> 01:30:42.120
That's actually a very short amount of time.

01:30:42.520 --> 01:30:44.680
And people like Dyson have thought

01:30:44.680 --> 01:30:47.360
about constructing Dyson spheres around stars

01:30:47.360 --> 01:30:50.480
to collect all the energy coming out of the star.

01:30:50.480 --> 01:30:51.880
There would be constructions like that

01:30:51.880 --> 01:30:54.200
would be visible across space,

01:30:54.200 --> 01:30:55.840
probably even across a galaxy.

01:30:56.760 --> 01:31:00.320
And then if you think about all of our radio, television,

01:31:00.320 --> 01:31:04.240
emissions that have gone out since the 30s and 40s,

01:31:05.200 --> 01:31:06.800
imagine a million years of that.

01:31:06.800 --> 01:31:10.040
And now hundreds of civilizations doing that.

01:31:10.040 --> 01:31:11.640
When we opened our ears,

01:31:11.640 --> 01:31:14.880
at the point we got technologically sophisticated enough

01:31:14.880 --> 01:31:19.200
in the space age, we should have heard a cacophony of voices.

01:31:19.200 --> 01:31:20.960
We should have joined that cacophony of voices.

01:31:20.960 --> 01:31:24.560
And what we did, we opened our ears and we heard nothing.

01:31:24.560 --> 01:31:27.200
And many people who argue that there are aliens

01:31:27.200 --> 01:31:30.000
would say, well, we haven't really done exhaustive search yet

01:31:30.000 --> 01:31:31.960
and maybe we're looking in the wrong bands

01:31:31.960 --> 01:31:33.840
and we've got the wrong devices

01:31:33.840 --> 01:31:36.160
and we wouldn't notice what an alien form was like

01:31:36.160 --> 01:31:38.320
because it'd be so different to what we're used to.

01:31:38.320 --> 01:31:40.680
But I don't really buy that,

01:31:40.680 --> 01:31:43.000
that it shouldn't be as difficult as that.

01:31:43.000 --> 01:31:44.360
I think we've searched enough.

01:31:44.360 --> 01:31:46.200
There should be everywhere.

01:31:46.200 --> 01:31:47.360
It should be everywhere.

01:31:47.360 --> 01:31:49.280
We should see Dyson spheres being put up,

01:31:49.280 --> 01:31:50.880
sun's blinking in and out.

01:31:50.880 --> 01:31:52.960
There should be a lot of evidence for those things.

01:31:52.960 --> 01:31:54.200
And then there are other people who argue,

01:31:54.200 --> 01:31:56.040
well, the sort of safari view of like,

01:31:56.040 --> 01:31:57.880
well, we're a primitive species still

01:31:57.880 --> 01:31:59.440
because we're not space faring yet.

01:31:59.440 --> 01:32:02.560
And there's some kind of universal rule

01:32:02.560 --> 01:32:04.640
not to interfere, Star Trek rule.

01:32:04.640 --> 01:32:07.400
But look, we can't even coordinate humans

01:32:07.400 --> 01:32:10.080
to deal with climate change and we're one species.

01:32:10.080 --> 01:32:12.440
What is the chance that of all of these different

01:32:12.440 --> 01:32:14.840
human civilization, you know, alien civilizations,

01:32:14.840 --> 01:32:16.800
they would have the same priorities

01:32:16.800 --> 01:32:20.240
and agree across these kinds of matters.

01:32:20.240 --> 01:32:21.880
And even if that was true

01:32:21.880 --> 01:32:25.080
and we were in some sort of safari for our own good,

01:32:25.080 --> 01:32:26.400
to me, that's not much different

01:32:26.400 --> 01:32:27.680
from the simulation hypothesis

01:32:27.680 --> 01:32:29.920
because what does it mean, the simulation hypothesis?

01:32:29.920 --> 01:32:31.400
I think in its most fundamental level,

01:32:31.400 --> 01:32:35.000
it means what we're seeing is not quite reality, right?

01:32:35.600 --> 01:32:37.760
There's something more deeper underlying it,

01:32:37.760 --> 01:32:39.120
maybe computational.

01:32:39.120 --> 01:32:42.600
Now, if we were in a sort of safari park

01:32:42.600 --> 01:32:44.440
and everything we were seeing was a hologram

01:32:44.440 --> 01:32:46.520
and it was projected by the aliens or whatever,

01:32:46.520 --> 01:32:47.800
that to me is not much different

01:32:47.800 --> 01:32:50.280
than thinking we're inside of another universe

01:32:50.280 --> 01:32:53.160
because we still can't see true reality, right?

01:32:53.160 --> 01:32:55.120
I mean, there's other explanations.

01:32:55.120 --> 01:32:58.000
It could be that the way they're communicating

01:32:58.000 --> 01:32:59.280
is just fundamentally different,

01:32:59.280 --> 01:33:01.200
that we're too dumb to understand

01:33:01.200 --> 01:33:03.880
the much better methods of communication they have.

01:33:03.880 --> 01:33:06.640
It could be, I mean, it's silly to say,

01:33:06.640 --> 01:33:10.000
but our own thoughts could be the methods

01:33:10.000 --> 01:33:11.200
by which they're communicating.

01:33:11.200 --> 01:33:13.280
Like the place from which our ideas,

01:33:13.280 --> 01:33:15.280
writers talk about this, like the muse.

01:33:17.160 --> 01:33:20.920
I mean, it sounds like very kind of wild,

01:33:20.920 --> 01:33:22.160
but it could be thoughts,

01:33:22.160 --> 01:33:24.640
it could be some interactions with our mind

01:33:24.640 --> 01:33:27.880
that we think are originating from us

01:33:27.880 --> 01:33:31.480
is actually something that is coming

01:33:31.480 --> 01:33:33.080
from other life forms elsewhere.

01:33:33.080 --> 01:33:34.880
Consciousness itself might be that.

01:33:34.880 --> 01:33:37.400
It could be, but I don't see any sensible argument

01:33:37.400 --> 01:33:40.560
to the why would all of the alien species

01:33:40.560 --> 01:33:41.640
be using this method?

01:33:41.640 --> 01:33:43.240
Yeah, some of them will be more primitive,

01:33:43.240 --> 01:33:45.640
they will be close to our level.

01:33:45.640 --> 01:33:46.720
There should be a whole sort of

01:33:46.720 --> 01:33:48.720
normal distribution of these things, right?

01:33:48.720 --> 01:33:52.160
Some would be aggressive, some would be curious,

01:33:52.160 --> 01:33:55.600
others would be very stoical and philosophical

01:33:55.600 --> 01:33:58.080
because maybe they're a million years older than us,

01:33:58.080 --> 01:34:00.200
but it shouldn't be like,

01:34:00.200 --> 01:34:03.040
I mean, one alien civilization might be like that,

01:34:04.040 --> 01:34:06.760
but I don't see why potentially the hundreds

01:34:06.760 --> 01:34:10.000
there should be would be uniform in this way, right?

01:34:10.000 --> 01:34:11.720
It could be a violent dictatorship

01:34:11.720 --> 01:34:14.240
that the people, the alien civilizations

01:34:14.240 --> 01:34:18.440
that become successful become,

01:34:20.520 --> 01:34:23.040
gain the ability to be destructive,

01:34:23.040 --> 01:34:25.040
an order of magnitude more destructive.

01:34:26.000 --> 01:34:28.720
But of course, the sad thought,

01:34:29.880 --> 01:34:32.640
well, either humans are very special.

01:34:33.280 --> 01:34:35.560
We took a lot of leaps that arrived

01:34:35.560 --> 01:34:37.160
at what it means to be human.

01:34:38.680 --> 01:34:41.240
There's a question there, which was the hardest,

01:34:41.240 --> 01:34:42.800
which was the most special?

01:34:42.800 --> 01:34:45.280
But also if others have reached this level,

01:34:45.280 --> 01:34:47.760
and maybe many others have reached this level,

01:34:47.760 --> 01:34:52.760
the great filter that prevented them from going farther

01:34:52.960 --> 01:34:55.120
to becoming a multi-planetary species

01:34:55.120 --> 01:34:57.800
are reaching out into the stars.

01:34:57.800 --> 01:35:00.280
And those are really important questions for us,

01:35:00.280 --> 01:35:05.000
whether there's other alien civilizations out there or not,

01:35:05.000 --> 01:35:07.280
this is very useful for us to think about.

01:35:07.280 --> 01:35:10.560
If we destroy ourselves, how will we do it,

01:35:10.560 --> 01:35:12.280
and how easy is it to do?

01:35:12.280 --> 01:35:14.480
Yeah, well, these are big questions,

01:35:14.480 --> 01:35:15.640
and I've thought about these a lot,

01:35:15.640 --> 01:35:19.880
but the interesting thing is that if we're alone,

01:35:19.880 --> 01:35:22.320
that's somewhat comforting from the great filter perspective

01:35:22.320 --> 01:35:25.560
because it probably means the great filters are past us,

01:35:25.560 --> 01:35:26.600
and I'm pretty sure they are.

01:35:26.600 --> 01:35:29.360
So going back to your origin of life question,

01:35:29.360 --> 01:35:30.840
there are some incredible things

01:35:30.840 --> 01:35:32.200
that no one knows how happened.

01:35:32.200 --> 01:35:35.560
Obviously the first life form from chemical soup,

01:35:35.560 --> 01:35:37.080
that seems pretty hard.

01:35:37.080 --> 01:35:39.040
But I would guess the multicellular,

01:35:39.040 --> 01:35:42.680
I wouldn't be that surprised if we saw single cell

01:35:42.680 --> 01:35:45.760
life forms elsewhere, bacteria type things.

01:35:45.760 --> 01:35:48.320
But multicellular life seems incredibly hard,

01:35:48.320 --> 01:35:50.520
that step of capturing mitochondria

01:35:50.520 --> 01:35:53.400
and then using that as part of yourself

01:35:53.400 --> 01:35:54.240
when you've just eaten it.

01:35:54.320 --> 01:35:57.480
Would you say that's the biggest, the most,

01:35:59.480 --> 01:36:02.480
if you had to choose one, sort of Hitchhiker's Guide

01:36:02.480 --> 01:36:04.640
to the Galaxy, one sentence summary of like,

01:36:04.640 --> 01:36:07.560
oh, those clever creatures did this,

01:36:07.560 --> 01:36:08.560
that would be the multicellular.

01:36:08.560 --> 01:36:10.880
I think that's probably the one that's the biggest.

01:36:10.880 --> 01:36:11.920
I mean, there's a great book called

01:36:11.920 --> 01:36:15.080
The 10 Great Inventions of Evolution by Nick Lane,

01:36:15.080 --> 01:36:17.720
and he speculates on 10 of these,

01:36:17.720 --> 01:36:20.120
what could be great filters.

01:36:20.120 --> 01:36:20.960
I think that's one.

01:36:20.960 --> 01:36:23.840
I think the advent of intelligence

01:36:24.400 --> 01:36:27.520
and conscious intelligence to us to be able to do science

01:36:27.520 --> 01:36:29.880
and things like that is huge as well.

01:36:29.880 --> 01:36:34.880
I mean, it's only evolved once as far as in Earth history.

01:36:34.880 --> 01:36:37.160
So that would be a later candidate,

01:36:37.160 --> 01:36:39.160
but certainly for the early candidates,

01:36:39.160 --> 01:36:41.440
I think multicellular life forms is huge.

01:36:41.440 --> 01:36:43.560
By the way, it's interesting to ask you

01:36:43.560 --> 01:36:45.760
if you can hypothesize about

01:36:45.760 --> 01:36:48.000
what is the origin of intelligence.

01:36:48.000 --> 01:36:53.000
Is it that we started cooking meat over fire?

01:36:53.600 --> 01:36:55.520
Is it that we somehow figured out

01:36:55.520 --> 01:36:57.040
that we could be very powerful

01:36:57.040 --> 01:36:58.080
when we started collaborating?

01:36:58.080 --> 01:37:03.080
So cooperation between our ancestors

01:37:03.520 --> 01:37:05.920
so that we can overthrow the alpha male?

01:37:07.000 --> 01:37:07.840
What is it, Richard?

01:37:07.840 --> 01:37:08.920
I talked to Richard Randham,

01:37:08.920 --> 01:37:10.720
who thinks we're all just beta males

01:37:10.720 --> 01:37:14.480
who figured out how to collaborate to defeat the dictator,

01:37:14.480 --> 01:37:19.160
the authoritarian alpha male that controlled the tribe.

01:37:19.160 --> 01:37:20.880
Is there other explanation?

01:37:20.920 --> 01:37:24.840
Was there a 2001 Space Odyssey type of monolith

01:37:24.840 --> 01:37:25.920
that came down to Earth?

01:37:25.920 --> 01:37:28.560
Well, I think all of those things you suggested

01:37:28.560 --> 01:37:31.320
are good candidates, fire and cooking, right?

01:37:31.320 --> 01:37:36.200
So that's clearly important for energy efficiency,

01:37:36.200 --> 01:37:40.240
cooking our meat and then being able to be more efficient

01:37:40.240 --> 01:37:42.680
about eating it and consuming the energy.

01:37:43.520 --> 01:37:44.360
I think that's huge.

01:37:44.360 --> 01:37:46.360
And then utilizing fire and tools.

01:37:46.360 --> 01:37:49.160
I think you're right about the tribal cooperation aspects

01:37:49.160 --> 01:37:51.600
and probably language is part of that

01:37:51.600 --> 01:37:52.920
because probably that's what allowed us

01:37:52.920 --> 01:37:54.200
to out-compete Neanderthals

01:37:54.200 --> 01:37:56.760
and perhaps less cooperative species.

01:37:56.760 --> 01:37:59.320
So that may be the case.

01:37:59.320 --> 01:38:02.960
Tool-making, spears, axes, I think that let us,

01:38:02.960 --> 01:38:04.440
I mean, I think it's pretty clear now

01:38:04.440 --> 01:38:05.640
that humans were responsible

01:38:05.640 --> 01:38:08.400
for a lot of the extinctions of megafauna,

01:38:08.400 --> 01:38:11.400
especially in the Americas when humans arrived.

01:38:11.400 --> 01:38:15.000
So you can imagine once you discover tool usage,

01:38:15.000 --> 01:38:16.320
how powerful that would have been

01:38:16.320 --> 01:38:18.040
and how scary for animals.

01:38:18.040 --> 01:38:21.320
So I think all of those could have been explanations for it.

01:38:21.320 --> 01:38:22.680
The interesting thing is

01:38:22.680 --> 01:38:24.560
that it's a bit like general intelligence too,

01:38:24.560 --> 01:38:28.560
is it's very costly to begin with to have a brain

01:38:28.560 --> 01:38:30.040
and especially a general purpose brain

01:38:30.040 --> 01:38:31.440
rather than a special purpose one

01:38:31.440 --> 01:38:32.840
because the amount of energy our brains use,

01:38:32.840 --> 01:38:34.880
I think it's like 20% of the body's energy

01:38:34.880 --> 01:38:36.120
and it's massive.

01:38:36.120 --> 01:38:37.160
And when you're thinking chess,

01:38:37.160 --> 01:38:39.480
one of the funny things that we used to say

01:38:39.480 --> 01:38:42.040
is it's as much as a racing driver uses

01:38:42.040 --> 01:38:44.080
for a whole Formula One race,

01:38:44.080 --> 01:38:46.400
just playing a game of serious high-level chess,

01:38:46.400 --> 01:38:49.320
which you wouldn't think just sitting there

01:38:49.320 --> 01:38:52.040
because the brain's using so much energy.

01:38:52.040 --> 01:38:54.760
So in order for an animal or an organism to justify that,

01:38:54.760 --> 01:38:57.880
there has to be a huge payoff.

01:38:57.880 --> 01:39:00.280
And the problem with half a brain

01:39:00.280 --> 01:39:05.280
or half intelligence, say an IQs of like a monkey brain,

01:39:06.720 --> 01:39:10.240
it's not clear you can justify that evolutionary

01:39:10.240 --> 01:39:12.440
until you get to the human level brain.

01:39:12.440 --> 01:39:14.760
And so, but how do you do that jump?

01:39:14.760 --> 01:39:15.600
It's very difficult,

01:39:15.600 --> 01:39:17.160
which is why I think it's only been done once

01:39:17.160 --> 01:39:19.840
from the sort of specialized brains that you see in animals

01:39:19.840 --> 01:39:22.520
to this sort of general purpose,

01:39:22.520 --> 01:39:25.040
truing powerful brains that humans have

01:39:26.200 --> 01:39:28.960
and which allows us to invent the modern world.

01:39:29.840 --> 01:39:33.640
And it takes a lot to cross that barrier.

01:39:33.640 --> 01:39:35.640
And I think we've seen the same with AI systems,

01:39:35.640 --> 01:39:38.200
which is that maybe until very recently,

01:39:38.200 --> 01:39:40.920
it's always been easier to craft a specific solution

01:39:40.920 --> 01:39:42.320
to a problem like chess

01:39:42.320 --> 01:39:44.480
than it has been to build a general learning system

01:39:44.480 --> 01:39:46.280
that could potentially do many things.

01:39:46.280 --> 01:39:49.480
Because initially, that system will be way worse

01:39:49.480 --> 01:39:52.120
than less efficient than the specialized system.

01:39:52.120 --> 01:39:55.880
So one of the interesting quirks of the human mind

01:39:55.880 --> 01:40:00.880
of this evolved system is that it appears to be conscious.

01:40:01.320 --> 01:40:02.960
This thing that we don't quite understand,

01:40:02.960 --> 01:40:06.600
but it seems very special,

01:40:06.600 --> 01:40:08.760
its ability to have a subjective experience

01:40:08.760 --> 01:40:12.280
that it feels like something to eat a cookie,

01:40:12.320 --> 01:40:13.520
the deliciousness of it,

01:40:13.520 --> 01:40:15.560
or see a color and that kind of stuff.

01:40:15.560 --> 01:40:17.960
Do you think in order to solve intelligence,

01:40:17.960 --> 01:40:20.720
we also need to solve consciousness along the way?

01:40:20.720 --> 01:40:23.960
Do you think AGI systems need to have consciousness

01:40:23.960 --> 01:40:28.000
in order to be truly intelligent?

01:40:28.000 --> 01:40:29.680
Yeah, we thought about this a lot actually.

01:40:29.680 --> 01:40:33.440
And I think that my guess is that consciousness

01:40:33.440 --> 01:40:35.800
and intelligence are double dissociable.

01:40:35.800 --> 01:40:38.360
So you can have one without the other both ways.

01:40:38.360 --> 01:40:40.920
And I think you can see that with consciousness

01:40:40.920 --> 01:40:44.120
in that I think some animals and pets,

01:40:44.120 --> 01:40:46.240
if you have a pet dog or something like that,

01:40:46.240 --> 01:40:48.520
you can see some of the higher animals and dolphins,

01:40:48.520 --> 01:40:51.640
things like that, have self-awareness

01:40:51.640 --> 01:40:55.720
and are very sociable, seem to dream.

01:40:57.360 --> 01:40:59.000
A lot of the traits one would regard

01:40:59.000 --> 01:41:01.560
as being kind of conscious and self-aware.

01:41:02.800 --> 01:41:05.080
But yet they're not that smart, right?

01:41:05.080 --> 01:41:08.040
So they're not that intelligent by say IQ standards

01:41:08.040 --> 01:41:08.880
or something like that.

01:41:09.200 --> 01:41:11.760
It's also possible that our understanding of intelligence

01:41:11.760 --> 01:41:14.920
is flawed, like putting an IQ to it.

01:41:14.920 --> 01:41:17.360
Maybe the thing that a dog can do

01:41:17.360 --> 01:41:20.640
is actually gone very far along the path of intelligence

01:41:20.640 --> 01:41:23.240
and we humans are just able to play chess

01:41:23.240 --> 01:41:25.120
and maybe write poems.

01:41:25.120 --> 01:41:27.040
But if we go back to the idea of AGI

01:41:27.040 --> 01:41:29.480
and general intelligence, dogs are very specialized.

01:41:29.480 --> 01:41:30.920
Most animals are pretty specialized.

01:41:30.920 --> 01:41:32.360
They can be amazing at what they do,

01:41:32.360 --> 01:41:35.840
but they're like kind of elite sports people or something.

01:41:35.840 --> 01:41:38.040
So they do one thing extremely well

01:41:38.040 --> 01:41:40.120
because their entire brain is optimized.

01:41:40.120 --> 01:41:41.920
They have somehow convinced the entirety

01:41:41.920 --> 01:41:44.560
of the human population to feed them and service them.

01:41:44.560 --> 01:41:46.440
So in some way they're controlling.

01:41:46.440 --> 01:41:47.280
Yes, exactly.

01:41:47.280 --> 01:41:50.120
Well, we co-evolved to some crazy degree, right?

01:41:50.120 --> 01:41:53.840
Including the way the dogs even wag their tails

01:41:53.840 --> 01:41:55.200
and twitch their noses, right?

01:41:55.200 --> 01:41:57.520
We find inexorably cute.

01:41:58.680 --> 01:42:01.880
But I think you can also see intelligence on the other side.

01:42:01.880 --> 01:42:03.840
So systems like artificial systems

01:42:03.840 --> 01:42:07.280
that are amazingly smart at certain things,

01:42:07.280 --> 01:42:09.840
like maybe playing Go and chess and other things,

01:42:09.840 --> 01:42:13.480
but they don't feel at all in any shape or form conscious

01:42:13.480 --> 01:42:17.280
in the way that you do to me or I do to you.

01:42:17.280 --> 01:42:22.360
And I think actually building AI is,

01:42:22.360 --> 01:42:24.240
these intelligent constructs,

01:42:24.240 --> 01:42:25.960
is one of the best ways to explore

01:42:25.960 --> 01:42:28.040
the mystery of consciousness, to break it down,

01:42:28.040 --> 01:42:31.240
because we're gonna have devices

01:42:31.240 --> 01:42:34.480
that are pretty smart at certain things

01:42:34.480 --> 01:42:36.240
or capable at certain things,

01:42:36.240 --> 01:42:39.160
but potentially won't have any semblance

01:42:39.160 --> 01:42:40.840
of self-awareness or other things.

01:42:40.840 --> 01:42:43.880
And in fact, I would advocate if there's a choice,

01:42:43.880 --> 01:42:45.680
building systems in the first place,

01:42:45.680 --> 01:42:49.160
AI systems that are not conscious to begin with,

01:42:49.160 --> 01:42:52.440
just tools until we understand them better

01:42:52.440 --> 01:42:53.960
and the capabilities better.

01:42:53.960 --> 01:42:58.360
So on that topic, just not as the CEO of DeepMind,

01:42:59.400 --> 01:43:01.520
just as a human being, let me ask you about this

01:43:01.520 --> 01:43:05.320
one particular anecdotal evidence of the Google engineer

01:43:05.360 --> 01:43:09.880
who made a comment or believed that there's some aspect

01:43:09.880 --> 01:43:13.280
of a language model, the Lambda language model

01:43:13.280 --> 01:43:16.000
that exhibited sentience.

01:43:16.000 --> 01:43:18.480
So you said you believe there might be a responsibility

01:43:18.480 --> 01:43:21.160
to build systems that are not sentient.

01:43:21.160 --> 01:43:23.560
And this experience of a particular engineer,

01:43:23.560 --> 01:43:25.880
I think, I'd love to get your general opinion

01:43:25.880 --> 01:43:28.040
on this kind of thing, but I think it will happen

01:43:28.040 --> 01:43:31.520
more and more and more, which not when engineers,

01:43:31.520 --> 01:43:33.600
but when people out there that don't have an engineer

01:43:33.600 --> 01:43:35.560
background start interacting with increasingly

01:43:35.560 --> 01:43:39.000
intelligent systems, we anthropomorphize them.

01:43:39.000 --> 01:43:44.000
They start to have deep impactful interactions with us

01:43:44.720 --> 01:43:47.960
in a way that we miss them when they're gone.

01:43:47.960 --> 01:43:52.000
And we sure as heck feel like they're living entities,

01:43:52.000 --> 01:43:55.240
self-aware entities, and maybe even we project sentience

01:43:55.240 --> 01:43:56.080
onto them.

01:43:56.080 --> 01:44:00.040
So what's your thought about this particular system?

01:44:00.120 --> 01:44:03.160
This is a, have you ever met a language model

01:44:03.160 --> 01:44:04.600
that's sentient?

01:44:04.600 --> 01:44:06.440
No, no.

01:44:06.440 --> 01:44:10.200
What do you make of the case of when you kind of feel

01:44:10.200 --> 01:44:12.960
that there's some elements of sentience to the system?

01:44:12.960 --> 01:44:15.080
Yeah, so this is an interesting question

01:44:15.080 --> 01:44:17.800
and obviously a very fundamental one.

01:44:17.800 --> 01:44:20.320
So the first thing to say is I think that none

01:44:20.320 --> 01:44:22.800
of the systems we have today, I would say even have

01:44:22.800 --> 01:44:26.360
one iota of semblance of consciousness or sentience.

01:44:26.360 --> 01:44:29.760
That's my personal feeling interacting with them every day.

01:44:29.760 --> 01:44:32.440
So I think that's way premature to be discussing

01:44:32.440 --> 01:44:34.200
what that engineer talked about.

01:44:34.200 --> 01:44:36.520
I think at the moment it's more of projection

01:44:36.520 --> 01:44:37.880
of other way our own minds work,

01:44:37.880 --> 01:44:42.880
which is to see sort of purpose and direction

01:44:43.160 --> 01:44:45.120
in almost anything that we, you know, our brains

01:44:45.120 --> 01:44:49.760
are trained to interpret agency basically in things,

01:44:49.760 --> 01:44:52.320
even inanimate things sometimes.

01:44:52.320 --> 01:44:54.920
And of course with a language system,

01:44:54.920 --> 01:44:57.120
because language is so fundamental to intelligence,

01:44:57.120 --> 01:45:00.440
it's gonna be easy for us to anthropomorphize that.

01:45:00.440 --> 01:45:03.840
I mean, back in the day, even the first, you know,

01:45:03.840 --> 01:45:05.800
the dumbest sort of template chatbots ever,

01:45:05.800 --> 01:45:10.320
Eliza and the ilk of the original chatbots back in the 60s

01:45:10.320 --> 01:45:12.600
fooled some people under certain circumstances, right?

01:45:12.600 --> 01:45:14.040
It pretended to be a psychologist.

01:45:14.040 --> 01:45:16.080
So just basically wrap it back to you,

01:45:16.080 --> 01:45:19.240
the same question you asked it back to you.

01:45:19.240 --> 01:45:21.320
And some people believe that.

01:45:21.320 --> 01:45:23.280
So I don't think we can, this is why I think

01:45:23.280 --> 01:45:25.400
the Turing test is a little bit flawed as a formal test

01:45:25.400 --> 01:45:29.200
because it depends on the sophistication of the judge,

01:45:29.200 --> 01:45:33.240
whether or not they are qualified to make that distinction.

01:45:33.240 --> 01:45:36.800
So I think we should talk to, you know,

01:45:36.800 --> 01:45:38.280
the top philosophers about this,

01:45:38.280 --> 01:45:40.800
people like Daniel Dennett and David Chalmers

01:45:40.800 --> 01:45:42.480
and others who've obviously thought deeply

01:45:42.480 --> 01:45:43.640
about consciousness.

01:45:43.640 --> 01:45:46.000
Of course, consciousness itself hasn't been well,

01:45:46.000 --> 01:45:47.720
there's no agreed definition.

01:45:47.720 --> 01:45:51.840
If I was to, you know, speculate about that,

01:45:51.840 --> 01:45:55.080
you know, I kind of, the working definition I like is,

01:45:55.080 --> 01:45:57.200
it's the way information feels when, you know,

01:45:57.200 --> 01:45:58.040
it gets processed.

01:45:58.040 --> 01:46:00.120
I think maybe Max Tegmark came up with that.

01:46:00.120 --> 01:46:01.040
I like that idea.

01:46:01.040 --> 01:46:02.240
I don't know if it helps us get towards

01:46:02.240 --> 01:46:03.880
any more operational thing,

01:46:03.880 --> 01:46:07.760
but I think it's a nice way of viewing it.

01:46:07.760 --> 01:46:09.920
I think we can obviously see from neuroscience

01:46:09.920 --> 01:46:11.680
certain prerequisites that are required,

01:46:11.680 --> 01:46:14.400
like self-awareness, I think is necessary,

01:46:14.400 --> 01:46:16.040
but not sufficient component.

01:46:16.040 --> 01:46:18.120
This idea of a self and other,

01:46:18.120 --> 01:46:20.480
and set of coherent preferences

01:46:20.480 --> 01:46:22.480
that are coherent over time.

01:46:22.480 --> 01:46:24.800
You know, these things are maybe memory,

01:46:24.800 --> 01:46:26.240
these things are probably needed

01:46:26.240 --> 01:46:29.360
for a sentient or conscious being.

01:46:29.360 --> 01:46:31.840
But the reason, the difficult thing I think for us

01:46:31.840 --> 01:46:33.440
when we get, and I think this is a really interesting

01:46:33.440 --> 01:46:37.320
philosophical debate, is when we get closer to AGI

01:46:37.320 --> 01:46:42.280
and much more powerful systems than we have today,

01:46:42.280 --> 01:46:44.480
how are we going to make this judgment?

01:46:44.480 --> 01:46:47.000
And one way, which is the Turing test,

01:46:47.000 --> 01:46:48.680
is sort of a behavioral judgment.

01:46:48.680 --> 01:46:52.120
Is the system exhibiting all the behaviors

01:46:52.360 --> 01:46:56.880
that a human sentient or a sentient being would exhibit?

01:46:56.880 --> 01:46:58.160
Is it answering the right questions?

01:46:58.160 --> 01:46:59.160
Is it saying the right things?

01:46:59.160 --> 01:47:03.360
Is it indistinguishable from a human and so on?

01:47:03.360 --> 01:47:05.760
But I think there's a second thing

01:47:05.760 --> 01:47:09.240
that makes us as humans regard each other as sentient.

01:47:09.240 --> 01:47:10.920
Why do we think this?

01:47:10.920 --> 01:47:12.720
And I debated this with Daniel Dennett.

01:47:12.720 --> 01:47:13.880
And I think there's a second reason

01:47:13.880 --> 01:47:15.600
that's often overlooked,

01:47:15.600 --> 01:47:18.280
which is that we're running on the same substrate.

01:47:18.280 --> 01:47:21.120
So if we're exhibiting the same behavior,

01:47:21.120 --> 01:47:22.680
more or less, as humans,

01:47:22.680 --> 01:47:25.160
and we're running on the same carbon-based

01:47:25.160 --> 01:47:28.440
biological substrate, the squishy few pounds of flesh

01:47:28.440 --> 01:47:31.800
in our skulls, then the most parsimonious, I think,

01:47:31.800 --> 01:47:33.760
explanation is that you're feeling

01:47:33.760 --> 01:47:35.520
the same thing as I'm feeling.

01:47:35.520 --> 01:47:37.840
But we will never have that second part,

01:47:37.840 --> 01:47:41.200
the substrate equivalence, with a machine.

01:47:41.200 --> 01:47:43.960
So we will have to only judge based on the behavior.

01:47:43.960 --> 01:47:46.840
And I think the substrate equivalence is a critical part

01:47:46.840 --> 01:47:49.080
of why we make assumptions that we're conscious.

01:47:49.080 --> 01:47:51.720
And in fact, even with animals, high-level animals,

01:47:51.720 --> 01:47:52.680
why we think they might be,

01:47:52.680 --> 01:47:54.160
because they're exhibiting some of the behaviors

01:47:54.160 --> 01:47:55.920
we would expect from a sentient animal,

01:47:55.920 --> 01:47:57.600
and we know they're made of the same things,

01:47:57.600 --> 01:47:58.640
biological neurons.

01:47:58.640 --> 01:48:02.880
So we're gonna have to come up with explanations

01:48:02.880 --> 01:48:06.320
or models of the gap between substrate differences

01:48:06.320 --> 01:48:08.080
between machines and humans

01:48:08.080 --> 01:48:10.880
to get anywhere beyond the behavioral.

01:48:10.880 --> 01:48:12.920
But to me, sort of the practical question

01:48:12.920 --> 01:48:16.080
is very interesting and very important.

01:48:16.080 --> 01:48:18.680
When you have millions, perhaps billions of people

01:48:18.680 --> 01:48:20.840
believing that you have a sentient AI,

01:48:20.840 --> 01:48:23.120
believing what that Google engineer believed,

01:48:24.040 --> 01:48:26.360
which I just see as an obvious,

01:48:26.360 --> 01:48:28.760
very near-term future thing,

01:48:28.760 --> 01:48:31.200
certainly on the path to AGI,

01:48:31.200 --> 01:48:33.160
how does that change the world?

01:48:33.160 --> 01:48:35.280
What's the responsibility of the AI system

01:48:35.280 --> 01:48:37.080
to help those millions of people?

01:48:38.160 --> 01:48:39.800
And also, what's the ethical thing?

01:48:39.800 --> 01:48:44.800
Because you can make a lot of people happy

01:48:45.000 --> 01:48:48.240
by creating a meaningful, deep experience

01:48:48.800 --> 01:48:52.360
with a system that's faking it before it makes it.

01:48:56.360 --> 01:49:00.000
Who is to say what's the right thing to do?

01:49:00.000 --> 01:49:02.000
Should AI always be tools?

01:49:02.000 --> 01:49:06.120
Like, why are we constraining AIs to always be tools

01:49:06.120 --> 01:49:07.960
as opposed to friends?

01:49:07.960 --> 01:49:12.080
Yeah, I think, well, I mean, these are fantastic questions

01:49:12.080 --> 01:49:14.120
and also critical ones.

01:49:14.120 --> 01:49:16.520
And we've been thinking about this

01:49:16.520 --> 01:49:18.320
since the start of DeepMind and before that,

01:49:18.320 --> 01:49:19.800
because we planned for success

01:49:19.800 --> 01:49:24.800
and however remote that looked like back in 2010.

01:49:24.920 --> 01:49:27.240
And we've always had sort of these ethical considerations

01:49:27.240 --> 01:49:28.720
as fundamental at DeepMind.

01:49:29.680 --> 01:49:32.200
And my current thinking on the language models

01:49:32.200 --> 01:49:34.200
and large models is they're not ready.

01:49:34.200 --> 01:49:36.760
We don't understand them well enough yet.

01:49:36.760 --> 01:49:40.520
And in terms of analysis tools and guardrails,

01:49:40.520 --> 01:49:42.360
what they can and can't do and so on

01:49:42.360 --> 01:49:44.000
to deploy them at scale,

01:49:44.000 --> 01:49:47.120
because I think there are big, still ethical questions

01:49:47.120 --> 01:49:48.920
like should an AI system always announce

01:49:48.920 --> 01:49:50.880
that it is an AI system to begin with?

01:49:50.880 --> 01:49:51.880
Probably yes.

01:49:53.080 --> 01:49:55.800
What do you do about answering those philosophical questions

01:49:55.800 --> 01:49:59.080
about the feelings people may have about AI systems,

01:49:59.080 --> 01:50:01.040
perhaps incorrectly attributed?

01:50:01.040 --> 01:50:03.120
So I think there's a whole bunch of research

01:50:03.120 --> 01:50:06.320
that needs to be done first to responsibly,

01:50:06.320 --> 01:50:09.400
before you can responsibly deploy these systems at scale.

01:50:09.400 --> 01:50:12.320
That will be at least be my current position.

01:50:12.320 --> 01:50:15.400
Over time, I'm very confident we'll have those tools

01:50:15.400 --> 01:50:20.400
like interpretability questions and analysis questions.

01:50:20.960 --> 01:50:23.520
And then with the ethical quandary,

01:50:23.520 --> 01:50:28.520
I think there it's important to look beyond just science.

01:50:28.840 --> 01:50:31.760
That's why I think philosophy, social sciences,

01:50:31.760 --> 01:50:34.760
even theology, other things like that come into it

01:50:34.760 --> 01:50:37.440
where arts and humanities,

01:50:37.440 --> 01:50:40.360
what does it mean to be human and the spirit of being human

01:50:40.360 --> 01:50:41.960
and to enhance that?

01:50:42.480 --> 01:50:43.680
And the human condition, right?

01:50:43.680 --> 01:50:45.080
And allow us to experience things

01:50:45.080 --> 01:50:46.400
we could never experience before

01:50:46.400 --> 01:50:49.080
and improve the overall human condition

01:50:49.080 --> 01:50:51.640
and humanity overall, you know, get radical abundance,

01:50:51.640 --> 01:50:54.120
solve many scientific problems, solve disease.

01:50:54.120 --> 01:50:55.240
So this is the era I think,

01:50:55.240 --> 01:50:57.520
this is the amazing era I think we're heading into

01:50:57.520 --> 01:50:58.560
if we do it right.

01:50:59.480 --> 01:51:00.800
But we've got to be careful.

01:51:00.800 --> 01:51:02.680
We've already seen with things like social media,

01:51:02.680 --> 01:51:05.920
how dual use technologies can be misused by,

01:51:05.920 --> 01:51:10.920
firstly, by bad actors or naive actors or crazy actors,

01:51:12.080 --> 01:51:12.920
right?

01:51:12.920 --> 01:51:15.720
So there's that set of just the common or garden misuse

01:51:15.720 --> 01:51:18.040
of existing dual use technology.

01:51:18.040 --> 01:51:21.000
And then of course, there's an additional thing

01:51:21.000 --> 01:51:21.960
that has to be overcome with AI

01:51:21.960 --> 01:51:24.520
that eventually it may have its own agency.

01:51:24.520 --> 01:51:28.760
So it could be a good or bad in of itself.

01:51:28.760 --> 01:51:31.520
So I think these questions have to be approached

01:51:31.520 --> 01:51:35.400
very carefully using the scientific method, I would say,

01:51:35.400 --> 01:51:38.720
in terms of hypothesis generation, careful control testing,

01:51:38.720 --> 01:51:40.760
not live AB testing out in the world

01:51:40.760 --> 01:51:44.400
because with powerful dual technologies like AI,

01:51:44.400 --> 01:51:47.680
if something goes wrong, it may cause a lot of harm

01:51:47.680 --> 01:51:49.160
before you can fix it.

01:51:49.160 --> 01:51:52.040
It's not like an imaging app or game app

01:51:52.040 --> 01:51:54.640
where if something goes wrong,

01:51:54.640 --> 01:51:58.000
it's relatively easy to fix and the harm's relatively small.

01:51:58.000 --> 01:52:02.600
So I think it comes with the usual cliche

01:52:02.600 --> 01:52:05.280
of like with a lot of power comes a lot of responsibility.

01:52:05.280 --> 01:52:07.840
And I think that's the case here with things like AI,

01:52:07.840 --> 01:52:11.080
given the enormous opportunity in front of us.

01:52:11.080 --> 01:52:14.080
And I think we need a lot of voices

01:52:14.080 --> 01:52:17.200
and as many inputs into things like the design

01:52:17.200 --> 01:52:19.920
of the systems and the values they should have

01:52:19.920 --> 01:52:22.440
and what goals should they be put to.

01:52:22.440 --> 01:52:24.600
I think as wide a group of voices as possible

01:52:24.600 --> 01:52:26.800
beyond just the technologists is needed

01:52:26.800 --> 01:52:29.120
to input into that and to have a say in that,

01:52:29.120 --> 01:52:31.840
especially when it comes to deployment of these systems,

01:52:31.840 --> 01:52:33.480
which is when the rubber really hits the road,

01:52:33.480 --> 01:52:35.480
it really affects the general person in the street

01:52:35.480 --> 01:52:37.400
rather than fundamental research.

01:52:37.400 --> 01:52:40.280
And that's why I say, I think as a first step,

01:52:40.280 --> 01:52:42.360
it would be better if we have the choice

01:52:42.360 --> 01:52:45.120
to build these systems as tools to give,

01:52:45.120 --> 01:52:47.960
and I'm not saying that they should never go beyond tools

01:52:47.960 --> 01:52:50.360
because of course the potential is there

01:52:50.360 --> 01:52:52.960
for it to go way beyond just tools.

01:52:52.960 --> 01:52:55.800
But I think that would be a good first step

01:52:55.800 --> 01:52:58.880
in order for us to allow us to carefully experiment

01:52:58.880 --> 01:53:01.000
and understand what these things can do.

01:53:01.000 --> 01:53:05.800
So the leap between tool to sentient entity being

01:53:05.840 --> 01:53:08.320
is one we should take very careful of.

01:53:08.320 --> 01:53:11.160
Let me ask a dark personal question.

01:53:11.160 --> 01:53:13.520
So you're one of the most brilliant people

01:53:13.520 --> 01:53:16.840
in the AI community, you're also one of the most kind,

01:53:16.840 --> 01:53:20.880
and if I may say sort of loved people in the community.

01:53:20.880 --> 01:53:25.880
That said, creation of a super intelligent AI system

01:53:27.800 --> 01:53:32.720
would be one of the most powerful things in the world,

01:53:32.720 --> 01:53:34.840
tools or otherwise.

01:53:34.840 --> 01:53:37.560
And again, as the old saying goes,

01:53:37.560 --> 01:53:40.640
power corrupts and absolute power corrupts, absolutely.

01:53:41.640 --> 01:53:46.640
You are likely to be one of the people,

01:53:47.240 --> 01:53:50.280
but I would say probably the most likely person

01:53:50.280 --> 01:53:53.240
to be in the control of such a system.

01:53:53.240 --> 01:53:57.120
Do you think about the corrupting nature of power

01:53:57.120 --> 01:53:59.560
when you talk about these kinds of systems

01:53:59.560 --> 01:54:04.560
that as all dictators and people have caused atrocities

01:54:04.920 --> 01:54:07.760
in the past, always think they're doing good,

01:54:07.760 --> 01:54:10.520
but they don't do good because the powers

01:54:10.520 --> 01:54:13.720
polluted their mind about what is good and what is evil.

01:54:13.720 --> 01:54:14.840
Do you think about this stuff

01:54:14.840 --> 01:54:16.440
or are we just focused on language model?

01:54:16.440 --> 01:54:18.720
No, I think about them all the time.

01:54:18.720 --> 01:54:22.360
And I think what are the defenses against that?

01:54:22.360 --> 01:54:24.840
I think one thing is to remain very grounded

01:54:24.840 --> 01:54:28.800
and sort of humble no matter what you do or achieve.

01:54:28.800 --> 01:54:30.400
And I try to do that.

01:54:30.400 --> 01:54:32.200
My best friends are still my set of friends

01:54:32.200 --> 01:54:34.680
from my undergraduate Cambridge days,

01:54:35.520 --> 01:54:38.080
my family and friends are very important.

01:54:39.240 --> 01:54:42.360
I think trying to be a multidisciplinary person,

01:54:42.360 --> 01:54:43.760
it helps to keep you humble

01:54:43.760 --> 01:54:45.880
because no matter how good you are at one topic,

01:54:45.880 --> 01:54:47.560
someone will be better than you at that.

01:54:47.560 --> 01:54:51.280
And always relearning a new topic again from scratch

01:54:51.280 --> 01:54:53.360
or new field is very humbling.

01:54:53.360 --> 01:54:56.400
So for me, that's been biology over the last five years,

01:54:56.400 --> 01:55:00.240
huge area topic and I just love doing that,

01:55:00.240 --> 01:55:01.600
but it helps to keep you grounded

01:55:01.600 --> 01:55:03.160
like it keeps you open-minded.

01:55:04.000 --> 01:55:06.800
And then the other important thing is to have

01:55:06.800 --> 01:55:10.040
a really good, amazing set of people around you

01:55:10.040 --> 01:55:11.840
at your company or your organization

01:55:11.840 --> 01:55:14.760
who are also very ethical and grounded themselves

01:55:14.760 --> 01:55:16.840
and help to keep you that way.

01:55:16.840 --> 01:55:18.880
And then ultimately, just to answer your question,

01:55:18.880 --> 01:55:22.000
I hope we're gonna be a big part of birthing AI

01:55:22.000 --> 01:55:24.440
and that being the greatest benefit to humanity

01:55:24.440 --> 01:55:26.840
of any tool or technology ever,

01:55:26.840 --> 01:55:29.560
and getting us into a world of radical abundance

01:55:29.560 --> 01:55:33.960
and curing diseases and solving many of the big challenges

01:55:33.960 --> 01:55:36.360
we have in front of us and then ultimately,

01:55:36.360 --> 01:55:38.240
help the ultimate flourishing of humanity

01:55:38.240 --> 01:55:41.160
to travel the stars and find those aliens if they are there.

01:55:41.160 --> 01:55:43.720
And if they're not there, find out why they're not there,

01:55:43.720 --> 01:55:46.520
what is going on here in the universe.

01:55:46.520 --> 01:55:49.560
This is all to come and that's what I've always dreamed about.

01:55:50.720 --> 01:55:52.960
But I don't think, I think AI is too big an idea.

01:55:52.960 --> 01:55:56.160
It's not going to be, there'll be a certain set of pioneers

01:55:56.160 --> 01:55:57.000
who get there first.

01:55:57.000 --> 01:55:58.600
I hope we're in the vanguard

01:55:58.600 --> 01:56:00.360
so we can influence how that goes.

01:56:00.360 --> 01:56:02.480
And I think it matters who builds,

01:56:02.480 --> 01:56:06.480
which cultures they come from and what values they have,

01:56:06.480 --> 01:56:07.840
the builders of AI systems.

01:56:07.840 --> 01:56:09.280
Because I think even though the AI system

01:56:09.280 --> 01:56:11.560
is gonna learn for itself most of its knowledge,

01:56:11.560 --> 01:56:14.760
there'll be a residue in the system of the culture

01:56:14.760 --> 01:56:17.680
and the values of the creators of that system.

01:56:17.680 --> 01:56:20.080
And there's interesting questions to discuss about that

01:56:20.080 --> 01:56:22.400
geopolitically, different cultures,

01:56:22.400 --> 01:56:24.880
we're in a more fragmented world than ever, unfortunately.

01:56:24.880 --> 01:56:27.480
I think in terms of global cooperation,

01:56:27.480 --> 01:56:29.240
we see that in things like climate

01:56:29.240 --> 01:56:32.000
where we can't seem to get our act together globally

01:56:32.000 --> 01:56:34.080
to cooperate on these pressing matters.

01:56:34.080 --> 01:56:35.600
I hope that will change over time.

01:56:35.600 --> 01:56:38.640
Perhaps if we get to an era of radical abundance,

01:56:38.640 --> 01:56:40.440
we don't have to be so competitive anymore.

01:56:40.440 --> 01:56:42.680
Maybe we can be more cooperative

01:56:42.680 --> 01:56:44.320
if resources aren't so scarce.

01:56:44.320 --> 01:56:48.240
It's true that in terms of power corrupting

01:56:48.240 --> 01:56:50.040
and leading to destructive things,

01:56:50.040 --> 01:56:53.160
it seems that some of the atrocities of the past happen

01:56:53.160 --> 01:56:56.680
when there's a significant constraint on resources.

01:56:56.680 --> 01:56:57.600
I think that's the first thing.

01:56:57.600 --> 01:56:58.440
I don't think that's enough.

01:56:58.440 --> 01:57:01.640
I think scarcity is one thing that's led to competition,

01:57:01.640 --> 01:57:04.000
destruct, you know, sort of zero sum game thinking.

01:57:04.000 --> 01:57:06.120
I would like us to all be in a positive sum world.

01:57:06.120 --> 01:57:08.520
And I think for that, you have to remove scarcity.

01:57:08.520 --> 01:57:09.880
I don't think that's enough, unfortunately,

01:57:09.880 --> 01:57:11.760
to get world peace because there's also

01:57:11.760 --> 01:57:14.680
other corrupting things like wanting power over people

01:57:14.680 --> 01:57:15.520
and this kind of stuff,

01:57:15.520 --> 01:57:19.040
which is not necessarily satisfied by just abundance.

01:57:19.040 --> 01:57:20.280
But I think it will help.

01:57:21.200 --> 01:57:23.320
And I think, but I think ultimately,

01:57:23.320 --> 01:57:25.920
AI is not gonna be run by any one person

01:57:25.920 --> 01:57:26.760
or one organization.

01:57:26.760 --> 01:57:28.040
I think it should belong to the world,

01:57:28.040 --> 01:57:29.560
belong to humanity.

01:57:29.560 --> 01:57:33.080
And I think there'll be many ways this will happen.

01:57:33.080 --> 01:57:36.840
And ultimately, everybody should have a say in that.

01:57:37.800 --> 01:57:41.320
Do you have advice for young people

01:57:41.320 --> 01:57:43.040
in high school and college,

01:57:43.040 --> 01:57:45.840
maybe if they're interested in AI

01:57:45.840 --> 01:57:50.680
or interested in having a big impact on the world,

01:57:50.680 --> 01:57:53.240
what they should do to have a career they can be proud of

01:57:53.240 --> 01:57:55.040
or to have a life they can be proud of?

01:57:55.040 --> 01:57:57.440
I love giving talks to the next generation.

01:57:57.440 --> 01:57:59.160
What I say to them is actually two things.

01:57:59.160 --> 01:58:02.160
I think the most important things to learn about

01:58:02.160 --> 01:58:04.520
and to find out about when you're young

01:58:04.520 --> 01:58:07.080
is what are your true passions is first of all,

01:58:07.080 --> 01:58:07.920
there's two things.

01:58:07.920 --> 01:58:09.720
One is find your true passions.

01:58:09.720 --> 01:58:12.520
And I think you can do that by the way to do that

01:58:12.520 --> 01:58:15.240
is to explore as many things as possible when you're young

01:58:15.240 --> 01:58:19.160
and you have the time and you can take those risks.

01:58:19.160 --> 01:58:21.080
I would also encourage people to look at the,

01:58:21.080 --> 01:58:24.600
finding the connections between things in a unique way.

01:58:25.160 --> 01:58:27.280
I think that's a really great way to find a passion.

01:58:27.280 --> 01:58:30.640
Second thing I would say, advice is know yourself.

01:58:30.640 --> 01:58:35.640
So spend a lot of time understanding how you work best,

01:58:35.640 --> 01:58:37.720
like what are the optimal times to work?

01:58:37.720 --> 01:58:39.920
What are the optimal ways that you study?

01:58:39.920 --> 01:58:42.280
What are your, how do you deal with pressure?

01:58:42.280 --> 01:58:44.600
Sort of test yourself in various scenarios

01:58:44.600 --> 01:58:47.280
and try and improve your weaknesses,

01:58:47.280 --> 01:58:50.720
but also find out what your unique skills and strengths are

01:58:50.720 --> 01:58:52.200
and then hone those.

01:58:52.200 --> 01:58:54.560
So then that's what will be your super value

01:58:54.560 --> 01:58:55.920
in the world later on.

01:58:55.920 --> 01:58:57.880
And if you can then combine those two things

01:58:57.880 --> 01:59:01.240
and find passions that you're genuinely excited about

01:59:01.240 --> 01:59:05.400
that intersect with what your unique strong skills are,

01:59:05.400 --> 01:59:07.920
then you're onto something incredible

01:59:07.920 --> 01:59:10.960
and I think you can make a huge difference in the world.

01:59:10.960 --> 01:59:12.760
So let me ask about know yourself.

01:59:12.760 --> 01:59:14.280
This is fun, this is fun.

01:59:14.280 --> 01:59:18.160
Quick questions about day in the life, the perfect day,

01:59:18.160 --> 01:59:21.200
the perfect productive day in the life of Demisys Hopps.

01:59:21.240 --> 01:59:26.240
Maybe these days you're, there's a lot involved.

01:59:26.440 --> 01:59:29.040
Maybe a slightly younger Demisys Hopps

01:59:29.040 --> 01:59:31.440
where you could focus on a single project maybe.

01:59:33.200 --> 01:59:34.480
How early do you wake up?

01:59:34.480 --> 01:59:35.680
Are you night owl?

01:59:35.680 --> 01:59:36.800
Do you wake up early in the morning?

01:59:36.800 --> 01:59:39.200
What are some interesting habits?

01:59:39.200 --> 01:59:42.440
How many dozens of cups of coffees do you drink a day?

01:59:42.440 --> 01:59:46.360
What's the computer that you use?

01:59:46.360 --> 01:59:47.200
What's the setup?

01:59:47.200 --> 01:59:48.020
How many screens?

01:59:48.020 --> 01:59:49.160
What kind of keyboard?

01:59:49.160 --> 01:59:51.440
Are we talking a EMAX VIM

01:59:51.440 --> 01:59:53.360
or are we talking something more modern?

01:59:53.360 --> 01:59:54.520
So there's a bunch of those questions.

01:59:54.520 --> 01:59:59.000
So maybe day in the life, what's the perfect day involved?

01:59:59.000 --> 02:00:00.880
Well, these days it's quite different

02:00:00.880 --> 02:00:02.680
from say 10, 20 years ago.

02:00:02.680 --> 02:00:05.440
Back 10, 20 years ago, it would have been

02:00:05.440 --> 02:00:10.080
a whole day of research, individual research

02:00:10.080 --> 02:00:12.000
or programming, doing some experiment,

02:00:12.000 --> 02:00:14.080
neuroscience, computer science experiment,

02:00:14.080 --> 02:00:16.640
reading lots of research papers

02:00:16.640 --> 02:00:18.400
and then perhaps at nighttime,

02:00:19.720 --> 02:00:24.720
reading science fiction books or playing some games.

02:00:25.440 --> 02:00:28.360
But lots of focus, so like deep focused work

02:00:28.360 --> 02:00:32.440
on whether it's programming or reading research papers.

02:00:32.440 --> 02:00:35.320
Yes, so that would be lots of deep, focused work.

02:00:35.320 --> 02:00:39.560
These days for the last sort of, I guess, five to 10 years,

02:00:39.560 --> 02:00:41.020
I've actually got quite a structure

02:00:41.020 --> 02:00:42.360
that works very well for me now,

02:00:42.360 --> 02:00:46.160
which is that I'm a complete night owl, always have been.

02:00:46.160 --> 02:00:47.680
So I optimize for that.

02:00:47.680 --> 02:00:50.760
So I basically do a normal day's work,

02:00:50.760 --> 02:00:52.560
get into work about 11 o'clock

02:00:52.560 --> 02:00:56.400
and sort of do work to about seven in the office.

02:00:56.400 --> 02:00:58.960
And I will arrange back to back meetings

02:00:58.960 --> 02:01:00.920
for the entire time of that.

02:01:00.920 --> 02:01:03.200
And with as many, meet as many people as possible.

02:01:03.200 --> 02:01:06.480
So that's my collaboration management part of the day.

02:01:06.480 --> 02:01:10.680
Then I go home, spend time with the family and friends,

02:01:10.680 --> 02:01:13.600
have dinner, relax a little bit.

02:01:13.600 --> 02:01:15.240
And then I start a second day of work.

02:01:15.240 --> 02:01:18.480
I call it my second day of work around 10 p.m., 11 p.m.

02:01:18.480 --> 02:01:20.840
And that's the time till about the small hours

02:01:20.840 --> 02:01:22.520
of the morning, four or five in the morning,

02:01:22.520 --> 02:01:26.480
where I will do my thinking and reading research,

02:01:26.480 --> 02:01:28.060
writing research papers.

02:01:29.000 --> 02:01:30.960
Sadly, I don't have time to code anymore,

02:01:30.960 --> 02:01:34.900
but it's not efficient to do that these days,

02:01:34.900 --> 02:01:37.120
given the amount of time I have.

02:01:37.120 --> 02:01:40.760
But that's when I do, maybe do the long kind of stretches

02:01:40.760 --> 02:01:42.440
of thinking and planning.

02:01:42.480 --> 02:01:45.280
And then probably, using email or other things,

02:01:45.280 --> 02:01:47.640
I would set, I would fire off a lot of things to my team

02:01:47.640 --> 02:01:49.360
to deal with the next morning.

02:01:49.360 --> 02:01:51.640
But actually thinking about this overnight,

02:01:51.640 --> 02:01:53.200
we should go for this project

02:01:53.200 --> 02:01:54.880
or arrange this meeting the next day.

02:01:54.880 --> 02:01:56.120
When you think it through a problem,

02:01:56.120 --> 02:01:58.160
are you talking about a sheet of paper or the pen?

02:01:58.160 --> 02:02:01.040
Is there some structured process?

02:02:01.040 --> 02:02:04.360
I still like pencil and paper best for working out things,

02:02:04.360 --> 02:02:06.720
but these days it's just so efficient

02:02:06.720 --> 02:02:08.720
to read research papers just on the screen.

02:02:08.720 --> 02:02:10.220
I still often print them out, actually.

02:02:10.220 --> 02:02:12.540
I still prefer to mark out things,

02:02:12.540 --> 02:02:14.500
and I find it goes into the brain quicker,

02:02:14.500 --> 02:02:16.000
better, and sticks in the brain better

02:02:16.000 --> 02:02:19.460
when you're still using physical pen and pencil and paper.

02:02:19.460 --> 02:02:20.820
So you take notes with the?

02:02:20.820 --> 02:02:22.460
I have lots of notes, electronic ones,

02:02:22.460 --> 02:02:27.460
and also whole stacks of notebooks that I use at home, yeah.

02:02:27.620 --> 02:02:29.860
On some of these most challenging next steps,

02:02:29.860 --> 02:02:32.660
for example, stuff none of us know about

02:02:32.660 --> 02:02:35.580
that you're working on, you're thinking,

02:02:35.580 --> 02:02:37.620
there's some deep thinking required there, right?

02:02:37.620 --> 02:02:39.420
Like what is the right problem?

02:02:39.420 --> 02:02:41.300
What is the right approach?

02:02:41.300 --> 02:02:42.900
Because you're gonna have to invest

02:02:42.900 --> 02:02:44.780
a huge amount of time for the whole team.

02:02:44.780 --> 02:02:46.740
They're going to have to pursue this thing.

02:02:46.740 --> 02:02:48.540
What's the right way to do it?

02:02:48.540 --> 02:02:50.060
Is RL gonna work here or not?

02:02:50.060 --> 02:02:50.900
Yes.

02:02:51.980 --> 02:02:53.140
What's the right thing to try?

02:02:53.140 --> 02:02:54.940
What's the right benchmark to use?

02:02:54.940 --> 02:02:57.340
Do we need to construct a benchmark from scratch?

02:02:57.340 --> 02:02:58.180
All those kinds of things.

02:02:58.180 --> 02:03:00.220
Yes, so I think of all those kind of things

02:03:00.220 --> 02:03:03.460
in the nighttime phase, but also much more,

02:03:03.460 --> 02:03:07.660
I find I've always found the quiet hours of the morning

02:03:07.660 --> 02:03:11.420
when everyone's asleep, it's super quiet outside.

02:03:11.420 --> 02:03:13.380
I love that time, it's the golden hours,

02:03:13.380 --> 02:03:16.500
like between one and three in the morning.

02:03:16.500 --> 02:03:18.900
Put some music on, some inspiring music on,

02:03:18.900 --> 02:03:21.580
and then think these deep thoughts.

02:03:21.580 --> 02:03:24.220
So that's when I would read my philosophy books

02:03:24.220 --> 02:03:28.820
and Spinoza is my recent favorite can, all these things.

02:03:28.820 --> 02:03:33.660
And I read about a great scientist of history,

02:03:33.660 --> 02:03:35.660
how they did things, how they thought things.

02:03:35.660 --> 02:03:37.220
So that's when you do all your create,

02:03:37.220 --> 02:03:39.180
that's when I do all my creative thinking.

02:03:39.180 --> 02:03:42.100
And it's good, I think people recommend,

02:03:42.100 --> 02:03:45.100
you do your sort of creative thinking in one block.

02:03:45.100 --> 02:03:47.140
And the way I organize the day,

02:03:47.140 --> 02:03:48.580
that way I don't get interrupted,

02:03:48.580 --> 02:03:51.500
because obviously no one else is up at those times.

02:03:51.500 --> 02:03:56.500
So I can go, I can sort of get super deep and super into flow.

02:03:57.580 --> 02:03:59.660
The other nice thing about doing it nighttime wise

02:03:59.660 --> 02:04:02.820
is if I'm really onto something

02:04:02.820 --> 02:04:04.980
or I've got really deep into something,

02:04:04.980 --> 02:04:06.900
I can choose to extend it

02:04:06.900 --> 02:04:09.020
and I'll go into six in the morning, whatever,

02:04:09.020 --> 02:04:10.820
and then I'll just pay for it the next day.

02:04:10.820 --> 02:04:13.020
So I'll be a bit tired and I won't be my best,

02:04:13.020 --> 02:04:13.940
but that's fine.

02:04:13.940 --> 02:04:16.740
I can decide looking at my schedule the next day,

02:04:16.740 --> 02:04:19.380
and given where I'm at with this particular thought

02:04:19.380 --> 02:04:20.820
or creative idea,

02:04:20.820 --> 02:04:22.860
that I'm gonna pay that cost the next day.

02:04:22.860 --> 02:04:25.420
So I think that's more flexible

02:04:25.420 --> 02:04:27.660
than morning people who do that.

02:04:27.660 --> 02:04:28.820
They get up at four in the morning,

02:04:28.820 --> 02:04:31.060
they can also do those golden hours then,

02:04:31.060 --> 02:04:32.700
but then their start of their scheduled day

02:04:32.700 --> 02:04:34.020
starts at breakfast,

02:04:34.500 --> 02:04:36.060
whatever they have their first meeting,

02:04:36.060 --> 02:04:36.900
and then it's hard,

02:04:36.900 --> 02:04:38.980
you have to reschedule a day if you're in flow.

02:04:38.980 --> 02:04:41.940
Yeah, that could be a true special thread of thoughts

02:04:41.940 --> 02:04:45.180
that you're too passionate about.

02:04:45.180 --> 02:04:46.780
This is where some of the greatest ideas

02:04:46.780 --> 02:04:47.780
could potentially come

02:04:47.780 --> 02:04:50.380
is when you just lose yourself late in the night.

02:04:51.380 --> 02:04:52.980
And for the meetings,

02:04:52.980 --> 02:04:54.900
I mean, you're loading in really hard problems

02:04:54.900 --> 02:04:56.540
in a very short amount of time.

02:04:56.540 --> 02:04:59.060
So you have to do some kind of first principles thinking here.

02:04:59.060 --> 02:05:00.220
It's like, what's the problem?

02:05:00.220 --> 02:05:01.380
What's the state of things?

02:05:01.380 --> 02:05:03.180
What's the right next step?

02:05:03.180 --> 02:05:05.140
You have to get really good at context switching,

02:05:05.140 --> 02:05:07.220
which is one of the hardest things,

02:05:07.220 --> 02:05:09.060
because especially as we do so many things,

02:05:09.060 --> 02:05:10.820
if you include all the scientific things we do,

02:05:10.820 --> 02:05:12.620
scientific fields we're working in,

02:05:12.620 --> 02:05:15.420
these are entire complex fields in themselves,

02:05:15.420 --> 02:05:19.020
and you have to sort of keep up to abreast of that.

02:05:19.020 --> 02:05:20.060
But I enjoy it.

02:05:20.060 --> 02:05:23.900
I've always been a sort of generalist in a way,

02:05:23.900 --> 02:05:24.820
and that's actually what happened

02:05:24.820 --> 02:05:26.420
in my games career after chess.

02:05:27.940 --> 02:05:29.300
One of the reasons I stopped playing chess

02:05:29.300 --> 02:05:30.380
was because I got into computers,

02:05:30.380 --> 02:05:31.700
but also I started realizing

02:05:31.700 --> 02:05:33.940
there were many other great games out there to play too.

02:05:33.940 --> 02:05:36.980
So I've always been that way inclined, multidisciplinary,

02:05:36.980 --> 02:05:39.180
and there's too many interesting things in the world

02:05:39.180 --> 02:05:41.740
to spend all your time just on one thing.

02:05:41.740 --> 02:05:43.300
So you mentioned Spinoza,

02:05:43.300 --> 02:05:47.700
gotta ask the big, ridiculously big question about life.

02:05:47.700 --> 02:05:50.540
What do you think is the meaning of this whole thing?

02:05:50.540 --> 02:05:52.620
Why are we humans here?

02:05:52.620 --> 02:05:53.620
You've already mentioned

02:05:53.620 --> 02:05:56.740
that perhaps the universe created us.

02:05:56.740 --> 02:05:58.980
Is that why you think we're here?

02:05:58.980 --> 02:06:00.180
To understand how the universe works.

02:06:00.380 --> 02:06:02.100
I think my answer to that would be,

02:06:02.100 --> 02:06:03.980
and at least the life I'm living

02:06:03.980 --> 02:06:08.100
is to gain and understand the knowledge,

02:06:08.100 --> 02:06:10.620
to gain knowledge and understand the universe.

02:06:10.620 --> 02:06:12.260
That's what I think.

02:06:12.260 --> 02:06:13.860
I can't see any higher purpose than that.

02:06:13.860 --> 02:06:15.700
If you think back to the classical Greeks,

02:06:15.700 --> 02:06:17.540
the virtue of gaining knowledge,

02:06:17.540 --> 02:06:20.460
I think it's one of the few true virtues

02:06:20.460 --> 02:06:23.580
is to understand the world around us

02:06:23.580 --> 02:06:25.660
and the context and humanity better.

02:06:25.660 --> 02:06:27.820
And I think if you do that,

02:06:27.820 --> 02:06:29.140
you become more compassionate

02:06:29.140 --> 02:06:32.060
and more understanding yourself and more tolerant

02:06:32.060 --> 02:06:32.900
and all these,

02:06:32.900 --> 02:06:34.740
I think all these other things may flow from that.

02:06:34.740 --> 02:06:37.660
And to me, understanding the nature of reality,

02:06:37.660 --> 02:06:38.740
that is the biggest question.

02:06:38.740 --> 02:06:41.420
What is going on here is sometimes the colloquial way I say.

02:06:41.420 --> 02:06:43.620
What is really going on here?

02:06:43.620 --> 02:06:44.900
It's so mysterious.

02:06:44.900 --> 02:06:46.820
I feel like we're in some huge puzzle.

02:06:48.180 --> 02:06:49.980
But the world is also seems to be,

02:06:49.980 --> 02:06:53.100
the universe seems to be structured in a way,

02:06:53.100 --> 02:06:55.860
why is it structured in a way that science is even possible?

02:06:56.820 --> 02:06:58.140
The scientific method works.

02:06:58.140 --> 02:06:59.260
Things are repeatable.

02:07:00.220 --> 02:07:02.540
It feels like it's almost structured in a way

02:07:02.540 --> 02:07:04.980
to be conducive to gaining knowledge.

02:07:04.980 --> 02:07:06.500
So I feel like,

02:07:06.500 --> 02:07:07.940
why should computers be even possible?

02:07:07.940 --> 02:07:10.740
Isn't that amazing that computational

02:07:10.740 --> 02:07:14.060
or electronic devices can be possible

02:07:14.060 --> 02:07:15.260
and they're made of sand,

02:07:15.260 --> 02:07:17.420
our most common element that we have,

02:07:17.420 --> 02:07:19.900
silicon on the earth's crust.

02:07:19.900 --> 02:07:21.460
It could be made of diamond or something

02:07:21.460 --> 02:07:23.740
and we would have only had one computer.

02:07:24.420 --> 02:07:26.540
A lot of things are slightly suspicious to me.

02:07:26.540 --> 02:07:27.700
It sure as heck sounds,

02:07:27.700 --> 02:07:29.180
this puzzle sure as heck sounds

02:07:29.180 --> 02:07:30.740
like something we talked about earlier,

02:07:30.740 --> 02:07:33.900
what it takes to design a game

02:07:33.900 --> 02:07:36.740
that's really fun to play for prolonged periods of time.

02:07:37.740 --> 02:07:39.420
And it does seem like this puzzle,

02:07:39.420 --> 02:07:42.300
like you mentioned, the more you learn about it,

02:07:42.300 --> 02:07:44.860
the more you realize how little you know.

02:07:44.860 --> 02:07:46.820
So it humbles you but excites you

02:07:46.820 --> 02:07:49.020
by the possibility of learning more.

02:07:49.020 --> 02:07:53.580
It's one heck of a puzzle we got going on here.

02:07:54.420 --> 02:07:56.460
So like I mentioned, of all the people in the world,

02:07:56.460 --> 02:07:59.580
you're very likely to be the one who creates

02:07:59.580 --> 02:08:04.580
the AGI system that achieves human level intelligence

02:08:04.980 --> 02:08:06.340
and goes beyond it.

02:08:06.340 --> 02:08:07.660
So if you got a chance,

02:08:07.660 --> 02:08:09.460
and very well you could be the person

02:08:09.460 --> 02:08:11.100
that goes into the room with the system

02:08:11.100 --> 02:08:13.180
and have a conversation,

02:08:13.180 --> 02:08:15.300
maybe you only get to ask one question.

02:08:15.300 --> 02:08:18.140
If you do, what question would you ask her?

02:08:19.140 --> 02:08:21.140
I would probably ask,

02:08:21.140 --> 02:08:23.380
what is the true nature of reality?

02:08:23.380 --> 02:08:24.300
I think that's the question.

02:08:24.300 --> 02:08:25.700
I don't know if I'd understand the answer

02:08:25.700 --> 02:08:28.260
because maybe it would be 42 or something like that.

02:08:28.260 --> 02:08:30.700
But that's the question I would ask.

02:08:32.140 --> 02:08:34.540
And then there'll be a deep sigh from the systems,

02:08:34.540 --> 02:08:37.180
like, all right, how do I explain to this human?

02:08:37.180 --> 02:08:41.580
All right, let me, I don't have time to explain.

02:08:41.580 --> 02:08:44.380
Maybe I'll draw you a picture that it is,

02:08:44.380 --> 02:08:46.260
I mean, how do you even begin

02:08:48.460 --> 02:08:49.900
to answer that question?

02:08:51.300 --> 02:08:53.260
Well, I think it would-

02:08:53.260 --> 02:08:55.700
What would you think the answer could possibly look like?

02:08:55.700 --> 02:08:58.420
I think it could start looking like

02:08:59.940 --> 02:09:02.100
more fundamental explanations of physics

02:09:02.100 --> 02:09:03.940
would be the beginning.

02:09:03.940 --> 02:09:05.780
More careful specification of that,

02:09:05.780 --> 02:09:07.740
taking, walking us through by the hand

02:09:07.740 --> 02:09:10.660
as to what one would do to maybe prove those things out.

02:09:10.660 --> 02:09:13.140
Maybe giving you glimpses of

02:09:13.140 --> 02:09:16.380
what things you totally missed in the physics of today.

02:09:16.380 --> 02:09:17.220
Exactly.

02:09:17.500 --> 02:09:18.900
Here's glimpses of,

02:09:18.900 --> 02:09:23.660
no, like there's a much more elaborate world

02:09:23.660 --> 02:09:25.500
or a much simpler world or something.

02:09:26.820 --> 02:09:30.300
A much deeper, maybe simpler explanation of things

02:09:30.300 --> 02:09:31.940
right than the standard model of physics,

02:09:31.940 --> 02:09:34.900
which we know doesn't work, but we still keep adding to.

02:09:34.900 --> 02:09:37.980
So, and that's how I think the beginning

02:09:37.980 --> 02:09:38.980
of an explanation would look.

02:09:38.980 --> 02:09:41.300
And it would start encompassing many of the mysteries

02:09:41.300 --> 02:09:43.420
that we have wondered about for thousands of years,

02:09:43.420 --> 02:09:47.180
like consciousness, dreaming, life,

02:09:48.140 --> 02:09:48.980
and gravity, all of these things.

02:09:48.980 --> 02:09:52.620
Yeah, giving us glimpses of explanations for those things.

02:09:52.620 --> 02:09:57.180
Well, Damis, you're one of the special human beings

02:09:57.180 --> 02:09:59.060
in this giant puzzle of ours,

02:09:59.060 --> 02:10:01.020
and it's a huge honor that you would take a pause

02:10:01.020 --> 02:10:03.220
from the bigger puzzle to solve this small puzzle

02:10:03.220 --> 02:10:04.740
of a conversation with me today.

02:10:04.740 --> 02:10:06.300
It's truly an honor and a pleasure.

02:10:06.300 --> 02:10:07.140
Thank you so much.

02:10:07.140 --> 02:10:07.960
Glad to have you.

02:10:07.960 --> 02:10:08.800
I really enjoyed it.

02:10:08.800 --> 02:10:09.640
Thanks, Lex.

02:10:09.640 --> 02:10:10.620
Thanks for listening to this conversation

02:10:10.620 --> 02:10:11.980
with Damis Ashabis.

02:10:11.980 --> 02:10:13.180
To support this podcast,

02:10:13.180 --> 02:10:15.820
please check out our sponsors in the description.

02:10:15.860 --> 02:10:17.920
And now, let me leave you with some words

02:10:17.920 --> 02:10:20.420
from Edgar Dijkstra.

02:10:20.420 --> 02:10:23.540
Computer science is no more about computers

02:10:23.540 --> 02:10:26.280
than astronomy is about telescopes.

02:10:27.260 --> 02:10:29.980
Thank you for listening and hope to see you next time.


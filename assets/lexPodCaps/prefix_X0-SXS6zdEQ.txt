WEBVTT

00:00.000 --> 00:03.400
The following is a conversation with Peter Wang, one of the most impactful

00:03.400 --> 00:07.960
leaders and developers in the Python community, former physicist, current

00:07.960 --> 00:13.080
philosopher, and someone who many people told me about and praised as a truly

00:13.080 --> 00:18.220
special mind that I absolutely should talk to, recommendations ranging from

00:18.220 --> 00:20.480
Travis Aliphant to Eric Weinstein.

00:20.900 --> 00:22.600
So here we are.

00:23.280 --> 00:25.400
This is the Lex Freedman podcast.

00:25.600 --> 00:28.560
To support it, please check out our sponsors in the description.

00:28.840 --> 00:32.240
And now here's my conversation with Peter Wang.

00:33.400 --> 00:37.080
You're one of the most impactful humans in the Python ecosystem.

00:38.240 --> 00:42.360
So you're an engineer, leader of engineers, but you're also a philosopher.

00:42.880 --> 00:46.920
So let's talk both in this conversation about programming and philosophy.

00:47.280 --> 00:52.320
First programming, what to you is the best or maybe the most beautiful

00:52.320 --> 00:57.360
feature of Python, or maybe the thing they made you fall in love or stay

00:57.360 --> 00:58.640
in love with Python?

00:58.960 --> 01:00.560
Well, those are three different things.

01:00.800 --> 01:03.520
What I think is most beautiful, what made me fall in love with me, stay in love.

01:03.880 --> 01:08.400
When I first started using it was when I was a C++ computer

01:08.400 --> 01:09.800
graphics performance nerd.

01:09.960 --> 01:10.600
In the nineties?

01:10.640 --> 01:11.840
In, yeah, in late nineties.

01:12.000 --> 01:13.680
And that was my first job out of college.

01:15.120 --> 01:19.680
And we kept trying to do more and more abstract and higher order programming

01:19.680 --> 01:24.800
in C++, which at the time was quite difficult with templates, the compiler

01:24.800 --> 01:26.120
support wasn't great, et cetera.

01:26.480 --> 01:29.600
So when I started playing around with Python, that was my first time

01:29.600 --> 01:34.000
encountering really first-class support for types, for functions and things like

01:34.000 --> 01:36.640
that, and it felt so incredibly expressive.

01:37.120 --> 01:39.720
So that was what kind of made me fall in love with it a little bit.

01:39.720 --> 01:44.560
And also once you spend a lot of time in a C++ dev environment, the ability to

01:44.560 --> 01:49.600
just whip something together that basically runs and works the first time is amazing.

01:49.600 --> 01:51.480
So really productive scripting language.

01:51.880 --> 01:53.560
I mean, I knew Perl, I knew Bash.

01:53.640 --> 01:57.120
I was decent at both, but Python just made everything.

01:57.160 --> 01:59.640
It made the whole world accessible, right?

01:59.640 --> 02:03.120
I could script this and that and the other network things, you know, little

02:03.120 --> 02:05.760
hard drive utilities, I could write all of these things in the space of an

02:05.760 --> 02:07.600
afternoon and that was really, really cool.

02:07.600 --> 02:08.560
So that's what made me fall in love.

02:08.600 --> 02:11.920
Is there something specific you could put your finger on that you're

02:11.920 --> 02:13.560
not programming in Perl today?

02:14.040 --> 02:16.200
Like why, why Python for scripting?

02:17.080 --> 02:22.240
I think there's not a specific thing as much as the design motif of both the,

02:22.280 --> 02:26.000
the creator of the language and the core group of people that built the standard

02:26.000 --> 02:32.120
library around him, there was definitely, there was a taste to it.

02:32.120 --> 02:35.520
I mean, Steve Jobs, you know, use that term, you know, in somewhat of a arrogant

02:35.520 --> 02:39.720
way, but I think it's a real thing that it was designed to fit a friend of mine

02:39.720 --> 02:40.880
actually expressed this really well.

02:40.880 --> 02:42.320
He said, Python just fits in my head.

02:42.960 --> 02:45.160
And there's nothing better to say than that.

02:45.160 --> 02:49.000
Now, now people might argue modern Python, there's a lot more complexity,

02:49.160 --> 02:53.480
but certainly as version five, one, five, two, I think was my first version that

02:53.480 --> 02:54.480
fit in my head very easily.

02:54.720 --> 02:56.040
So that's what made me fall in love with it.

02:56.480 --> 02:56.800
Okay.

02:56.800 --> 03:03.680
So the most beautiful feature of Python that made you stay in love over the

03:03.680 --> 03:08.880
years, what has like, you know, you do a double take and you, you return too

03:08.880 --> 03:11.440
often as a thing that just brings you a smile.

03:11.480 --> 03:17.440
I really still like the, um, the ability to play with meta classes and express

03:17.440 --> 03:22.640
higher order things when I have to create some new object model to model

03:22.640 --> 03:23.360
something, right?

03:23.360 --> 03:26.680
It's easy for me, cause I'm pretty expert as a Python programmer.

03:27.080 --> 03:31.000
I can easily put all sorts of lovely things together and use properties and

03:31.000 --> 03:34.680
decorators and other kinds of things and create something that feels very nice.

03:34.680 --> 03:38.680
So that, that to me, I would say that's tied with the NumPy and

03:38.680 --> 03:40.640
vectorization capabilities.

03:40.640 --> 03:44.040
I love thinking in terms of the matrices and the vectors and these

03:44.040 --> 03:46.080
kind of data structures.

03:46.080 --> 03:49.360
So I would say those two are kind of, uh, tied for me.

03:49.360 --> 03:53.840
So the elegance of the NumPy data structure, like slicing through the

03:53.840 --> 03:54.840
different multidimensionalities.

03:54.840 --> 03:54.920
Yeah.

03:54.920 --> 03:56.200
There's just enough things there.

03:56.200 --> 03:59.360
It's like a very, it's a very simple, comfortable tool.

04:00.000 --> 04:04.280
Just it's easy to reason about what it does when you don't stray too far a field.

04:04.960 --> 04:10.960
Can you put your finger on how to design a language such that it fits in your

04:10.960 --> 04:17.160
head, certain things like the colon or the certain notation aspects of Python.

04:17.160 --> 04:18.360
They just kind of work.

04:18.640 --> 04:23.200
Is it, uh, something you have to kind of write out on paper, look and

04:23.200 --> 04:24.560
say it's just right?

04:24.640 --> 04:27.360
Is it a taste thing or is there a systematic process?

04:27.560 --> 04:28.280
What's your sense?

04:28.800 --> 04:33.680
I think it's more of a taste thing, but one, one thing that should be said is

04:33.680 --> 04:36.360
that you have to pick your audience, right?

04:36.360 --> 04:41.480
So the better defined the user audience is, or the users are the easier it is to

04:41.480 --> 04:46.320
build something that fits in their minds because their needs will be more compact

04:46.320 --> 04:49.160
and coherent, it is possible to find a projection, right?

04:49.160 --> 04:50.480
A compact projection for their needs.

04:50.920 --> 04:54.120
The more diverse the user base, um, the harder that is.

04:54.480 --> 04:59.120
And so as Python has grown in popularity, that's also naturally created more

04:59.120 --> 05:03.120
complexity as people try to design any given thing, there'll be multiple valid

05:03.560 --> 05:05.960
opinions about a particular design approach.

05:06.320 --> 05:10.280
And so I do think that's the, that's the downside of popularity.

05:10.280 --> 05:12.800
It's almost an intrinsic aspect of the complexity of the problem.

05:13.080 --> 05:17.760
Well, at the very beginning, aren't you an audience of one isn't ultimately, aren't

05:17.760 --> 05:19.520
all the greatest projects in history.

05:19.520 --> 05:21.840
We're just solving a problem that you yourself had.

05:21.880 --> 05:26.080
Well, so Clay Shirky in his, um, book on crowdsourcing or in his kind of

05:26.080 --> 05:29.560
thoughts on crowdsourcing, he identifies the first step of crowdsourcing

05:29.560 --> 05:31.120
is me first collaboration.

05:31.480 --> 05:33.760
You first have to make something that works well for yourself.

05:33.800 --> 05:38.080
Yeah, it's very telling that when you look at all of the impactful big

05:38.080 --> 05:41.480
projects, while they're fundamental projects now in the SciPy and Pi data

05:41.480 --> 05:47.040
ecosystem, they all started with the, uh, people in the domain trying to

05:47.040 --> 05:49.840
scratch their own itch and the whole idea of scratching your own itch is

05:49.840 --> 05:52.880
something that the open source or the free software world has known for a long

05:52.880 --> 05:57.560
time, but in the scientific computing areas, you know, these are assistant

05:57.560 --> 06:02.040
professors or electrical engineering grad students, they didn't have really

06:02.040 --> 06:05.720
a lot of programming skill necessarily, but Python was just good enough for

06:05.720 --> 06:09.160
them to put something together that fit in their domain, right?

06:09.440 --> 06:13.920
So it's almost like a, it's a necessity as a mother of invention aspect.

06:13.960 --> 06:19.760
And also it was a really harsh filter for utility and compactness and

06:19.760 --> 06:23.280
expressiveness, like it was too hard to use, then they wouldn't have built

06:23.280 --> 06:24.960
it because there was just too much trouble, right?

06:24.960 --> 06:26.080
It was a side project for them.

06:26.160 --> 06:28.120
And also necessity creates a kind of deadline.

06:28.120 --> 06:30.560
It seems like a lot of these projects are quickly thrown together.

06:31.080 --> 06:36.320
And if in the first step and that, even though it's flawed, that just seems

06:36.320 --> 06:38.680
to work well for software projects.

06:38.840 --> 06:41.280
Well, it does work well for software projects in general.

06:41.280 --> 06:45.000
And in this particular space, um, but one, one of my colleagues, uh, Stan

06:45.000 --> 06:50.240
Sieber identified this, that all the projects in the SciPy ecosystem, um, you

06:50.240 --> 06:53.440
know, if we just rattle them off, there's NumPy, there's SciPy built by

06:53.440 --> 06:54.800
different collaborations of people.

06:55.040 --> 06:56.600
Although Travis is the heart of both of them.

06:57.040 --> 07:00.160
Um, but NumPy coming from Numeric and Numeric, these are different people.

07:00.680 --> 07:04.200
And then you've got Pandas, you've got Jupiter, uh, or IPython.

07:04.480 --> 07:06.720
There's, um, there's Matplotlib.

07:06.880 --> 07:08.040
There's just so many others.

07:08.040 --> 07:11.200
I'm, you know, not going to do justice if I try to name them all, but all of

07:11.200 --> 07:12.320
them are actually different people.

07:12.800 --> 07:17.360
And as they rolled out their projects, the fact that they had limited resources

07:17.560 --> 07:19.360
meant that they were humble about scope.

07:20.160 --> 07:25.200
Um, uh, a great, um, famous hacker, Jamie Zawiski once said that every, every

07:25.200 --> 07:29.280
geek's dream is to build the, uh, the ultimate middleware, right?

07:29.280 --> 07:32.920
And the thing is with these scientists turned programmers, they had no such

07:32.920 --> 07:35.720
team, they were just trying to write something that was a little bit better

07:35.720 --> 07:38.720
for what they needed, the MATLAB, and they were going to leverage what

07:38.720 --> 07:39.560
everyone else had built.

07:39.920 --> 07:43.960
So naturally almost in kind of this annealing process or whatever, we built

07:43.960 --> 07:49.680
a very modular cover of the basic needs of a scientific computing library.

07:50.280 --> 07:53.080
If you look at the whole human story, how much of a leap is it?

07:53.680 --> 07:55.960
We've, uh, developed all kinds of languages, all kinds of

07:55.960 --> 07:57.520
methodologies for communication.

07:57.720 --> 08:01.240
It just kind of like grew this collective intelligence, the civilization

08:01.240 --> 08:06.920
grew, it expanded, wrote a bunch of books and now we tweet, uh, how big

08:06.920 --> 08:11.440
of a leap is programming if programming is yet another language, is it just

08:11.440 --> 08:16.280
a nice little trick that's temporary in our human history, or is it like a

08:16.280 --> 08:23.520
big leap in the, uh, almost us becoming, uh, another organism at a higher

08:23.520 --> 08:25.880
level of abstraction, something else.

08:26.160 --> 08:32.560
I think the act of programming or, uh, using grammatical constructions of

08:32.560 --> 08:37.680
some underlying primitives, that is something that humans do learn, but

08:37.680 --> 08:40.520
every human learns this, anyone who can speak learns how to do this.

08:41.120 --> 08:45.400
What makes programming different has been that up to this point, when we

08:45.400 --> 08:50.600
try to give instructions to computing systems, all of our computers, well,

08:50.640 --> 08:53.640
actually, this is not quite true, but I'll first say it and then I'll tell

08:54.040 --> 08:56.920
you why it's not true, but for the most part, we can think of computers as

08:56.920 --> 08:58.480
being these iterated systems.

08:58.840 --> 09:03.800
So when we program, we're giving very precise instructions to, uh, iterated

09:03.800 --> 09:07.800
systems that then run at, um, incomprehensible speed and run those

09:07.800 --> 09:08.360
instructions.

09:08.800 --> 09:15.000
In my experience, some people are just better equipped to model systematic

09:15.040 --> 09:18.640
iterated systems, well, whatever, iterated systems in their head.

09:20.080 --> 09:22.560
Some people are really good at that and other people are not.

09:23.080 --> 09:27.040
Um, and so when you have like, for instance, sometimes people have tried to

09:27.040 --> 09:30.440
build systems that, uh, make programming easier by making it visual drag and

09:30.440 --> 09:34.280
drop, and the issue is you can have a drag and drop thing, but once you start

09:34.280 --> 09:36.720
having to iterate the system with conditional logic, handling case

09:36.720 --> 09:40.120
statements and branch statements and all of these other things, the visual

09:40.120 --> 09:41.960
drag and drop part doesn't save you anything.

09:42.040 --> 09:45.080
You still have to reason about this giant iterated system with all these

09:45.080 --> 09:46.280
different conditions around it.

09:46.360 --> 09:47.600
That's the hard part, right?

09:48.160 --> 09:52.240
So handling iterated logic, um, that's the hard part.

09:52.240 --> 09:55.400
The languages we use then emerge to give us an ability and

09:55.400 --> 09:56.640
capability over these things.

09:57.240 --> 09:59.960
Now the one exception to this rule, of course, is the most popular programming

09:59.960 --> 10:04.240
system in the world, which is Excel, which is a data flow and a data driven

10:04.440 --> 10:07.360
immediate mode data transformation oriented programming system.

10:07.800 --> 10:11.760
Um, and this actually not an accident that that system is the most popular

10:11.760 --> 10:15.680
programming system because it's so accessible to a much, a much broader

10:15.680 --> 10:21.360
group of people, I do think as, um, we build future computing systems, you

10:21.360 --> 10:22.640
actually already seeing this a little bit.

10:22.920 --> 10:25.240
It's much more about composition of modular blocks.

10:25.680 --> 10:29.880
They themselves, um, actually maintain all their internal state and the

10:29.880 --> 10:32.680
interfaces between them are well-defined data schemas.

10:33.120 --> 10:37.400
And so to stitch these things together using like IFTTT or Zapier or any of

10:37.400 --> 10:41.160
these kinds of, you know, uh, I would say compositional scripting kinds of things.

10:41.560 --> 10:44.200
Um, I mean, hypercard was also a little bit in this vein.

10:44.680 --> 10:47.120
Um, that's much more accessible to most people.

10:47.600 --> 10:51.560
It's, it's really that implicit state that's so hard for people to track.

10:52.000 --> 10:52.200
Yeah.

10:52.200 --> 10:52.360
Okay.

10:52.360 --> 10:54.960
So that's modular stuff, but there's also an aspect where you're standing

10:54.960 --> 10:55.920
on the shoulders of giants.

10:55.920 --> 10:59.560
So you're building like, uh, higher and higher levels of abstraction.

10:59.920 --> 11:02.080
But you, you do that a little bit with language.

11:02.160 --> 11:07.320
So with language, you develop sort of ideas, philosophies from Plato and so on.

11:07.600 --> 11:10.360
And then you kind of leverage those philosophies as you try to have

11:10.840 --> 11:12.280
deeper and deeper conversations.

11:12.600 --> 11:16.280
But with programming, it seems like you can build much more complicated

11:16.280 --> 11:19.520
systems, like without knowing how everything works, you can build

11:19.520 --> 11:20.880
on top of the work of others.

11:21.360 --> 11:27.280
And it seems like you're developing more and more sophisticated, uh, expressions.

11:27.920 --> 11:31.280
Ability to express ideas in a computational space.

11:31.280 --> 11:35.400
I think it's worth pondering the difference here between

11:35.400 --> 11:38.480
complexity and complication.

11:39.560 --> 11:40.800
Uh, okay.

11:40.960 --> 11:41.160
Right.

11:41.160 --> 11:41.800
Back to Excel.

11:42.240 --> 11:46.040
Well, not quite back to Excel, but, but the, the idea is, um, you

11:46.040 --> 11:50.880
know, when we have a human conversation, all languages, uh, for humans emerged

11:51.200 --> 11:56.680
to support, um, human, uh, relational communications, which is that the person

11:56.680 --> 12:00.760
we're communicating with is a person and they would communicate back to us.

12:01.240 --> 12:05.600
And so we sort of, um, hit a resonance point, right?

12:05.600 --> 12:07.440
When we actually agree on some concepts.

12:07.440 --> 12:08.840
So there's a messiness to it.

12:08.840 --> 12:11.400
And there's a fluidity to it with computing systems.

12:11.840 --> 12:14.760
When we express something to the computer and it's wrong, we just try

12:14.760 --> 12:18.320
again, so we can basically live many virtual worlds of having failed at

12:18.320 --> 12:22.360
expressing ourselves to the computer until the one time we expressed ourselves right.

12:22.800 --> 12:25.280
Then we kind of put in production and then discover that it's still wrong,

12:25.480 --> 12:26.720
you know, a few days down the road.

12:27.120 --> 12:32.760
So I think the, the sophistication of things that we build with computing, one

12:32.760 --> 12:36.840
has to really pay attention to the difference between when an end user is

12:36.840 --> 12:41.440
expressing something onto a system that exists versus when they're extending

12:41.440 --> 12:46.320
the system to, to increase the system's capability for someone else to that

12:46.320 --> 12:49.520
interface with, and we happen to use the same language for both of those things

12:49.520 --> 12:53.000
and usually in most cases, but it doesn't have to be that in Excel is actually a

12:53.000 --> 12:55.720
great example of this, uh, of kind of a counterpoint to that.

12:56.200 --> 12:56.520
Okay.

12:56.520 --> 13:04.360
So what about the idea of, you said messiness, wouldn't you put the software

13:04.440 --> 13:12.840
2.0 idea, this idea of machine learning into the further and further steps into

13:12.840 --> 13:17.240
the world of messiness, the same kind of beautiful messiness of human communication.

13:17.560 --> 13:23.720
Isn't that what machine learning is, is a building on levels of abstraction that

13:23.720 --> 13:27.760
don't have messiness in them that, uh, at the operating system level, then there's

13:27.760 --> 13:32.040
Python, the programming languages that have more and more power, but then finally

13:32.440 --> 13:39.080
there's a neural networks that ultimately work with data and so the programming is

13:39.080 --> 13:42.040
almost in the space of data and the data is allowed to be messy.

13:42.400 --> 13:43.840
Isn't that a kind of program?

13:43.840 --> 13:49.640
So the idea of software 2.0 is a lot of the programming happens in the space of

13:49.640 --> 13:55.560
data, so back to Excel, all roads lead back to Excel in the space of data and also

13:55.560 --> 14:01.400
the hyperparameters of the neural networks and all of those allow the same

14:01.400 --> 14:03.520
kind of messiness that human communication allows.

14:04.320 --> 14:07.640
It does, but you know, my background is a physics.

14:07.720 --> 14:11.800
I took like two CS courses in college, so I don't have, um, now I did cram a

14:11.800 --> 14:16.920
bunch of CS, uh, in prep when I applied for grad school, but, um, but still I,

14:16.960 --> 14:20.200
I don't have a formal background in computer science, um, but what I have

14:20.200 --> 14:23.720
observed in studying programming languages and programming systems and,

14:23.720 --> 14:27.280
and things like that is that there seems to be this, this, this triangle.

14:27.280 --> 14:30.840
It's one of these beautiful little iron triangles, uh, in it, that you find in

14:30.840 --> 14:35.800
life sometimes, and it's the connection between the code correctness and kind

14:35.800 --> 14:41.240
of expressiveness of code, the semantics of the data, and then the kind of

14:41.280 --> 14:44.240
correctness or parameters of the underlying hardware compute system.

14:44.840 --> 14:47.560
So there's the algorithms that you want to apply.

14:47.960 --> 14:52.680
Um, there's what the bits that are stored on whatever media actually

14:52.680 --> 14:56.720
represents, the semantics of the data, you know, within the representation.

14:57.000 --> 14:58.520
And then there's what the computer can actually do.

14:59.360 --> 15:03.600
Um, and every programming system, every information system ultimately

15:03.920 --> 15:06.720
finds some spot in the middle of this little triangle.

15:07.400 --> 15:10.920
Sometimes some systems collapse them into just one edge.

15:11.080 --> 15:13.080
Are we, are we including humans as a system?

15:13.080 --> 15:15.320
Um, no, no, I'm just thinking about computing systems here.

15:15.560 --> 15:15.920
Okay.

15:15.920 --> 15:18.680
And the reason I bring this up is because I believe there's no

15:18.680 --> 15:20.040
free lunch around this stuff.

15:20.080 --> 15:24.480
So if we build, if we build machine learning systems to sort of write the

15:24.480 --> 15:28.280
correct code that is at a certain level of performance, so it'll sort of select

15:28.640 --> 15:32.080
right with hyper hyper parameters, we can tune kind of how we want the

15:32.080 --> 15:36.880
performance boundary and isolated to look like for transforming some set of

15:36.880 --> 15:38.720
inputs into certain kinds of outputs.

15:39.560 --> 15:44.240
That training process itself is intrinsically sensitive to the kinds of

15:44.240 --> 15:45.160
inputs we put into it.

15:45.560 --> 15:48.320
It's, and it's quite sensitive to the boundary conditions we put around

15:48.320 --> 15:48.960
the performance.

15:49.320 --> 15:52.720
So I think even as we move to using automated systems to build this

15:52.720 --> 15:56.720
transformation, as opposed to humans explicitly from a top-down perspective,

15:56.920 --> 16:00.080
figuring out, well, this schema and this database and these columns get

16:00.080 --> 16:01.320
selected for this algorithm.

16:01.680 --> 16:04.000
And here we'll put a, you know, a Fibonacci heap for some other thing,

16:04.480 --> 16:06.480
um, human design or computer design.

16:06.760 --> 16:09.680
Ultimately, what we hit the boundaries that we hit with these information

16:09.680 --> 16:14.560
systems is when the representation of the data hits the real world is where

16:14.560 --> 16:16.880
there's a lot of slop and a lot of interpretation.

16:17.520 --> 16:21.000
And that's where actually, I think a lot of the work will go in the future is

16:21.000 --> 16:25.320
actually understanding kind of how to better in the, in the view of these live

16:25.320 --> 16:30.000
data systems, how to better encode the semantics of the world for those things.

16:30.000 --> 16:33.040
So it'll be less about the details of how we write a particular SQL query.

16:33.440 --> 16:33.760
Okay.

16:33.800 --> 16:37.080
But given the semantics of the real world and the messiness of that, what

16:37.080 --> 16:39.880
does the word correctness mean when you're talking about code?

16:40.760 --> 16:43.760
There's a lot of dimensions to correctness historically.

16:43.800 --> 16:46.320
And this is one of the reasons I say that we're coming to the end of the

16:46.320 --> 16:51.120
era of software, because for the last 40 years or so, software correctness was

16:51.120 --> 16:54.520
really defined, uh, about functional correctness.

16:54.800 --> 16:56.320
I read a function, it's got some inputs.

16:56.320 --> 16:57.440
Does it produce the right outputs?

16:57.800 --> 17:01.800
If so, then I can turn it on, hook it up to the live database and it goes in more

17:01.800 --> 17:05.320
and more now we have, I mean, in fact, I think the bright line in the sand between

17:05.320 --> 17:09.560
machine learning systems or modern data driven systems versus software, classical

17:09.560 --> 17:16.480
software systems is that the values of the input actually have to be considered

17:16.480 --> 17:19.080
with the function together to say this whole thing is correct or not.

17:19.440 --> 17:21.520
And usually there's a performance SLA as well.

17:21.760 --> 17:22.880
Like, did it actually finish?

17:22.880 --> 17:23.720
What's SLA?

17:23.760 --> 17:25.200
Sorry, service level agreement.

17:25.360 --> 17:27.040
So it has to return within some time.

17:27.080 --> 17:30.240
You have a 10 millisecond time budget to return a prediction of

17:30.240 --> 17:31.320
this level of accuracy.

17:31.720 --> 17:32.000
Right.

17:32.320 --> 17:35.640
Um, so these are things that were not traditionally in most business

17:35.640 --> 17:38.720
computing systems for the last 20 years at all, people didn't think about it.

17:39.320 --> 17:42.400
But now we have value dependence on functional correctness.

17:42.640 --> 17:45.600
So that, that question of correctness is becoming a bigger and bigger question.

17:45.680 --> 17:47.440
What does that map to the end of software?

17:48.080 --> 17:51.920
We've thought about software as just this thing that you can do in isolation

17:52.080 --> 17:57.880
with some test trial inputs and in a very, you know, um, very sort of sandboxed

17:57.880 --> 18:02.040
environment and we can quantify how does it scale, how does it, you know, perform?

18:02.040 --> 18:03.240
How many nodes do we need to allocate?

18:03.240 --> 18:06.920
If we want to scale this many inputs, when we start turning this stuff into

18:06.920 --> 18:11.560
prediction systems, real cybernetic systems, you're going to find scenarios

18:11.560 --> 18:13.640
where you get inputs that you're going to want to spend a little more time thinking

18:13.640 --> 18:16.840
about, you're gonna find inputs that are not, it's not clear what you should do.

18:16.880 --> 18:17.200
Right.

18:17.440 --> 18:21.120
So then the software has a varying amount of runtime and correctness

18:21.240 --> 18:22.120
with regard to input.

18:22.240 --> 18:23.960
And that is a different kind of system altogether.

18:23.960 --> 18:25.600
Now it's a full on cybernetic system.

18:25.960 --> 18:28.360
It's a next generation information system that is not like

18:28.560 --> 18:29.760
traditional software systems.

18:30.160 --> 18:32.880
Can you maybe describe what is a cybernetic system?

18:33.120 --> 18:35.040
Do you include humans in that picture?

18:35.040 --> 18:39.880
So is it, is it human in the loop kind of complex mess of the whole kind of

18:40.160 --> 18:44.480
interactivity of software with the real world or, or is it something more concrete?

18:44.720 --> 18:48.360
Well, when I say cybernetic, I really do mean that the software itself is closing

18:48.360 --> 18:51.160
the observe, orient, decide, act loop by itself.

18:51.560 --> 18:56.840
So humans being out of the loop is, is the fact of what for me, uh, makes it

18:57.320 --> 19:00.640
a cybernetic system and humans are out of that loop.

19:00.880 --> 19:04.240
When humans are out of the loop, when the machine is actually sort of deciding on

19:04.240 --> 19:09.080
its own, what it should do next to get more information that makes it a cybernetic

19:09.080 --> 19:09.480
system.

19:09.680 --> 19:11.320
So we're just at the dawn of this, right?

19:11.360 --> 19:15.920
I think everyone talking about MLAI it's, it's, it's, it's great, but really

19:15.920 --> 19:19.840
the thing we should be talking about is when we really enter the cybernetic era

19:20.360 --> 19:23.880
and all of the questions of ethics and governance and correctness and all these

19:23.880 --> 19:26.840
things, they really are the most important questions.

19:27.160 --> 19:27.480
Okay.

19:27.520 --> 19:28.560
Can we just linger on this?

19:28.560 --> 19:31.760
What does it mean for the human to be out of the loop in a cybernetic system?

19:31.760 --> 19:36.720
Because isn't the cybernetic system that's ultimately accomplished in some

19:36.720 --> 19:41.480
kind of purpose that at the, at the bottom, you know, the turtles all the way

19:41.480 --> 19:43.320
down at the bottom turtles, a human.

19:44.160 --> 19:47.200
Well, the human may have set some criteria, but the human wasn't precise.

19:47.240 --> 19:51.560
So for instance, I just read the other day that earlier this year, or maybe it

19:51.560 --> 19:56.080
was last year at some point, the Libyan army, I think, sent out some automated

19:56.080 --> 19:59.920
killer drones with explosives and there was no human in the loop at that point.

19:59.920 --> 20:03.520
They basically put them in a geofenced area, said, find any moving target, like

20:03.520 --> 20:08.080
a truck or vehicle that looks like this and boom, that's not a human in the loop.

20:08.240 --> 20:08.520
Right.

20:09.240 --> 20:13.800
So increasingly the less human there is in the loop, the more concerned you are

20:13.800 --> 20:18.480
about these kinds of systems because there's unintended consequences, like

20:18.480 --> 20:24.000
less the original designer and engineer of the system is able to predict even

20:24.000 --> 20:27.280
one with good intent is able to predict the consequences of such a system.

20:27.280 --> 20:27.640
Is that

20:27.680 --> 20:28.320
That's right.

20:28.520 --> 20:30.080
There are some software systems, right?

20:30.080 --> 20:32.680
That run without humans in the loop that are quite complex.

20:32.680 --> 20:35.840
And that's like the electronic markets and we get flash crashes all the time.

20:35.840 --> 20:40.600
We get, you know, in the, in the heyday of high frequency trading, there's a

20:40.600 --> 20:44.240
lot of market microstructure of people doing all sorts of weird stuff that the

20:44.280 --> 20:48.320
market designers had never really thought about contemplated or intended.

20:48.760 --> 20:53.360
So when we run these full on systems with these automated trading bots, now

20:53.360 --> 20:56.800
they become automated, you know, killer drones and then all sorts of other stuff.

20:57.200 --> 21:00.720
We, we are, that's what I mean by we're at the dawn of the cybernetic era and

21:00.720 --> 21:02.560
the end of the era of just pure software.

21:03.520 --> 21:08.320
Are you more concerned if you're thinking about cybernetic systems or even

21:08.320 --> 21:12.800
like self replicating systems, so systems that aren't just doing a particular

21:12.800 --> 21:17.240
task, but are able to sort of multiply and scale in some dimension in the

21:17.240 --> 21:19.760
digital or even the physical world.

21:20.040 --> 21:24.440
Are you more concerned about, uh, like the lobster being boiled?

21:24.480 --> 21:32.040
So a gradual with us not noticing, um, collapse of civilization or a big

21:32.320 --> 21:39.000
explosion, uh, like, oops, kind of a big thing where everyone notices, but it's

21:39.000 --> 21:44.840
too late, I think that it will be a different experience for different people.

21:45.920 --> 21:51.560
Um, I do, I do, um, share a common point of view with some of the climate, um,

21:52.000 --> 21:55.760
you know, people who are concerned about climate change and, and just the, uh,

21:55.760 --> 22:00.560
this, uh, the, the, the big existential risks that we have, but unlike a lot of

22:00.560 --> 22:04.000
people who are, who share my level of concern, I think the collapse will not

22:04.000 --> 22:07.520
be quite so dramatic as some of them think.

22:07.560 --> 22:11.840
And what I mean is that I think that for certain tiers of, let's say economic

22:11.840 --> 22:16.960
class or certain locations in the world, people will experience dramatic collapse

22:16.960 --> 22:20.840
scenarios, but for a lot of people, especially in the developed world, the,

22:20.880 --> 22:24.000
um, realities of collapse will be managed.

22:24.040 --> 22:28.560
There will be narrative management around it so that they essentially

22:28.560 --> 22:32.520
insulate the middle-class will be used to insulate the upper-class from the

22:32.520 --> 22:35.880
pitchforks and the, and the, um, flaming torches and everything.

22:35.880 --> 22:39.560
It's interesting because, uh, so my specific question wasn't as general as

22:39.600 --> 22:42.360
my question was more about cybernetic systems or software.

22:42.520 --> 22:42.920
Okay.

22:43.040 --> 22:46.720
Uh, it's interesting, but it would nevertheless perhaps be about class.

22:46.720 --> 22:50.200
So the effect of algorithms might affect certain classes more than others.

22:50.240 --> 22:50.880
Absolutely.

22:51.040 --> 22:56.000
I was more thinking about whether it's social media algorithms or actual robots.

22:57.000 --> 23:02.800
Is there going to be a gradual effect on us where we wake up one day and don't

23:02.800 --> 23:07.800
recognize the humans we are, or, or is it something truly dramatic where there's,

23:07.840 --> 23:12.680
you know, like, uh, a meltdown of a nuclear reactor kind of thing, Chernobyl,

23:12.720 --> 23:19.080
like a catastrophic events that, um, are almost bugs in a program that

23:19.080 --> 23:20.640
scaled itself too quickly.

23:20.840 --> 23:25.480
Yeah, I'm not as concerned about the visible stuff.

23:26.240 --> 23:29.840
And the reason is because the big visible explosions, I mean, this is

23:29.840 --> 23:32.600
something I said about social media is that, you know, at least with nuclear

23:32.600 --> 23:35.600
weapons, when a nuke goes off, you can see it and they're like, well, that's

23:35.600 --> 23:37.520
really, wow, that's kind of bad.

23:37.520 --> 23:37.720
Right.

23:37.720 --> 23:40.560
I mean, uh, Oppenheimer was reciting the Bhagavad Gita, right.

23:40.560 --> 23:41.760
When he saw one of those things go off.

23:42.160 --> 23:44.680
So we can see nukes are really bad.

23:45.120 --> 23:47.000
He's not reciting anything about Twitter.

23:47.920 --> 23:49.280
Well, but right.

23:49.280 --> 23:52.840
But then when, when you have social media, when you have, um, uh, all these

23:52.880 --> 23:57.120
different things that conspire to create a layer of virtual experience for people

23:57.120 --> 24:02.400
that alienates them from reality and from each other, that's very pernicious.

24:02.400 --> 24:03.520
That's impossible to see.

24:03.560 --> 24:03.840
Right.

24:03.840 --> 24:05.840
And it slowly gets in there.

24:06.240 --> 24:11.120
So you've written about this idea of virtuality on this topic, which you

24:11.120 --> 24:16.280
define as the subjective phenomenon of knowingly engaging with virtual

24:16.280 --> 24:20.760
sensation and perception and suspending or forgetting the context that it's, uh,

24:20.800 --> 24:21.480
simulacrum.

24:22.000 --> 24:25.800
So let me ask, uh, what is real?

24:26.440 --> 24:30.200
Is there a hard line between reality and virtuality?

24:30.400 --> 24:33.160
Like perception drifts from some kind of physical reality.

24:33.440 --> 24:37.560
We have to kind of have a sense of what is the line that's to we've gone too far.

24:37.600 --> 24:38.080
Right.

24:38.080 --> 24:38.360
Right.

24:38.720 --> 24:44.640
For me, it's not about any hard line about physical reality as much as, um, a

24:44.640 --> 24:51.880
simple question of, um, does the particular technology help people connect in a more

24:51.880 --> 24:57.000
integral way with other people, with their environment, with all of the full

24:57.000 --> 24:58.320
spectrum of things around them.

24:58.520 --> 25:02.640
So it's less about, oh, this is a virtual thing and this is a hard, real thing.

25:02.960 --> 25:06.520
More about when we create virtual representations of the real things.

25:07.000 --> 25:10.440
Um, always some things are lost in translation.

25:10.720 --> 25:14.080
Usually many, many dimensions are lost in translation, right?

25:14.280 --> 25:17.840
We're now coming to almost two years of COVID people on zoom all the time.

25:17.880 --> 25:20.320
You know, it's different when you meet somebody in person than when you see them

25:20.320 --> 25:21.880
on, I've seen you on YouTube lots, right?

25:22.280 --> 25:23.800
Um, but the senior person is very different.

25:24.160 --> 25:30.920
And so I think when we engage in virtual experiences all the time and we only do

25:30.920 --> 25:34.200
that, there is absolutely a level of embodiment.

25:34.240 --> 25:39.320
There's a level of embodied experience and participatory interaction that is lost.

25:40.000 --> 25:42.400
Um, and it's very hard to put your finger on exactly what it is.

25:42.440 --> 25:44.960
It's hard to say, oh, we're going to spend a hundred million dollars building a new

25:44.960 --> 25:51.160
system that captures this five, five, 5% better, higher fidelity, human expression.

25:51.160 --> 25:51.880
No one's going to pay for that.

25:51.880 --> 25:52.160
Right.

25:52.480 --> 26:00.320
So when we rush madly into a world of simulacrum and, and virtuality, um, you

26:00.320 --> 26:05.320
know, the things that are lost are, it's difficult once everyone moves there, it's,

26:05.360 --> 26:07.960
it can be hard to look back and see what we've, what we've lost.

26:08.040 --> 26:14.400
So is it irrecoverably lost or rather when you put it all on the table, is it

26:14.400 --> 26:16.800
possible for more to be gained than is lost?

26:17.080 --> 26:22.960
If you look at video games, they create virtual experiences that are surreal and

26:22.960 --> 26:27.320
can bring joy to a lot of people, can connect a lot of people, uh, and can get

26:27.320 --> 26:31.880
people to talk a lot of trash, uh, so they can bring out the best and the worst in

26:31.880 --> 26:32.280
people.

26:32.520 --> 26:37.760
So is it possible to have a future world where the pros outweigh the cons?

26:38.520 --> 26:38.960
It is.

26:38.960 --> 26:43.440
I mean, it's possible to have that in the, in the current world, but, um, when

26:44.680 --> 26:49.400
literally trillions of dollars of capital are tied to using those things to

26:49.880 --> 26:54.960
groom the worst of our inclinations and to attack our weaknesses, uh, in the

26:54.960 --> 26:58.680
limbic system to create these things into id machines versus connection machines,

26:59.080 --> 27:02.840
then, um, then the, the, those good things don't stand a chance.

27:03.120 --> 27:05.840
Can you make a lot of money by building connection machines?

27:06.120 --> 27:07.600
Is it possible?

27:07.600 --> 27:12.360
Do you think to bring out the best in human nature, to, uh, create fulfilling

27:12.360 --> 27:16.600
connections and relationships in the digital world and make a shit ton of

27:16.600 --> 27:16.880
money?

27:18.000 --> 27:20.160
Um, if I figured out, I'll let you know,

27:21.120 --> 27:25.960
what's your intuition without concretely knowing what's the solution is that a

27:25.960 --> 27:30.800
lot of our digital technologies give us the ability to have synthetic connections

27:30.800 --> 27:32.400
or to experience virtuality.

27:33.000 --> 27:38.880
They have co-evolved with sort of the human expectations.

27:38.880 --> 27:40.440
It's sort of like sugary drinks.

27:40.800 --> 27:44.160
As people have more sugary drinks, they get, they need more sugary drinks to

27:44.160 --> 27:45.560
get that same hit, right?

27:45.800 --> 27:50.880
So with these virtual things, um, and with TV, uh, and fast cuts and, you

27:50.880 --> 27:53.880
know, TikToks and all these different kinds of things, we're co-creating

27:54.080 --> 27:57.240
essentially humanity that sort of asks and needs those things.

27:57.280 --> 28:00.320
And now it becomes very difficult to get people to slow down.

28:00.320 --> 28:05.040
It gets difficult for people to hold their attention on, on slow things and

28:05.040 --> 28:07.320
actually feel that embodied experience, right?

28:07.320 --> 28:12.080
So mindfulness now more than ever is so important in schools and, um, as a

28:12.080 --> 28:15.640
therapy technique for people, because our environment has been accelerated.

28:15.640 --> 28:19.000
And, and McLuhan actually talks about this in the electric environment of the

28:19.000 --> 28:22.160
television, and that was before TikTok and before front facing cameras.

28:22.400 --> 28:27.160
So I think for me, the, the concern is that it's not like we can ever switch to

28:27.160 --> 28:31.520
doing something better, but more of the humans and technology.

28:32.040 --> 28:33.360
They're not independent of each other.

28:33.560 --> 28:37.960
The technology that we use kind of molds what we need for the

28:37.960 --> 28:39.080
next generation of technology.

28:39.080 --> 28:39.240
Yeah.

28:39.240 --> 28:44.280
But humans are intelligent and they're, uh, introspective and they can reflect

28:44.280 --> 28:45.760
on the experiences of their life.

28:45.800 --> 28:49.200
So for example, there's been many years in my life where I ate an

28:49.200 --> 28:53.480
excessive amount of sugar, and then a certain moment I woke up and said,

28:53.520 --> 28:55.880
uh, why do I keep doing this?

28:55.880 --> 28:56.880
This doesn't feel good.

28:57.640 --> 28:58.840
I like long-term.

28:59.000 --> 29:03.960
And I think, uh, so going through the TikTok process of realizing, okay,

29:03.960 --> 29:08.800
when I shorten my attention span, actually that does not make me feel good

29:08.840 --> 29:14.240
longer term and realizing that, and then going to platforms, going to places

29:14.240 --> 29:17.880
that, um, are away from the sugar.

29:18.240 --> 29:21.720
So, so in, in, in so doing, you can create platforms that can make a lot of

29:21.720 --> 29:25.400
money when, so to help people wake up to what actually makes them feel good.

29:25.400 --> 29:27.920
Long-term develop, grow as human beings.

29:28.280 --> 29:33.760
And it just feels like humans are more intelligent than, uh, mice looking for

29:33.760 --> 29:36.720
cheese, they're able to sort of think.

29:36.720 --> 29:41.160
I mean, we can think we can contemplate our mortality and contemplate things

29:41.160 --> 29:46.840
like long-term love, and we can have a long-term fear of certain things like

29:46.840 --> 29:52.800
mortality, we can contemplate whether the experiences, the sort of the drugs of

29:52.800 --> 29:58.120
daily life that we've been partaking in is making us happier, better people.

29:58.360 --> 30:03.240
And then once we contemplate that, we can make financial decisions in using

30:03.240 --> 30:06.520
services and paying for services that are making us better people.

30:06.880 --> 30:14.080
So it just seems that we're in the very first stages of social networks that

30:14.080 --> 30:18.600
just were able to make a lot of money really quickly, but in bringing out

30:18.600 --> 30:23.160
sometimes the bad parts of human nature, they didn't destroy humans.

30:23.160 --> 30:26.880
They just, they just fed everybody a lot of sugar and now everyone's going to

30:26.880 --> 30:31.680
wake up and say, Hey, we're going to start having like sugar-free social media.

30:32.800 --> 30:33.240
Right.

30:33.240 --> 30:34.800
Well, there's a lot to unpack there.

30:34.800 --> 30:37.240
I think some people certainly have the capacity for that.

30:37.520 --> 30:39.640
And I certainly think, I mean, it's very interesting.

30:39.640 --> 30:42.400
Even the way you said it, you woke up one day and you thought, well, this

30:42.400 --> 30:43.320
doesn't feel very good.

30:44.120 --> 30:46.600
Well, that's still your limbic system saying this doesn't feel very good.

30:47.240 --> 30:47.480
Right.

30:47.480 --> 30:49.920
You have a cat brain's worth of neurons around your gut.

30:50.040 --> 30:50.400
Right.

30:50.720 --> 30:54.520
And so maybe that saturated and that was telling you, Hey, this isn't good.

30:55.040 --> 30:58.760
Humans are more than just mice looking for cheese or monkeys

30:58.760 --> 30:59.920
looking for sex and power.

31:00.240 --> 31:00.560
Right.

31:00.800 --> 31:02.280
So let's slow down.

31:02.640 --> 31:05.960
Now you're, um, now a lot of people would argue with you on that one.

31:05.960 --> 31:09.680
But we're more than just that, but we're at least that, and we're very,

31:09.680 --> 31:11.160
very seldom not that.

31:11.800 --> 31:16.200
So, um, my, I don't actually disagree with you that we could be better and

31:16.200 --> 31:19.800
that we can, that better platforms exist and people are voluntarily noping

31:19.800 --> 31:21.480
out of things like Facebook and noping out.

31:21.720 --> 31:22.720
That's an awesome verb.

31:22.760 --> 31:23.680
It's a great term.

31:23.680 --> 31:23.880
Yeah.

31:23.880 --> 31:24.240
I love it.

31:24.240 --> 31:24.920
I use it all the time.

31:25.720 --> 31:26.240
You're welcome.

31:27.240 --> 31:28.280
I want to nope out of that.

31:28.280 --> 31:28.600
Right.

31:28.600 --> 31:29.720
It's going to be a hard pass.

31:29.760 --> 31:32.520
And, and that's, and that's, that's great.

31:32.840 --> 31:36.720
But that's again, to your point, that's the first generation of front-facing

31:36.720 --> 31:42.440
cameras of social pressures and you as a, you know, self-starter, self-aware

31:42.600 --> 31:46.360
adult have the capacity to say, yeah, I'm not going to do that.

31:46.360 --> 31:48.480
I'm going to go and spend time on long form reads.

31:48.480 --> 31:50.280
I'm going to spend time managing my attention.

31:50.280 --> 31:51.400
I'm going to do some yoga.

31:52.080 --> 31:57.240
If you're a 15 year old in high school and your entire social environment is

31:57.240 --> 31:58.960
everyone doing these things, guess what you're going to do?

31:59.520 --> 32:01.760
You're going to kind of have to do that because your limbic system says,

32:01.760 --> 32:04.240
Hey, I need to get the guy or the girl or the whatever.

32:04.520 --> 32:05.320
And that's what I'm going to do.

32:05.640 --> 32:09.000
And so one of the things that we have to reason out here is the social media

32:09.000 --> 32:14.800
systems or, you know, social media, I think is a first, our first encounter

32:14.800 --> 32:22.240
with a technological system that runs a bit of a loop around our

32:22.240 --> 32:23.640
own cognition and attention.

32:24.120 --> 32:27.520
It's not the last it's, it's far from the last.

32:28.080 --> 32:32.720
And it gets to the heart of some of the philosophical Achilles heel of the

32:32.720 --> 32:35.680
Western philosophical system, which is each person gets to make their own

32:35.720 --> 32:36.400
determination.

32:36.760 --> 32:40.600
Each person is an individual that's, you know, sacrosanct and their agency and

32:40.600 --> 32:41.760
their sovereignty and all these things.

32:42.360 --> 32:45.800
The problem with these systems is they come down and they are able to

32:45.800 --> 32:47.400
manage everyone on mass.

32:47.920 --> 32:52.440
And so every person is making their own decision, but together the bigger system

32:52.440 --> 32:58.080
is causing them to act with a group dynamic that's very profitable for people.

32:58.600 --> 33:03.520
So this is the issue that we have is that our philosophies are actually not geared

33:03.520 --> 33:10.440
to understand what is it for a person to be, to have a high trust connection as

33:10.440 --> 33:14.560
part of a collective and for that collective to have its right to coherency

33:14.560 --> 33:20.160
and agency, that's something like when, when a social media app causes a family

33:20.160 --> 33:24.520
to break apart, it's done harm to more than just individuals, right?

33:24.520 --> 33:27.760
So that concept is not something we really talk about or think about very

33:27.760 --> 33:31.880
much, but that's actually the problem is that we're, we're vaporizing molecules

33:31.880 --> 33:35.000
into atomic units, and then we're hitting all the atoms with certain things.

33:35.000 --> 33:37.640
That's like, yeah, well, that person chose to look at my app.

33:38.200 --> 33:43.280
So our understanding of human nature is at the individual level, it emphasizes the

33:43.280 --> 33:47.320
individual too much because ultimately society operates at the collective level.

33:47.400 --> 33:48.520
And these apps do as well.

33:48.800 --> 33:49.800
And the apps do as well.

33:49.800 --> 33:55.000
So for us to understand the progression and development of this organism, we call

33:55.000 --> 33:57.720
human civilization, we have to think of the collective level too.

33:57.960 --> 33:59.000
I would say multi-tiered.

33:59.200 --> 33:59.880
Multi-tiered.

33:59.920 --> 34:00.200
Multi-tiered.

34:00.200 --> 34:01.520
So individual as well.

34:01.560 --> 34:06.160
Individuals, family units, social collectives, and, and all the way up.

34:06.320 --> 34:06.560
Okay.

34:07.080 --> 34:11.600
So you've said that individual humans are multi-layered, susceptible to signals

34:11.600 --> 34:16.080
and waves and multiple strata, the physical, the biological, social, cultural,

34:16.080 --> 34:22.360
intellectual, so sort of going along these lines, can you describe the layers of the

34:22.360 --> 34:28.640
cake that, that is a human being and maybe the human collective, human society?

34:29.160 --> 34:33.840
So I'm just stealing wholesale here from Robert Persig, who is the author of Zen

34:33.840 --> 34:39.960
and the Art of Motorcycle Maintenance and his follow on book has a sequel to it

34:39.960 --> 34:44.240
called Lila, he goes into this in a little more detail, but it's, it's a, it's

34:44.240 --> 34:49.040
a crude approach to thinking about people, but I think it's still an advancement over

34:49.040 --> 34:53.640
traditional subject object metaphysics, where we look at people as a dualist

34:53.640 --> 34:58.760
would say, well, is your mind, you know, your consciousness, is that, is that just

34:58.760 --> 35:03.200
merely the matter that's in your brain or is there something kind of more beyond

35:03.200 --> 35:07.560
that? And they would say, yes, there's a soul, sort of ineffable soul beyond just

35:07.560 --> 35:09.000
merely the physical body, right?

35:09.320 --> 35:11.320
And then, and I'm not one of those people, right?

35:11.320 --> 35:16.560
I think that we don't have to draw a line between are things only this or only

35:16.560 --> 35:20.480
that, collectives of things can emerge structures and patterns that are just as

35:20.480 --> 35:24.400
real as the underlying pieces, but, you know, they're transcendent, but they're

35:24.400 --> 35:26.000
still of the underlying pieces.

35:26.480 --> 35:28.360
So your body is this way.

35:28.360 --> 35:32.760
I mean, we just know physically you're, you consist of atoms and, and, and whatnot.

35:32.760 --> 35:36.360
And then the atoms are arranged into molecules, which then arrange into certain

35:36.360 --> 35:40.360
kinds of structures that seem to have a homeostasis to them, we'll call them cells.

35:40.640 --> 35:45.200
And those cells form, you know, sort of biological structures, those biological

35:45.200 --> 35:49.040
structures give your body its physical ability and the biological ability to

35:49.040 --> 35:51.320
consume energy and to maintain homeostasis.

35:51.720 --> 35:53.920
But humans are social animals.

35:53.920 --> 35:56.720
I mean, human by themselves is not very long for the world.

35:57.120 --> 36:02.160
So we also part of our biology is wired to connect to other people, to, you know,

36:02.160 --> 36:05.680
from the mirror neurons to our language centers and all these other things.

36:06.160 --> 36:11.160
So we are intrinsically, there's a layer, there's a part of us that wants to be

36:11.160 --> 36:12.520
part of a thing.

36:12.520 --> 36:15.560
If we're around other people not saying a word, but they're just up and down

36:15.560 --> 36:17.800
jumping and dancing, laughing, we're going to feel better.

36:18.200 --> 36:18.520
Right.

36:18.520 --> 36:21.720
And they didn't, there was no exchange of physical anything.

36:21.720 --> 36:24.040
They didn't give us like five atoms of happiness.

36:24.240 --> 36:24.560
Right.

36:24.800 --> 36:29.080
But there's an induction in our own sense of self that is at that social level.

36:29.640 --> 36:34.320
And then beyond that, Pirsic puts the intellectual level kind of one level

36:34.320 --> 36:35.040
higher than social.

36:35.040 --> 36:38.200
I think they're actually more intertwined than, than that, but the intellectual

36:38.200 --> 36:42.760
level is the level of pure ideas that you are a vessel for memes.

36:42.760 --> 36:44.440
You're a vessel for philosophies.

36:44.960 --> 36:47.200
You will conduct yourself in a particular way.

36:47.640 --> 36:49.920
I mean, I think part of this is if we think about it from a physics

36:49.920 --> 36:53.760
perspective, you're not, you know, there's the joke that physicists like to

36:53.800 --> 36:57.400
approximate things and we'll say, well, approximate a spherical cow, right?

36:57.400 --> 36:58.240
You're not a spherical cow.

36:58.240 --> 36:59.120
You're not a spherical human.

36:59.480 --> 37:00.400
You're a messy human.

37:00.640 --> 37:05.120
And we can't even say what the dynamics of your emotion will be unless we

37:05.120 --> 37:07.840
analyze all four of these layers, right?

37:08.400 --> 37:11.600
If it's, if you're, if you're Muslim at a certain time of day, guess what?

37:11.600 --> 37:13.880
You're going to be on the ground kneeling and praying, right?

37:13.880 --> 37:16.240
And that has nothing to do with your biological need to get on the

37:16.240 --> 37:17.880
ground or physics of gravity.

37:18.400 --> 37:20.320
It is an intellectual drive that you have.

37:20.320 --> 37:23.360
It's a cultural phenomenon and an intellectual belief that you carry.

37:23.640 --> 37:28.000
So that's what the four layered stack, um, is, is all about.

37:28.000 --> 37:30.240
It's that a person is not only one of these things.

37:30.240 --> 37:31.640
They're all of these things at the same time.

37:31.640 --> 37:36.680
It's a superposition of dynamics that run through us that make us who we are.

37:37.200 --> 37:39.240
So no layers is special.

37:39.920 --> 37:40.920
Um, not so much.

37:40.920 --> 37:41.600
No layer is special.

37:41.600 --> 37:42.800
Each layer is just different.

37:43.520 --> 37:44.920
Um, but we are.

37:45.160 --> 37:47.520
But each layer gets the participation trophy.

37:48.040 --> 37:48.280
Yeah.

37:48.280 --> 37:50.080
Each layer is a part of what you are.

37:50.080 --> 37:51.240
You are a layer cake, right?

37:51.240 --> 37:51.840
Of all these things.

37:51.840 --> 37:57.240
And if we try to deny, right, so many philosophies do try to deny the reality

37:57.240 --> 37:58.680
of some of these things, right?

37:58.680 --> 38:00.800
Some people will say, well, we're only atoms.

38:01.160 --> 38:03.840
Well, we're not only atoms because there's a lot of other things that are only atoms.

38:03.840 --> 38:08.040
I can reduce a human being to a bunch of soup and it's not, they're not the same

38:08.040 --> 38:09.200
thing, even though it's the same atoms.

38:09.560 --> 38:14.400
So I think the, the order and the patterns that emerge within humans, um, to

38:14.400 --> 38:18.520
to understand, to really think about what a next generation of philosophy would

38:18.520 --> 38:22.200
look like that would allow us to reason about extending humans into the digital

38:22.200 --> 38:26.680
realm or to interact with autonomous intelligences that are not biological

38:26.680 --> 38:30.840
nature, we really need to appreciate these, that human, what human beings

38:30.840 --> 38:34.160
actually are is the superposition of these different layers.

38:34.720 --> 38:36.040
You mentioned consciousness.

38:36.560 --> 38:39.320
Are each of these layers of cake conscious?

38:39.760 --> 38:43.360
Is consciousness a particular quality of one of the layers?

38:43.680 --> 38:44.920
Is there like a spike?

38:44.960 --> 38:48.800
If you have a consciousness detector at these layers, or is something that just

38:48.800 --> 38:51.520
permeates all of these layers and just takes different form?

38:51.880 --> 38:55.560
I believe what humans experience as consciousness is something that sits on

38:55.560 --> 39:03.000
a gradient scale of a general principle in the universe that seems to look for

39:03.080 --> 39:07.360
order and reach for order when there's an excess of energy, you know, it's, it's,

39:07.400 --> 39:09.360
it would be odd to say a proton is alive, right?

39:09.360 --> 39:14.640
It'd be odd to say like this particular atom or molecule of, of hydrogen gas is

39:14.640 --> 39:21.320
alive, but there's certainly something we can make assemblages of these things

39:21.360 --> 39:25.040
that, that ought, that have autopoetic aspects to them that will create

39:25.040 --> 39:28.280
structures that will, you know, crystalline solids will form very

39:28.280 --> 39:29.600
interesting and beautiful structures.

39:29.640 --> 39:32.960
Um, this gets kind of into weird mathematical territories.

39:32.960 --> 39:36.120
You start thinking about Penrose and game of life stuff about the

39:36.280 --> 39:39.400
generativity and math itself, like the hyperreal numbers, things like that.

39:39.440 --> 39:43.480
But, um, without going down that rabbit hole, I would say that there seems to

39:43.480 --> 39:49.680
be a tendency in the world that when there is excess energy, things will

39:49.680 --> 39:52.480
structure and pattern themselves and they will then actually furthermore

39:52.480 --> 39:57.200
try to create an environment that furthers their continued stability.

39:57.760 --> 40:01.720
Uh, it's the concept of, uh, externalized extended phenotype or niche construction.

40:02.200 --> 40:08.200
So, um, this is ultimately what leads to certain kinds of amino acids forming

40:08.200 --> 40:10.760
certain kinds of structures and so on and so forth until you get the ladder of life.

40:11.080 --> 40:14.520
So what we experience as consciousness, no, I don't think cells are conscious

40:14.520 --> 40:19.520
of that level, but is there something beyond mere equilibrium state biology

40:19.520 --> 40:24.880
and, and chemistry and biochemistry that drives what makes things work?

40:25.400 --> 40:26.240
I think there is.

40:26.600 --> 40:29.520
Um, uh, so Adrian Bazhan has this constructal law.

40:29.560 --> 40:32.760
There's other things you look at when you look at the life sciences and you look

40:32.760 --> 40:37.640
at, um, any kind of, uh, statistical physics and statistical mechanics, when

40:37.640 --> 40:42.040
you look at things far out of equilibrium, uh, when you have excess energy,

40:42.120 --> 40:46.840
what happens then life doesn't just make a hot soup, it starts making structure.

40:47.360 --> 40:48.360
There's something there.

40:48.640 --> 40:54.520
The poetry of reaches for order when there's an excess of energy, because

40:54.520 --> 40:55.840
you brought up game of life.

40:57.120 --> 40:57.800
You did it.

40:57.880 --> 41:00.360
Not me, my, I love cellular automata.

41:00.360 --> 41:04.280
So I have to sort of linger on that for a little bit.

41:06.400 --> 41:10.760
So cellular automata, I guess is a, or game of life is a very simple example

41:10.760 --> 41:15.200
of reaching for order when there's an excess of energy or reaching for order

41:15.200 --> 41:22.080
and somehow creating complexity within like this explosion of just turmoil,

41:22.400 --> 41:25.320
somehow trying to construct structures.

41:25.480 --> 41:31.320
And so doing, uh, create very elaborate organism looking type things.

41:32.400 --> 41:35.440
What intuition do you draw from the simple mechanism?

41:35.760 --> 41:40.560
Well, I like to turn that around in its head and, um, and look at it as what

41:40.560 --> 41:45.520
if every single one of the patterns created life or created, you know, not

41:45.520 --> 41:47.320
life, but created interesting patterns.

41:47.320 --> 41:48.320
Cause you know, some of them don't.

41:48.640 --> 41:51.640
And sometimes you make cool gliders and other times, you know, you start

41:51.640 --> 41:54.880
with certain things and you make gliders and other things that then construct

41:54.880 --> 41:56.680
like, you know, and gates and not gates.

41:56.680 --> 41:56.960
Right.

41:56.960 --> 41:57.920
And you build computers on them.

41:58.400 --> 42:02.520
Um, all of these rules that create these patterns that we can see, those are

42:02.520 --> 42:06.640
just the patterns we can see what if our subjectivity is actually limiting our

42:06.640 --> 42:11.360
ability to perceive the order and all of it, you know, what are some of the

42:11.360 --> 42:13.680
things that we think are random or actually not that random, we're simply

42:13.680 --> 42:17.760
not integrating at a final level across a broad enough time horizon.

42:18.440 --> 42:21.000
Um, and this is again, I said, we go down the rabbit holes and the pen row

42:21.000 --> 42:23.400
stuff or like Wolfram's explorations on these things.

42:23.920 --> 42:28.520
Um, there is something deep and beautiful in the mathematics of all this.

42:28.560 --> 42:31.160
That is hopefully one day I'll have enough money to work and retire and

42:31.160 --> 42:33.400
just ponder those, those questions.

42:33.400 --> 42:34.400
But there's something there.

42:34.480 --> 42:37.320
But you're saying there's a ceiling to when you have enough money and you

42:37.320 --> 42:40.680
retire and you ponder it, there's a ceiling to how much you can truly ponder

42:40.680 --> 42:45.840
because there's cognitive limitations in what you're able to perceive as a pattern.

42:46.280 --> 42:46.680
Yeah.

42:46.720 --> 42:51.680
So, and maybe mathematics extends your perception capabilities, but

42:51.680 --> 42:53.360
it's still, it's still finite.

42:53.800 --> 42:57.600
It's just like, yeah, the, the mathematics we use is the mathematics

42:57.600 --> 42:58.400
that can fit in our head.

42:58.960 --> 42:59.280
Yeah.

43:00.600 --> 43:03.720
You know, did God really create the integers or did God create all of it?

43:03.720 --> 43:06.440
And we just happen at this point in time to be able to perceive integers.

43:07.160 --> 43:09.000
Well, he just did the positive energy.

43:11.280 --> 43:15.920
And then we, um, she, she, she just created the natural numbers and then

43:15.920 --> 43:19.440
we screwed all up with zero and then I guess, okay, but we did, we created

43:19.440 --> 43:24.440
mathematical, uh, operations so that we can have iterated steps to approach

43:24.440 --> 43:25.960
bigger problems, right?

43:26.000 --> 43:29.560
I mean, the entire, the entire point of the Arabic numeral system, and it's

43:29.560 --> 43:33.440
a rubric for mapping a certain set of operations of folding them into a

43:33.440 --> 43:38.120
simple little expression, but that's just the operations that we can fit in our heads.

43:38.720 --> 43:40.480
There are many other operations besides, right?

43:41.120 --> 43:46.400
The thing that worries me the most about aliens and humans is that

43:46.720 --> 43:51.600
they're aliens are all around us and we're too dumb to see them.

43:51.680 --> 43:52.280
Oh, certainly.

43:52.320 --> 43:52.600
Yeah.

43:52.840 --> 43:57.600
Or life, let's say just life, life of all kinds of forms or organisms.

43:58.080 --> 43:58.520
You know what?

43:58.520 --> 44:04.440
Just even the intelligence of organisms is, uh, imperceptible to us because

44:04.440 --> 44:06.120
we're too dumb and self-centered.

44:06.560 --> 44:07.040
That worries me.

44:07.040 --> 44:09.360
Well, we're looking for a particular kind of thing.

44:09.600 --> 44:13.840
Um, when I was at Cornell, I had a lovely professor of Asian religions,

44:13.840 --> 44:18.320
Jamery Law, and she would tell this, um, story about a musical, a musician,

44:18.320 --> 44:21.840
a Western musician who went to Japan and he taught, you know, classical music

44:21.840 --> 44:24.000
and could play all sorts of instruments.

44:24.000 --> 44:27.920
He went to Japan, um, and he would ask people, you know, he would basically

44:27.920 --> 44:32.640
be looking for things in the style of Western, you know, uh, chromatic

44:32.640 --> 44:34.040
scale and these kinds of things.

44:34.040 --> 44:36.600
And then finding none of it, he would say, well, there's really no music

44:36.600 --> 44:39.160
in Japan, but they're using a different scale, they're playing

44:39.160 --> 44:40.240
different kinds of instruments, right?

44:40.440 --> 44:43.640
The same thing she was using as sort of a metaphor for religion as well.

44:43.640 --> 44:45.760
And the West, we center a lot of religion.

44:45.760 --> 44:50.000
Certainly the, the religions of Abraham, we center them around belief.

44:50.040 --> 44:52.440
And in the East, it's more about practice, right?

44:52.440 --> 44:54.320
Spirituality and practice rather than belief.

44:54.560 --> 44:59.240
So anyway, the point is here to your point, um, life, we, I think so

44:59.240 --> 45:03.880
many people are so fixated on certain aspects of self-replication or, you

45:03.880 --> 45:05.560
know, homeostasis or whatever.

45:06.080 --> 45:09.760
But if we kind of broaden and generalize this thing of things, reaching for

45:09.800 --> 45:13.760
order under which conditions can they then create an environment that

45:13.760 --> 45:18.760
sustains that order that, um, allows them, you know, the, the invention

45:18.760 --> 45:20.120
of death is an interesting thing.

45:20.160 --> 45:24.000
There are some organisms on earth that are thousands of years old, and it's

45:24.000 --> 45:25.520
not like they're incredibly complex.

45:25.520 --> 45:29.560
They're actually simpler than the cells that comprise us, but they never die.

45:29.600 --> 45:33.600
So at some point, um, death was invented, you know, uh, somewhere

45:33.600 --> 45:36.480
along the eukaryotic scale, I mean, even the protists, right, there's death.

45:37.120 --> 45:41.360
And why is that along with the sexual reproduction, right?

45:41.400 --> 45:45.840
There is something about the renewal process, something about the ability

45:45.840 --> 45:50.360
to respond to a changing environment where it just become, you know, just

45:50.360 --> 45:54.680
killing off the old generation and letting new generations try seems to be

45:54.680 --> 45:56.600
the best way to fit into the niche.

45:56.640 --> 46:00.800
You know, human historian seems to write about wheels and fires, the greatest

46:00.840 --> 46:05.520
inventions, but it seems like death and sex are pretty good and they're kind

46:05.520 --> 46:08.040
of essential inventions at the very beginning at the very beginning.

46:08.120 --> 46:08.440
Yeah.

46:08.520 --> 46:09.800
Well, we didn't invent them, right?

46:10.360 --> 46:13.200
Well, broad, we, you didn't invent them.

46:13.240 --> 46:18.760
I see us as one, uh, you particular homo sapien did not invent them, but, uh, we

46:18.760 --> 46:22.400
together, it's a team project, just like you're saying, I think the greatest

46:22.400 --> 46:25.320
homo sapien invention is collaboration.

46:25.560 --> 46:32.760
So when you say collaboration, Peter, where do ideas come from and how do

46:32.760 --> 46:34.720
they take hold in society?

46:34.760 --> 46:36.880
What's, is that the nature of collaboration?

46:36.880 --> 46:39.840
Is that the basic atom of collaboration is ideas?

46:40.360 --> 46:42.960
It's not not ideas, but it's not only ideas.

46:43.120 --> 46:45.640
There's a book I just started reading called death from a distance.

46:45.840 --> 46:46.400
Have you heard of this?

46:46.440 --> 46:51.320
No, it's a really fascinating thesis, which is that humans are the only

46:51.320 --> 46:56.720
cons specific, the only species that can kill other members of the species from

46:56.720 --> 47:01.080
range and maybe there's a few exceptions, but if you look in the animal world, you

47:01.080 --> 47:02.960
see like prong horns, butting heads, right?

47:02.960 --> 47:07.760
You see the alpha, uh, lion and the beta lion and they take each other down humans.

47:07.760 --> 47:10.960
We developed the ability to chuck rocks at each other and while at prey, but

47:10.960 --> 47:15.480
also at each other, and that means the beta male can chunk a rock at the alpha

47:15.480 --> 47:20.000
male and take them down and with very, and he can throw a lot of rocks actually

47:20.040 --> 47:22.000
miss a bunch of times, which is hit once and be good.

47:22.360 --> 47:27.240
So, um, this ability to actually kill members of our own species from range

47:27.240 --> 47:29.280
without a threat of harm to ourselves.

47:29.880 --> 47:33.960
Create essentially mutually assured destruction where we had to evolve cooperation.

47:33.960 --> 47:38.440
If we didn't, then if we just continue to try to do like, I'm the biggest

47:38.440 --> 47:42.600
monkey in the tribe and I'm going to, you know, own this tribe and you have to go.

47:43.200 --> 47:47.400
If we do it that way, then those tribes basically failed and the tribes that's

47:47.400 --> 47:51.720
that persisted and that have now given rise to the modern homo sapiens are the

47:51.720 --> 47:56.160
ones where respecting the fact that we can kill each other from a range, uh,

47:56.200 --> 48:00.080
without heart, like there's an asymmetric ability to, to snipe the leader from

48:00.080 --> 48:04.880
range that meant that we sort of had to learn how to cooperate with each other.

48:05.160 --> 48:05.360
Right.

48:05.360 --> 48:05.760
Come back here.

48:05.760 --> 48:06.560
Don't throw that rock at me.

48:06.560 --> 48:07.920
Let's talk our differences out.

48:07.920 --> 48:09.920
So violence is also part of collaboration.

48:10.200 --> 48:11.320
The threat of violence, let's say.

48:12.280 --> 48:16.680
Well, the recognition I was maybe the better way to put it is the recognition

48:16.680 --> 48:20.840
that we have more to gain by working together than the prisoner's dilemma

48:21.120 --> 48:22.400
of both of us defecting.

48:23.400 --> 48:26.680
So, uh, mutually assured destruction in all its forms is part

48:26.680 --> 48:28.200
of this idea of collaboration.

48:28.600 --> 48:31.280
Well, and Eric Weinstein talks about our nuclear peace, right?

48:31.280 --> 48:33.920
I mean, it kind of sucks with thousands of warheads aimed at each other.

48:33.920 --> 48:38.040
We mean Russia and the U S but it's like, on the other hand, you know,

48:38.040 --> 48:39.760
we only fought proxy wars, right?

48:39.760 --> 48:42.920
We did not have another world war three of like hundreds of millions of people

48:43.040 --> 48:47.360
dying to like machine gun fire and, and, you know, giant, you know, guided missiles.

48:47.720 --> 48:52.160
So the original nuclear weapon is a rock that we learned how to throw essentially.

48:52.200 --> 48:53.000
The original, yeah.

48:53.000 --> 48:56.120
Well, the original scope of the world for any human being was their little tribe.

48:58.720 --> 49:04.120
I would say it still is to the most, for the most part, Eric Weinstein speaks

49:04.120 --> 49:08.680
very highly of you, which was very surprising to me at first, cause I didn't

49:09.000 --> 49:10.760
know there's this depth to you.

49:10.800 --> 49:15.680
Cause I knew you as a, as a, as an amazing leader of engineers and

49:15.680 --> 49:17.080
engineer yourself and so on.

49:17.360 --> 49:18.160
So it's fascinating.

49:18.440 --> 49:24.760
Maybe just as a comment, a side tangent that we can take, uh, what's your nature

49:24.760 --> 49:27.120
of your friendship with Eric Weinstein?

49:27.120 --> 49:30.640
How did the two, how did such two interesting paths cross?

49:30.640 --> 49:32.920
Is it your origins in physics?

49:32.960 --> 49:37.240
Is it your interest in philosophy and the ideas of how the world works?

49:37.280 --> 49:37.680
What is it?

49:37.680 --> 49:38.880
It's actually, it's very random.

49:38.880 --> 49:40.320
It's a, Eric found me.

49:40.480 --> 49:46.400
Um, he actually found Travis and I, um, yeah, we were both working at a company

49:46.400 --> 49:50.200
called and thought, uh, back in the mid two thousands and we're doing, um, a lot

49:50.200 --> 49:51.960
of consulting around scientific Python.

49:52.360 --> 49:56.240
Um, and we'd made some, some tools and, uh, Eric was trying to use some of these

49:56.240 --> 50:01.640
Python tools to visualize that he had a fiber bundle approach to, um, modeling

50:01.840 --> 50:03.200
certain aspects of economics.

50:03.440 --> 50:05.560
He was doing this and that's how he kind of got in touch with us.

50:05.560 --> 50:13.000
And so, um, this was in the early mid two thousands, uh, uh, oh seven timeframe.

50:13.000 --> 50:13.640
Oh six or seven.

50:13.640 --> 50:15.920
Eric Weinstein trying to use Python.

50:16.160 --> 50:16.400
Right.

50:16.400 --> 50:19.800
To visualize fiber bundles, uh, using some of the tools that we're, that

50:19.800 --> 50:20.880
we'd built in the open source.

50:21.240 --> 50:22.720
That's somehow entertaining to me.

50:24.120 --> 50:24.800
It's really funny.

50:25.040 --> 50:27.640
But then, um, you know, we've met with him a couple of times, a really interesting

50:27.640 --> 50:32.600
guy, and then in the wake of the 07 08 kind of financial collapse, he, uh, helped

50:32.600 --> 50:37.800
organize with Lee Smolin, um, a symposium at the perimeter Institute about, um,

50:37.840 --> 50:41.800
okay, well clearly, you know, big finance can't be trusted governments in its

50:41.800 --> 50:43.160
pockets with regulatory capture.

50:43.480 --> 50:44.920
What the F do we do?

50:45.320 --> 50:48.600
Um, and all sorts of people in the scene to leave was there and Andy Lowe from

50:48.600 --> 50:51.240
MIT was there and, you know, uh, Bill Janeway.

50:51.240 --> 50:54.360
I mean, just a lot of, you know, top billing people were there.

50:54.800 --> 50:59.080
And he invited me and, uh, Travis and, uh, another one of our coworkers,

50:59.120 --> 51:02.920
uh, Robert Kern, uh, who was a, anyone in the SciPy NumPy community knows

51:02.920 --> 51:04.520
Robert, um, really great guy.

51:04.520 --> 51:06.600
So the three of us also got invited to go to this thing.

51:06.600 --> 51:08.880
And that's where I met Brett Weinstein for the first time as well.

51:09.440 --> 51:09.560
Yeah.

51:09.560 --> 51:13.360
I knew him before he got all famous for unfortunate reasons, I guess.

51:13.360 --> 51:18.440
But, uh, but, but anyway, we, um, so we met then and kind of had a friendship,

51:18.480 --> 51:21.040
um, you know, throughout since, since then,

51:21.320 --> 51:27.400
you have a depth of thinking that, uh, kind of runs with Eric in terms of just

51:27.400 --> 51:30.800
thinking about the world deeply and thinking philosophically, and then

51:30.800 --> 51:32.920
there's Eric's interest in programming.

51:33.400 --> 51:38.640
I actually never, um, you know, he'll bring up programming to me quite

51:38.640 --> 51:43.480
a bit as a metaphor for stuff, but I never kind of pushed the point of like,

51:44.440 --> 51:46.400
what's the nature of your interest in programming?

51:46.760 --> 51:51.440
I think he saw it probably as a tool that to visualize, to explore

51:51.480 --> 51:53.000
mathematics and explore physics.

51:53.040 --> 51:58.240
But, and I was wondering like, what's the, is, uh, depth of interest and also

51:58.240 --> 52:05.240
his vision for what programming would look like in the future.

52:05.640 --> 52:08.120
Have you, have you had interaction with him, like discussion in the

52:08.120 --> 52:09.800
space of Python and programming?

52:09.840 --> 52:13.480
Well, um, in the sense of sometimes he asked me, why is this stuff still so hard?

52:15.480 --> 52:20.000
Um, uh, yeah, you know, everybody's a critic, but, uh, but actually, no, Eric,

52:20.280 --> 52:22.160
programming, you mean like, yes, yes.

52:22.200 --> 52:25.160
Well, not programming in general, but certain things in the Python ecosystem.

52:26.440 --> 52:30.080
But he, uh, but he actually, I think what I find in listening to some of his stuff

52:30.080 --> 52:33.240
is that he does use programming metaphors a lot, right?

52:33.240 --> 52:36.120
He'll talk about APIs or object oriented and things like that.

52:36.360 --> 52:41.880
So I think that's a useful set of frames for him to draw upon for, uh, discourse.

52:42.240 --> 52:45.400
Um, I haven't pair programmed with him in a very long time.

52:45.400 --> 52:50.160
You've, you've previously, well, I mean, try to, try to help like put together

52:50.160 --> 52:53.440
some of the visualizations around these things, but it's been a very, not really

52:53.440 --> 52:55.760
pair program, but like even looked at his code, right?

52:55.760 --> 53:01.440
I mean, how legendary would be, is that like, uh, get repo with Peter

53:01.440 --> 53:02.640
Wang and Eric Weinstein.

53:02.680 --> 53:05.440
Honestly, honestly, Robert Kern did all the heavy lifting.

53:05.440 --> 53:06.720
So I have to give credit where credit is due.

53:06.880 --> 53:11.600
Robert is, is the silent, but incredibly deep, um, quiet, not silent, but quiet,

53:11.600 --> 53:14.920
but incredibly deep individual at the heart of a lot of those things that Eric

53:14.920 --> 53:19.240
was trying to do, um, but we did have, you know, in the, as Travis and I were

53:19.240 --> 53:24.960
starting our company in, um, 2012 timeframe, we went to New York, Eric was

53:24.960 --> 53:28.440
still in New York at the time he hadn't moved to this is before he joined Teal

53:28.440 --> 53:32.400
capital, we just had like a steak dinner somewhere, maybe it was Keen's, I don't

53:32.400 --> 53:33.160
know, somewhere in New York.

53:33.400 --> 53:37.440
So it was me, Travis, Eric, and then Wes McKinney, the creative pandas, and

53:37.440 --> 53:41.720
then Wes is, um, then business partner, Adam, the five of us sat around having

53:41.720 --> 53:44.840
this, just a hilarious time, amazing dinner.

53:44.880 --> 53:48.400
Um, I forget what all we talked about, but it was, it was one of those

53:48.400 --> 53:52.400
conversations, which I wish, um, as soon as COVID is over, maybe Eric and I can

53:52.440 --> 53:56.680
sit down, recreate it somewhere in, uh, in LA or maybe he comes here.

53:56.680 --> 53:58.160
Cause a lot of cool people are here in Austin, right?

53:58.160 --> 53:58.640
Exactly.

53:58.680 --> 53:58.800
Yeah.

53:58.800 --> 53:59.240
We're all here.

53:59.240 --> 53:59.920
He should come here.

53:59.960 --> 54:00.400
Come here.

54:00.640 --> 54:00.960
Yeah.

54:00.960 --> 54:05.240
So he uses, uh, the metaphor source code sometimes to talk about physics.

54:05.280 --> 54:06.760
We figure out our own source code.

54:07.120 --> 54:09.120
So you were the physics background.

54:10.000 --> 54:14.120
Um, and, uh, somebody who's quite a bit of an expert in source code.

54:14.120 --> 54:16.720
Do you think we'll ever figure out our own source code?

54:17.720 --> 54:20.280
In the way that Eric means, do you think we'll figure out the nature?

54:20.280 --> 54:21.640
We're constantly working on that problem.

54:21.720 --> 54:24.280
I mean, I think we'll, we'll make more and more progress.

54:24.360 --> 54:28.080
For me, there's some things I don't really doubt too much.

54:28.120 --> 54:33.000
Like I don't really doubt that one day we will create, um, a synthetic, maybe

54:33.000 --> 54:40.080
not, maybe not fully in Silicon, but a synthetic approach to, um, Cognition

54:40.280 --> 54:44.160
that rivals, uh, the biological 20 watt computers in our heads.

54:44.720 --> 54:45.880
What's cognition here?

54:45.920 --> 54:51.160
Well, cognition, perception, attention, memory, recall, asking better questions.

54:51.800 --> 54:53.160
That for me is a measure of intelligence.

54:53.160 --> 54:55.280
Doesn't Roomba vacuum clean already do that?

54:55.400 --> 54:57.080
Or do you mean, Oh, it doesn't ask questions.

54:57.080 --> 55:02.320
I mean, no, it's, I mean, I have a Roomba, but it's not even as smart as my cat.

55:02.400 --> 55:02.720
Right.

55:02.800 --> 55:03.240
So, yeah.

55:03.240 --> 55:05.240
But it asks questions about what is this wall?

55:05.360 --> 55:08.280
It now new feature asks, is this poop or not?

55:08.280 --> 55:08.800
Apparently.

55:08.920 --> 55:09.400
Yes.

55:09.440 --> 55:12.440
A lot of our current cybernetic system, it's a cybernetic system.

55:12.640 --> 55:14.480
It will go and it will happily vacuum up some poop.

55:14.520 --> 55:14.800
Right.

55:14.840 --> 55:16.000
Uh, the older generations would.

55:16.400 --> 55:20.720
Um, you one just released, not vacuum up the commercial.

55:20.800 --> 55:23.160
I wonder if it still gets stuck under my first rung of my stair.

55:23.200 --> 55:29.520
Um, in any case, I, these cybernetic systems we have, they are mold, they're

55:29.520 --> 55:35.040
designed to be sent off into a relatively static environment and whatever dynamic

55:35.040 --> 55:38.280
things happen in the environment, they have a very limited capacity to respond to.

55:38.840 --> 55:44.680
A human baby, a human toddler of 18 months of age has more capacity to

55:44.680 --> 55:48.080
manage its own attention and its own capacity to make better sense of the

55:48.080 --> 55:51.280
world than the most advanced robots today.

55:51.640 --> 55:55.480
So, um, again, my cat, I think can do a better job of my two and they're both

55:55.480 --> 55:59.760
pretty clever, so I do think though, back to my kind of original point, I think

55:59.760 --> 56:04.360
that it's not for me, it's not question at all that we will be able to create

56:04.360 --> 56:09.440
synthetic systems that are able to do this, um, better than the human at an

56:09.440 --> 56:11.120
equal level or better than the human mind.

56:11.640 --> 56:19.200
It's also for me, not a question that we will be able to, um, put them alongside

56:19.200 --> 56:24.240
humans so that they capture the full broad spectrum of what we are seeing as

56:24.240 --> 56:29.200
well, and also looking at our responses, listening to our responses, even maybe

56:29.200 --> 56:31.560
measuring certain vital signs about us.

56:31.920 --> 56:37.680
So in this kind of sidecar mode, a greater intelligence could use us and

56:37.680 --> 56:43.040
our whatever 80 years of life to train itself up and then be a very good

56:43.040 --> 56:44.600
simulacrum of us moving forward.

56:45.000 --> 56:49.880
So, uh, so who is in the sidecar in that picture of the future?

56:49.880 --> 56:50.360
Exactly.

56:50.360 --> 56:52.760
Is that the baby version of our immortal selves?

56:52.920 --> 56:53.200
Okay.

56:53.200 --> 56:57.760
So once the baby grows up, is there any use for humans?

56:58.440 --> 56:59.320
I think so.

56:59.960 --> 57:03.680
I think that out of, out of epistemic humility, we need to keep

57:03.680 --> 57:04.920
humans around for a long time.

57:05.600 --> 57:09.400
And I would hope that anyone making those systems would believe that to be true.

57:10.000 --> 57:13.400
Out of epistemic humility, what's the nature of the humility that

57:13.440 --> 57:14.920
that we don't know what we don't know.

57:16.400 --> 57:19.240
So we don't, right?

57:19.360 --> 57:20.160
So we don't know.

57:20.160 --> 57:23.360
I mean, first we have to build systems that, that help us do the things that we

57:23.360 --> 57:26.960
do know about that can then probe the unknowns that we know about, but the

57:26.960 --> 57:28.480
unknown unknowns, we don't know.

57:28.520 --> 57:33.360
We could always know nature is the one thing that is infinitely able to surprise us.

57:33.720 --> 57:36.960
So we should keep biological humans around for a very, very, very long

57:36.960 --> 57:41.280
time, even after our immortal selves have transcended and have gone off to

57:41.280 --> 57:44.680
explore other worlds, gone to go communicate with the life forms, living

57:44.680 --> 57:45.840
in the sun or whatever else.

57:45.840 --> 57:51.360
So, um, you know, I think that's, that's for me, these are, these seem like

57:51.560 --> 57:52.800
things that are going to happen.

57:53.000 --> 57:55.200
Like I don't really question that, that they're going to happen.

57:55.720 --> 57:58.040
Assuming we don't completely, you know, destroy ourselves.

57:58.200 --> 58:04.840
Is it possible to create an AI system that you fall in love with and it

58:04.840 --> 58:09.040
falls in love with you and you have a romantic relationship with it or a

58:09.040 --> 58:10.160
deep friendship, let's say.

58:10.680 --> 58:13.480
I would hope that that is the design criteria for any of these systems.

58:14.400 --> 58:18.880
If we cannot have a meaningful relationship with it, then it's still

58:18.880 --> 58:20.160
just a chunk of silicon.

58:20.200 --> 58:21.560
So then what is meaningful?

58:21.560 --> 58:23.640
Because, um, back to sugar.

58:23.680 --> 58:25.280
Well, sugar doesn't love you back, right?

58:25.280 --> 58:26.720
So the computer has to love you back.

58:26.760 --> 58:27.680
And what does love mean?

58:28.080 --> 58:31.160
Well, in this context for me, love, I'm going to take a page from Alain

58:31.160 --> 58:35.880
de Botton, love means that it wants to help us become the best version of ourselves.

58:36.520 --> 58:36.760
Yes.

58:37.080 --> 58:38.600
Um, that's, that's beautiful.

58:38.600 --> 58:40.680
I think that's a beautiful definition of love.

58:40.680 --> 58:46.280
So what, what role does love play in the human condition at the individual

58:46.280 --> 58:48.400
level and at the group level?

58:48.640 --> 58:51.920
Cause you were kind of saying that humans, we should really consider

58:51.920 --> 58:54.880
humans, both at the individual and the group and the societal level.

58:55.240 --> 58:56.880
What's the role of love in this whole thing?

58:56.920 --> 58:58.080
We talked about sex.

58:58.080 --> 59:00.720
We talked about death thanks to the bacteria.

59:00.720 --> 59:04.280
They invented it at which point did we invent love by the way?

59:04.280 --> 59:05.480
I mean, is that, is that also?

59:05.480 --> 59:08.840
No, I think, I think love is, is the start of it all.

59:08.880 --> 59:15.400
And the feelings of, and this gets sort of beyond just, you know, romantic,

59:15.400 --> 59:18.600
sensual, whatever kind of things, but actually genuine love as we

59:18.600 --> 59:21.920
have for another person, love as it would be used in a religious text.

59:21.920 --> 59:22.240
Right.

59:22.640 --> 59:26.680
I think that capacity to feel love more than consciousness,

59:26.880 --> 59:28.160
that is the universal thing.

59:28.360 --> 59:31.240
Our feeling of love is actually a sense of that generativity.

59:31.240 --> 59:34.680
When we can look at another person and see that they can be something more

59:35.040 --> 59:40.760
than, than they are, and more than just what we, you know, a pigeonhole

59:40.760 --> 59:44.080
we might stick them in, we see, I mean, I think there's in any religious text,

59:44.080 --> 59:48.920
you'll find, um, voiced some concept of this, that you should see the grace

59:48.920 --> 59:50.560
of God in the other person, right?

59:50.560 --> 59:54.440
You, that they're, they're made in the spirit of, of what, you know, the, the

59:54.440 --> 59:56.920
love that God feels for his creation or her creation.

59:57.200 --> 01:00:00.120
And so I think this thing is actually the root of it.

01:00:00.120 --> 01:00:05.000
So I would say before, I don't think, I don't think molecules of water feel

01:00:05.000 --> 01:00:09.880
consciousness, have consciousness, but there is some proto micro quantum thing

01:00:09.880 --> 01:00:14.520
of love that's the generativity, um, when there's more energy than what they

01:00:14.520 --> 01:00:15.560
need to maintain equilibrium.

01:00:16.200 --> 01:00:20.800
And that, when you sum it all up is something that leads to, I mean, I

01:00:20.800 --> 01:00:24.520
had my mind blown one day as an undergrad at the physics computer lab.

01:00:24.520 --> 01:00:28.360
I logged in and you know, uh, when you log into bash for a long time, there

01:00:28.360 --> 01:00:31.840
was a little fortune that would come out and it said, man was created by

01:00:31.840 --> 01:00:33.760
water to carry itself uphill.

01:00:35.400 --> 01:00:38.760
And I was logging in to work on some, you know, problem set and I logged

01:00:38.760 --> 01:00:42.160
in and I saw that and I just said, son of a bitch, you know, I just, I

01:00:42.160 --> 01:00:45.600
logged out and I went to the coffee shop and I got a coffee and I sat there

01:00:45.600 --> 01:00:52.400
on the quad and like, you know, it's not wrong and yet WTF, right?

01:00:53.120 --> 01:00:56.320
Um, so when you look at it that way, it's like, yeah, okay.

01:00:56.320 --> 01:00:58.320
Non-equilibrium physics is a thing.

01:00:59.040 --> 01:01:02.480
Um, and so when we think about love, when we think about these kinds of things,

01:01:03.040 --> 01:01:09.960
um, I would say that in the modern day human condition, there's a lot of talk

01:01:09.960 --> 01:01:16.080
about freedom and individual liberty and rights and all these things, but that's

01:01:16.080 --> 01:01:17.240
a, that's very Hegelian.

01:01:17.240 --> 01:01:20.920
It's very kind of following from the Western philosophy of, of the, the

01:01:20.920 --> 01:01:25.920
individual as, as sacrosanct, but it's not really couched, I think the right

01:01:25.920 --> 01:01:30.280
way, because it should be, how do we maximize people's ability to love each

01:01:30.280 --> 01:01:35.000
other, to love themselves first, to love each other, their responsibilities to

01:01:35.240 --> 01:01:37.320
the previous generation, to the future generations.

01:01:37.760 --> 01:01:41.800
And those are the kinds of things that should be our design criteria, right?

01:01:41.800 --> 01:01:47.400
Those should be what we start with to then come up with the philosophies of

01:01:47.400 --> 01:01:49.240
self and of rights and responsibilities.

01:01:49.640 --> 01:01:52.760
Um, but that, that love being at the center of it, I think when we

01:01:52.800 --> 01:01:58.520
design systems for cognition, um, it should absolutely be built that way.

01:01:58.640 --> 01:02:03.040
I think if we simply focus on efficiency and productivity, these kind of very,

01:02:03.360 --> 01:02:07.840
uh, industrial era, um, you know, all the things that Marx had issues with, right?

01:02:08.120 --> 01:02:11.800
Those that's, that's a way to go and, and really, I think, go off

01:02:11.800 --> 01:02:13.040
the deep end in the wrong way.

01:02:13.520 --> 01:02:20.520
So one of the interesting consequences of thinking of life in this hierarchical way

01:02:21.040 --> 01:02:25.880
of an individual human, and then there's groups and there are societies is, uh,

01:02:26.440 --> 01:02:30.480
I believe that you believe that corporations are people.

01:02:31.600 --> 01:02:36.920
So this is a, this is a kind of a politically dense idea and all those

01:02:36.920 --> 01:02:40.000
kinds of things, if we just throw politics aside, if we throw all of that

01:02:40.000 --> 01:02:44.400
aside, in which sense do you believe that corporations are people?

01:02:44.720 --> 01:02:47.480
So, um, and how does love connect to that?

01:02:47.600 --> 01:02:48.120
Right.

01:02:48.400 --> 01:02:54.720
So the belief is that groups of people have some kind of higher level, I would

01:02:54.720 --> 01:02:56.480
say, mesoscopic claim to agency.

01:02:56.880 --> 01:03:00.520
I, you know, so, so where do I, you know, let's, let's start with this.

01:03:00.840 --> 01:03:04.720
Most people would say, okay, individuals have claims to agency and sovereignty.

01:03:05.080 --> 01:03:07.320
Nations, we certainly act as if nations.

01:03:07.560 --> 01:03:12.880
So at a very large, large scale, nations have rights to sovereignty and agency.

01:03:13.040 --> 01:03:16.280
Like everyone plays the game of modernity as if that's true, right?

01:03:16.280 --> 01:03:17.320
We believe France is a thing.

01:03:17.320 --> 01:03:18.520
We believe the United States is a thing.

01:03:18.760 --> 01:03:24.560
But to say that groups of people at a smaller level than that, um,

01:03:25.080 --> 01:03:26.640
like a family unit is the thing.

01:03:26.640 --> 01:03:30.000
Well, in our law, in our laws, we actually do encode this concept.

01:03:30.440 --> 01:03:33.560
Uh, I believe that, um, in a relationship and a marriage, right.

01:03:33.760 --> 01:03:37.720
One partner can sue for loss of consortium, right?

01:03:37.720 --> 01:03:39.800
If someone breaks up the marriage or whatever.

01:03:39.800 --> 01:03:42.520
So these are concepts that even in law, we do respect that there is

01:03:42.520 --> 01:03:45.720
something about the union and about the family.

01:03:45.960 --> 01:03:50.680
So for me, I don't think it's so weird to think that groups of people have a

01:03:50.680 --> 01:03:54.600
right to a claim to rights and sovereignty of some degree.

01:03:54.640 --> 01:03:58.880
I mean, we, and we, we, uh, look at our clubs, we look at churches.

01:03:59.000 --> 01:04:02.440
These are, we, we talk about these collectives of people as if they

01:04:02.440 --> 01:04:05.400
have a real agency to them and then they do.

01:04:05.760 --> 01:04:09.520
But I think if we take that one step further and say, okay, they can accrue

01:04:09.520 --> 01:04:14.720
resources, well, yes, check, you know, and by law they can, um, they can own land.

01:04:14.760 --> 01:04:17.000
They can engage in contracts.

01:04:17.000 --> 01:04:18.480
They can do all these different kinds of things.

01:04:18.760 --> 01:04:24.440
So we in legal terms, uh, support this idea that groups of people have rights.

01:04:25.040 --> 01:04:31.160
Um, where we go wrong on this stuff is that the most popular version of this

01:04:31.200 --> 01:04:36.800
is the for-profit absentee owner corporation that then is able to amass

01:04:36.800 --> 01:04:41.080
larger resources than anyone else in the landscape, anything else, any other

01:04:41.120 --> 01:04:44.760
entity of equivalent size, and they're able to essentially bully around

01:04:44.760 --> 01:04:47.560
individuals, whether it's laborers, whether it's people whose resources

01:04:47.560 --> 01:04:50.920
they want to capture, they're also able to bully around our system of

01:04:50.920 --> 01:04:55.080
representation, which is still tied to individuals, right?

01:04:55.520 --> 01:04:58.520
So, um, I don't believe that's correct.

01:04:58.520 --> 01:05:02.280
I don't think it's good that they, you know, they're people, but they're assholes.

01:05:02.280 --> 01:05:05.040
I don't think that corporations as people acting like assholes is a good thing.

01:05:05.480 --> 01:05:09.040
But the idea that collectives and collections of people that we should

01:05:09.040 --> 01:05:13.760
treat them philosophically as having some agency, some agency, and some,

01:05:13.760 --> 01:05:18.000
some mass, um, at a mesoscopic level, I think that's an important thing.

01:05:18.000 --> 01:05:23.440
Because one, one thing I do think we underappreciate sometimes is the

01:05:23.440 --> 01:05:25.800
fact that relationships have relationships.

01:05:26.160 --> 01:05:28.480
So it's not just individuals having relationships with each other.

01:05:29.080 --> 01:05:32.080
But if you have eight people seated around a table, right?

01:05:32.080 --> 01:05:35.120
Each person has a relationship with each of the others and that's obvious.

01:05:35.560 --> 01:05:39.800
But then if it's four couples, each couple also has a relationship with

01:05:39.800 --> 01:05:41.640
each of the other couples, right?

01:05:41.640 --> 01:05:42.520
The dyads do.

01:05:42.840 --> 01:05:47.920
And if it's couples, but one is the father, mother older, and then, you

01:05:47.920 --> 01:05:53.280
know, one of their children and their spouse, that, that family unit of four

01:05:53.280 --> 01:05:55.360
has a relationship with the other family unit of four.

01:05:55.680 --> 01:05:58.480
So the idea that relationships have relationships is something that we

01:05:58.480 --> 01:06:02.680
intuitively know in navigating the social landscape, but it's not something

01:06:02.680 --> 01:06:04.040
I hear expressed like that.

01:06:04.440 --> 01:06:07.720
Um, it's certainly not something that is, I think, taken into account very

01:06:07.720 --> 01:06:09.360
well when we design these kinds of things.

01:06:09.360 --> 01:06:14.760
So I think, um, the reason why I care a lot about this is because I think the

01:06:14.760 --> 01:06:20.280
future of humanity requires us to form better sense, make collective sense

01:06:20.280 --> 01:06:25.960
making units at something, you know, around Dunbar number, you know, half

01:06:25.960 --> 01:06:32.040
to five X Dunbar, and that's very different than right now where we, um,

01:06:32.360 --> 01:06:36.200
defer sense making to massive aging zombie institutions.

01:06:36.720 --> 01:06:38.440
Um, or we just do it ourselves.

01:06:38.440 --> 01:06:40.400
We go to loan, go to the dark force of the internet.

01:06:41.040 --> 01:06:42.280
So that's really interesting.

01:06:42.280 --> 01:06:44.720
So you've, you've talked about agency.

01:06:45.280 --> 01:06:49.040
I think maybe calling it a convenient fiction at all these different levels.

01:06:49.560 --> 01:06:53.080
So even at the human individual level, it's kind of a fiction.

01:06:53.120 --> 01:06:55.680
We all believe because we are, like you said, made of cells

01:06:55.680 --> 01:06:56.840
and cells are made of atoms.

01:06:57.640 --> 01:06:58.920
So that's a useful fiction.

01:06:58.920 --> 01:07:04.720
And then there's nations that seems to be a useful fiction, but it seems

01:07:04.720 --> 01:07:06.840
like some fictions are better than others.

01:07:06.840 --> 01:07:10.600
You know, there's a lot of people that argue the fiction of nation is a bad idea.

01:07:10.920 --> 01:07:14.600
One of them lives two doors down from me, Michael Malice.

01:07:14.600 --> 01:07:15.400
He's an anarchist.

01:07:16.080 --> 01:07:19.600
You know, I'm sure there's a lot of people who are into meditation that

01:07:19.600 --> 01:07:26.160
believe the idea, this useful fiction of agency of an individual is troublesome

01:07:26.160 --> 01:07:31.080
as well, but we need to let go of that in order to truly like to transcend.

01:07:31.760 --> 01:07:32.200
I don't know.

01:07:32.240 --> 01:07:35.440
I don't know what words you want to use, but suffering or to, uh, to

01:07:35.440 --> 01:07:37.960
elevate the experience of life.

01:07:38.400 --> 01:07:43.120
So you're kind of arguing that, okay, so we have some of these useful fictions

01:07:43.120 --> 01:07:49.880
of agency, we should add a stronger fiction that we tell ourselves about the

01:07:49.920 --> 01:07:55.960
agency of groups in the hundreds of, uh, the, uh, half a Dunbar's number,

01:07:56.160 --> 01:07:57.560
five X Dunbar's number.

01:07:57.880 --> 01:07:58.040
Yeah.

01:07:58.040 --> 01:07:58.840
Something on that order.

01:07:58.960 --> 01:08:01.480
And we call them fictions, but really the rules of the game, right?

01:08:01.560 --> 01:08:05.360
Rules that we, we, we feel are fair or rules that we consent to.

01:08:05.720 --> 01:08:08.840
I always question the rules when I lose, like a monopoly, that's

01:08:08.840 --> 01:08:09.840
when I usually question them.

01:08:09.840 --> 01:08:11.440
When I'm winning, I don't question the rules.

01:08:11.560 --> 01:08:12.840
We should play a game monopoly someday.

01:08:12.840 --> 01:08:14.840
There's a trippy version of it that we could do.

01:08:15.200 --> 01:08:19.040
What, what kind of contract monopoly is introduced by a friend of mine to me

01:08:19.120 --> 01:08:24.200
where you can write contracts on, uh, future earnings or landing on various

01:08:24.200 --> 01:08:27.760
things and you can hand out like, you know, you can land the first three times

01:08:27.760 --> 01:08:31.000
you land a park places free or whatever, just, and then you can start trading

01:08:31.000 --> 01:08:37.120
those contracts for money and then you create a human civilization and somehow

01:08:37.120 --> 01:08:38.200
Bitcoin comes into it.

01:08:38.240 --> 01:08:38.680
Okay.

01:08:38.760 --> 01:08:43.320
Uh, but some of these, actually, I bet if me and you and Eric sat down to play a

01:08:43.320 --> 01:08:46.400
game of monopoly and we were to make NFTs out of the contracts we wrote, we

01:08:46.400 --> 01:08:47.200
could make a lot of money.

01:08:47.960 --> 01:08:48.960
Now it's a terrible idea.

01:08:49.280 --> 01:08:52.680
I would never do it, but I bet we could actually sell the NFTs around.

01:08:52.880 --> 01:08:57.280
I have other ideas to make money that I could tell you, and they're all

01:08:57.280 --> 01:09:01.600
terrible ideas, including cat videos on the internet.

01:09:02.200 --> 01:09:02.600
Okay.

01:09:02.640 --> 01:09:06.400
But some of these rules of the game, some of these fictions are, it seems

01:09:06.400 --> 01:09:08.840
like they're better than others.

01:09:09.200 --> 01:09:14.560
They have worked this far to cohere, um, human to organize human collective action.

01:09:14.840 --> 01:09:18.320
But you're saying something about, especially this technological age

01:09:19.240 --> 01:09:23.160
requires modified fictions, stories of agency.

01:09:23.600 --> 01:09:25.000
Why the Dunbar number?

01:09:25.000 --> 01:09:27.920
And also, you know, how do you select the group of people?

01:09:28.200 --> 01:09:34.840
You know, Dunbar numbers, I think I have the sense that it's overused as a kind

01:09:34.840 --> 01:09:41.720
of law that somehow we can have deep human connection at this scale, like some

01:09:41.720 --> 01:09:44.880
of it feels like an interface problem too.

01:09:45.360 --> 01:09:49.200
It feels like if I have the right tools, I can deeply connect with a

01:09:49.200 --> 01:09:51.440
large number, larger number of people.

01:09:51.840 --> 01:09:57.960
It just feels like, uh, there's a huge value to interacting just in person,

01:09:57.960 --> 01:10:02.360
getting to share traumatic experiences together, beautiful experiences together.

01:10:02.720 --> 01:10:07.520
There's other experiences like, um, that in the digital space that you can share.

01:10:07.520 --> 01:10:11.360
It just feels like Dunbar's number can be expanded significantly, perhaps

01:10:11.360 --> 01:10:16.240
not to the level of millions and billions, but it feels like it could be expanded.

01:10:16.240 --> 01:10:23.480
So how do we find the right interface, you think, um, for, uh, having a

01:10:23.480 --> 01:10:25.640
little bit of a collective here that has agency.

01:10:26.000 --> 01:10:29.840
You're right that there's many different ways that we can build trust with each other.

01:10:30.000 --> 01:10:35.800
Um, my friend, Joe Edelman talks about a few different ways that, um, you

01:10:35.800 --> 01:10:40.680
know, mutual appreciation, trustful conflict, um, just experiencing something

01:10:40.680 --> 01:10:44.440
like, you know, there's a variety of different things that we can do, but, um,

01:10:44.600 --> 01:10:49.360
all those things take time and you have to be present the less presence you are.

01:10:49.360 --> 01:10:51.440
I mean, there's just, again, a no free lunch principle here.

01:10:51.480 --> 01:10:54.480
The less present you are, the more of them you can do, but then the

01:10:54.480 --> 01:10:56.080
less, the less connection you build.

01:10:56.720 --> 01:11:00.160
So I think there is sort of a human capacity issue around some of these things.

01:11:00.160 --> 01:11:04.440
Now that being said, if we can use certain technologies.

01:11:04.720 --> 01:11:08.880
So for instance, if I write a little monograph on my view of the world, you

01:11:08.880 --> 01:11:11.840
read it asynchronously at some point and you're like, wow, Peter, this is great.

01:11:11.840 --> 01:11:12.560
Here's mine.

01:11:12.760 --> 01:11:13.160
I read it.

01:11:13.160 --> 01:11:14.680
I'm like, wow, Lex, this is awesome.

01:11:15.240 --> 01:11:19.640
We can be friends without having to spend 10 years, you know, figuring

01:11:19.640 --> 01:11:20.480
all this stuff out together.

01:11:20.480 --> 01:11:23.680
We just read each other's thing and be like, oh yeah, this guy's exactly

01:11:23.880 --> 01:11:25.600
in my wheelhouse and vice versa.

01:11:26.040 --> 01:11:30.600
And we can then, um, you know, connect just a few times a year and

01:11:30.600 --> 01:11:32.680
maintain a high trust relationship.

01:11:33.000 --> 01:11:36.160
It can be expanded a little bit, but it also requires these things are

01:11:36.160 --> 01:11:37.240
not all technological in nature.

01:11:37.240 --> 01:11:40.560
It requires the individual themselves to have a certain level of

01:11:40.560 --> 01:11:44.720
capacity, to have a certain lack of neuroticism, right?

01:11:44.720 --> 01:11:47.480
If you want to use like the, the ocean big five sort of model,

01:11:48.040 --> 01:11:49.360
people have to be pretty centered.

01:11:49.640 --> 01:11:52.280
The less centered you are, the fewer authentic connections you can really

01:11:52.280 --> 01:11:54.200
build for a particular unit of time.

01:11:54.800 --> 01:11:55.800
It just takes more time.

01:11:55.840 --> 01:11:57.080
Other people had to put up with your crap.

01:11:57.120 --> 01:12:00.440
Like there's just a lot of the stuff that you have to deal with if you

01:12:00.440 --> 01:12:02.000
are not so well balanced, right?

01:12:02.200 --> 01:12:05.680
So yes, we can help people get better to where they can develop more

01:12:05.680 --> 01:12:06.560
relationships faster.

01:12:06.880 --> 01:12:09.760
And then you can maybe expand Dunbar number by quite a bit, but

01:12:09.760 --> 01:12:10.520
you're not going to do it.

01:12:10.640 --> 01:12:13.240
I think it's gonna be hard to get it beyond 10 X kind of the

01:12:13.240 --> 01:12:14.600
rough swag of what it is.

01:12:15.160 --> 01:12:20.800
You know, well, don't you think that AI systems could be an addition

01:12:20.800 --> 01:12:22.160
to the Dunbar's number?

01:12:22.640 --> 01:12:25.520
So like why do you count as one system or multiple AI systems,

01:12:25.560 --> 01:12:26.520
multiple AI systems.

01:12:26.520 --> 01:12:30.560
So I do believe that AI systems for them to integrate into human society

01:12:30.560 --> 01:12:32.600
as it is now have to have a sense of agency.

01:12:32.600 --> 01:12:37.160
So there has to be an individual because otherwise we wouldn't relate to them.

01:12:37.560 --> 01:12:41.600
We could engage certain kinds of individuals to make sense of them for us

01:12:41.600 --> 01:12:45.800
and be almost like, did you ever watch Star Trek, like Voyager, like there's

01:12:45.800 --> 01:12:49.720
the Volta who were like the interfaces, the ambassadors for the Dominion.

01:12:50.400 --> 01:12:54.320
We may have ambassadors that speak on behalf of these systems.

01:12:54.320 --> 01:12:56.840
They're like the Mentats of Dune maybe, or something like this.

01:12:57.200 --> 01:12:58.840
I mean, we already have this to some extent.

01:12:59.240 --> 01:13:02.840
If you look at the biggest sort of, I wouldn't say AI system, but the biggest

01:13:02.840 --> 01:13:05.080
cybernetic system in the world is the financial markets.

01:13:05.080 --> 01:13:09.040
It runs outside of any individual's control and you have an entire stack of

01:13:09.040 --> 01:13:12.800
people on Wall Street, Wall Street analysts to CNBC reporters, whatever.

01:13:13.120 --> 01:13:15.200
They're all helping to communicate.

01:13:15.200 --> 01:13:16.280
What does this mean?

01:13:16.800 --> 01:13:19.200
You know, you got Jim Cramer like running around and yelling and stuff.

01:13:19.440 --> 01:13:24.400
Like all of these people are part of that lowering of the complexity there to

01:13:24.800 --> 01:13:29.120
meet sense, you know, to help do sense making for people at whatever capacity

01:13:29.120 --> 01:13:29.520
they're at.

01:13:29.760 --> 01:13:31.520
And I don't see this changing with AI systems.

01:13:31.520 --> 01:13:34.200
I think you would have ringside commentators talking about all this stuff

01:13:34.480 --> 01:13:37.280
that this AI system is trying to do over here, over here, because it's a, it's

01:13:37.280 --> 01:13:38.640
actually a super intelligence.

01:13:39.040 --> 01:13:41.560
So if you want to talk about humans interfacing, making first contact with

01:13:41.560 --> 01:13:43.240
the super intelligence, we're already there.

01:13:43.520 --> 01:13:44.560
We do it pretty poorly.

01:13:44.720 --> 01:13:48.160
And if you look at the gradient of power and money, what happens is people

01:13:48.160 --> 01:13:53.920
closest to it will absolutely exploit their distance for personal financial gain.

01:13:54.280 --> 01:13:57.000
So we should look at that and be like, oh, well, that's probably what the

01:13:57.000 --> 01:13:58.120
future will look like as well.

01:13:58.800 --> 01:14:01.320
But, um, but nonetheless, I mean, we're already doing this kind of thing.

01:14:01.320 --> 01:14:05.200
So in the future we can have AI systems, but you're still gonna have to trust

01:14:05.200 --> 01:14:07.400
people to bridge the sense making gap to them.

01:14:08.320 --> 01:14:12.840
See, I don't, I just feel like there could be of like millions of AI systems

01:14:12.840 --> 01:14:16.640
that have, um, have agencies.

01:14:16.640 --> 01:14:20.480
You have, when you say one super intelligence, super intelligence in that

01:14:20.480 --> 01:14:26.320
context means it's able to solve particular problems extremely well, but

01:14:26.320 --> 01:14:30.200
there's some aspect of human like intelligence that's necessary to be

01:14:30.200 --> 01:14:31.840
integrated into human society.

01:14:32.280 --> 01:14:37.640
So not financial markets, not sort of weather prediction systems or I don't

01:14:37.640 --> 01:14:39.240
know, logistics optimization.

01:14:39.680 --> 01:14:44.400
I'm more referring to things that you interact with on the intellectual level.

01:14:45.120 --> 01:14:48.600
And that I think requires, there has to be a backstory.

01:14:48.920 --> 01:14:50.040
There has to be a personality.

01:14:50.040 --> 01:14:53.240
I believe it has to fear its own mortality in a genuine way.

01:14:53.280 --> 01:14:59.800
Like there has to be all many of the elements that we humans experience that

01:14:59.800 --> 01:15:03.800
are fundamental to the human condition, because otherwise we would not have

01:15:03.800 --> 01:15:04.880
a deep connection with it.

01:15:05.800 --> 01:15:09.400
But I don't think having a deep connection with it is necessarily going to stop us

01:15:09.400 --> 01:15:12.200
from building a thing that has quite an alien intelligence aspect.

01:15:12.200 --> 01:15:12.440
Sure.

01:15:13.000 --> 01:15:16.760
Um, so another, the other kind of alien intelligence on this planet is

01:15:16.760 --> 01:15:19.320
the octopuses or octopodes or whatever you want to call them.

01:15:20.200 --> 01:15:20.840
Octopi.

01:15:20.840 --> 01:15:21.000
Yeah.

01:15:21.000 --> 01:15:25.320
There's a, there's a little controversy as to what the plural is, I guess, but an

01:15:25.320 --> 01:15:32.120
octopus, an octopus, um, you know, it really acts as a collective intelligence

01:15:32.120 --> 01:15:34.320
of eight intelligent arms, right?

01:15:34.360 --> 01:15:36.440
Its arms have a tremendous amount of neural density to them.

01:15:37.000 --> 01:15:41.960
And I see if we can build, I mean, just let's, let's go with what you're saying.

01:15:42.000 --> 01:15:47.360
If we build a singular intelligence that interfaces with humans that has a sense

01:15:47.360 --> 01:15:51.040
of agency so it can run the cybernetic loop and develop its own theory of mind,

01:15:51.040 --> 01:15:52.640
as well as it's a theory of action.

01:15:52.920 --> 01:15:56.200
All of these things, I agree with you that that's the necessary components

01:15:56.200 --> 01:15:57.760
to build a real intelligence, right?

01:15:57.760 --> 01:15:58.680
There's gotta be something at stake.

01:15:58.680 --> 01:15:59.760
It's got to make a decision.

01:15:59.960 --> 01:16:01.160
It's got to then run the OODA loop.

01:16:01.240 --> 01:16:01.560
Okay.

01:16:01.600 --> 01:16:02.440
So we build one of those.

01:16:02.960 --> 01:16:05.040
Well, if we can build one of those, we can probably build 5 million of them.

01:16:05.600 --> 01:16:06.920
So we'll 5 million of them.

01:16:07.400 --> 01:16:12.200
And if their cognitive systems are already digitized and already kind of there, we

01:16:12.200 --> 01:16:15.880
stick our antenna on each of them, bring it all back to a hive mind that maybe

01:16:15.880 --> 01:16:19.840
doesn't make all the individual decisions for them, but treats each one as almost

01:16:19.840 --> 01:16:24.280
like a neural, neuronal input of a much higher bandwidth and fidelity, going back

01:16:24.280 --> 01:16:30.400
to a central system that is then able to perceive much broader dynamics that we

01:16:30.400 --> 01:16:32.560
can't see in the same way that a phased array radar, right?

01:16:32.560 --> 01:16:34.040
You think about how phased array radar works.

01:16:34.400 --> 01:16:35.600
It's just sensitivity.

01:16:36.160 --> 01:16:39.600
It's just radars and then it's hypersensitivity and really great

01:16:39.600 --> 01:16:40.720
timing between all of them.

01:16:41.120 --> 01:16:44.600
And with a flat array, it's as good as a curved radar dish, right?

01:16:44.680 --> 01:16:48.800
So with these things, it's a phased array of cybernetic systems that'll give the

01:16:48.800 --> 01:16:55.000
centralized intelligence a much, much better, a much higher fidelity understanding

01:16:55.000 --> 01:16:56.520
of what's actually happening in the environment.

01:16:56.520 --> 01:17:02.680
But the more power, the more understanding the central superintelligence has, the

01:17:02.680 --> 01:17:07.800
dumber the individual like fingers of this intelligence are, I think.

01:17:08.040 --> 01:17:11.720
I think, I don't see what has to be this argument.

01:17:11.760 --> 01:17:16.720
There has to be the experience of the individual agent has to have the full

01:17:16.960 --> 01:17:20.520
richness of the human-like experience.

01:17:20.760 --> 01:17:24.600
You have to be able to be driving the car in the rain, listening to Bruce

01:17:24.600 --> 01:17:29.280
Springsteen and all of a sudden break out in tears because remembering some,

01:17:29.600 --> 01:17:30.960
something that happened to you in high school.

01:17:30.960 --> 01:17:32.800
We can implant those memories if that's really needed.

01:17:32.800 --> 01:17:37.920
But no, no, but the central agency, like I guess I'm saying for, in my view, for

01:17:37.960 --> 01:17:43.440
intelligence to be born, you have to have a decentralization.

01:17:43.840 --> 01:17:46.960
Like each one has to struggle and reach.

01:17:47.280 --> 01:17:52.720
So each one in the excess of energy has to reach for order as opposed to a

01:17:52.720 --> 01:17:54.320
central place doing so.

01:17:54.400 --> 01:17:57.840
Have you ever read like some sci-fi where there's like hive minds?

01:17:58.800 --> 01:18:02.760
Like the Verner Vinge, I think has one of these and then some of the stuff from,

01:18:03.640 --> 01:18:07.240
yes, on the Commonwealth saga, the idea that you're an individual, but you're

01:18:07.240 --> 01:18:10.280
connected with like a few other individuals telepathically as well.

01:18:10.280 --> 01:18:11.760
And together you form a swarm.

01:18:12.560 --> 01:18:17.560
So if you are, I need to ask you, what do you think is the experience of if you

01:18:17.560 --> 01:18:18.920
are like, well, a Borg, right?

01:18:18.920 --> 01:18:23.760
If you are one, if you're part of this hive mind, outside of all the aesthetics,

01:18:23.760 --> 01:18:28.040
forget the aesthetics internally, what is your experience like?

01:18:28.360 --> 01:18:30.040
Cause I have a theory as to what that looks like.

01:18:30.600 --> 01:18:35.200
The one question I have for you about that experience is how much is there

01:18:35.240 --> 01:18:42.000
feeling of freedom, free will, because I obviously as a, as a human, very

01:18:42.000 --> 01:18:47.360
unbiased, but also somebody who values freedom and biased, it feels like the

01:18:47.360 --> 01:18:55.320
experience of freedom is essential for, um, trying stuff out to being, to being

01:18:55.320 --> 01:18:58.560
creative and doing something truly novel, which is at the core of.

01:18:59.120 --> 01:18:59.480
Yeah.

01:18:59.480 --> 01:19:01.400
Well, I don't think you have to lose any freedom when you're in that

01:19:01.400 --> 01:19:05.800
mode, because I think what happens is we think we still think, I mean, you're

01:19:05.800 --> 01:19:09.560
still thinking about this in a sense of a top-down command and control hierarchy,

01:19:09.800 --> 01:19:11.280
which is not what it has to be at all.

01:19:11.880 --> 01:19:16.040
Um, I think the experience, so I'll just, you know, show by cards here.

01:19:16.040 --> 01:19:20.280
I think the experience of being a robot in that robot swarm, a robot who has

01:19:20.280 --> 01:19:24.280
agency over their own local environment, that's doing sense-making and reporting

01:19:24.280 --> 01:19:25.720
it back to the hive mind.

01:19:26.360 --> 01:19:30.720
Um, I think that robot's experience would be one of when the hive mind is

01:19:30.720 --> 01:19:35.280
working well, it would be an experience of like talking to God, right?

01:19:35.280 --> 01:19:39.640
That you essentially are reporting to, you're sort of saying, here's what I see.

01:19:39.640 --> 01:19:41.080
I think this is what's going to happen over here.

01:19:41.120 --> 01:19:44.000
I'm going to go do this thing because I think if I'm going to do this, this will

01:19:44.000 --> 01:19:45.760
make this change happen in the environment.

01:19:46.360 --> 01:19:51.240
And, and then, and God, she may tell you that's great.

01:19:51.240 --> 01:19:53.560
And in fact, your, your brothers and sisters will join you to help

01:19:53.560 --> 01:19:54.640
make this go better, right?

01:19:55.000 --> 01:19:58.400
And then she can let your brothers and sisters know, Hey, you know, Peter is

01:19:58.400 --> 01:19:59.280
going to go do this thing.

01:19:59.440 --> 01:20:00.400
Would you like to help him?

01:20:00.680 --> 01:20:02.360
Because we think that this will make this thing go better.

01:20:02.360 --> 01:20:03.400
And they'll say, yes, we'll help him.

01:20:03.760 --> 01:20:08.080
So the whole thing could be actually a very emergent, the sense of, you know,

01:20:08.080 --> 01:20:12.480
what does it feel like to be a cell in a network that is alive, that is generative.

01:20:12.720 --> 01:20:18.400
And I think actually the feeling is serendipity that, that there's random

01:20:18.480 --> 01:20:23.800
order, not random disorder or chaos, but random order, just when you need it to

01:20:23.800 --> 01:20:27.600
hear Bruce Springsteen, you turn on the radio and bam, it's Bruce Springsteen.

01:20:28.000 --> 01:20:28.280
Right.

01:20:28.520 --> 01:20:32.440
That feeling of serendipity, I feel like, um, this is a bit of a flight of fancy,

01:20:32.440 --> 01:20:37.000
but every cell in your body must have, like, what does it feel like to be a cell

01:20:37.000 --> 01:20:41.400
in your body when it needs sugar, there's sugar, when he's oxygen, there's just

01:20:41.400 --> 01:20:45.280
oxygen now when it needs to go and do its work and pull like as one of your

01:20:45.280 --> 01:20:46.160
muscle fibers, right.

01:20:46.720 --> 01:20:48.760
It does its work and it's great.

01:20:48.760 --> 01:20:49.800
It contributes to the cause.

01:20:49.800 --> 01:20:50.080
Right.

01:20:50.160 --> 01:20:54.240
So this is all, again, a fight, a flight of fancy, but I think as we extrapolate

01:20:54.240 --> 01:20:57.840
up, what does it feel like to be an independent individual with some bounded

01:20:57.840 --> 01:21:00.840
sense of freedom, all sense of freedom is actually bounded, but it was about a

01:21:00.840 --> 01:21:04.840
sense of freedom that still lives within a network that has order to it.

01:21:04.920 --> 01:21:06.920
And I feel like it has to be a feeling of serendipity.

01:21:07.160 --> 01:21:10.720
So the cell, there's a feeling of serendipity, even though

01:21:11.240 --> 01:21:14.040
it has no way of explaining why it's getting oxygen and sugar when it gets it.

01:21:14.040 --> 01:21:18.080
So you have to, each individual component has to be too dumb to

01:21:18.080 --> 01:21:19.600
understand the big picture.

01:21:20.280 --> 01:21:22.480
No, the big picture is bigger than what it can understand.

01:21:22.920 --> 01:21:26.480
But isn't that an essential characteristic of the individual is to

01:21:26.480 --> 01:21:31.400
be too dumb to understand the bigger picture, like not dumb necessarily, but

01:21:31.400 --> 01:21:33.880
limited in its capacity to understand.

01:21:34.080 --> 01:21:34.800
Cause the moment, okay.

01:21:35.160 --> 01:21:41.560
The moment you understand, I feel like that leads to, if you tell me now that

01:21:41.640 --> 01:21:46.400
there are some bigger intelligence controlling everything I do, intelligence

01:21:46.400 --> 01:21:50.360
broadly defined, meaning like, you know, even the Sam Harris thing, there's no

01:21:50.360 --> 01:21:55.320
free will, if I'm smart enough to truly understand that that's the case,

01:21:56.320 --> 01:21:58.920
that's kind of, I don't know if I.

01:21:58.920 --> 01:22:00.880
Well, you have philosophical breakdown, right?

01:22:00.920 --> 01:22:04.760
Because we're in the West and we're pumped full of this stuff of like, you are

01:22:05.080 --> 01:22:08.640
a golden, fully free individual with all your freedoms and all your liberties and

01:22:08.640 --> 01:22:10.200
go grab a gun and shoot whatever you want to.

01:22:10.480 --> 01:22:15.520
No, it's actually, you don't actually have a lot of these, you're not unconstrained,

01:22:15.800 --> 01:22:21.680
but the areas where you can manifest agency, you're free to do those things.

01:22:21.800 --> 01:22:23.200
You can say whatever you want on this podcast.

01:22:23.200 --> 01:22:24.360
You can create a podcast, right?

01:22:24.840 --> 01:22:28.480
You're not, I mean, you have a lot of this kind of freedom, but even as you're

01:22:28.480 --> 01:22:32.480
doing this, you are actually, I guess, where the, the, the denouement of this

01:22:32.480 --> 01:22:37.480
is that we are already intelligent agents in such a system, right?

01:22:37.760 --> 01:22:42.360
In that one of these, these like robots of one to 5 million little swarm robots

01:22:42.360 --> 01:22:45.120
or one of the Borg, they're just posting on internal bulletin board.

01:22:45.360 --> 01:22:48.320
I mean, maybe the Borg cube is just a giant Facebook machine floating in space.

01:22:48.600 --> 01:22:50.360
And everyone's just posting on there.

01:22:50.480 --> 01:22:52.760
They're just posting really fast and like, oh yeah.

01:22:52.760 --> 01:22:53.720
It's called the metaverse now.

01:22:53.760 --> 01:22:54.600
The nest called the metaverse.

01:22:54.600 --> 01:22:54.880
That's right.

01:22:54.880 --> 01:22:55.560
Here's the enterprise.

01:22:55.560 --> 01:22:56.440
Maybe we should all go shoot it.

01:22:56.440 --> 01:22:56.720
Yeah.

01:22:56.720 --> 01:22:58.160
Everyone uploads and they're going to go shoot it.

01:22:58.160 --> 01:22:58.440
Right.

01:22:58.840 --> 01:23:03.800
But we already are part of a human online collaborative environment and

01:23:03.800 --> 01:23:05.360
collaborative sense-making system.

01:23:05.640 --> 01:23:06.880
It's not very good yet.

01:23:07.280 --> 01:23:11.520
It's got the overhangs of zombie sense-making institutions all over it.

01:23:11.960 --> 01:23:16.440
But as that washes away, and as we get better at this, we are going to see

01:23:16.440 --> 01:23:22.360
humanity improving at speeds that are unthinkable in the past, and it's not

01:23:22.360 --> 01:23:23.720
because anyone's freedoms were limited.

01:23:23.760 --> 01:23:26.360
In fact, the open store, and we started this with open source software, right?

01:23:26.760 --> 01:23:30.440
The collaboration, what the internet surfaced was the ability for people

01:23:30.440 --> 01:23:33.560
all over the world to collaborate and produce some of the most foundational

01:23:33.560 --> 01:23:35.920
software that's in use today, right?

01:23:35.920 --> 01:23:38.200
That entire ecosystem was created by collaborators all over the place.

01:23:38.880 --> 01:23:44.360
So these online kind of swarm kind of things are not novel.

01:23:44.360 --> 01:23:48.240
It's just, I'm just suggesting that future AI systems, if you can build one

01:23:48.240 --> 01:23:52.200
smart system, you have no reason not to build multiple, if you build multiple,

01:23:52.200 --> 01:23:55.920
there's no reason not to integrate them all into a collective sense-making

01:23:56.760 --> 01:24:00.840
substrate, and that thing will certainly have emergent intelligence that none

01:24:00.840 --> 01:24:04.080
of the individuals and probably not any of the human designers will be able to

01:24:04.080 --> 01:24:06.760
really put a bow around and explain.

01:24:06.800 --> 01:24:13.640
But in some sense, would that AI system still be able to go like rural Texas,

01:24:13.640 --> 01:24:17.000
buy a ranch, go off the grid, go full survivalist?

01:24:17.640 --> 01:24:20.360
Can you disconnect from the hive mind?

01:24:22.520 --> 01:24:23.400
You may not want to.

01:24:25.240 --> 01:24:29.840
So to be ineffective, to be intelligent, you have access to way more intelligence

01:24:29.840 --> 01:24:32.360
capability if you're plugged into 5 million other really, really smart

01:24:32.360 --> 01:24:34.200
cyborgs, why would you leave?

01:24:34.880 --> 01:24:37.600
So like there's a word control that comes to mind.

01:24:38.240 --> 01:24:43.200
So it doesn't, it doesn't feel like control, like over, over, uh, overbearing

01:24:43.200 --> 01:24:47.880
control, it's, it's just, I think systems to your point, I mean, look at, look at

01:24:47.880 --> 01:24:50.240
how much, how uncomfortable you are with this concept, right?

01:24:50.560 --> 01:24:55.280
I think systems that feel like overbearing control will not evolutionarily went out.

01:24:55.760 --> 01:24:59.000
I think systems that give their individual elements, the feeling of

01:24:59.000 --> 01:25:04.720
serendipity and the feeling of agency, that that will, um, those systems will

01:25:04.720 --> 01:25:07.360
win, but that's not to say that there will not be emergent higher level

01:25:07.360 --> 01:25:08.600
order on top of it.

01:25:10.160 --> 01:25:12.440
And that's the thing, that's the philosophical breakdown that we're

01:25:12.720 --> 01:25:16.320
staring right at, which is in the Western mind, I think there's a very

01:25:16.600 --> 01:25:23.240
sharp delineation between explicit control, you know, Cartesian, like what

01:25:23.240 --> 01:25:25.760
is the vector, where is the position, where is it going?

01:25:26.240 --> 01:25:31.240
It's completely deterministic and kind of this idea that things emerge.

01:25:31.480 --> 01:25:34.640
Everything we see is the emergent patterns of other things.

01:25:35.400 --> 01:25:37.960
And there is agency when there's extra energy.

01:25:40.320 --> 01:25:44.680
So you have spoken about a kind of meaning crisis that we're going through,

01:25:44.680 --> 01:25:53.160
but it feels like since, uh, since we invented sex and death, we broadly

01:25:53.160 --> 01:25:55.960
speaking, we've been searching for a kind of meaning.

01:25:56.240 --> 01:25:59.720
So it feels like a human civilization has been going through a meaning crisis

01:25:59.720 --> 01:26:01.920
of different flavors throughout its history.

01:26:02.240 --> 01:26:08.480
Why is, how is this particular meaning crisis different, or is it really a

01:26:08.480 --> 01:26:10.240
crisis and it wasn't previously?

01:26:10.480 --> 01:26:11.280
What's your sense?

01:26:11.560 --> 01:26:14.720
A lot of human history, there wasn't so much a meaning crisis.

01:26:14.720 --> 01:26:18.120
There was just a like food and not getting eaten by bears crisis, right?

01:26:18.600 --> 01:26:21.320
Once you get to a point where you can make food, there was the, like, not

01:26:21.320 --> 01:26:23.200
getting killed by other humans crisis.

01:26:23.560 --> 01:26:28.040
So sitting around wondering what is it all about is actually a relatively recent luxury.

01:26:28.640 --> 01:26:33.640
Um, and the, and to some extent the meaning crisis coming out of that is precisely

01:26:33.640 --> 01:26:38.880
because, well, not precisely because I believe that meaning is the consequence

01:26:38.880 --> 01:26:45.280
of, um, when we make consequential decisions, it's tied to agency, right?

01:26:45.520 --> 01:26:48.960
When we make consequential decisions, that generates meaning.

01:26:49.720 --> 01:26:53.440
So if we make a lot of decisions, but we don't see the consequences of them, then

01:26:53.440 --> 01:26:55.360
it feels like, what was the point, right?

01:26:55.400 --> 01:26:57.880
But if there's all these big things happening, but we're just along for the

01:26:57.880 --> 01:27:01.280
ride, then it also does not feel very meaningful meaning.

01:27:01.320 --> 01:27:06.360
As far as I can tell, this is my working definition of Serga 2021 is, uh, generally

01:27:06.440 --> 01:27:10.600
the result of a person making a consequential decision, acting on it, and

01:27:10.600 --> 01:27:11.680
then seeing the consequences of it.

01:27:12.120 --> 01:27:17.080
So historically, um, just when humans are in survival mode, you're making

01:27:17.080 --> 01:27:18.560
consequential decisions all the time.

01:27:19.440 --> 01:27:22.760
So there's not a lack of meaning because like you either got eaten or you didn't.

01:27:23.240 --> 01:27:23.400
Right.

01:27:23.400 --> 01:27:24.560
You got some food and that's great.

01:27:24.560 --> 01:27:25.200
You feel good.

01:27:25.520 --> 01:27:30.480
Like these are all consequential decisions only in, you know, the post

01:27:30.960 --> 01:27:33.000
fossil fuel and industrial revolution.

01:27:33.680 --> 01:27:36.240
Could we create a massive leisure class?

01:27:36.880 --> 01:27:40.320
I could sit around, not being threatened by bears, not starving to death.

01:27:41.120 --> 01:27:46.560
Um, making and making decisions somewhat, but a lot of times not making, not

01:27:46.560 --> 01:27:50.520
seeing the consequences of any decisions they make the general sort of sense of

01:27:50.520 --> 01:27:54.960
anomie, I think that was the French term for it in the, in the wake of the consumer

01:27:54.960 --> 01:28:00.240
society, in the wake of mass mass media, telling everyone, Hey, you know, choosing

01:28:00.240 --> 01:28:03.120
between Hermes and Chanel is a meaningful decision.

01:28:03.160 --> 01:28:03.800
No, it's not.

01:28:04.280 --> 01:28:10.720
It's a high end luxury, um, purses and crap like that.

01:28:10.960 --> 01:28:14.760
But the point is that we, we give people the idea that consumption is meaning

01:28:15.120 --> 01:28:19.720
that making a choice of this team versus that team spectating has meaning.

01:28:20.160 --> 01:28:25.080
So we produce all of these different things that are as if meaning, right.

01:28:25.320 --> 01:28:28.400
But really making a decision that has no consequences for us.

01:28:28.920 --> 01:28:31.040
And so that creates the meaning crisis.

01:28:31.040 --> 01:28:33.960
Well, you're saying, uh, choosing between Chanel and the other one

01:28:33.960 --> 01:28:35.240
isn't has no consequence.

01:28:35.440 --> 01:28:38.240
I mean, why is one more meaningful than the other?

01:28:38.280 --> 01:28:39.640
It's not that it's more meaningful than the other.

01:28:39.640 --> 01:28:43.880
It's that you make a decision between these two brands and you're told this

01:28:43.880 --> 01:28:45.960
brand will make me look better in front of other people.

01:28:45.960 --> 01:28:50.080
If I buy this brand of car, if I wear that brand of apparel, right.

01:28:50.200 --> 01:28:54.920
The idea, like a lot of decisions we make are around consumption, but consumption

01:28:54.920 --> 01:28:56.760
by itself doesn't actually yield meaning.

01:28:57.160 --> 01:28:59.800
Gaining social status does provide meaning.

01:29:00.000 --> 01:29:05.960
So that's why in this era of, um, abundant production, we, uh, so many

01:29:05.960 --> 01:29:07.200
things turn into status games.

01:29:07.200 --> 01:29:10.840
Then the NFT kind of explosion is a similar kind of thing everywhere.

01:29:10.840 --> 01:29:15.280
There are status games because, you know, we just have so much excess production.

01:29:15.920 --> 01:29:18.400
Um, aren't those status games a source of meaning?

01:29:18.400 --> 01:29:23.440
Like what, why do the games we play have to be grounded in physical reality?

01:29:23.440 --> 01:29:25.840
Like they are when you're trying to run away from lions.

01:29:26.160 --> 01:29:30.360
Why can't we in this virtual reality world on social media?

01:29:30.360 --> 01:29:32.200
Why can't we play the games on social media?

01:29:32.200 --> 01:29:33.320
Even the dark ones.

01:29:33.360 --> 01:29:33.600
Right.

01:29:33.600 --> 01:29:37.360
We can, but you're saying that's creating a meaning crisis.

01:29:37.680 --> 01:29:39.600
Well, there's a meaning crisis in that.

01:29:39.640 --> 01:29:40.840
There's two aspects of it.

01:29:41.080 --> 01:29:45.160
Number one, playing those kinds of status games, uh, oftentimes

01:29:45.160 --> 01:29:46.720
requires destroying the planet.

01:29:47.360 --> 01:29:53.520
Because, um, it's, it's, it ties to consumption, consuming the latest and

01:29:53.520 --> 01:29:57.760
greatest version of a thing, buying the latest limited edition sneaker and

01:29:57.760 --> 01:30:00.520
throwing out all the old ones, maybe it keeps them the old ones, but the amount

01:30:00.520 --> 01:30:04.560
of sneakers we have to cut up and destroy every year to create artificial

01:30:04.560 --> 01:30:05.880
scarcity for the next generation.

01:30:05.880 --> 01:30:06.120
Right.

01:30:06.400 --> 01:30:08.280
This is kind of stuff that's not great.

01:30:08.480 --> 01:30:09.320
It's not great at all.

01:30:09.840 --> 01:30:15.000
Um, so can speakers consumption, uh, fueling status games is really

01:30:15.000 --> 01:30:16.800
bad for the planet, not sustainable.

01:30:17.160 --> 01:30:21.320
The second thing is you can play these kinds of status games, but then what

01:30:21.320 --> 01:30:24.680
it does is it renders you captured to the virtual environment.

01:30:25.360 --> 01:30:28.360
The status games that really wealthy people are playing are all around

01:30:28.360 --> 01:30:31.440
the hard resources where they're going to build the factories.

01:30:31.440 --> 01:30:34.240
They'd have the fuel in the rare earths to make the next generation of robots.

01:30:34.240 --> 01:30:35.400
They're then going to one game.

01:30:35.640 --> 01:30:37.680
So one circles around you and your, your children.

01:30:38.160 --> 01:30:40.840
So that's another reason not to play those virtual status games.

01:30:41.080 --> 01:30:46.720
So you're saying ultimately the, the, the big picture game is one by people who

01:30:46.720 --> 01:30:50.520
have access or control over actual hard resources, so you can't.

01:30:51.320 --> 01:30:56.840
You don't see a society where most of the games are played in the virtual space.

01:30:56.840 --> 01:30:58.400
Don't be captured in the physical space.

01:30:58.400 --> 01:30:59.480
It's it all, it all builds.

01:30:59.520 --> 01:31:01.280
It's just like the stat, the stack of human being.

01:31:01.640 --> 01:31:02.000
Right.

01:31:02.360 --> 01:31:07.920
If you only play the game at the cultural and intellectual level and the people,

01:31:07.920 --> 01:31:12.320
the hard resources and access to layer zero physical are going to own you.

01:31:12.960 --> 01:31:17.400
But isn't money not connected to or less and less connected to hard resources

01:31:17.400 --> 01:31:18.760
and money still seems to work.

01:31:18.920 --> 01:31:20.160
It's a virtual technology.

01:31:20.560 --> 01:31:21.560
There's different kinds of money.

01:31:22.360 --> 01:31:26.440
Part of the reason that some of the stuff is able to go a little unhinged is

01:31:26.440 --> 01:31:33.320
because of the, the, the, the big sovereignty is where one spends money

01:31:33.320 --> 01:31:37.680
and uses money and plays money games and inflates money their, their ability to

01:31:37.720 --> 01:31:42.200
adjudicate the physical resources and hard resources on land and things like that.

01:31:42.360 --> 01:31:44.240
Those have not been challenged in a very long time.

01:31:45.560 --> 01:31:47.640
So, you know, we went off the gold standard.

01:31:47.680 --> 01:31:50.600
Most money is not connected to physical resources.

01:31:51.520 --> 01:31:57.680
Uh, it's an idea and that idea is very closely connected to status.

01:31:58.840 --> 01:32:02.920
Um, so why, but it's also tied to, like, it's actually tied to law.

01:32:02.960 --> 01:32:04.880
It's, it is tied to some physical hard things.

01:32:04.880 --> 01:32:06.000
So you have to pay your taxes.

01:32:06.080 --> 01:32:06.560
Yes.

01:32:06.600 --> 01:32:10.800
So it's always at the end going to be connected to the, uh, the

01:32:10.840 --> 01:32:12.720
blockchain of physical reality.

01:32:12.760 --> 01:32:18.160
So in the case of law and taxes is connected to government and, uh,

01:32:18.200 --> 01:32:20.640
government is what violence is.

01:32:20.640 --> 01:32:28.680
The, I'm playing with devil's advocates here, uh, and popping one

01:32:28.680 --> 01:32:32.280
devil off the stack at a time isn't ultimately, of course, it'll be connected

01:32:32.280 --> 01:32:35.720
to physical reality, but just because people control the physical reality,

01:32:35.720 --> 01:32:40.120
it doesn't mean the status LeBron James in theory could make more money than

01:32:40.160 --> 01:32:43.000
the owners of the teams in theory.

01:32:43.440 --> 01:32:45.040
And to me, that's a virtual idea.

01:32:45.040 --> 01:32:49.360
So somebody else constructed a game and now you're playing in the space of virtual,

01:32:49.720 --> 01:32:51.560
uh, in the virtual space of the game.

01:32:51.880 --> 01:32:57.200
And so it just feels like there could be games where status, we build realities

01:32:57.560 --> 01:32:59.840
that give us meaning in the virtual space.

01:33:00.080 --> 01:33:02.800
Like I can imagine such things being possible.

01:33:02.920 --> 01:33:03.520
Oh yeah.

01:33:03.520 --> 01:33:03.760
Okay.

01:33:03.760 --> 01:33:04.520
So I see what you're saying.

01:33:04.520 --> 01:33:07.360
I think I see what you're saying there with the idea there.

01:33:07.400 --> 01:33:10.720
I mean, we'll take the LeBron James side and put in like some YouTube influencer.

01:33:10.800 --> 01:33:11.440
Yes, sure.

01:33:11.440 --> 01:33:11.640
Right.

01:33:11.640 --> 01:33:16.800
So the YouTube influencer, it is status games, but at a certain level, it

01:33:16.800 --> 01:33:20.480
precipitates into real dollars and into like, well, you look at Mr.

01:33:20.480 --> 01:33:21.240
Beast, right?

01:33:21.280 --> 01:33:23.960
He's like sending off half a million dollars worth of fireworks or

01:33:23.960 --> 01:33:24.600
something, right?

01:33:24.600 --> 01:33:25.320
Not a YouTube video.

01:33:25.800 --> 01:33:28.600
And also like saving, you know, like saving trees and so on.

01:33:28.600 --> 01:33:28.920
Sure.

01:33:28.920 --> 01:33:29.200
Right.

01:33:29.200 --> 01:33:31.040
They're trying to find a million trees with Mark Rober or whatever it was.

01:33:31.040 --> 01:33:31.280
Yeah.

01:33:31.320 --> 01:33:34.720
Like it's not that those kinds of games can't lead to real consequences.

01:33:35.440 --> 01:33:41.680
It's that for the vast majority of people in consumer culture, they are

01:33:41.680 --> 01:33:47.240
incented by the, I would say mostly I'm thinking about middle-class consumers.

01:33:47.800 --> 01:33:49.680
They're incented by advertisements.

01:33:49.680 --> 01:33:54.600
They're centered by their mimetic environment to treat the purchasing

01:33:54.600 --> 01:33:57.480
of certain things, the need to buy the latest model of whatever that

01:33:57.480 --> 01:34:02.720
need to appear, however, the need to pursue status games as a driver of

01:34:02.720 --> 01:34:03.280
meaning.

01:34:03.760 --> 01:34:06.600
And my point would be that it's a very hollow driver of meaning.

01:34:07.200 --> 01:34:11.040
And that is what creates a meaning crisis because at the end of the day,

01:34:11.320 --> 01:34:13.360
it's like eating a lot of empty calories, right?

01:34:13.360 --> 01:34:13.520
Yeah.

01:34:13.520 --> 01:34:16.440
It tasted good going down a lot of sugar, but man, it did not, it was not

01:34:16.440 --> 01:34:19.480
enough protein to help build your muscles and you kind of feel that in

01:34:19.480 --> 01:34:19.960
your gut.

01:34:20.520 --> 01:34:23.320
And I think that's, I mean, all this stuff aside and setting aside our

01:34:23.320 --> 01:34:26.880
discussion on currency, which I hope we get back to, that's what I mean

01:34:27.080 --> 01:34:31.040
about the meaning crisis, part of it being created by the fact that we

01:34:31.040 --> 01:34:36.280
don't, we're not encouraged to have more and more direct relationships.

01:34:36.560 --> 01:34:42.640
We're actually alienated from relating to even our family members sometimes,

01:34:42.640 --> 01:34:42.840
right?

01:34:42.840 --> 01:34:46.120
We're encouraged to relate to brands.

01:34:46.440 --> 01:34:50.520
We're encouraged to relate to these kinds of things that then tell us to

01:34:51.280 --> 01:34:53.280
do things that are really of low consequence.

01:34:53.720 --> 01:34:55.280
And that's where the meaning crisis comes from.

01:34:55.400 --> 01:34:59.200
So the role of technology in this, so there's somebody you mentioned, Jacques

01:34:59.880 --> 01:35:06.040
Eliel, his view of technology, he warns about the towering piles of

01:35:06.040 --> 01:35:09.760
technique, which I guess is a broad idea of technology.

01:35:09.760 --> 01:35:10.160
Yes.

01:35:10.200 --> 01:35:15.600
So I think, correct me if I'm wrong for him, technology is bad, it's

01:35:15.600 --> 01:35:18.840
moving away from human nature and it's ultimately is destructive.

01:35:19.280 --> 01:35:22.600
My question broadly speaking in this meaning crisis, can technology, what

01:35:22.600 --> 01:35:24.000
are the pros and cons of technology?

01:35:24.320 --> 01:35:25.280
Can it be a good?

01:35:25.520 --> 01:35:26.560
Yeah, I think it can be.

01:35:26.840 --> 01:35:31.040
I certainly draw on some of the little ideas and I think some of them are

01:35:31.080 --> 01:35:37.160
pretty good, but the way he defines technique is, well, also Simondon as

01:35:37.160 --> 01:35:41.640
well, I mean, he speaks to the general mentality of efficiency, homogenized

01:35:41.640 --> 01:35:45.240
processes, homogenized production, homogenized labor to produce homogenized

01:35:45.560 --> 01:35:52.720
artifacts that then are not actually, they don't sit well in the environment.

01:35:52.800 --> 01:35:58.240
It's essentially, you can think of it as the antonym of craft, whereas a

01:35:58.240 --> 01:36:03.040
craftsman will come to a problem, maybe a piece of wood and they need to

01:36:03.040 --> 01:36:06.560
make into a chair, it may be a site to build a house or build a stable or

01:36:06.560 --> 01:36:12.320
build whatever, and they will consider how to bring various things in to

01:36:12.320 --> 01:36:18.640
build something well contextualized that's in right relationship with

01:36:18.640 --> 01:36:19.360
that environment.

01:36:20.240 --> 01:36:24.680
But the way we have driven technology over the last 100, 150 years is not

01:36:24.680 --> 01:36:31.440
that at all, it is how can we make sure the input materials are homogenized,

01:36:31.480 --> 01:36:36.360
cut to the same size, diluted and doped to exactly the right alloy concentrations?

01:36:36.800 --> 01:36:39.040
How do we create machines that then consume exactly the right kind of

01:36:39.040 --> 01:36:42.800
energy to be able to run at this high speed to stamp out the same parts, which

01:36:42.800 --> 01:36:46.200
then go out the door, everyone gets the same tickle me Elmo and the reason why

01:36:46.200 --> 01:36:49.520
everyone wants it is because we have broadcast that tells everyone this is

01:36:49.520 --> 01:36:50.360
the cool thing.

01:36:50.520 --> 01:36:52.600
So homogenized demand, right?

01:36:52.600 --> 01:36:56.480
And we're like Baudrillard and other critiques of modernity coming from that

01:36:56.480 --> 01:37:01.440
direction, the situation list as well, it's that their point is that at this

01:37:01.440 --> 01:37:05.760
point in time, consumption is the thing that drives a lot of the economic stuff,

01:37:05.760 --> 01:37:08.840
not the need, but the need to consume and build status games on top.

01:37:09.400 --> 01:37:13.760
So we have homogenized, when we discovered, I think this is really like

01:37:13.800 --> 01:37:14.800
Bernays and stuff, right?

01:37:14.840 --> 01:37:19.400
In the early 20th century, we discovered we can create, we can create demand, we

01:37:19.400 --> 01:37:24.400
can create desire in a way that was not possible before because of broadcast

01:37:24.400 --> 01:37:29.000
media and one, not only do we create desire, we don't create desire for each

01:37:29.000 --> 01:37:32.040
person to connect to some bespoke thing, to build a relationship with their

01:37:32.040 --> 01:37:33.200
neighbor or their spouse.

01:37:33.640 --> 01:37:35.880
We are telling them, you need to consume this brand.

01:37:36.120 --> 01:37:37.240
You need to drive this vehicle.

01:37:37.240 --> 01:37:38.320
You got to listen to this music.

01:37:38.320 --> 01:37:39.200
Have you heard this?

01:37:39.400 --> 01:37:40.240
Have you seen this movie?

01:37:40.280 --> 01:37:40.600
Right?

01:37:40.840 --> 01:37:45.640
So creating homogenized demand makes it really cheap to create homogenized

01:37:45.640 --> 01:37:48.280
product and now you have economics of scale.

01:37:48.480 --> 01:37:52.600
So we make the same tickle me Elmo, give it to all the kids and all the kids are

01:37:52.600 --> 01:37:53.800
like, Hey, I got a tickle me Elmo.

01:37:53.800 --> 01:37:54.080
Right?

01:37:54.360 --> 01:38:00.720
So this is ultimately where this ties in then to runaway hyper-capitalism is

01:38:00.720 --> 01:38:05.480
that we then have capitalism is always looking for growth, it's always looking

01:38:05.480 --> 01:38:09.120
for growth and growth only happens to the margins, so you have to squeeze more

01:38:09.120 --> 01:38:11.560
and more demand out, you got to make it cheaper and cheaper to make the same

01:38:11.560 --> 01:38:15.040
thing, but tell everyone they're still getting meaning from it.

01:38:15.080 --> 01:38:17.120
You're still like, this is still your tickle me Elmo.

01:38:17.480 --> 01:38:17.840
Right?

01:38:18.160 --> 01:38:21.680
And we, we see little bits of this dripping critiques of this

01:38:21.680 --> 01:38:22.800
dripping in popular culture.

01:38:22.800 --> 01:38:26.960
You see it sometimes it's when Buzz Lightyear walks into the thing.

01:38:26.960 --> 01:38:30.600
He's like, Oh my God, at the toy store, I'm just a toy.

01:38:30.640 --> 01:38:33.400
Like there's millions of other, or there's hundreds of other buzz light years,

01:38:33.400 --> 01:38:34.200
just like me, right?

01:38:34.760 --> 01:38:40.000
That is, I think, you know, a fun Pixar critique on this homogenization dynamic.

01:38:40.120 --> 01:38:42.920
I agree with you on most of the things you're saying.

01:38:42.920 --> 01:38:49.000
So I'm playing devil's advocate here, but you know, this homogenized machine

01:38:49.040 --> 01:38:55.880
of capitalism is also the thing that is able to fund if channeled correctly

01:38:56.240 --> 01:39:01.760
innovation, invention and development of totally new things that in the best

01:39:01.760 --> 01:39:05.440
possible world, create all kinds of new experiences that can enrich lives.

01:39:06.280 --> 01:39:09.840
Um, the quality of lives for, uh, all kinds of people.

01:39:09.840 --> 01:39:15.520
So isn't this the machine that actually enables the experiences and more and

01:39:15.520 --> 01:39:17.560
more experiences that would then give meaning.

01:39:18.640 --> 01:39:20.680
It has done that to some extent.

01:39:21.040 --> 01:39:23.960
I mean, it's not all good or bad in my perspective.

01:39:24.400 --> 01:39:28.720
You know, we can always look backwards and offer a critique of the path we've

01:39:28.720 --> 01:39:30.520
taken to get to this point in time.

01:39:31.360 --> 01:39:34.320
Um, but that's a different, that's somewhat different and informs the

01:39:34.320 --> 01:39:38.120
discussion, um, but it's somewhat different than the question of where

01:39:38.120 --> 01:39:39.640
do we go in the future, right?

01:39:40.120 --> 01:39:43.400
Um, is this still the same rocket we need to ride to get to the next point?

01:39:43.400 --> 01:39:44.520
We'll even get us to the next point.

01:39:44.520 --> 01:39:46.200
Well, how does this, so you're predicting the future.

01:39:46.200 --> 01:39:48.040
How does it go wrong in your view?

01:39:48.720 --> 01:39:54.280
We have the mechanisms we have now explored enough technologies to where

01:39:54.320 --> 01:40:00.720
we can actually, I think sustainably produce, um, what most people in

01:40:00.720 --> 01:40:02.240
the world need to live.

01:40:03.360 --> 01:40:09.560
We have also created the infrastructures to allow continued research and

01:40:09.560 --> 01:40:14.320
development of additional science and medicine and various other kinds of things.

01:40:15.080 --> 01:40:20.120
Um, the organizing principles that we use to govern all these things today,

01:40:20.880 --> 01:40:26.440
um, have been, a lot of them have been just inherited from honestly,

01:40:26.440 --> 01:40:30.960
medieval times, um, some of them have refactored a little bit in the industrial

01:40:30.960 --> 01:40:38.120
era, but a lot of these modes of organizing people are deeply problematic.

01:40:38.680 --> 01:40:44.800
Um, and furthermore, they're rooted in, uh, I think a very industrial mode

01:40:45.360 --> 01:40:49.720
perspective on human labor, and this is one of those things I'm going to go back

01:40:49.720 --> 01:40:54.200
to the open source thing, there was a point in time when, well, let me ask you

01:40:54.200 --> 01:40:58.560
this, if you look at the core SciPy sort of collection of libraries, that's

01:40:58.560 --> 01:41:00.640
SciPy, NumPy, Matplotlib, right?

01:41:00.640 --> 01:41:01.640
There's iPython notebook.

01:41:01.680 --> 01:41:02.680
Let's throw pandas in there.

01:41:02.720 --> 01:41:04.280
Scikit-learn a few of these things.

01:41:05.080 --> 01:41:09.960
Um, how much value do you think economic value would you say

01:41:09.960 --> 01:41:11.240
they drive in the world today?

01:41:13.040 --> 01:41:16.760
That's one of the fascinating things about talking to you and Travis is like,

01:41:17.720 --> 01:41:19.360
it, it's, it's a measure.

01:41:19.360 --> 01:41:23.240
It's like, uh, at least a billion dollars a day, maybe billion dollars.

01:41:23.240 --> 01:41:23.560
Sure.

01:41:23.560 --> 01:41:26.800
I mean, it's, it's like, it's similar question of like, how much value

01:41:26.800 --> 01:41:28.080
does Wikipedia create?

01:41:28.440 --> 01:41:28.800
Right.

01:41:29.120 --> 01:41:32.080
It's like all of it.

01:41:32.120 --> 01:41:32.520
I don't know.

01:41:32.640 --> 01:41:35.960
Well, I mean, if you look at our systems, when you do a Google search, right.

01:41:35.960 --> 01:41:39.200
Now some of that stuff runs through TensorFlow, but when you look at, you

01:41:39.200 --> 01:41:43.040
know, Siri, when you do credit card transaction fraud, like just everything,

01:41:43.040 --> 01:41:46.560
right, every intelligence agency under the sun, they're using some aspect of

01:41:46.560 --> 01:41:47.520
these kinds of tools.

01:41:47.520 --> 01:41:51.480
So I would say that, um, these create billions of dollars of value.

01:41:51.520 --> 01:41:54.080
Oh, you mean like direct use of tools that leverage this?

01:41:54.080 --> 01:41:54.720
Yeah.

01:41:54.720 --> 01:41:54.840
Yeah.

01:41:54.840 --> 01:41:56.240
Even that's billions a day.

01:41:56.240 --> 01:41:56.440
Yeah.

01:41:56.560 --> 01:41:56.840
Yeah.

01:41:56.880 --> 01:41:57.160
Right.

01:41:57.160 --> 01:41:57.560
Easily.

01:41:57.960 --> 01:42:00.640
I think like the things they could not do if they didn't have these tools.

01:42:00.680 --> 01:42:00.960
Right.

01:42:01.040 --> 01:42:01.320
Yes.

01:42:01.440 --> 01:42:04.000
So that's billions of dollars a day.

01:42:04.000 --> 01:42:04.400
Great.

01:42:04.800 --> 01:42:05.640
I think that's about right.

01:42:05.680 --> 01:42:09.880
Now, if we take how many people did take to make that, right.

01:42:09.880 --> 01:42:12.360
And there was a point in time, not anymore, but there was a point in time

01:42:12.360 --> 01:42:13.400
when they could fit in a van.

01:42:13.440 --> 01:42:15.320
I could have fit them in my, in my Mercedes Sminder.

01:42:15.360 --> 01:42:15.600
Right.

01:42:16.080 --> 01:42:21.240
And so if you look at that, like, holy crap, literally a van of maybe a dozen

01:42:21.240 --> 01:42:27.800
people could create value to the tune of billions of dollars a day.

01:42:28.240 --> 01:42:29.680
What lesson do you draw from that?

01:42:29.960 --> 01:42:31.000
Well, here's the thing.

01:42:31.280 --> 01:42:34.000
What can we do to do more of that?

01:42:35.120 --> 01:42:36.240
Like that's open source.

01:42:36.240 --> 01:42:41.480
The way I've talked about this in other, um, environments is when we use generative

01:42:41.520 --> 01:42:48.680
participatory crowdsourced approaches, we unlock human potential at a level that

01:42:48.680 --> 01:42:50.600
is better than what capitalism can do.

01:42:52.160 --> 01:42:56.760
I would challenge, you know, anyone to go and try to hire the right 12 people in

01:42:56.760 --> 01:43:01.640
the world to build that entire stack the way those 12 people did that.

01:43:01.920 --> 01:43:02.240
Right.

01:43:02.440 --> 01:43:04.080
They would be very, very hard to press to do that.

01:43:04.120 --> 01:43:08.200
If a hedge fund could just hire a dozen people and create like something that is

01:43:08.200 --> 01:43:11.480
worth billions of dollars a day, every single one of them will be racing to do it.

01:43:11.480 --> 01:43:11.760
Right.

01:43:12.320 --> 01:43:15.520
But finding the right people, fostering the right collaborations, getting it

01:43:15.520 --> 01:43:17.600
adopted by the right other people to then refine it.

01:43:18.040 --> 01:43:20.680
That is a thing that was organic in nature.

01:43:20.680 --> 01:43:22.120
That, that took crowdsourcing.

01:43:22.120 --> 01:43:25.600
That took a lot of the open source ethos and it took the right kinds of people.

01:43:26.040 --> 01:43:26.400
Right.

01:43:26.400 --> 01:43:29.200
Now those people who started that said, I need to have a part of a

01:43:29.200 --> 01:43:32.080
multi-billion dollar a day sort of enterprise.

01:43:32.400 --> 01:43:34.840
They're like, I'm doing this cool thing to solve my problem for my friends.

01:43:34.880 --> 01:43:35.120
Right.

01:43:35.440 --> 01:43:40.480
So the point of telling the story is to say that our way of thinking about value,

01:43:40.720 --> 01:43:43.960
our way of thinking about allocation of resources, our ways of thinking about

01:43:44.000 --> 01:43:48.760
property rights and all these kinds of things, they come from finite game,

01:43:48.800 --> 01:43:51.120
scarcity mentality, medieval institutions.

01:43:52.120 --> 01:43:57.000
As we are now entering to some extent, we are sort of in a post-scarcity era,

01:43:57.040 --> 01:43:59.080
although some people are hoarding a whole lot of stuff.

01:43:59.760 --> 01:44:03.520
We are at a point where if not now, soon we'll be in a post-scarcity era.

01:44:03.880 --> 01:44:08.000
The question of how we allocate resources has to be revisited at a fundamental

01:44:08.000 --> 01:44:12.600
level because the kind of software these people built, the modalities that those

01:44:13.000 --> 01:44:18.200
human ecologies that built that software, it treats offers unproperty actually

01:44:18.200 --> 01:44:22.360
sharing creates value, restricting and forking reduces value.

01:44:23.000 --> 01:44:26.960
So that's different than any other physical resource that we've ever dealt with.

01:44:26.960 --> 01:44:30.840
It's different than how most corporations treat software IP, right?

01:44:31.200 --> 01:44:36.960
So if treating software in this way created this much value, so efficiently,

01:44:36.960 --> 01:44:40.800
so cheaply, because feeding a dozen people for 10 years is really cheap, right?

01:44:41.560 --> 01:44:45.440
That's the, that's the reason I care about this right now is because looking

01:44:45.440 --> 01:44:50.040
forward when we can automate a lot of labor where we can, in fact, the, the

01:44:50.040 --> 01:44:53.360
programming for your robot in your part, neck of the woods and your part of the

01:44:53.360 --> 01:44:57.560
Amazon to build something sustainable for you and your tribe to deliver the right

01:44:57.600 --> 01:44:59.400
medicines to take care of the kids.

01:45:00.200 --> 01:45:01.960
That's just software.

01:45:01.960 --> 01:45:05.280
That's just code that could be totally open-sourced, right?

01:45:05.400 --> 01:45:10.880
So we can actually get to a mode where all of this additional generative things

01:45:10.880 --> 01:45:15.920
that humans are doing, they, they don't have to be wrapped up in a container.

01:45:16.200 --> 01:45:18.680
And then we charge for all the exponential dynamics out of it.

01:45:19.120 --> 01:45:20.320
That's what Facebook did.

01:45:20.360 --> 01:45:22.360
That's what modern social media did, right?

01:45:22.360 --> 01:45:24.640
Cause the old internet was connecting people just fine.

01:45:25.000 --> 01:45:26.880
Facebook came along and said, well, anyone can post a picture.

01:45:26.880 --> 01:45:29.960
Anyone can post some text and we're going to amplify the crap out of it

01:45:29.960 --> 01:45:34.520
to everyone else and it exploded this generative network of human interaction.

01:45:34.680 --> 01:45:35.960
And then I said, how do I make money off that?

01:45:36.000 --> 01:45:36.320
Oh yeah.

01:45:36.320 --> 01:45:39.120
I'm going to be a gatekeeper on everybody's attention.

01:45:39.800 --> 01:45:40.920
And that's how I'm going to make money.

01:45:40.960 --> 01:45:45.080
So how do we create a more than one van?

01:45:45.560 --> 01:45:48.760
How do we have millions of vans full of people that create NumPy,

01:45:48.760 --> 01:45:50.640
SciPy, they create Python.

01:45:50.920 --> 01:45:56.080
So, you know, the story of those people is often they have some kind of job

01:45:56.080 --> 01:45:56.960
outside of this.

01:45:56.960 --> 01:45:58.400
This is what they're doing for fun.

01:45:58.800 --> 01:46:00.280
Don't you need to have a job?

01:46:00.840 --> 01:46:04.880
Don't you have to be connected, plugged in to the capitalist system?

01:46:04.880 --> 01:46:11.680
Isn't that what like, um, isn't this consumerism, the engine that results

01:46:12.160 --> 01:46:15.880
in the individuals that kind of take a break from it every once in a while to

01:46:15.880 --> 01:46:18.760
create something magical, like at the edges, right?

01:46:18.760 --> 01:46:20.280
The question of surplus, right?

01:46:20.280 --> 01:46:21.320
This is the, this is the question.

01:46:21.320 --> 01:46:25.040
Like if everyone were to go and run their own farm, no one would have time

01:46:25.040 --> 01:46:26.520
to go and write NumPy, SciPy, right?

01:46:27.280 --> 01:46:29.920
Maybe, but that's, that's what I'm talking about.

01:46:29.920 --> 01:46:34.360
When I say we're maybe at a post scarcity point for a lot of people, the

01:46:34.360 --> 01:46:39.440
question that we're never encouraged to ask in a Super Bowl ad is how much do

01:46:39.440 --> 01:46:41.600
you need, how much is enough?

01:46:41.960 --> 01:46:44.720
Do you need to have a new car every two years, every five?

01:46:45.040 --> 01:46:47.200
If you have a reliable car, can you drive one for 10 years?

01:46:47.200 --> 01:46:47.680
Is that all right?

01:46:48.080 --> 01:46:49.880
You know, I had a car for 10 years and it was fine.

01:46:50.120 --> 01:46:52.560
You know, your iPhone, do you have to upgrade every two years?

01:46:52.800 --> 01:46:56.520
I mean, it's sort of, you're using the same apps you did four years ago, right?

01:46:56.680 --> 01:46:58.120
This should be a Super Bowl ad.

01:46:58.320 --> 01:46:59.240
This should be a Super Bowl ad.

01:46:59.240 --> 01:46:59.680
That's great.

01:46:59.680 --> 01:47:03.520
Maybe somebody, maybe one of our listeners will, will fund something

01:47:03.520 --> 01:47:07.440
like this of like, no, but, but just actually bringing it back, bringing it

01:47:07.440 --> 01:47:12.840
back to actually the question of what do you need, how do we create the

01:47:12.840 --> 01:47:19.000
infrastructure for collectives of people to live on the basis of providing, you

01:47:19.000 --> 01:47:22.440
know, what we need, meeting people's needs with a little bit of access to

01:47:22.440 --> 01:47:23.840
handle emergencies and things like that.

01:47:24.440 --> 01:47:28.120
Pulling our resources together to handle the really, really big, you

01:47:28.120 --> 01:47:31.320
know, emergencies, somebody with a really rare catform cancer or some

01:47:31.320 --> 01:47:34.600
massive fire sweeps through, you know, half the, the village or whatever,

01:47:34.920 --> 01:47:42.520
but can we actually unscale things and solve for people's needs and then give

01:47:42.520 --> 01:47:46.880
them the capacity to explore how to be the best version of themselves.

01:47:47.360 --> 01:47:51.040
And for Travis, that was, you know, throwing away his shot of tenure

01:47:51.040 --> 01:47:52.160
in order to write NumPy.

01:47:52.880 --> 01:47:57.040
For others, it's, there is a saying in the, in the sci-fi community that,

01:47:57.040 --> 01:47:59.680
you know, sci-fi advances one failed postdoc at a time.

01:48:00.920 --> 01:48:03.800
And that's, you know, we can do these things.

01:48:03.800 --> 01:48:07.160
We can actually do this kind of collaboration because code, software

01:48:07.160 --> 01:48:09.040
information organization, that's cheap.

01:48:09.520 --> 01:48:12.040
That, that, those bits are very cheap to fling across the oceans.

01:48:13.000 --> 01:48:14.360
So you mentioned Travis.

01:48:14.760 --> 01:48:17.520
We've been talking and we'll continue to talk about open source.

01:48:19.400 --> 01:48:20.360
Maybe you can comment.

01:48:20.360 --> 01:48:21.520
How did you meet Travis?

01:48:21.560 --> 01:48:23.360
Who, who is Travis Alphont?

01:48:24.000 --> 01:48:27.200
What's, what's your relationship been like through the years?

01:48:27.760 --> 01:48:30.080
Oh, where did you work together?

01:48:30.080 --> 01:48:34.760
How did you meet what's the present and the future look like?

01:48:35.040 --> 01:48:35.320
Yeah.

01:48:35.320 --> 01:48:38.640
So the first time I met Travis was at a sci-fi conference in Pasadena.

01:48:39.280 --> 01:48:40.160
Do you remember the year?

01:48:40.840 --> 01:48:41.480
2005.

01:48:42.040 --> 01:48:44.800
I was working at, again, at nthought, you know, working on

01:48:44.800 --> 01:48:46.520
scientific computing consulting.

01:48:47.000 --> 01:48:53.840
And, um, a couple of years later, he joined us at nthought, I think 2007.

01:48:54.320 --> 01:48:59.000
Um, and he came in, uh, as the president, uh, the, the, one of the founders

01:48:59.000 --> 01:49:00.560
of nthought was the CEO, Eric Jones.

01:49:01.040 --> 01:49:04.040
Um, and we were all very excited that Travis was joining us.

01:49:04.040 --> 01:49:05.040
And that was, you know, great fun.

01:49:05.040 --> 01:49:09.120
And so I worked with Travis, um, on a number of consulting projects and we

01:49:09.160 --> 01:49:12.120
worked on, um, some open source stuff.

01:49:12.120 --> 01:49:14.680
I mean, it was just a really, it was a good, a good time there.

01:49:15.080 --> 01:49:17.720
And then it was primarily Python related.

01:49:17.800 --> 01:49:18.040
Oh yeah.

01:49:18.040 --> 01:49:20.240
It was all Python, NumPy, sci-fi consulting kind of stuff.

01:49:20.720 --> 01:49:24.440
Um, towards the end of that time, uh, we started getting called

01:49:24.440 --> 01:49:26.800
into more and more finance shops.

01:49:27.280 --> 01:49:29.440
Um, they were adopting Python pretty heavily.

01:49:29.840 --> 01:49:33.560
I did some work on like, uh, at a high frequency trading shop, um, working

01:49:33.560 --> 01:49:37.200
some stuff, and then we worked together on some, um, at a couple of

01:49:37.200 --> 01:49:39.200
investment banks in, in Manhattan.

01:49:39.840 --> 01:49:44.200
And so we started seeing that there was a potential to take Python in the

01:49:44.200 --> 01:49:47.000
direction of business computing more than just being this niche, like

01:49:47.000 --> 01:49:49.840
Matlab replacement for big vector computing.

01:49:50.520 --> 01:49:53.320
What we were seeing was, oh yeah, you could actually use Python as a Swiss

01:49:53.320 --> 01:49:56.520
army knife to do a lot of shadow data transformation kind of stuff.

01:49:56.840 --> 01:50:00.480
So that's when we realized the potential is much greater.

01:50:00.560 --> 01:50:03.800
And so, um, we started Anaconda, I mean, it was called

01:50:03.800 --> 01:50:07.120
Continuum Analytics at the time, but we started in January of 2012.

01:50:07.440 --> 01:50:11.520
With a vision of, um, shoring up the parts of Python that needed to get

01:50:11.760 --> 01:50:15.280
expanded to handle data at scale, to do web visualization, application

01:50:15.280 --> 01:50:16.600
development, et cetera.

01:50:17.160 --> 01:50:17.760
And that was that.

01:50:17.760 --> 01:50:17.960
Yeah.

01:50:17.960 --> 01:50:23.800
So he was CEO and I was president, um, for the first five years.

01:50:23.800 --> 01:50:28.920
And then, um, we raised some money and then the board sort of put in a new CEO.

01:50:28.920 --> 01:50:33.200
They hired a kind of professional CEO and then Travis, you laugh at that.

01:50:33.480 --> 01:50:35.200
Um, I took over the CTO role.

01:50:35.200 --> 01:50:39.840
Travis then left after a year to do his own thing, to do Quonsight, um, which

01:50:39.840 --> 01:50:43.760
was more oriented around some of the bootstrap years that we did at Continuum

01:50:43.840 --> 01:50:45.840
where it was, you know, open source and consulting.

01:50:46.120 --> 01:50:49.640
It wasn't sort of like gung-ho product development and it wasn't focused on,

01:50:49.920 --> 01:50:53.960
you know, we accidentally stumbled into the package management problem, um, at

01:50:53.960 --> 01:50:57.840
Anaconda, uh, but then we had a lot of other visions of other technology that

01:50:57.840 --> 01:51:02.600
we built in the open source and Travis was really trying to push, again, the

01:51:02.600 --> 01:51:06.400
frontiers of numerical computing, vector computing, handling things like

01:51:06.400 --> 01:51:09.480
auto-differentiation and stuff intrinsically in the open ecosystem.

01:51:09.880 --> 01:51:14.480
So I think that's the, the, you know, that's kind of the direction he's,

01:51:14.520 --> 01:51:17.560
he's, uh, working on and some of his, his work.

01:51:17.920 --> 01:51:22.440
Um, we remain great friends and, um, you know, and colleagues and collaborators,

01:51:22.440 --> 01:51:26.040
even though he's no longer a day to day, you know, working at Anaconda, but, um,

01:51:26.080 --> 01:51:28.520
he gives me a lot of feedback about, you know, this and that and the other.

01:51:28.960 --> 01:51:33.360
What's, uh, what's the big lesson you've learned from Travis about life or about

01:51:33.360 --> 01:51:35.040
programming or about leadership?

01:51:35.400 --> 01:51:35.720
Wow.

01:51:35.720 --> 01:51:36.400
There's a lot.

01:51:36.440 --> 01:51:37.080
There's a lot.

01:51:37.160 --> 01:51:39.040
Travis is a really, really good guy.

01:51:39.600 --> 01:51:41.880
He really, his heart is really in it.

01:51:41.880 --> 01:51:42.920
He cares a lot.

01:51:43.680 --> 01:51:44.280
Um,

01:51:44.720 --> 01:51:46.880
I've gotten that sense having to interact with them.

01:51:46.880 --> 01:51:47.640
It's so interesting.

01:51:47.640 --> 01:51:48.560
Such a good,

01:51:48.560 --> 01:51:49.680
he's a really good dude.

01:51:49.680 --> 01:51:51.320
And he, and I, you know, it's so interesting.

01:51:51.320 --> 01:51:54.720
We come from very different backgrounds where we're quite different as people.

01:51:55.240 --> 01:52:02.120
Um, but we, uh, I think we can like not talk for a long time and then, and then

01:52:02.120 --> 01:52:05.920
be on a conversation and be eye to eye on like 90% of things.

01:52:06.360 --> 01:52:10.400
And so he's someone who I believe no matter how much fog settles into the ocean,

01:52:10.600 --> 01:52:13.800
his ship, my ship are pointed sort of in the same direction of the same star.

01:52:14.080 --> 01:52:14.560
Wow.

01:52:14.560 --> 01:52:16.400
So that's a beautiful way to phrase it.

01:52:16.840 --> 01:52:20.000
No matter how much fog there is appointed at the same star.

01:52:20.360 --> 01:52:20.680
Yeah.

01:52:20.680 --> 01:52:21.840
And I hope he feels the same way.

01:52:21.880 --> 01:52:23.400
I mean, I hope he knows that over the years now.

01:52:23.440 --> 01:52:26.080
Um, we both care a lot about the community.

01:52:26.640 --> 01:52:28.880
Um, for someone who cares so deeply, I would say this about Travis.

01:52:28.880 --> 01:52:33.600
That's interesting for someone who cares so deeply about the nerd details of

01:52:33.600 --> 01:52:37.400
like type system design and vector computing and efficiency of expressing

01:52:37.400 --> 01:52:41.280
this and that and the other memory layouts and all that stuff, he cares even

01:52:41.280 --> 01:52:48.600
more about the people in the ecosystem, the community, and I have, um, a

01:52:48.600 --> 01:52:49.720
similar kind of alignment.

01:52:49.720 --> 01:52:51.760
I care a lot about the tech.

01:52:51.760 --> 01:52:52.640
I really do.

01:52:53.040 --> 01:52:59.760
But for me, the, the, the beauty of what this human ecology has produced is I

01:52:59.760 --> 01:53:02.640
think a touchstone, it's an early version.

01:53:02.640 --> 01:53:05.800
We can look at it and say, how do we replicate this for humanity at scale?

01:53:05.840 --> 01:53:08.280
What this open source collaboration was able to produce.

01:53:08.800 --> 01:53:12.680
How can we be generative in human collaboration moving forward and create

01:53:12.680 --> 01:53:14.520
that as a civilizational kind of dynamic?

01:53:15.120 --> 01:53:17.480
Like, can we seize this moment to do that?

01:53:17.480 --> 01:53:20.560
Cause like a lot of the other open source movements, it's all nerds

01:53:20.560 --> 01:53:21.880
nerding out on code for nerds.

01:53:21.920 --> 01:53:26.320
You know, um, and the, this, because it's scientists, because it's

01:53:26.320 --> 01:53:29.960
people working on data that all of it faces real human problems.

01:53:30.840 --> 01:53:33.960
Um, I think we have an opportunity to actually make a bigger impact.

01:53:34.440 --> 01:53:38.360
Is there a way for this kind of open source vision to make money?

01:53:39.120 --> 01:53:39.800
Absolutely.

01:53:40.160 --> 01:53:41.480
To fund the people involved.

01:53:41.680 --> 01:53:43.080
Is that an essential part of it?

01:53:43.120 --> 01:53:47.960
It's hard, but, but we're trying to do that in our own way, um, at Anaconda,

01:53:48.400 --> 01:53:51.120
uh, because we know that business users, as they use more of the stuff, they

01:53:51.120 --> 01:53:54.760
have needs that like business specific needs around security provenance.

01:53:55.080 --> 01:54:00.400
Um, you know, they really can't tell their VPs and their investors, Hey,

01:54:00.400 --> 01:54:02.720
we're having, you know, our data scientists are installing random

01:54:02.720 --> 01:54:05.600
packages from who, who knows where and running on customer data.

01:54:05.600 --> 01:54:06.800
So they have to have someone to talk to.

01:54:06.800 --> 01:54:07.920
And that's what Anaconda does.

01:54:08.320 --> 01:54:11.400
So we are, um, you know, a governed source of packages for them.

01:54:11.400 --> 01:54:11.920
And that's great.

01:54:11.920 --> 01:54:12.640
That makes some money.

01:54:13.080 --> 01:54:16.960
We take some of that and we just, um, take that as a dividend.

01:54:17.040 --> 01:54:19.120
We take a percentage of our revenues and write that as a dividend

01:54:19.120 --> 01:54:20.560
for the open source community.

01:54:20.960 --> 01:54:26.800
But beyond that, um, I really see the development of a marketplace for people

01:54:26.800 --> 01:54:31.560
to create notebooks, models, data sets, curation of these different kinds of

01:54:31.560 --> 01:54:36.960
things, um, and to really have a long tail marketplace dynamic with that.

01:54:38.000 --> 01:54:41.600
Can you speak about this problem that you stumbled into of package

01:54:41.600 --> 01:54:43.840
management, Python package management?

01:54:43.840 --> 01:54:44.840
What is that?

01:54:45.720 --> 01:54:50.240
Um, a lot of people speak very highly of condo, which is part of Anaconda,

01:54:50.280 --> 01:54:51.480
which is a package manager.

01:54:51.480 --> 01:54:52.560
There's a ton of packages.

01:54:53.120 --> 01:54:55.040
So first, what are package managers?

01:54:55.040 --> 01:54:57.240
And second, what was there before?

01:54:57.240 --> 01:55:01.640
What is PIP and why is condo more awesome?

01:55:01.800 --> 01:55:08.680
The package problem is this, which is that in order to do, um, numerical

01:55:08.680 --> 01:55:14.440
computing efficiently with Python, there are a lot of low level libraries

01:55:14.480 --> 01:55:18.000
that need to be compiled, compiled with a C compiler or C plus plus

01:55:18.000 --> 01:55:19.160
compiler, a Fortran compiler.

01:55:19.840 --> 01:55:21.880
They need to not just be compiled, but they need to be compiled with

01:55:21.880 --> 01:55:23.080
all of the right settings.

01:55:23.800 --> 01:55:26.800
And oftentimes those settings are tuned for specific chip architectures.

01:55:27.400 --> 01:55:32.160
And when you add GPUs to the mix, when you look at different operating systems,

01:55:32.240 --> 01:55:35.840
you know, you may be on the same chip, but if you're running Mac versus

01:55:36.200 --> 01:55:39.480
Linux versus windows on the same x86 chip, you compile link differently.

01:55:39.960 --> 01:55:45.880
All of this complexity is beyond the capability of most data scientists to

01:55:45.880 --> 01:55:50.080
reason about, and it's also beyond what most of the package developers

01:55:50.160 --> 01:55:53.040
want to deal with too, because if you're a package developer, you're like,

01:55:53.080 --> 01:55:54.960
I code on Linux, this works for me.

01:55:54.960 --> 01:55:55.400
I'm good.

01:55:55.760 --> 01:55:58.880
It is not my problem to figure out how to build this on an ancient version of

01:55:58.880 --> 01:55:59.960
windows, right?

01:55:59.960 --> 01:56:01.200
That's just simply not my problem.

01:56:01.840 --> 01:56:06.960
So what we end up with is we have a creator or create a, a very creative

01:56:06.960 --> 01:56:10.640
crowdsourced environment where people want to use this stuff, but they can't.

01:56:11.360 --> 01:56:16.720
And so we ended up creating a new set of technologies, like a build recipe

01:56:16.720 --> 01:56:24.200
system, a build system, and an installer system that is able to, well, to put

01:56:24.200 --> 01:56:28.440
it simply, it's able to build these packages correctly on each of these

01:56:28.440 --> 01:56:30.920
different kinds of platforms and operating systems and make it so when

01:56:30.920 --> 01:56:34.320
people want to install something, they can, it's just one command.

01:56:34.320 --> 01:56:37.640
They don't have to, you know, set up a big compiler system and do all these things.

01:56:38.240 --> 01:56:39.760
So when it works well, it works great.

01:56:40.400 --> 01:56:45.160
Now the difficulty is we have literally thousands of people writing code in the

01:56:45.160 --> 01:56:48.960
ecosystem, building all sorts of stuff and each person writing code, they may

01:56:48.960 --> 01:56:50.320
take a dependence on something else.

01:56:50.720 --> 01:56:54.120
And so all this web, incredibly complex web of dependencies.

01:56:54.760 --> 01:57:00.280
So installing the correct package for any given set of packages you want,

01:57:00.280 --> 01:57:03.680
getting that right sub graph is an incredibly hard problem.

01:57:04.560 --> 01:57:06.600
And again, most data scientists don't want to think about this.

01:57:06.600 --> 01:57:08.880
They're like, I want to install NumPy and pandas.

01:57:09.120 --> 01:57:11.640
I want this version of some like geospatial library.

01:57:11.640 --> 01:57:12.600
I want this other thing.

01:57:13.000 --> 01:57:13.920
Like, why is this hard?

01:57:13.920 --> 01:57:14.960
These exist, right?

01:57:15.960 --> 01:57:19.120
And it is hard because it's, well, you're installing this on a, on a

01:57:19.120 --> 01:57:20.360
version of windows, right?

01:57:20.680 --> 01:57:24.040
And half of these libraries are not built for windows or the

01:57:24.040 --> 01:57:25.040
latest version of the mailable.

01:57:25.040 --> 01:57:27.640
But the old version was, if you go to the old version of this library, that

01:57:27.640 --> 01:57:29.280
means you need to go to a different version of that library.

01:57:30.000 --> 01:57:35.320
And so the Python ecosystem, by virtue of being crowdsourced, we were able to

01:57:35.320 --> 01:57:39.920
fill a hundred thousand different niches, but then we also suffer this problem

01:57:40.280 --> 01:57:44.200
that because it's crowdsourced and no one, it's like a tragedy of the commons,

01:57:44.200 --> 01:57:44.400
right?

01:57:44.400 --> 01:57:48.720
No one really needs, wants to support their thousands of other dependencies.

01:57:49.120 --> 01:57:52.080
So we end up sort of having to do a lot of this.

01:57:52.080 --> 01:57:54.680
And of course the condo forge community also steps up as an open source

01:57:54.680 --> 01:57:57.000
community that, you know, maintains some of these recipes.

01:57:57.440 --> 01:57:58.560
That's what conda does.

01:57:58.600 --> 01:58:03.760
Now, pip is a tool that came along after conda to some extent, it came along as

01:58:03.760 --> 01:58:09.640
an easier way for the, for the Python developers writing Python code that

01:58:09.640 --> 01:58:14.280
didn't have as much compiled, you know, stuff, they could then install different

01:58:14.280 --> 01:58:14.880
packages.

01:58:15.280 --> 01:58:19.080
And what ended up happening in the Python ecosystem was that a lot of the core

01:58:19.080 --> 01:58:22.960
Python and web Python developers, they never ran into any of this compilation

01:58:22.960 --> 01:58:23.600
stuff at all.

01:58:24.080 --> 01:58:29.640
So even we have, you know, on video, we have Guido Van Rossum saying, you know

01:58:29.640 --> 01:58:32.840
what, the scientific community's packaging problems are just too exotic and different.

01:58:33.080 --> 01:58:34.520
I mean, you're talking about Fortran compilers, right?

01:58:34.920 --> 01:58:38.240
Like you guys just need to build your own solution perhaps, right?

01:58:38.880 --> 01:58:43.400
So the Python core Python community went and built its own sort of packaging

01:58:44.040 --> 01:58:48.760
technologies, not really contemplating the complexity of this stuff over here.

01:58:49.400 --> 01:58:53.160
And so now we have the challenge where you can pip install some things and some

01:58:53.160 --> 01:58:56.000
libraries, if you just want to get started with them, you can pip install

01:58:56.000 --> 01:58:57.240
TensorFlow and that works great.

01:58:57.480 --> 01:59:00.680
The instant you want to also install some other packages that use different

01:59:00.680 --> 01:59:05.760
versions of NumPy or some like graphics library or some OpenCV thing or some

01:59:05.760 --> 01:59:10.400
other thing, you now run into dependency hell because you cannot, you know, OpenCV

01:59:10.400 --> 01:59:13.960
can't have a different version of libjpeg over here than PyTorch over here.

01:59:14.240 --> 01:59:17.080
Like they actually, they all have to use the, if you want to use GPU acceleration,

01:59:17.360 --> 01:59:20.120
they have to always the same underlying drivers and same GPU CUDA things.

01:59:20.320 --> 01:59:24.960
So it's, it gets to be very gnarly and it's a level of technology that both the

01:59:24.960 --> 01:59:27.560
makers and the users don't really want to think too much about.

01:59:28.480 --> 01:59:30.400
And that's where you step in and try to solve this.

01:59:30.440 --> 01:59:31.160
We try to solve it.

01:59:32.040 --> 01:59:32.800
How much is that?

01:59:32.920 --> 01:59:35.920
I mean, you said that you don't want to think, they don't want to think about it,

01:59:35.920 --> 01:59:40.800
but how much is it a little bit on the developer and providing them tools to, to

01:59:40.800 --> 01:59:44.840
be a little bit more clear of that sub graph of dependency that's necessary?

01:59:44.880 --> 01:59:48.920
It is, it is getting to a point where we do have to think about, look, can we pull

01:59:48.920 --> 01:59:52.320
some of the most popular packages together and get them to work on a

01:59:52.320 --> 01:59:55.480
coordinated release timeline, get them to build against the same test matrix,

01:59:55.800 --> 01:59:56.840
et cetera, et cetera, right.

01:59:57.080 --> 02:00:00.360
And there is a little bit of dynamic around this, but again,

02:00:00.400 --> 02:00:01.680
it is a volunteer community.

02:00:02.600 --> 02:00:05.480
You know, people working on these different projects have their own

02:00:05.480 --> 02:00:07.080
timelines and their own things they're trying to meet.

02:00:07.520 --> 02:00:10.920
So we end up trying to pull these things together.

02:00:11.320 --> 02:00:14.880
And then it's, it's this incredibly, and I would recommend just as a business tip,

02:00:15.400 --> 02:00:19.360
don't ever go into business where when your hard work works, you're invisible.

02:00:19.560 --> 02:00:23.120
And when it breaks because of someone else's problem, you get flagged for it.

02:00:23.120 --> 02:00:25.200
Cause that's, that's in our situation, right?

02:00:25.560 --> 02:00:28.600
When something doesn't conda install properly, usually it's some upstream issue,

02:00:28.880 --> 02:00:30.160
but it looks like condas broken.

02:00:30.160 --> 02:00:33.640
It looks like, you know, anaconda screw something up when things do work though.

02:00:33.640 --> 02:00:34.480
It's like, oh yeah, cool.

02:00:34.480 --> 02:00:37.360
I just worked assuming naturally, of course, that's very easy to make that work.

02:00:37.520 --> 02:00:37.840
Right.

02:00:38.120 --> 02:00:43.360
So we end up in this kind of, um, problematic scenario, but, uh, but it's okay

02:00:43.360 --> 02:00:46.360
because I think we're still, um, you know, our hearts in the right place.

02:00:46.720 --> 02:00:49.160
We're trying to move this forward as a community sort of affair.

02:00:49.160 --> 02:00:51.720
I think most of the people in the community also appreciate the work we've

02:00:51.720 --> 02:00:55.720
done over the years to try to move these things forward in a, in a collaborative

02:00:55.720 --> 02:00:56.240
fashion.

02:00:56.360 --> 02:01:03.680
So one of the sub graphs of dependencies that, uh, became super complicated is

02:01:03.680 --> 02:01:05.720
the move from Python two to Python three.

02:01:05.720 --> 02:01:10.200
So there's all these ways to mess with these kinds of, uh, ecosystems of

02:01:10.200 --> 02:01:11.280
packages and so on.

02:01:11.480 --> 02:01:13.440
So I just want to ask you about that particular one.

02:01:13.720 --> 02:01:17.160
What do you think about the move from Python two to three?

02:01:17.960 --> 02:01:19.440
Uh, why did it take so long?

02:01:19.440 --> 02:01:23.880
What were, from your perspective, just seeing the packages all struggle in the

02:01:23.880 --> 02:01:25.800
community, all struggled through this process.

02:01:26.200 --> 02:01:27.760
What lessons do you take away from it?

02:01:27.760 --> 02:01:28.720
Why did it take so long?

02:01:29.400 --> 02:01:35.840
Looking back, some people perhaps underestimated how much adoption Python

02:01:35.840 --> 02:01:42.000
two had, I think, uh, some people also underestimated how much, or they

02:01:42.000 --> 02:01:46.240
overestimated how much value some of the new features in Python three really

02:01:46.240 --> 02:01:50.040
provided, like the things they really loved about Python three just didn't

02:01:50.040 --> 02:01:51.480
matter to some of these people on Python two.

02:01:52.440 --> 02:01:57.320
Cause this change was happening as Python, SciPy was starting to take off

02:01:57.360 --> 02:02:01.200
really like past, like a hockey stick of adoption in the early data science era

02:02:01.200 --> 02:02:05.360
in the early 2010s, a lot of people were learning and onboarding in whatever

02:02:05.360 --> 02:02:09.040
just worked and the teachers were like, well, yeah, these libraries I need are

02:02:09.040 --> 02:02:10.200
not supported on Python three yet.

02:02:10.240 --> 02:02:11.320
I'm going to teach you Python two.

02:02:11.880 --> 02:02:15.040
Took a lot of advocacy to get people to move over to Python three.

02:02:15.560 --> 02:02:19.400
So I think it wasn't any particular single thing, but it was one of those

02:02:19.480 --> 02:02:24.240
death by, you know, a dozen cuts, which just really made it hard to move off

02:02:24.240 --> 02:02:28.440
of Python two and also Python three itself, as they were kind of breaking

02:02:28.440 --> 02:02:30.680
things and changing these around and reorganizing the standard library,

02:02:30.680 --> 02:02:35.240
there's a lot of stuff that was happening there that kept giving people

02:02:35.240 --> 02:02:38.480
an excuse to say, I'll put off till the next version two is working fine

02:02:38.480 --> 02:02:39.240
enough for me right now.

02:02:39.600 --> 02:02:41.360
So I think that's essentially what happened there.

02:02:41.400 --> 02:02:47.920
And I will say this though, the strength of the Python data science movement,

02:02:48.600 --> 02:02:53.520
I think is what kept Python alive in that transition, because a lot of languages

02:02:53.520 --> 02:02:56.560
have died in left, left their user bases behind.

02:02:56.760 --> 02:03:00.720
If there wasn't the use of Python for data, there's a good chunk of Python

02:03:00.720 --> 02:03:04.800
users that during that transition would have just left for go and rust and state.

02:03:04.840 --> 02:03:08.200
In fact, some people did, they moved to go and rust and they just never look back.

02:03:08.800 --> 02:03:14.680
The fact that we were able to grow by the, by millions of users, the Python data

02:03:14.680 --> 02:03:18.040
community, that is what kept the momentum for Python going.

02:03:18.240 --> 02:03:23.800
And now the usage of Python for data is over 50% of the overall Python user base.

02:03:24.240 --> 02:03:27.880
So I will put, I will make, I'm happy to debate that on stage somewhere.

02:03:27.880 --> 02:03:30.280
If I con with someone, if they really want to take issue with that statement,

02:03:30.280 --> 02:03:32.280
but from my, where I sit, I think that's true.

02:03:32.480 --> 02:03:36.960
The statement there, the idea is that the switch from Python two to Python three

02:03:37.840 --> 02:03:43.680
would have probably destroyed Python if it didn't also coincide with Python for

02:03:43.680 --> 02:03:50.240
whatever reason, just overtaking the data science community, anything that

02:03:50.240 --> 02:03:56.880
processes data, so like the timing was perfect that this maybe imperfect decision

02:03:56.880 --> 02:04:01.960
was coupled with a great timing on the value of data in our world.

02:04:02.000 --> 02:04:04.200
I would say the troubled execution of a good decision.

02:04:04.720 --> 02:04:06.280
It was a decision that was necessary.

02:04:07.320 --> 02:04:09.800
It's possible if we had more resources, we could have done it in a way that

02:04:09.840 --> 02:04:15.520
was a little bit smoother, but ultimately, the arguments for Python three, I

02:04:15.520 --> 02:04:17.360
bought them at the time and I buy them now, right?

02:04:17.360 --> 02:04:21.520
Having great text handling is like a non-negotiable table stakes thing

02:04:21.520 --> 02:04:22.880
you need to have in a language.

02:04:23.400 --> 02:04:32.960
So that's great, but the execution, Python is the, it's volunteer driven.

02:04:33.000 --> 02:04:36.640
It's like now the most popular language on the planet, but it's all literally volunteers.

02:04:37.040 --> 02:04:41.080
So the lack of resources meant that they had to really, they had to do

02:04:41.080 --> 02:04:42.960
things in a very hamstrung way.

02:04:43.560 --> 02:04:47.840
And I think to carry the Python momentum and the language through that time, the

02:04:47.840 --> 02:04:49.600
data movement was a critical part of that.

02:04:49.840 --> 02:04:51.320
So some of it is carrot and stick.

02:04:51.360 --> 02:04:57.320
I actually have to shamefully admit that it took me a very long time to switch

02:04:57.320 --> 02:05:00.240
from Python two and Python three because I'm a machine learning person.

02:05:00.240 --> 02:05:03.600
It was just for the longest time, you could just do fine with Python two.

02:05:03.760 --> 02:05:04.000
Right.

02:05:04.880 --> 02:05:10.880
But I think the moment where I switched everybody I worked with and switched

02:05:10.920 --> 02:05:17.800
myself for small projects and big is when finally, when NumPy announced that

02:05:17.800 --> 02:05:22.080
they're going to end support, like in 2020 or something like that.

02:05:22.080 --> 02:05:22.360
Right.

02:05:23.000 --> 02:05:27.200
So like when I realized, oh, this isn't going, this is going to end.

02:05:27.320 --> 02:05:27.640
Right.

02:05:27.680 --> 02:05:28.720
So that's the stick.

02:05:28.760 --> 02:05:29.680
That's not a carrot.

02:05:29.680 --> 02:05:31.640
That's not, so for the longest time it was carrots.

02:05:31.680 --> 02:05:35.920
It was like all of these packages were saying, okay, we have Python three support

02:05:35.920 --> 02:05:40.440
now come join us with Python two and Python three, but when NumPy, one of the

02:05:40.440 --> 02:05:46.840
packages I sort of love and depend on, uh, said like, Nope, it's over.

02:05:47.640 --> 02:05:50.360
Uh, that's, that's when I, uh, decided to switch.

02:05:50.360 --> 02:05:56.280
I wonder if you think it was possible much earlier for somebody like, uh, like

02:05:56.280 --> 02:06:03.120
NumPy or some major package to step into the cold and say, like,

02:06:03.120 --> 02:06:04.680
it's a chicken and egg problem too.

02:06:04.800 --> 02:06:05.200
Right.

02:06:05.280 --> 02:06:09.080
You don't want to cut off a lot of users unless you see the user momentum going

02:06:09.080 --> 02:06:13.600
too, so the decisions for the scientific community, for each of the different

02:06:13.600 --> 02:06:15.080
projects, you know, there's not a monolith.

02:06:15.240 --> 02:06:18.360
Some projects are like, we'll only be releasing new features on Python three.

02:06:18.880 --> 02:06:23.360
And that was more of a sticky carrot or a firm carrot, if you will, a firm carrot.

02:06:23.800 --> 02:06:27.200
Um, a stick shaped carrot.

02:06:27.920 --> 02:06:29.400
But then for others, yeah.

02:06:29.480 --> 02:06:33.240
NumPy in particular, cause it's at the base of the dependency stack for so many

02:06:33.240 --> 02:06:36.080
things, um, that was the final stick.

02:06:36.160 --> 02:06:37.160
That was a stick shape stick.

02:06:37.560 --> 02:06:41.160
People were saying, look, if I have to keep maintaining my releases for Python

02:06:41.160 --> 02:06:45.840
two, that's that much less energy that I can put into making things better for

02:06:45.840 --> 02:06:49.080
the Python three folks or in my new version, which is of course going to be

02:06:49.080 --> 02:06:53.000
Python three, so people were also getting kind of pulled by this tension.

02:06:53.240 --> 02:06:57.720
So the overall community sort of had a lot of input into when the NumPy core

02:06:57.720 --> 02:07:01.000
folks decided that they would end of life on Python two.

02:07:01.280 --> 02:07:05.840
So as this, these numbers are a little bit loose, but there are about 10 million

02:07:05.840 --> 02:07:07.280
Python programmers in the world.

02:07:07.320 --> 02:07:11.000
You could argue that number, but let's say 10 million, uh, there's actually

02:07:11.440 --> 02:07:16.480
where I was looking, said 27 million total programmers, developers in the world.

02:07:17.160 --> 02:07:20.760
You mentioned in a talk that, uh, changes need to be made for there

02:07:20.760 --> 02:07:23.520
to be 100 million Python programmers.

02:07:23.880 --> 02:07:27.560
So first of all, do you see a future where there's a hundred million Python

02:07:27.560 --> 02:07:30.760
programmers and second, what kind of changes need to be made?

02:07:31.240 --> 02:07:34.200
So Anaconda, Miniconda get downloaded about a million times a week.

02:07:34.800 --> 02:07:39.000
So I think the idea that there's only 10 million Python programmers in the

02:07:39.000 --> 02:07:41.000
world is a little bit under counting.

02:07:41.880 --> 02:07:46.120
There are a lot of people who escaped traditional counting that are using

02:07:46.120 --> 02:07:47.960
Python and data in their jobs.

02:07:48.400 --> 02:07:52.920
I do believe that the future world for it to, well, the world I would like

02:07:52.920 --> 02:07:58.480
to see is one where people are data literate, so they are able to use tools

02:07:58.520 --> 02:08:04.560
that let them express their questions, ideas fluidly, um, and the data variety

02:08:04.840 --> 02:08:06.400
and data complexity will not go down.

02:08:06.400 --> 02:08:07.520
It will only keep increasing.

02:08:08.240 --> 02:08:14.960
So I think some level of code or code like things will continue to be relevant.

02:08:15.440 --> 02:08:21.280
And so my, my hope is that we can build systems that allow people to more

02:08:21.280 --> 02:08:26.320
seamlessly integrate Python kinds of expressivity with data systems and

02:08:26.320 --> 02:08:29.800
operationalization methods that are much more seamless.

02:08:30.480 --> 02:08:33.480
Um, and what I mean by that is, you know, right now you can't punch

02:08:33.480 --> 02:08:35.480
Python code into an Excel cell.

02:08:35.640 --> 02:08:37.360
I mean, there's some tools you can do to kind of do this.

02:08:37.920 --> 02:08:41.840
We didn't build a thing for doing this back in the day, but, um, but I feel

02:08:41.840 --> 02:08:46.520
like the total addressable market for Python users, if we do the things right,

02:08:46.840 --> 02:08:50.480
is on the order of the Excel users, which is, you know, a few hundred million.

02:08:51.160 --> 02:08:57.800
So, um, I think Python has to get better at being embedded, um, you know, being

02:08:57.800 --> 02:09:03.400
a smaller thing that pulls in just the right parts of the ecosystem to, um, run

02:09:03.400 --> 02:09:08.120
numerics and do data exploration, meeting people where they're already at with

02:09:08.120 --> 02:09:09.320
their data and their data tools.

02:09:09.760 --> 02:09:13.840
And then I think also it has to be easier to take some of those things they've

02:09:13.840 --> 02:09:19.000
written and flow those back into deployed systems or apps or visualizations.

02:09:19.440 --> 02:09:22.680
I think if we don't do those things, then we will always be kept in a

02:09:22.680 --> 02:09:27.280
silo as sort of a, you know, an expert user's tool and not a tool for the masses.

02:09:27.360 --> 02:09:33.840
You know, I work with a bunch of folks in the Adobe creative suite, and I'm kind

02:09:33.840 --> 02:09:38.040
of forcing them or inspired them to learn Python, uh, to do a bunch of stuff that

02:09:38.040 --> 02:09:40.640
helps them and it's interesting because they probably wouldn't call themselves a

02:09:40.840 --> 02:09:43.040
Python programmers, but all using Python.

02:09:43.800 --> 02:09:47.160
I would love it if the tools like Photoshop and Premiere and all those kinds

02:09:47.160 --> 02:09:50.640
of tools that are targeted towards creative people, I guess that's where

02:09:50.640 --> 02:09:54.640
Excel, Excel is targeted towards a certain kind of audience that works with

02:09:54.640 --> 02:09:57.040
data, uh, financial people, all that kind of stuff.

02:09:58.080 --> 02:10:02.480
If they, there will be easy ways to leverage to use Python for quick

02:10:02.480 --> 02:10:07.320
scripting tasks and I, you know, there's an exciting application of, um,

02:10:07.560 --> 02:10:12.160
artificial intelligence in this space that I'm hopeful about looking at open

02:10:12.160 --> 02:10:16.080
AI codecs with, um, generating programs.

02:10:16.880 --> 02:10:23.760
So almost helping people bridge the gap from kind of visual interface to

02:10:23.760 --> 02:10:28.920
generating programs to something formal and then they can modify it and so on.

02:10:28.920 --> 02:10:33.320
But kind of, uh, without having to read the manual, without having to do a

02:10:33.320 --> 02:10:36.440
Google search and stack overflow, which is essentially what a neural network

02:10:36.440 --> 02:10:41.360
does when it's doing code generation, uh, is, is actually generating code and

02:10:41.360 --> 02:10:45.440
allowing a human to communicate with multiple programs and then maybe even

02:10:45.440 --> 02:10:47.720
programs to communicate with each other via Python.

02:10:48.360 --> 02:10:52.120
So that, that to me is a really exciting possibility because I think there's,

02:10:52.120 --> 02:10:57.840
um, there's a friction to kind of like, how do I learn how to use Python in my

02:10:57.840 --> 02:10:58.280
life?

02:10:58.840 --> 02:11:03.920
There's a, oftentimes you kind of what sort of class you start learning about

02:11:03.920 --> 02:11:09.120
types is, I don't know, functions like this is, you know, Python is the first

02:11:09.120 --> 02:11:14.960
language with which you start to learn to program, but I feel like that's going

02:11:14.960 --> 02:11:18.240
to take a long time for you to understand why it's useful.

02:11:18.560 --> 02:11:20.200
You almost want to start with a script.

02:11:20.280 --> 02:11:23.840
Well, you, you do in fact, I think starting with the theory, the theory

02:11:23.840 --> 02:11:27.560
behind programming languages and types and all that, I mean, types are there to

02:11:27.560 --> 02:11:29.560
make the compiler writers jobs easier.

02:11:30.000 --> 02:11:33.440
Types are not, I mean, heck, do you have an ontology of types for just the

02:11:33.440 --> 02:11:34.280
objects on this table?

02:11:34.320 --> 02:11:34.680
No.

02:11:35.520 --> 02:11:39.800
So types are there because compiler writers are human and they're limited in

02:11:39.800 --> 02:11:45.560
what they can do, but, um, I think that the beauty of scripting, like there's a,

02:11:45.560 --> 02:11:49.680
there's a Python book that's called automate the boring stuff, which is

02:11:49.680 --> 02:11:50.960
exactly the right mentality.

02:11:51.480 --> 02:11:57.920
Um, I grew up with computers in a time when I could, uh, when, when Steve's job

02:11:57.920 --> 02:12:00.280
was still pitching these things as bicycles for the mind, they were supposed

02:12:00.280 --> 02:12:02.600
to not be just media consumption devices.

02:12:02.920 --> 02:12:05.920
Um, but they were, they were actually, you could, you could write some code.

02:12:05.920 --> 02:12:08.080
You can write basic, you could write some stuff to do some things.

02:12:09.000 --> 02:12:13.760
And that feeling of a computer as a thing that we can use to extend

02:12:13.760 --> 02:12:17.240
ourselves has all but evaporated for a lot of people.

02:12:17.760 --> 02:12:21.480
So you see a little bit in parts and the current, the generation of youth

02:12:21.480 --> 02:12:23.320
around Minecraft or roadblocks, right?

02:12:23.640 --> 02:12:27.560
And I think Python circuit, Python, these things, um, could be a

02:12:27.560 --> 02:12:32.840
renaissance of that, of people actually shaping and using their computers as

02:12:32.840 --> 02:12:37.400
computers, um, as an extension of their minds and the curiosity, their creativity.

02:12:37.800 --> 02:12:41.640
So, you know, you talk about scripting the Adobe suite with Python, um, in

02:12:41.640 --> 02:12:46.600
the 3d graphics world, Python, um, is a scripting language that some of these

02:12:46.600 --> 02:12:49.440
3d graphics suites use, and I think it's great.

02:12:49.440 --> 02:12:51.000
We should better support those kinds of things.

02:12:51.240 --> 02:12:55.440
But ultimately the idea that I should be able to have power over my computing

02:12:55.440 --> 02:12:59.920
environment, if I want these things to happen repeatedly all the time, I should

02:12:59.920 --> 02:13:02.360
be able to say that somehow to the computer, right?

02:13:02.600 --> 02:13:07.400
Now, whether, um, the operating systems get there faster by having some, you

02:13:07.400 --> 02:13:10.480
know, Siri backed with open AI with whatever, so you can just say, Siri, make

02:13:10.480 --> 02:13:12.040
this, do this, do this every other Friday, right?

02:13:12.640 --> 02:13:14.200
We probably will get there somewhere.

02:13:14.200 --> 02:13:16.720
And Apple's always had these ideas, you know, there's the, the Apple

02:13:16.720 --> 02:13:21.360
script in the menu that no one ever uses, but, um, you can do these kinds of things.

02:13:21.600 --> 02:13:24.880
But when you start doing that kind of scripting, the challenge isn't learning

02:13:24.880 --> 02:13:28.720
the type system or even the syntax, the language, the challenges, all of the

02:13:28.720 --> 02:13:31.720
dictionaries and all the objects of all their properties and attributes and

02:13:31.720 --> 02:13:34.960
parameters, like who's got time to learn all that stuff, right?

02:13:35.400 --> 02:13:40.440
So that's when then programming by prototype or by example, becomes the

02:13:40.440 --> 02:13:42.960
right way to get the user to express their desire.

02:13:43.400 --> 02:13:45.720
Um, so there's a lot of these, these different ways that we can approach

02:13:45.720 --> 02:13:49.280
programming, but I do think when, as you were talking about the Adobe

02:13:49.280 --> 02:13:52.840
scripting thing, I was thinking about, you know, when we do use something like

02:13:52.840 --> 02:13:58.040
NumPy, when we use things in the Python data and scientific, uh, they'd say

02:13:58.040 --> 02:14:02.520
expression system, there's a reason we use that, which is that it gives us

02:14:02.560 --> 02:14:03.760
mathematical precision.

02:14:04.400 --> 02:14:08.280
It gives us actually quite a lot of precision over precisely what we mean

02:14:08.320 --> 02:14:10.640
about this data set, that data set.

02:14:11.040 --> 02:14:15.360
And it's the fact that we can have that precision that lets Python be powerful

02:14:15.360 --> 02:14:18.160
over, uh, as a duct tape for data.

02:14:18.680 --> 02:14:20.720
You know, you give me a TSV or a CSV.

02:14:21.160 --> 02:14:25.440
And you, if you give me some massively expensive vendor tool for data

02:14:25.440 --> 02:14:28.000
transformation, I don't know, I'm going to be able to solve your problem.

02:14:28.200 --> 02:14:32.320
But if you give me a Python prompt, you can throw whatever data you want at me.

02:14:32.440 --> 02:14:33.840
I will be able to mash it into shape.

02:14:34.320 --> 02:14:39.320
So that ability to take it as sort of this like, um, you know, machete out of

02:14:39.320 --> 02:14:41.280
the data jungle is really powerful.

02:14:41.600 --> 02:14:45.960
And I think that's why at some level we're not, we're not going to get away

02:14:45.960 --> 02:14:50.840
from some of these expressions and APIs and libraries in Python for, for data

02:14:50.840 --> 02:14:51.480
transformation.

02:14:53.920 --> 02:14:58.280
You've been at the center of the Python community for many years.

02:14:58.360 --> 02:15:05.000
If you could change one thing about the community to help it grow, to help it

02:15:05.000 --> 02:15:09.240
improve, to help it flourish and prosper, what would it be?

02:15:09.440 --> 02:15:12.920
I mean, not, you know, it doesn't have to be one thing, but what, what kind of

02:15:12.920 --> 02:15:14.880
comes to mind, what are the challenges?

02:15:15.360 --> 02:15:18.120
Humility is one of the values that we have at Anaconda at the company, but it's

02:15:18.120 --> 02:15:23.360
also one of the values in the, in the community that it's been breached a

02:15:23.360 --> 02:15:24.440
little bit in the last few years.

02:15:24.440 --> 02:15:28.640
But in general, people are quite decent and reasonable and nice.

02:15:29.320 --> 02:15:35.800
And, um, that humility prevents them from seeing, um, the greatness that they

02:15:35.800 --> 02:15:42.160
could have, I don't know how many people in the core Python community really

02:15:42.200 --> 02:15:48.760
understand that they stand perched at the edge of an opportunity to transform

02:15:48.760 --> 02:15:50.080
how people use computers.

02:15:51.120 --> 02:15:55.360
And actually PyCon, I think his last physical PyCon I went to, uh, Russell

02:15:55.360 --> 02:16:00.240
Keith McGee gave a great keynote, uh, about, you know, uh, very much along the

02:16:00.240 --> 02:16:04.440
lines of the challenges I have, which is, you know, Python for a language that

02:16:04.440 --> 02:16:08.160
doesn't actually, they can't put an interface up on like the most popular

02:16:08.160 --> 02:16:11.000
computing devices, it's done really well as a language, hasn't it?

02:16:11.760 --> 02:16:13.640
You can't write a web front end with Python really.

02:16:13.680 --> 02:16:14.760
I mean, everyone uses JavaScript.

02:16:15.000 --> 02:16:16.400
You certainly can't write native apps.

02:16:16.960 --> 02:16:21.800
So for a language that you can't actually write apps in any front end

02:16:21.800 --> 02:16:24.120
runtime environments, Python has done exceedingly well.

02:16:25.920 --> 02:16:28.600
And so that, that wasn't to pat ourselves in the back.

02:16:28.680 --> 02:16:31.440
That was to challenge ourselves as a community to say, we, through our

02:16:31.440 --> 02:16:35.600
current volunteer dynamic have gotten to this point, what comes next and how do

02:16:35.600 --> 02:16:39.080
we seize, you know, we've caught the tiger by the tail, how do we make sure

02:16:39.080 --> 02:16:40.720
we keep up with it as it goes forward?

02:16:40.920 --> 02:16:43.520
So that's one of the questions I have about sort of open source

02:16:43.520 --> 02:16:47.520
communities is, um, at its best, there's a kind of humility.

02:16:48.560 --> 02:16:53.600
Is that humility prevent you to have a vision for creating something

02:16:53.600 --> 02:16:55.520
like very new and powerful.

02:16:55.520 --> 02:16:58.520
And you've brought us back to consciousness again, the collaboration

02:16:58.520 --> 02:17:00.360
is a swarm emergent dynamic.

02:17:00.640 --> 02:17:04.160
Humility lets these people work together without anyone trouncing anyone else.

02:17:04.760 --> 02:17:08.240
How do they, you know, in consciousness, there's the question of the binding

02:17:08.240 --> 02:17:12.200
problem, how does a singular, our attention, how does that emerge from,

02:17:12.320 --> 02:17:13.200
you know, billions of neurons?

02:17:13.200 --> 02:17:13.440
Yes.

02:17:13.800 --> 02:17:19.120
So how can you have a swarm of people emerge a consensus that has a singular

02:17:19.120 --> 02:17:20.560
vision to say, we will do this.

02:17:20.560 --> 02:17:22.760
And most importantly, we're not going to do these things.

02:17:23.840 --> 02:17:30.000
Emerging a coherent, pointed, focused leadership dynamic from a collaboration,

02:17:30.240 --> 02:17:34.080
being able to do that kind of, and then dissolve it so people can still do the

02:17:34.080 --> 02:17:36.680
swarm thing, that's a problem.

02:17:36.680 --> 02:17:37.160
That's a question.

02:17:37.160 --> 02:17:39.440
So do you have, do you have to have a charismatic leader?

02:17:40.800 --> 02:17:43.560
For some reason, Linus Torvald comes to mind, but he, you know, there's

02:17:43.560 --> 02:17:48.280
people who criticize the rules, the iron fist, man, but there's still charisma.

02:17:48.520 --> 02:17:49.520
There's a charisma, right?

02:17:49.560 --> 02:17:51.400
There's a charisma to that iron fist.

02:17:51.480 --> 02:17:56.720
Uh, there's a, every leader is different, I would say in their success.

02:17:56.720 --> 02:18:00.720
So he doesn't, I don't even know if you can say he doesn't have humility.

02:18:01.720 --> 02:18:08.880
Uh, there's such a meritocracy of ideas that like, this is a good idea

02:18:08.880 --> 02:18:10.200
and this is a bad idea.

02:18:10.320 --> 02:18:11.440
There's a step function to it.

02:18:11.520 --> 02:18:13.560
Once you clear a threshold, he's open.

02:18:13.800 --> 02:18:16.880
Once you clear the bozo threshold, he's open to your ideas, I think.

02:18:17.240 --> 02:18:17.440
Right.

02:18:17.440 --> 02:18:21.600
But see the, the interesting thing is obviously that will not stand in

02:18:21.600 --> 02:18:26.400
an open source community if that threshold that is defined by that one

02:18:26.400 --> 02:18:29.520
particular person is not actually that good.

02:18:30.240 --> 02:18:32.920
So you actually have to be really excellent at what you do.

02:18:33.640 --> 02:18:36.760
So the, the, he's very good at what he does.

02:18:37.080 --> 02:18:41.040
And so there's some aspect of leadership where you can get thrown out.

02:18:41.120 --> 02:18:45.480
People can just leave, you know, that's how it works with open source of the fork.

02:18:45.880 --> 02:18:50.680
But at the same time, you want to sometimes be a leader, like with a strong

02:18:50.680 --> 02:18:54.800
opinion, because people, I mean, there's some kind of balance here for this,

02:18:54.800 --> 02:18:59.000
like hive mind to get like behind leadership is a big topic and I didn't,

02:18:59.000 --> 02:19:01.280
you know, I'm not one of these guys that went to MBA school and said, I'm

02:19:01.280 --> 02:19:03.880
going to be an entrepreneur and I'm going to be a leader and I'm going to

02:19:04.240 --> 02:19:06.640
read all these Harvard business review articles on leadership and all this

02:19:06.640 --> 02:19:10.640
other stuff, like it, it, I was a physicist turned into a software nerd

02:19:10.640 --> 02:19:12.360
who then really like nerded out on Python.

02:19:12.800 --> 02:19:14.800
Um, I know I am entrepreneurial, right?

02:19:14.800 --> 02:19:18.720
I saw a business opportunity around the use of Python or data, but, um, for me,

02:19:19.080 --> 02:19:22.960
what has been interesting over this journey with the last 10 years is how

02:19:22.960 --> 02:19:28.640
much I started really enjoying the understanding, thinking deeper about

02:19:28.640 --> 02:19:30.080
organizational dynamics and leadership.

02:19:30.720 --> 02:19:35.200
Um, and leadership does come down to, uh, a few core things.

02:19:35.200 --> 02:19:42.080
Number one, a leader has to create belief or at least has to dispel disbelief.

02:19:43.200 --> 02:19:48.840
Um, leadership also, you have to have vision, loyalty and experience.

02:19:49.240 --> 02:19:52.520
So can you say belief in a singular vision?

02:19:52.520 --> 02:19:53.520
Like what, what does belief?

02:19:53.560 --> 02:19:55.120
Yeah, belief means a few things.

02:19:55.120 --> 02:19:56.920
Belief means here's what we need to do.

02:19:56.920 --> 02:19:58.200
And this is a valid thing to do.

02:19:58.640 --> 02:20:05.240
Um, and we can do it, um, that you have to be able to drive that belief.

02:20:05.920 --> 02:20:10.600
Um, and every step of leadership along the way has to help you amplify

02:20:10.600 --> 02:20:11.800
that belief to more people.

02:20:12.560 --> 02:20:15.800
I mean, I think at a fundamental level, that's what it is.

02:20:15.800 --> 02:20:16.760
You have to have a vision.

02:20:17.000 --> 02:20:21.920
You have to be able to, um, show people that, or you have to convince people

02:20:21.960 --> 02:20:25.120
to believe in the vision and to, to get behind you.

02:20:25.120 --> 02:20:27.400
And that's where the loyalty part comes in and the experience part comes in.

02:20:28.120 --> 02:20:30.120
There's all different flavors of leadership.

02:20:30.120 --> 02:20:35.280
So if we talk about Linus, we could talk about Elon Musk and Steve

02:20:35.280 --> 02:20:40.160
Jobs, there's Sunder Prachai, there's people that kind of put themselves at

02:20:40.160 --> 02:20:42.120
the center and are strongly opinionated.

02:20:42.480 --> 02:20:44.960
And some people are more like consensus builders.

02:20:45.960 --> 02:20:47.680
What works well for open source?

02:20:47.680 --> 02:20:49.680
What works well in the space of programmers?

02:20:49.680 --> 02:20:53.920
So you've been a programmer, you've led many programmers and are now sort

02:20:53.920 --> 02:20:55.560
of at the center of this ecosystem.

02:20:55.560 --> 02:20:58.240
What works well in the programming world would you say?

02:20:58.920 --> 02:21:02.520
It really depends on the people, what style of leadership is best.

02:21:02.560 --> 02:21:04.080
And it depends on the programming community.

02:21:04.120 --> 02:21:08.240
I think for the Python community, um, servant leadership is one of the values.

02:21:08.240 --> 02:21:14.240
Like at the end of the day, the leader has to also be, um, the high priest of values.

02:21:15.000 --> 02:21:15.360
Right.

02:21:15.480 --> 02:21:19.080
So any kind of, any collection of people has values of their living.

02:21:19.920 --> 02:21:26.560
And if you want to maintain, uh, certain values and those values help you as an

02:21:26.560 --> 02:21:30.280
organization become more powerful, then the leader has to live those values

02:21:30.520 --> 02:21:33.960
unequivocally and has to, you know, has to hold the values.

02:21:34.320 --> 02:21:41.440
So in our case, in this collaborative community around Python, I think that the

02:21:41.440 --> 02:21:44.200
humility is one of those values, servant leadership, you know, you actually

02:21:44.200 --> 02:21:45.200
have to kind of do the stuff.

02:21:45.240 --> 02:21:47.280
You have to walk the walk, not just talk the talk.

02:21:47.920 --> 02:21:52.320
Um, I don't feel like the Python community really demands that much

02:21:52.320 --> 02:21:53.440
from a vision standpoint.

02:21:53.800 --> 02:21:55.520
And they should, and I think they should.

02:21:57.120 --> 02:22:02.760
This is the interesting thing is like so many people use Python from where it

02:22:02.760 --> 02:22:08.160
comes the vision, you know, like you have a Elon Musk type character who has

02:22:08.200 --> 02:22:13.840
mixed bold statements about the vision for particular companies he's involved

02:22:13.840 --> 02:22:14.080
with.

02:22:14.600 --> 02:22:21.760
And it's like, I think a lot of people that work at those companies kind of can

02:22:21.760 --> 02:22:23.880
only last if they believe that vision.

02:22:24.400 --> 02:22:26.200
Cause in some of it is super bold.

02:22:26.200 --> 02:22:31.840
So my question is, and by the way, those companies often use Python, uh, what,

02:22:32.040 --> 02:22:33.720
you know, how do you establish a vision?

02:22:33.720 --> 02:22:36.360
Like get to a hundred million users, right?

02:22:36.960 --> 02:22:44.560
Uh, get to where, you know, the Python is at the center of the machine learning

02:22:44.640 --> 02:22:49.000
and, um, was it data science, machine learning, deep learning, artificial

02:22:49.000 --> 02:22:51.560
intelligence revolution, right?

02:22:51.680 --> 02:22:55.800
Like in many ways, perhaps the Python community is not thinking of it that way,

02:22:55.800 --> 02:23:01.520
but it's leading the way on this, like the tooling is like a central, right?

02:23:01.800 --> 02:23:06.000
Well, you know, for a while, um, PyCon people in the scientific Python and the

02:23:06.000 --> 02:23:09.280
PyData community, um, they would submit talks.

02:23:09.320 --> 02:23:14.640
Those early to early 2010s, mid 2010s, they would submit talks to PyCon and the

02:23:14.640 --> 02:23:18.120
talks would all be rejected because there was the separate sort of PyData

02:23:18.120 --> 02:23:20.880
conferences and like, well, these should, these probably belong more to PyData.

02:23:21.400 --> 02:23:24.920
And instead there'd be yet another talk about, you know, threads and, you know,

02:23:24.920 --> 02:23:26.000
whatever, some web framework.

02:23:26.440 --> 02:23:31.320
And it's like, um, that was an interesting dynamic to see that there was, I mean,

02:23:31.360 --> 02:23:34.320
at the time it was a little annoying because we want to try to get more users

02:23:34.320 --> 02:23:35.800
and get more people talking about these things.

02:23:35.800 --> 02:23:37.360
And PyCon is a huge venue, right?

02:23:37.360 --> 02:23:39.720
It's thousands of Python programmers.

02:23:40.240 --> 02:23:44.240
But then also came to appreciate that, you know, parallel, having an ecosystem

02:23:44.520 --> 02:23:47.440
that allows parallel innovation is not bad, right?

02:23:47.440 --> 02:23:49.040
There are people doing embedded Python stuff.

02:23:49.080 --> 02:23:51.480
There's people doing web programming, people doing scripting.

02:23:51.480 --> 02:23:52.760
There's cyber uses of Python.

02:23:53.320 --> 02:23:57.360
I think the, ultimately at some point, if your slide mode, mold cover so much

02:23:57.360 --> 02:24:00.600
stuff, you have to respect that different things are growing in different

02:24:00.840 --> 02:24:01.920
areas and different niches.

02:24:02.280 --> 02:24:05.560
Now some, at some point that has to come together and the central body has to

02:24:06.120 --> 02:24:07.400
provide resources.

02:24:07.520 --> 02:24:08.880
The principle here is subsidiarity.

02:24:09.120 --> 02:24:13.760
Give resources to the various groups to then allocate as they see fit in their

02:24:13.760 --> 02:24:14.280
niches.

02:24:14.720 --> 02:24:17.160
Um, that would be a really helpful dynamic, but again, it's a volunteer

02:24:17.160 --> 02:24:17.480
community.

02:24:17.480 --> 02:24:19.520
It's not like they had that many resources to start with.

02:24:21.080 --> 02:24:24.920
What was, or is your favorite programming setup, what operating system, what

02:24:24.920 --> 02:24:31.480
keyboard, how many screens you're listening to, what time of day I drinking

02:24:31.480 --> 02:24:35.360
coffee, tea, tea, um, sometimes coffee, depending on how well I slept.

02:24:35.880 --> 02:24:37.520
Um, I used to have sleep.

02:24:37.520 --> 02:24:41.920
Do you get a night all, I remember somebody asked you somewhere, a question

02:24:41.920 --> 02:24:46.080
about work-life balance and, and, or like not just work-life balance, but

02:24:46.080 --> 02:24:51.120
like a family, you know, you lead a company and your answer was, was

02:24:51.120 --> 02:24:53.560
basically like, I still haven't figured it out.

02:24:54.160 --> 02:24:56.240
Yeah, I think I've gotten a little bit better balance.

02:24:56.240 --> 02:24:58.800
I have a really great leadership team now supporting me.

02:24:58.800 --> 02:25:00.800
And so that takes a lot of the day-to-day stuff.

02:25:01.320 --> 02:25:04.880
Um, off my plate and, um, my kids are getting a little older, so that helps.

02:25:05.080 --> 02:25:08.680
So, um, and of course I have a wonderful wife who takes care of a lot of the

02:25:08.920 --> 02:25:10.120
things that I'm not able to take care of.

02:25:10.120 --> 02:25:11.280
And, and she's, she's great.

02:25:11.600 --> 02:25:14.720
I try to get to sleep earlier now, um, because I have to get up every morning

02:25:14.720 --> 02:25:16.440
at six to take my kid down to the bus stop.

02:25:16.920 --> 02:25:19.800
So there's a hard, there's a hard thing for a while.

02:25:19.800 --> 02:25:21.920
I was doing polyphasic sleep, which is really interesting.

02:25:21.960 --> 02:25:25.680
Like I go to bed at nine, wake up at like 2 AM, work till five, sleep

02:25:25.680 --> 02:25:27.200
three hours, wake up at eight.

02:25:27.200 --> 02:25:29.240
Like that was actually, it was interesting.

02:25:29.240 --> 02:25:29.920
It wasn't too bad.

02:25:29.960 --> 02:25:30.520
How did it feel?

02:25:31.120 --> 02:25:31.520
It was good.

02:25:31.520 --> 02:25:36.080
I didn't keep it up for years, but once I've traveled, then it just, everything

02:25:36.080 --> 02:25:37.320
goes out the window, right?

02:25:37.320 --> 02:25:39.720
Cause then you're like time zones and all these things socially.

02:25:39.720 --> 02:25:43.440
Was it except like, were you able to live outside of how you felt?

02:25:43.480 --> 02:25:45.600
Were you able to live normal society?

02:25:45.680 --> 02:25:46.080
Oh yeah.

02:25:46.080 --> 02:25:48.920
Because like on the nights that wasn't out, hanging out with people or whatever,

02:25:48.920 --> 02:25:50.240
going to bed at nine, no one cares.

02:25:50.640 --> 02:25:53.760
I wake up at two, I'm still responding to their slacks emails, whatever.

02:25:54.160 --> 02:25:58.000
And, you know, uh, shit posting on Twitter or whatever at two in the morning is

02:25:58.520 --> 02:25:59.120
exactly right.

02:25:59.120 --> 02:26:02.360
And then you go to bed for a few hours and you wake up.

02:26:02.360 --> 02:26:03.800
It's like you had an extra day in the middle.

02:26:03.960 --> 02:26:04.200
Yes.

02:26:04.240 --> 02:26:06.400
Uh, and I'd read somewhere that, you know, humans naturally have

02:26:06.400 --> 02:26:07.560
biphasic sleep or something.

02:26:07.560 --> 02:26:11.080
I don't know, but, um, I read basically everything somewhere.

02:26:11.080 --> 02:26:14.240
So every option of everything is every option of everything.

02:26:14.440 --> 02:26:17.680
I will say that that worked out for me for a while, although I don't do it anymore.

02:26:18.080 --> 02:26:22.680
Um, in terms of programming setup, I had a 27 inch, um, high, high DPI, um,

02:26:22.680 --> 02:26:24.440
set up that I really liked.

02:26:24.520 --> 02:26:27.840
Um, but then I moved to a curved monitor just because when I moved to the

02:26:27.840 --> 02:26:32.360
new house, I want to have a bit more screen for zoom plus communications,

02:26:32.360 --> 02:26:35.640
plus, you know, like various kinds of things, like one large monitor,

02:26:35.720 --> 02:26:40.000
one large curve monitor, um, what operating system, Mac.

02:26:40.560 --> 02:26:40.840
Okay.

02:26:41.040 --> 02:26:41.360
Yeah.

02:26:41.600 --> 02:26:43.800
Is that, is that what happens when you become important?

02:26:43.880 --> 02:26:46.080
Is you stop using, uh, Linux and windows?

02:26:46.080 --> 02:26:49.440
I w no, I actually have, I have a windows box as well on the next table over.

02:26:49.720 --> 02:26:54.000
Um, but, uh, but I have, I have three desks, right?

02:26:54.040 --> 02:26:54.280
Yes.

02:26:54.280 --> 02:26:57.520
So a main one is the standing desk so that I can, you know, whatever,

02:26:57.520 --> 02:26:59.840
what I'm like, I have a teleprompter set up and everything else.

02:26:59.880 --> 02:27:06.160
And then, um, I've got my iMac, uh, and then eGPU and then windows, uh, PC.

02:27:06.400 --> 02:27:10.840
The reason I moved to Mac was, uh, it's, it's got a Linux prompt or not.

02:27:10.840 --> 02:27:11.000
Sorry.

02:27:11.000 --> 02:27:15.520
It's got a, it's got a unix prompt so I can do all my stuff, but then, um,

02:27:16.240 --> 02:27:20.080
I, uh, I don't have to worry like when I'm presenting for clients or investors,

02:27:20.080 --> 02:27:25.640
whatever, like it, I don't have to worry about any like ACPI related F sick

02:27:25.760 --> 02:27:27.120
things in the middle of a presentation.

02:27:27.120 --> 02:27:29.440
Like none of that, it just, it will always wake from sleep.

02:27:30.160 --> 02:27:32.280
Um, and it won't kernel panic on me.

02:27:32.280 --> 02:27:38.400
And this is not a dig against Linux, except that I just, um, I feel really bad.

02:27:38.400 --> 02:27:40.160
I feel like a traitor to my community saying this, right?

02:27:40.160 --> 02:27:43.200
But in 2012, I was just like, okay, start my own company.

02:27:43.200 --> 02:27:43.800
What do I get?

02:27:44.000 --> 02:27:46.520
And Linux laptops were just not quite there.

02:27:46.840 --> 02:27:48.520
Um, and so I've just stuck with Max.

02:27:48.520 --> 02:27:53.520
Can I just defend something that nobody respectable seems to do, which is, uh,

02:27:53.640 --> 02:27:59.520
so I do a boot on Linux windows, but in windows, I have, uh, windows subsystems

02:27:59.520 --> 02:28:06.120
for Linux or whatever, WSL, and I find myself being able to handle everything

02:28:06.120 --> 02:28:10.160
I need and almost everything I need in Linux for basic sort of tasks, scripting

02:28:10.160 --> 02:28:12.560
tasks within WSL and it creates a really nice environment.

02:28:12.920 --> 02:28:16.320
So I've been, but like whenever I hang out with like, especially important

02:28:16.320 --> 02:28:23.280
people, like they're all on iPhone and a Mac and it's like, yeah, like what

02:28:23.360 --> 02:28:28.200
there, there is a messiness to windows and a messiness to Linux that makes

02:28:28.200 --> 02:28:31.240
me feel like you're still in it.

02:28:31.880 --> 02:28:37.080
Well, the Linux stuff, Windows subsystem for Linux is very tempting, but there's

02:28:37.080 --> 02:28:41.000
still the windows on the outside where I don't know where, and I've been, okay.

02:28:41.000 --> 02:28:45.160
I've been, I've, I've used DOS since version 1.11 or 1.21 or something.

02:28:45.520 --> 02:28:47.920
So I've been a long time Microsoft user.

02:28:48.360 --> 02:28:52.440
And I will say that like, like, it's really hard for me to

02:28:52.440 --> 02:28:55.120
know where anything is, how to get to the details behind something when

02:28:55.120 --> 02:28:56.920
something screws up as it invariably does.

02:28:57.600 --> 02:29:01.160
And just things like changing group permissions on some shared folders and

02:29:01.160 --> 02:29:04.680
stuff, just everything seems a little bit more awkward, more clicks than it needs

02:29:04.680 --> 02:29:08.720
to be, not to say that there aren't any weird things like hidden attributes and

02:29:08.720 --> 02:29:14.600
all this other happy stuff on, on, on Mac, but for the most part, and well,

02:29:14.600 --> 02:29:17.000
actually, especially now with the new hardware coming out on Mac, it'll be

02:29:17.000 --> 02:29:21.120
very interesting, you know, with the new M1, there were some dark years in the

02:29:21.120 --> 02:29:24.160
last few years when I was like, I think maybe I have to move off of Mac as a

02:29:24.160 --> 02:29:29.040
platform, but this, I mean, like my keyboard was just not working.

02:29:29.080 --> 02:29:30.640
Like literally my keyboard just wasn't working.

02:29:30.640 --> 02:29:30.920
Right.

02:29:31.240 --> 02:29:34.120
I had this touch bar, didn't have a physical escape button like I needed to

02:29:34.120 --> 02:29:36.840
cause I use Vim and now I think we're back.

02:29:36.920 --> 02:29:37.400
So, yeah.

02:29:37.440 --> 02:29:40.000
So you use Vim and you have a, what kind of keyboard?

02:29:40.320 --> 02:29:42.240
So I use a real force 87U.

02:29:42.720 --> 02:29:46.080
Uh, it's a mechanical as a Topra key switch is a weird shape.

02:29:46.080 --> 02:29:46.960
There's a normal shape.

02:29:47.240 --> 02:29:47.480
Okay.

02:29:48.480 --> 02:29:52.440
Well, no, cause I say that because I use a Kinesis and I had, you said some

02:29:52.440 --> 02:29:54.960
dark, you said, yeah, dark moments.

02:29:54.960 --> 02:29:57.400
I've, I've recently had a dark moment.

02:29:57.400 --> 02:29:58.760
It's like, what am I doing with my life?

02:29:58.760 --> 02:30:04.880
So I remember sort of flying in a very kind of tight space and as I'm working,

02:30:05.080 --> 02:30:06.520
this is what I do on an airplane.

02:30:06.560 --> 02:30:10.960
I pull out a laptop and on top of the laptop, I'll put a Kinesis keyboard.

02:30:11.040 --> 02:30:11.960
That's hardcore, man.

02:30:12.040 --> 02:30:13.720
I was thinking, is this who I am?

02:30:13.800 --> 02:30:15.000
Is this what I'm becoming?

02:30:15.040 --> 02:30:16.120
Will that be this person?

02:30:16.120 --> 02:30:21.360
Cause I'm on Emacs with this Kinesis keyboard sitting like with everybody

02:30:21.360 --> 02:30:24.840
around Emacs on windows on WSL.

02:30:24.840 --> 02:30:25.040
Yeah.

02:30:25.520 --> 02:30:25.720
Yeah.

02:30:25.720 --> 02:30:27.240
Emacs on Linux on windows.

02:30:27.280 --> 02:30:27.560
Yeah.

02:30:27.720 --> 02:30:33.080
On windows and like everybody around me is using their iPhone to look at TikTok.

02:30:33.080 --> 02:30:37.600
So I'm like in this land and I thought, you know what, maybe I need to become

02:30:37.640 --> 02:30:42.920
an adult and put the nineties behind me and use like a normal keyboard.

02:30:43.280 --> 02:30:46.720
And then I did some soul searching and I decided like, this is who I am.

02:30:46.720 --> 02:30:50.120
This is me like coming out of the closet to saying I'm Kinesis keyboard all the

02:30:50.120 --> 02:30:53.040
way, I'm going to use Emacs, you know.

02:30:53.040 --> 02:30:54.160
You know who else is a Kinesis fan?

02:30:54.160 --> 02:30:56.520
Um, uh, uh, Wes McKinney, the creator of pandas.

02:30:56.680 --> 02:31:00.000
Oh, he, he, he just, he banged out pandas on a Kinesis keyboard, I believe.

02:31:00.000 --> 02:31:03.560
I don't know if he's still using one maybe, but certainly 10 years ago, like

02:31:03.560 --> 02:31:06.880
he was, if anyone's out there, maybe we need to have a Kinesis support

02:31:06.880 --> 02:31:08.280
group, please reach out.

02:31:08.320 --> 02:31:09.160
Isn't there a ready one?

02:31:09.440 --> 02:31:10.040
Is there one?

02:31:10.080 --> 02:31:10.320
I don't know.

02:31:10.320 --> 02:31:11.520
There's gotta be an IRC channel, man.

02:31:13.120 --> 02:31:15.240
Oh no.

02:31:15.280 --> 02:31:16.760
And you access it through Emacs.

02:31:16.840 --> 02:31:17.240
Okay.

02:31:18.000 --> 02:31:19.360
Do you still program these days?

02:31:19.560 --> 02:31:20.360
I do a little bit.

02:31:20.640 --> 02:31:26.440
Um, honestly, the last thing I did was, um, I had written, I was working with

02:31:26.440 --> 02:31:28.040
my son to script some Minecraft stuff.

02:31:28.480 --> 02:31:29.520
So I was doing a little bit of that.

02:31:29.520 --> 02:31:32.160
That was the last, literally the last code I wrote.

02:31:33.040 --> 02:31:33.440
Oh, you know what?

02:31:33.440 --> 02:31:36.920
I also, I wrote some code to do some, um, cap table evaluation, waterfall

02:31:36.920 --> 02:31:37.800
modeling kind of stuff.

02:31:39.200 --> 02:31:41.240
What advice would you give to a young person?

02:31:41.240 --> 02:31:45.200
You said your son today in high school, maybe even college

02:31:45.240 --> 02:31:47.080
about career, about life.

02:31:48.480 --> 02:31:50.000
This may be where I get into trouble a little bit.

02:31:51.000 --> 02:31:52.720
We are coming to the end.

02:31:53.280 --> 02:31:55.960
We're, we're rapidly entering a time between worlds.

02:31:56.400 --> 02:32:00.800
So we have a world now that's starting to really crumble under the weight of

02:32:00.800 --> 02:32:04.560
aging institutions that no longer even pretend to serve the purposes they were

02:32:04.560 --> 02:32:09.360
created for, we are creating technologies that are hurtling billions of people

02:32:09.360 --> 02:32:13.560
headlong into philosophical crises who they don't even know the philosophical

02:32:13.560 --> 02:32:16.480
operating systems in their firmware and they're heading into a time when that

02:32:16.480 --> 02:32:17.160
gets vaporized.

02:32:17.720 --> 02:32:21.600
So for people in high school, and certainly I tell my son this as well, he's

02:32:21.600 --> 02:32:28.880
in middle school, people in college, you are going to have to find your own way.

02:32:29.600 --> 02:32:33.960
You're going to have to have a pioneer spirit, even if you live in the middle

02:32:33.960 --> 02:32:36.120
of the most dense urban environment.

02:32:37.080 --> 02:32:44.120
All of human reality around you is the result of the last few generations of

02:32:44.240 --> 02:32:47.280
humans agreeing to play certain kinds of games.

02:32:48.000 --> 02:32:54.000
A lot of those games no longer, no longer operate according to the rules they used

02:32:54.000 --> 02:32:58.840
to, collapse is nonlinear, but it will be managed.

02:32:58.840 --> 02:33:04.960
And so if you are in a particular social caste or economic caste, and it's

02:33:05.800 --> 02:33:10.880
not, I think it's not kosher to say that about America, but America is a very

02:33:10.880 --> 02:33:13.520
stratified and classist society.

02:33:14.080 --> 02:33:17.000
There's some mobility, but it's really quite classist.

02:33:17.040 --> 02:33:21.720
And in America, unless you're in the upper middle class, you are headed into

02:33:21.720 --> 02:33:22.880
very choppy waters.

02:33:23.480 --> 02:33:28.440
So it is really, really good to think and understand the fundamentals of what you

02:33:28.440 --> 02:33:33.640
need to build a meaningful life for you, your loved ones, with your family.

02:33:35.440 --> 02:33:40.280
And almost all of the technology being created that's consumer facing is designed

02:33:40.280 --> 02:33:48.280
to own people, to take the four stack of people, to delaminate them and to own

02:33:48.280 --> 02:33:49.560
certain portions of that stack.

02:33:50.360 --> 02:33:53.760
And so if you want to be an integral human being, if you want to have your

02:33:53.760 --> 02:33:58.680
agency and you want to find your own way in the world, when you're young would be

02:33:58.680 --> 02:34:04.920
a great time to spend time looking at some of the classics around what it means

02:34:04.920 --> 02:34:07.600
to live a good life, what it means to build connection with people.

02:34:08.000 --> 02:34:13.480
And so much of the status game, so much of the stuff, you know, one of the things

02:34:13.480 --> 02:34:18.840
that I sort of talk about as we create more and more technology, there's a

02:34:18.840 --> 02:34:21.680
gradient in technology and a gradient in technology always leads to a gradient

02:34:21.680 --> 02:34:22.160
in power.

02:34:22.600 --> 02:34:24.760
And this is Jacques Elul's point to some extent as well.

02:34:25.120 --> 02:34:26.920
That gradient in power is not going to go away.

02:34:27.200 --> 02:34:31.960
The technologies are going so fast that even people like me who helped create

02:34:31.960 --> 02:34:33.440
some of the stuff, I'm being left behind.

02:34:33.440 --> 02:34:34.600
That's on the cutting edge research.

02:34:34.600 --> 02:34:36.680
I don't know what's going on against today.

02:34:36.680 --> 02:34:38.120
You know, I go read some proceedings.

02:34:38.560 --> 02:34:43.360
So as the world gets more and more technological, it will create more and

02:34:43.360 --> 02:34:48.960
more gradients where people will seize power, economic fortunes, and the way

02:34:48.960 --> 02:34:52.920
they make the people who are left behind okay with their lot in life is they

02:34:52.920 --> 02:34:54.240
create lottery systems.

02:34:55.400 --> 02:35:01.520
They make you take part in the narrative of your own being trapped in your own

02:35:01.520 --> 02:35:03.600
economic sort of zone.

02:35:04.080 --> 02:35:08.800
So avoiding those kinds of things is really important, knowing when someone

02:35:08.800 --> 02:35:10.120
is running game on you basically.

02:35:10.640 --> 02:35:12.160
So these are things I would tell young people.

02:35:12.160 --> 02:35:13.920
It's a dark message, but it's realism.

02:35:14.120 --> 02:35:15.000
I mean, that's what I see.

02:35:15.760 --> 02:35:19.800
So after you gave some realism, you sit back, you sit back with your son, you

02:35:19.800 --> 02:35:26.520
looking out at the sunset, what to him can you give as words of hope and to

02:35:26.520 --> 02:35:31.800
you from where do you derive hope for the future of our world?

02:35:32.040 --> 02:35:37.200
So you said at the individual level, you have to have a pioneer mindset to go back

02:35:37.200 --> 02:35:41.720
to the classics, to understand what is in human nature, you can find meaning, but

02:35:41.720 --> 02:35:45.960
at the societal level, what trajectory, when you look at possible trajectories,

02:35:45.960 --> 02:35:46.880
what gives you hope?

02:35:47.400 --> 02:35:53.680
What gives me hope is that we have little tremors now shaking people out of the

02:35:53.680 --> 02:35:57.440
reverie of the fiction of modernity that they've been living in kind of a late

02:35:57.440 --> 02:35:59.280
20th century style modernity.

02:36:00.280 --> 02:36:06.760
That's good, I think, because, and also to your point earlier, people are burning

02:36:06.760 --> 02:36:07.880
out on some of the social media stuff.

02:36:07.880 --> 02:36:11.360
They're sort of seeing the ugly side, especially the latest news with Facebook

02:36:11.360 --> 02:36:12.560
and the whistleblower, right?

02:36:12.560 --> 02:36:16.480
It's quite clear these things are not all they're cracked up to be.

02:36:16.480 --> 02:36:20.840
So do you believe, like I believe better social media can be built because they

02:36:20.840 --> 02:36:24.640
are burning out and incentivize other competitors to be built.

02:36:25.000 --> 02:36:25.960
Do you think that's possible?

02:36:26.440 --> 02:36:33.640
Well, the thing about it is that when you have extractive return on returns, you

02:36:33.640 --> 02:36:37.000
know, capital coming in and saying, look, you own a network, give me some

02:36:37.000 --> 02:36:38.480
exponential dynamics out of this network.

02:36:38.960 --> 02:36:39.680
What are you going to do?

02:36:39.680 --> 02:36:42.960
You're going to just basically put a toll keeper at every single node and every

02:36:42.960 --> 02:36:46.720
single graph edge, every node, every vertex, every edge.

02:36:48.000 --> 02:36:51.400
But if you don't have that need for it, if no one's sitting there saying, hey,

02:36:51.440 --> 02:36:55.440
Wikipedia monetize every, every character, every bite, every phrase, then

02:36:55.480 --> 02:36:59.520
generative human dynamics will naturally sort of arise, assuming we do, we

02:36:59.520 --> 02:37:02.600
respect a few principles around online communications.

02:37:03.000 --> 02:37:06.560
So the greatest and biggest social network in the world is still like

02:37:06.600 --> 02:37:08.320
email SMS, right?

02:37:08.320 --> 02:37:08.600
Yes.

02:37:08.720 --> 02:37:10.360
So we're fine there.

02:37:10.520 --> 02:37:13.920
The issue with the social media, as we call it now, is they're all, they're

02:37:13.920 --> 02:37:16.600
actually just new amplification systems, right?

02:37:16.600 --> 02:37:19.880
Now it's benefited certain people like yourself who have interesting content,

02:37:20.440 --> 02:37:22.440
to, to, to, to be amplified.

02:37:23.080 --> 02:37:25.120
So it's created a creator economy and that's, that's cool.

02:37:25.120 --> 02:37:28.680
There's a lot of great content out there, but giving everyone a shot at the fame

02:37:28.680 --> 02:37:32.440
lottery saying, Hey, you could also have your, if you wiggle your butt the right

02:37:32.440 --> 02:37:35.800
way on TikTok, you can have your 15 seconds of micro fame.

02:37:36.040 --> 02:37:37.800
That's not healthy for society at large.

02:37:38.120 --> 02:37:44.200
So I think if we can create tools that help people be, um, uh, conscientious

02:37:44.240 --> 02:37:48.760
about their attention, spend time looking at the past and really retrieving memory

02:37:49.000 --> 02:37:52.800
and calling, not calling, but processing and thinking about that.

02:37:53.200 --> 02:37:56.240
I think that's certainly possible and hopefully that's what we get.

02:37:56.720 --> 02:38:00.960
Um, so I'm, so the bigger picture, the bigger question about, uh, you're asking

02:38:00.960 --> 02:38:06.520
about what gives me hope is that, um, these early shocks of, you know, COVID

02:38:06.520 --> 02:38:11.920
lockdowns and remote work and all these different kinds of things, I think it's

02:38:11.920 --> 02:38:17.960
getting people to a point where they are looking, they're, they're sort of no

02:38:17.960 --> 02:38:19.520
longer in the reverie, right?

02:38:19.560 --> 02:38:23.640
Uh, as my friend, Jim Rutt says, there's more people with ears to hear now, right?

02:38:23.640 --> 02:38:27.400
With pandemic, um, and education, everyone's like, wait, wait, what have

02:38:27.400 --> 02:38:28.600
you guys been doing with my kids?

02:38:28.600 --> 02:38:29.600
Like, how are you teaching them?

02:38:29.600 --> 02:38:32.000
That was, what is this crap you're giving them as homework, right?

02:38:32.320 --> 02:38:35.840
So I think these are the kinds of things that are getting, um, in the

02:38:35.840 --> 02:38:38.480
supply chain disruptions, getting more people to think about how do

02:38:38.480 --> 02:38:39.520
we actually just make stuff?

02:38:40.120 --> 02:38:41.200
This is all good.

02:38:41.880 --> 02:38:48.240
Um, but the concern is that it's still going to take a while for these things,

02:38:48.320 --> 02:38:52.880
for people to learn how to be agentic again, um, and to be in right

02:38:52.880 --> 02:38:55.240
relationship with each other and with the world.

02:38:55.800 --> 02:38:59.040
So the, the message of hope is still people are resilient and we're built.

02:38:59.040 --> 02:39:01.080
We are building some really amazing technology.

02:39:01.320 --> 02:39:07.200
And I also like to me, I derive a lot of hope from, from individuals in that van,

02:39:08.080 --> 02:39:12.680
the power of a single individual to, uh, transform the world, to do positive

02:39:12.680 --> 02:39:14.320
things for the world is quite incredible.

02:39:14.640 --> 02:39:17.280
Now you've been talking about, it's nice to have as many of those

02:39:17.280 --> 02:39:21.360
individuals as possible, but even the power of one, it's kind of magical.

02:39:21.840 --> 02:39:22.600
It is, it is.

02:39:22.600 --> 02:39:24.160
We're in a mode now where we can do that.

02:39:24.440 --> 02:39:28.560
I think also, you know, part of what I try to do is in coming to podcasts

02:39:28.560 --> 02:39:31.680
like yours, and then, you know, spamming with all this philosophical stuff

02:39:31.680 --> 02:39:35.120
that I've got going on, um, there are a lot of good people out there trying to

02:39:35.160 --> 02:39:41.960
put, um, words around the current technological, social, economic crises

02:39:41.960 --> 02:39:45.560
that we're facing and the space of a few short years, I think there has been a

02:39:45.560 --> 02:39:48.920
lot of great content produced around this stuff for people who want to see,

02:39:49.560 --> 02:39:51.800
want to find out more or think more about this.

02:39:51.880 --> 02:39:55.680
Um, we're popularizing certain kinds of philosophical ideas that, um, move

02:39:55.680 --> 02:39:58.680
field beyond just the, oh, you're communist, oh, you're capitalist kind of stuff.

02:39:58.680 --> 02:40:00.680
Like it's sort of, we're way past that now.

02:40:01.160 --> 02:40:05.520
So, um, that also gives me hope that I feel like I myself am getting a handle

02:40:05.720 --> 02:40:06.880
on how to think about these things.

02:40:07.360 --> 02:40:11.880
Um, it makes me feel like I can, you know, hopefully affect, uh, change for the better.

02:40:12.520 --> 02:40:15.680
We've been sneaking up on this question all over the place.

02:40:15.680 --> 02:40:17.480
Let me ask the big ridiculous question.

02:40:17.520 --> 02:40:19.640
What is the meaning of life?

02:40:20.360 --> 02:40:21.040
Wow.

02:40:21.600 --> 02:40:25.080
Um, the meaning of life.

02:40:28.840 --> 02:40:29.680
Yeah, I don't know.

02:40:29.680 --> 02:40:31.840
I mean, I've never really understood that question.

02:40:32.080 --> 02:40:40.600
When you say meeting crisis, you're, you're saying that there is a search for a kind

02:40:40.600 --> 02:40:47.760
of experience that's could be described as fulfillment as like the, ah, like the

02:40:47.760 --> 02:40:50.120
aha moment of just like joy.

02:40:50.160 --> 02:40:54.440
And maybe when you see something beautiful, or maybe you have created

02:40:54.480 --> 02:41:01.560
something beautiful, that experience that you get, it feels like it all makes sense.

02:41:02.600 --> 02:41:05.560
So some of that is just chemicals coming together in your mind and all kinds of

02:41:05.560 --> 02:41:12.600
things, but it seems like we're building a sophisticated, uh, collective intelligence

02:41:12.640 --> 02:41:16.600
that's providing meaning in all kinds of ways to its members.

02:41:17.120 --> 02:41:20.400
And it, there's a theme to that meaning.

02:41:20.600 --> 02:41:26.920
So for, for a lot of history, I think faith played an important role, faith

02:41:26.920 --> 02:41:28.680
in, in God's sort of religion.

02:41:29.640 --> 02:41:34.880
I think, uh, technology in the modern era is kind of serving a little bit of a

02:41:34.880 --> 02:41:37.720
source of meaning for people, like innovation of different kinds.

02:41:39.560 --> 02:41:46.320
I think the old school things of love and the basics of just being good at stuff.

02:41:47.200 --> 02:41:53.120
But you were a physicist, so there's a desire to say, okay, yeah, but these seem

02:41:53.120 --> 02:41:58.760
to be like symptoms of something deeper, like why, what's capital I'm meaning.

02:41:58.760 --> 02:41:59.000
Yeah.

02:41:59.000 --> 02:42:00.320
What's capital I'm meaning.

02:42:00.640 --> 02:42:04.640
Why are we reaching for order when there is excess of energy?

02:42:06.600 --> 02:42:08.400
I don't know if I can answer the why.

02:42:09.040 --> 02:42:14.320
Any why that I come up with, I think is going to be, um, I, I'd have to think

02:42:14.600 --> 02:42:18.040
about that a little more, maybe, maybe get back to you on that, but I will say this.

02:42:18.840 --> 02:42:23.200
Um, we do look at the world through a traditional, I think most people look

02:42:23.200 --> 02:42:26.280
at the world through, uh, what I would say is a subject, object, kind of

02:42:26.280 --> 02:42:31.520
metaphysical lens that we have our own subjectivity and then there's, there's

02:42:31.520 --> 02:42:34.120
all of these object things that are not us.

02:42:34.200 --> 02:42:37.000
So I'm, I'm me and this is, these things are not me, right.

02:42:37.240 --> 02:42:38.040
And I'm interacting with them.

02:42:38.040 --> 02:42:41.880
I'm doing things to them, but a different view of the world that looks

02:42:41.880 --> 02:42:48.600
at it as much more connected that realizes, Oh, I'm, I'm really quite

02:42:48.600 --> 02:42:50.640
embedded in a soup of other things.

02:42:50.640 --> 02:42:54.880
And I'm simply, um, almost like a standing wave pattern of different things.

02:42:54.880 --> 02:42:55.240
Right.

02:42:55.760 --> 02:43:00.040
Um, so when you look at the world in that kind of connected sense, I, I

02:43:00.320 --> 02:43:04.720
I've recently taken a shine to this particular thought experiment, which

02:43:04.720 --> 02:43:12.160
is what if it was the case that everything that we touch with our hands,

02:43:12.360 --> 02:43:16.680
that we pay attention to, that we actually give intimacy to, what if

02:43:16.680 --> 02:43:23.320
there's actually, you know, all the, all the mumbo jumbo, like, you know, people

02:43:23.320 --> 02:43:26.840
with the magnetic healing crystals and all this other kind of stuff and quantum

02:43:26.840 --> 02:43:29.640
energy stuff, what if that was a thing?

02:43:30.240 --> 02:43:34.600
What if when you literally, when your hand touches an object, when you really

02:43:34.600 --> 02:43:37.320
look at something and you concentrate and you focus on it and you really

02:43:37.320 --> 02:43:43.720
give it attention, you actually give it, there is some physical residue of

02:43:43.720 --> 02:43:47.880
something, a part of you, a bit of your life force that goes into it.

02:43:48.440 --> 02:43:48.760
Okay.

02:43:48.960 --> 02:43:51.440
Now this is of course, completely mumbo jumbo stuff.

02:43:51.440 --> 02:43:54.640
This is not like, I don't actually think this is real, but let's do

02:43:54.640 --> 02:43:55.480
the thought experiment.

02:43:55.560 --> 02:44:01.960
What if it was, what if there actually was some quantum magnetic crystal and,

02:44:02.000 --> 02:44:06.360
you know, energy field thing that just by touching this can, this can has

02:44:07.080 --> 02:44:11.760
changed a little bit somehow, and it's not much unless you put a lot into it

02:44:11.920 --> 02:44:14.480
and you touch it all the time, like your phone, right?

02:44:14.960 --> 02:44:20.840
These things gain, they gain meaning to you a little bit, but what if there's

02:44:20.840 --> 02:44:25.880
something that technical objects, the phone is a technical object, it does not

02:44:25.920 --> 02:44:31.120
really receive attention or intimacy and then allow itself to be transformed by

02:44:31.120 --> 02:44:36.040
it, but if it's a piece of wood, if it's the handle of a knife that your mother

02:44:36.040 --> 02:44:42.800
used for 20 years to make dinner for you, right, what if it's a keyboard that

02:44:42.800 --> 02:44:47.320
you banged out your world transforming software library on, these are technical

02:44:47.320 --> 02:44:51.360
objects and these are physical objects, but somehow there's something to them.

02:44:51.360 --> 02:44:55.080
We feel an attraction to these objects as if we have imbued them with life

02:44:55.080 --> 02:44:56.760
energy, right?

02:44:56.920 --> 02:44:59.680
So if you walk that thought experiment through, what happens when we touch

02:44:59.720 --> 02:45:01.720
another person, when we hug them, when we hold them?

02:45:03.160 --> 02:45:12.600
And the reason this ties into my answer for your question is that there is, if

02:45:12.600 --> 02:45:16.360
there is such a thing, if we were to hypothesize, you know, hypothesize such a

02:45:16.360 --> 02:45:26.200
thing, it could be that the purpose of our lives is to imbue as many things with

02:45:26.200 --> 02:45:27.400
that love as possible.

02:45:30.640 --> 02:45:34.000
That's a, that's a beautiful answer and a beautiful way to end it.

02:45:34.040 --> 02:45:36.560
Peter, you're an incredible person.

02:45:36.600 --> 02:45:36.960
Thank you.

02:45:36.960 --> 02:45:43.200
Spending so much in the space of engineering and in the space of philosophy.

02:45:43.680 --> 02:45:51.080
I, I'm really proud to be living in the same city as you and I'm really grateful

02:45:51.080 --> 02:45:52.960
that you would spend your valuable time with me today.

02:45:52.960 --> 02:45:53.720
Well, thank you.

02:45:53.720 --> 02:45:55.320
I appreciate the opportunity to speak with you.

02:45:56.520 --> 02:45:59.360
Thanks for listening to this conversation with Peter Wang to

02:45:59.360 --> 02:46:00.520
support this podcast.

02:46:00.560 --> 02:46:03.080
Please check out our sponsors in the description.

02:46:03.520 --> 02:46:07.000
And now let me leave you with some words for Peter Wang himself.

02:46:07.920 --> 02:46:12.760
We tend to think of people as either malicious or incompetent, but in a

02:46:12.760 --> 02:46:17.840
world filled with corruptible and unchecked institutions, there exists a

02:46:17.840 --> 02:46:20.640
third thing, malicious incompetence.

02:46:21.040 --> 02:46:25.960
It's a social cancer and it only appears once human organizations scale

02:46:25.960 --> 02:46:27.640
beyond personal accountability.

02:46:27.640 --> 02:46:31.400
Thank you for listening and hope to see you next time.


WEBVTT

00:00.000 --> 00:04.700
The following is a conversation with Boris Sofman, who is the senior director of engineering

00:04.700 --> 00:10.740
and head of trucking at Waymo, the autonomous vehicle company, formerly the Google self-driving

00:10.740 --> 00:12.700
car project.

00:12.700 --> 00:18.820
Before that, Boris was the co-founder and CEO of Anki, a robotics company that created

00:18.820 --> 00:24.420
Cosmo, which, in my opinion, is one of the most incredible social robots ever built.

00:24.420 --> 00:30.160
It's a toy robot, but one with an emotional intelligence that creates a fun and engaging

00:30.160 --> 00:31.600
human-robot interaction.

00:31.600 --> 00:36.200
It was truly sad for me to see Anki shut down when he did.

00:36.200 --> 00:39.680
I had high hopes for those little robots.

00:39.680 --> 00:45.900
We talk about this story and the future of autonomous trucks, vehicles, and robotics

00:45.900 --> 00:46.900
in general.

00:46.900 --> 00:53.340
I spoke with Steve Viseli recently on episode 237 about the human side of trucking.

00:53.340 --> 00:57.200
This episode looks more at the robotics side.

00:57.200 --> 00:59.300
This is the Lex Fridman Podcast.

00:59.300 --> 01:03.060
To support it, please check out our sponsors in the description.

01:03.060 --> 01:08.460
And now, here's my conversation with Boris Sofman.

01:08.460 --> 01:12.620
Who is your favorite robot in science fiction, books, or movies?

01:12.620 --> 01:19.620
Wally and R2D2, where they were able to convey such an incredible degree of intent, emotion,

01:19.860 --> 01:26.860
and character attachment without having any language whatsoever, and just purely through

01:26.860 --> 01:29.180
the richness of emotional interaction.

01:29.180 --> 01:30.180
Those are fantastic.

01:30.180 --> 01:36.380
And then, the Terminator series, just like really, pretty wide range, right?

01:36.380 --> 01:41.400
But I kind of love this dynamic where you have this incredible Terminator itself that

01:41.400 --> 01:42.900
Arnold played.

01:42.900 --> 01:47.700
And then, he was kind of like the inferior, previous generation version that was like

01:47.740 --> 01:52.940
totally outmatched in terms of kind of specs by the new one, but still kind of like held

01:52.940 --> 01:53.940
his own.

01:53.940 --> 01:58.620
And so, it was kind of interesting where you realize how many levels there are on the spectrum

01:58.620 --> 02:02.820
from human to kind of potentials in AI and robotics to futures.

02:02.820 --> 02:08.780
And so, yeah, that movie really, as much as it was like kind of a dark world in a way,

02:08.780 --> 02:09.780
was actually quite fascinating.

02:09.780 --> 02:11.140
It gets the imagination going.

02:11.140 --> 02:16.860
Well, from an engineering perspective, both the movies you mentioned, Wally and Terminator,

02:16.860 --> 02:23.780
the first one is probably achievable, humanoid robot, maybe not with like the realism in

02:23.780 --> 02:28.620
terms of skin and so on, but that humanoid form, we have that humanoid form.

02:28.620 --> 02:30.620
It seems like a compelling form.

02:30.620 --> 02:35.340
Maybe the challenge is it's super expensive to build, but you can imagine, maybe not a

02:35.340 --> 02:40.780
machine of war, but you can imagine Terminator type robots walking around.

02:40.780 --> 02:44.180
And then, the same, obviously, with Wally, you've basically, so for people who don't

02:44.180 --> 02:50.340
know, you created the company Anki that created a small robot with a big personality called

02:50.340 --> 02:57.900
Cosmo that just does exactly what Wally does, which is somehow with very few basic visual

02:57.900 --> 03:00.820
tools is able to communicate a depth of emotion.

03:00.820 --> 03:02.500
And that's fascinating.

03:02.500 --> 03:07.100
But then, again, the humanoid form is super compelling.

03:07.100 --> 03:11.180
So Cosmo is very distant from a humanoid form.

03:11.180 --> 03:13.260
And then, the Terminator has a humanoid form.

03:13.260 --> 03:16.100
And you can imagine both of those actually being in our society.

03:16.100 --> 03:17.100
That's true.

03:17.100 --> 03:23.180
And it's interesting because it was very intentional to go really far away from human form when

03:23.180 --> 03:30.260
you think about a character like Cosmo or like Wally, where you can completely rethink

03:30.260 --> 03:34.860
the constraints you put on that character, what tools you leverage, and then how you

03:34.860 --> 03:41.260
actually create a personality and a level of intelligence interactivity that actually

03:41.500 --> 03:46.820
matches the constraints that you're under, whether it's mechanical or sensors or AI of

03:46.820 --> 03:47.820
the day.

03:47.820 --> 03:52.140
This is why I almost was always very surprised by how much energy people put towards trying

03:52.140 --> 03:57.420
to replicate human form in a robot because you actually take on some pretty significant

03:57.420 --> 04:02.420
kind of constraints and downsides when you do that, the first of which is obviously the

04:02.420 --> 04:09.100
cost where it's just the articulation of a human body is just so magical in both the

04:09.100 --> 04:13.700
precision as well as the dimensionality that to replicate that even in its reasonably

04:13.700 --> 04:19.460
close form takes like a giant amount of joints and actuators and motion and sensors and encoders

04:19.460 --> 04:20.460
and so forth.

04:20.460 --> 04:25.620
But then, you're almost like setting an expectation that the closer you try to get to human form,

04:25.620 --> 04:28.100
the more you expect the strengths to match.

04:28.100 --> 04:31.780
And that's not the way AI works is there's places where you're way stronger and there's

04:31.780 --> 04:33.420
places where you're weaker.

04:33.420 --> 04:38.260
And by moving away from human form, you can actually change the rules and embrace your

04:38.300 --> 04:39.980
strengths and bypass your weaknesses.

04:39.980 --> 04:45.700
And at the same time, the human form like has way too many degrees of freedom to play

04:45.700 --> 04:49.340
with. It's it's kind of counterintuitive, just as you're saying.

04:49.580 --> 04:56.500
But when you have fewer constraints, it's almost harder to master the communication

04:56.500 --> 05:01.340
of emotion. Like you see this with cartoons like stick figures, you can communicate quite

05:01.340 --> 05:07.300
a lot with just very minimal, like two dots for eyes and a line for for a smile.

05:07.300 --> 05:12.340
I think you can almost communicate arbitrary levels of emotion with just two dots and a

05:12.340 --> 05:14.220
line. Yeah. And like that's enough.

05:14.220 --> 05:18.140
And if you focus on just that, you can communicate the full range.

05:18.420 --> 05:25.660
And then you like if you do that, then you can focus on the actual magic of of human

05:25.660 --> 05:30.620
and dot line interaction versus all the engineering mess.

05:30.620 --> 05:33.820
That's right. Like dimensionality, voice, all these sort of things actually become a

05:33.820 --> 05:36.380
crutch where you get lost in a search space almost.

05:37.180 --> 05:43.380
And so some of the best animators that we've worked with, they almost like study when

05:43.380 --> 05:49.540
they come up, you know, kind of in building their expertise by forcing these projects

05:49.540 --> 05:54.060
where all you have is like a ball that can like kind of jump and manipulate itself or

05:54.060 --> 05:59.100
like really, really like aggressive constraints where you're forced to kind of extract the

05:59.100 --> 06:02.860
deepest love of emotion. And so in a lot of ways, you know, we thought when we thought

06:02.860 --> 06:07.100
about Cosmo, you're right. Like if we had to like describe it in like one small phrase,

06:07.100 --> 06:09.980
it was bringing a Pixar character to life in the real world.

06:09.980 --> 06:11.780
And so it's what we were going for.

06:11.780 --> 06:15.860
And in a lot of ways, what was interesting is that with like Wall-E, which we studied

06:15.860 --> 06:20.660
incredibly deeply and in fact, some of our team were, you know, kind of had worked

06:20.660 --> 06:25.660
previously at Pixar on our project, they intentionally constrained Wall-E as well,

06:25.660 --> 06:30.180
even though in an animated film you could do whatever you wanted to, because it forced

06:30.220 --> 06:36.420
you to like really saturate the smaller amount of dimensions, but you sometimes end up getting

06:36.420 --> 06:42.460
a far more beautiful output because you're pushing at the extremes of this emotional

06:42.460 --> 06:47.060
space in a way that you just wouldn't because you get lost in a surface area if you have

06:47.060 --> 06:49.180
like something that is just infinitely articulable.

06:49.180 --> 06:56.540
So if we backtrack a little bit and you thought of Cosmo in 2011 and 2013 actually designed

06:56.540 --> 07:04.740
and built it, what is Anki, what is Cosmo, I guess who is Cosmo and what was the vision

07:04.740 --> 07:06.820
behind this incredible little robot?

07:06.820 --> 07:10.820
We started Anki back in like while we were still in graduate school.

07:10.820 --> 07:15.540
So myself and my two co-founders, we were PhD students in the Robotics Institute at

07:15.540 --> 07:16.740
Carnegie Mellon.

07:16.740 --> 07:23.020
And so we were studying robotics, AI, machine learning, kind of different areas.

07:23.020 --> 07:27.180
One of my co-founders was working on walking robots for a period of time.

07:27.180 --> 07:36.020
And so we all had a bit of a deeper passion for applications of robotics and AI where

07:36.020 --> 07:38.940
there's like a spectrum where there's people that get like really fascinated by the theory

07:38.940 --> 07:43.700
of AI and machine learning robotics where whether it gets applied in the near future

07:43.700 --> 07:48.020
or not is less of a kind of factor on them, but they love the pursuit of the challenge

07:48.020 --> 07:49.020
and that's necessary.

07:49.020 --> 07:50.940
And there's a lot of incredible breakthroughs that happen there.

07:50.940 --> 07:54.940
We're probably closer to the other end of the spectrum where we love the technology

07:54.940 --> 07:59.460
and all the evolution of it, but we were really driven by applications, like how can you really

07:59.460 --> 08:05.120
reinvent experiences and functionality and build value that wouldn't have been possible

08:05.120 --> 08:07.340
without these approaches?

08:07.340 --> 08:08.340
And that's what drove us.

08:08.340 --> 08:12.540
And we had kind of some experiences through previous jobs and internships where we got

08:12.540 --> 08:14.580
to see the applied side of robotics.

08:14.580 --> 08:19.600
And at that time, there was actually relatively few applications of robotics that were outside

08:19.680 --> 08:25.280
of, you know, pure research or industrial applications, military applications and so

08:25.280 --> 08:26.280
forth.

08:26.280 --> 08:27.780
There were very few outside of it.

08:27.780 --> 08:31.640
So maybe, you know, iRobot was like one exception and maybe there were a few others, but for

08:31.640 --> 08:32.640
the most part, there weren't that many.

08:32.640 --> 08:38.160
And so we got excited about consumer applications of robotics where you could leverage way higher

08:38.160 --> 08:43.040
levels of intelligence through software to create value and experiences that were just

08:43.040 --> 08:48.040
not possible in those fields today.

08:48.040 --> 08:53.800
And we saw kind of a pretty wide range of applications that varied in the complexity

08:53.800 --> 08:55.960
of what it would take to actually solve those.

08:55.960 --> 09:00.400
And what we wanted to do was to commercialize this into a company, but actually do a bottoms

09:00.400 --> 09:04.680
up approach where we could have a huge impact in a space that was ripe to have an impact

09:04.680 --> 09:07.880
at that time and then build up off of that and move into other areas.

09:07.880 --> 09:12.400
And entertainment became the place to start because you had relatively little innovation

09:12.400 --> 09:17.280
in a toy space, an entertainment space, you had these really rich experiences in video

09:17.320 --> 09:21.280
games and movies, but there was like this chasm in between.

09:21.280 --> 09:25.320
And so we thought that we could really reinvent that experience.

09:25.320 --> 09:29.280
And there was a really fascinating transition technically that was happening at the time

09:29.280 --> 09:34.040
where the cost of components was plummeting because of the mobile phone industry and then

09:34.040 --> 09:35.480
the smartphone industry.

09:35.480 --> 09:40.760
And so the cost of a microcontroller, of a camera, of a motor, of memory, of microphones,

09:40.760 --> 09:44.000
cameras was dropping by orders of magnitude.

09:44.000 --> 09:52.520
And then on top of that with the iPhone coming out in 2000, I think it was 2007, I believe,

09:52.520 --> 09:57.560
it started to become apparent within a couple of years that this could become a really incredible

09:57.560 --> 10:03.040
interface device and the brain with much more computation behind a physical world experience

10:03.040 --> 10:05.560
that wouldn't have been possible previously.

10:05.560 --> 10:11.680
And so we really got excited about that and how we push all the complexity from the physical

10:11.680 --> 10:16.240
world into software by using really inexpensive components, but putting huge amounts of complexity

10:16.240 --> 10:17.640
into the AI side.

10:17.640 --> 10:20.960
And so Cosmo became our second product and then the one that we're probably most proud

10:20.960 --> 10:21.960
of.

10:21.960 --> 10:26.880
The idea there was to create a physical character that had enough understanding and awareness

10:26.880 --> 10:33.960
of the physical world around it and the context that mattered to feel like he was alive and

10:33.960 --> 10:38.760
to be able to have these emotional kind of connections and experiences with people that

10:38.760 --> 10:41.560
you would typically only find inside of a movie.

10:42.040 --> 10:44.360
The motivation very much was Pixar.

10:44.360 --> 10:49.720
We had an incredible respect and appreciation for what they were able to build in this really

10:49.720 --> 10:55.920
beautiful fashion and film, but it was always, one, it was virtual, and two, it was a story

10:55.920 --> 10:57.920
on rails that had no interactivity to it.

10:57.920 --> 11:03.240
It was very fixed and it obviously had a magic to it, but where you really start to hit a

11:03.240 --> 11:06.120
different level of experiences when you're actually able to physically interact with

11:06.120 --> 11:07.120
our robot.

11:07.120 --> 11:10.320
And then that was your idea with Anki, the first product was the cars.

11:10.320 --> 11:18.520
So basically you take a toy, you add intelligence into it in the same way you would add intelligence

11:18.520 --> 11:23.360
into AI systems within a video game, but you're now bringing it into the physical space.

11:23.360 --> 11:29.600
So the idea is really brilliant, which is you're basically bringing video games to life.

11:29.600 --> 11:30.600
Exactly.

11:30.600 --> 11:31.600
That's exactly right.

11:31.600 --> 11:35.400
We literally use that exact same phrase because in the case of Drive, this was a parallel

11:35.480 --> 11:42.320
of the racing genre and the goal was to effectively have a physical racing experience, but have

11:42.320 --> 11:47.280
a virtual state at all times that matches what's happening in the physical world.

11:47.280 --> 11:51.720
And then you can have a video game off of that and you can have different characters,

11:51.720 --> 11:56.800
different traits for the cars, weapons and interactions and special abilities and all

11:56.800 --> 12:00.600
these sort of things that you think of virtually, but then you can have it physically.

12:00.600 --> 12:05.360
And one of the things that we were really surprised by that really stood out and immediately

12:05.600 --> 12:11.480
led us to really accelerate the path towards Cosmo is that things that feel like they're

12:11.480 --> 12:14.800
really constrained and simple in the physical world, they have an amplified impact on people

12:14.800 --> 12:18.240
where the exact same experience virtually would not have anywhere near the impact, but

12:18.240 --> 12:20.680
seeing it physically really stood out.

12:20.680 --> 12:26.920
And so effectively with Drive, we were creating a video game engine for the physical world.

12:26.920 --> 12:34.400
And then with Cosmo, we expanded that video game engine to create a character and an animation

12:34.400 --> 12:40.040
and an interaction engine on top of it that allowed us to start to create these much more

12:40.040 --> 12:41.040
rich experiences.

12:41.040 --> 12:45.520
And a lot of those elements were almost like a proving ground for what would human robot

12:45.520 --> 12:50.560
interaction feel like in a domain that's much more forgiving, where you can make mistakes

12:50.560 --> 12:51.560
in a game.

12:51.560 --> 12:57.720
It's okay if a car goes off the track or if Cosmo makes a mistake.

12:57.720 --> 13:00.340
And what's funny is actually we were so worried about that.

13:00.340 --> 13:04.080
In reality, we realized very quickly that those mistakes can be endearing and if you

13:04.080 --> 13:07.200
make a mistake, as long as you realize you make a mistake and have the right emotional

13:07.200 --> 13:09.920
reaction to it, it builds even more empathy with the character.

13:09.920 --> 13:10.920
That's brilliant.

13:10.920 --> 13:11.920
Exactly.

13:11.920 --> 13:16.600
So when the thing you're optimizing for is fun, you have so much more freedom to fail,

13:16.600 --> 13:18.200
to explore.

13:18.200 --> 13:21.400
And also in the toy space, like all of this is really brilliant.

13:21.400 --> 13:24.360
I got to ask you backtrack.

13:24.360 --> 13:32.920
It seems for a roboticist to take a jump in into the direction of fun is a brilliant move

13:32.920 --> 13:37.040
because one, you have the freedom to explore and to design all those kinds of things.

13:37.040 --> 13:41.520
And you can also build cheap robots.

13:41.520 --> 13:47.480
If you're not chasing perfection and toys, it's understood that you can go cheaper, which

13:47.480 --> 13:52.760
means in robot, it's still expensive, but it's actually affordable by a large number

13:52.760 --> 13:53.760
of people.

13:53.760 --> 13:55.720
So it's a really brilliant space to explore.

13:55.720 --> 13:56.720
Yeah, that's right.

13:56.720 --> 14:01.000
And in fact, we realized pretty quickly that perfection is actually not fun because in

14:01.000 --> 14:06.880
a traditional roboticist sense, the first kind of path planner, and this is the part

14:06.880 --> 14:12.000
that I worked on out of the gate, was a lot of the AI systems where you have these vehicles

14:12.000 --> 14:16.680
and cars racing making optimal maneuvers to try to get ahead.

14:16.680 --> 14:23.240
And you realize very quickly that that's actually not fun because you want the chaos from mistakes.

14:23.240 --> 14:27.920
And so you start to intentionally almost add noise to the system in order to create more

14:27.920 --> 14:32.400
of a realism in the exact same way the human player might start really ineffective and

14:32.400 --> 14:37.560
inefficient and then start to increase their quality bar as they progress.

14:37.560 --> 14:42.520
And there is a really, really aggressive constraint that's forced on you by being a consumer product

14:42.520 --> 14:49.200
where the price point matters a ton, particularly in an entertainment where you can't make a

14:49.200 --> 14:53.400
thousand dollar product unless you're going to meet the expectations of a thousand dollar

14:53.400 --> 14:54.400
product.

14:54.960 --> 15:01.080
In order to make this work, your cost of goods had to be well under $100.

15:01.080 --> 15:04.640
In the case of Cosmo, we got it under $50, end to end, fully packaged and delivered.

15:04.640 --> 15:05.640
And it was under $200.

15:05.640 --> 15:06.640
At retail.

15:06.640 --> 15:07.640
At retail.

15:07.640 --> 15:08.640
Yeah.

15:08.640 --> 15:15.400
So, okay, if we sit down at the early stages, if we go back to that, and you're sitting

15:15.400 --> 15:19.640
down and thinking about what Cosmo looks like from a design perspective and from a cost

15:19.640 --> 15:24.240
perspective, I imagine that was part of the conversation.

15:24.280 --> 15:26.080
First of all, what came first?

15:26.080 --> 15:28.000
Did you have a cost in mind?

15:28.000 --> 15:30.400
Is there a target you're trying to chase?

15:30.400 --> 15:32.840
Did you have a vision in mind, like size?

15:32.840 --> 15:35.720
Did you have, because there's a lot of unique qualities to Cosmo.

15:35.720 --> 15:38.520
So for people who don't know, they should definitely check it out.

15:38.520 --> 15:43.280
There's a display, there's eyes on the little display and those eyes can, it's pretty low

15:43.280 --> 15:44.720
resolution eyes, right?

15:44.720 --> 15:47.760
But they still able to convey a lot of emotion.

15:47.760 --> 15:53.080
And there's this arm, like that lift sort of lift stuff.

15:53.320 --> 15:59.720
But there's something about arm movement that adds even more kind of depth.

15:59.720 --> 16:06.160
It's like the face communicates emotion and sadness and disappointment and happiness.

16:06.160 --> 16:10.200
And then the arms kind of communicates, I'm trying here.

16:10.200 --> 16:11.200
Yeah.

16:11.200 --> 16:14.040
I'm doing my best in this complicated world.

16:14.040 --> 16:15.040
Exactly.

16:15.040 --> 16:20.600
So it's interesting because like all of Cosmo is only four degrees of freedom.

16:20.600 --> 16:23.440
And two of them are the two treads, which is for basic movement.

16:23.440 --> 16:27.960
And so you literally have only a head that goes up and down, a lift that goes up and

16:27.960 --> 16:30.160
down, and then your two wheels.

16:30.160 --> 16:34.600
And you have sound and a screen, and a low resolution screen.

16:34.600 --> 16:38.800
And with that, it's actually pretty incredible what you can come up with where, like you

16:38.800 --> 16:43.760
said, it's a really interesting give and take because there's a lot of ideas far beyond

16:43.760 --> 16:46.320
that, obviously, as you can imagine where, like you said, how big is it?

16:46.320 --> 16:47.320
How many degrees of freedom?

16:47.320 --> 16:48.320
What does he look like?

16:48.320 --> 16:50.520
What does he sound like?

16:50.520 --> 16:52.040
How does he communicate?

16:52.040 --> 16:54.420
It's a formula that actually scales way beyond entertainment.

16:54.420 --> 16:59.440
This is the formula for human kind of robot interface more generally is you almost have

16:59.440 --> 17:04.520
this triangle between the physical aspects of it, the mechanics, the industrial design,

17:04.520 --> 17:07.560
what's mass-producible, the cost constraints and so forth.

17:07.560 --> 17:12.800
You have the AI side of how do you understand the world around you, interact intelligently

17:12.800 --> 17:14.600
with it, execute what you want to execute.

17:14.600 --> 17:19.840
So perceive the environment, make intelligent decisions, and move forward.

17:19.840 --> 17:23.000
And then you have the character side of it.

17:23.000 --> 17:27.800
Most companies that have done anything in human-robot interaction really miss the mark

17:27.800 --> 17:30.280
or under-invest in the character side of it.

17:30.280 --> 17:35.800
They over-invest in the mechanical side of it, and then varied results on the AI side

17:35.800 --> 17:36.800
of it.

17:36.800 --> 17:41.200
And so the thinking is that you put more mechanical flexibility into it, you're going to do better.

17:41.200 --> 17:42.200
You don't necessarily.

17:42.200 --> 17:46.440
You actually create a much higher bar for a high ROI because now your price point goes

17:46.440 --> 17:48.280
up, your expectations go up.

17:48.320 --> 17:53.400
And if the AI can't meet it or the overall experience isn't there, you miss the mark.

17:53.400 --> 18:01.080
So how did you, through those conversations, get the cost down so much and made it so simple?

18:01.080 --> 18:05.440
There's a big theme here because you come from the mecca of robotics, which is Carnegie

18:05.440 --> 18:08.240
Mellon University.

18:08.240 --> 18:13.960
For all the people I've interacted with that come from there or just from the world experts

18:13.960 --> 18:19.520
at robotics, they would never build something like Cosmo.

18:19.520 --> 18:22.840
And so where did that come from, the simplicity?

18:22.840 --> 18:27.080
It came from this combination of a team that we had, and it was quite cool because we,

18:27.080 --> 18:32.360
and by the way, you ask anybody that's experienced in the toy entertainment space, you'll never

18:32.360 --> 18:34.240
sell a product over $99.

18:34.240 --> 18:37.040
That was fundamentally false, and we believed it to be false.

18:37.040 --> 18:40.100
It was because experience had to meet the mark.

18:40.100 --> 18:43.840
And so we pushed past that amount, but there was a pressure where the higher you go, the

18:43.840 --> 18:46.240
more seasonal you become and the tougher it becomes.

18:46.240 --> 18:51.800
And so on the cost side, we very quickly partnered up with some previous contacts that we worked

18:51.800 --> 18:57.980
with where, just as an example, our head of mechanical engineering was one of the earliest

18:57.980 --> 19:02.680
heads of engineering at Logitech and has a billion units of consumer products and circulation

19:02.680 --> 19:03.680
that he's worked on.

19:03.680 --> 19:08.400
So like crazy, low cost, high volume consumer product experience, we had a really great

19:08.400 --> 19:12.600
mechanical engineering team and just a very practical mindset where we were not going

19:12.600 --> 19:16.480
to compromise on feasibility in the market in order to chase something that would be

19:16.480 --> 19:17.480
an enabler.

19:17.480 --> 19:20.880
And we pushed a huge amount of expectations onto the software team where, yes, we're going

19:20.880 --> 19:28.200
to use cheap, noisy motors and sensors, but we're going to fix it on the software side.

19:28.200 --> 19:32.000
Then we found on the design and character side, there was a faction that was more from

19:32.000 --> 19:36.720
like a game design background that thought that it should be very games driven, Cosmo,

19:36.720 --> 19:41.080
where you create a whole bunch of games experiences and it's all about game mechanics.

19:41.080 --> 19:46.120
And then there was a faction which my co-founder and I, they're most involved in this, really

19:46.120 --> 19:47.840
believed in, which was character driven.

19:47.840 --> 19:51.280
And the argument is that you will never compete with what you can do virtually from a game

19:51.280 --> 19:56.140
standpoint, but you actually, on the character side, put this into your wheelhouse and put

19:56.140 --> 20:02.760
it more towards your advantage because a physical character has a massively higher impact physically

20:02.760 --> 20:03.760
than virtually.

20:03.760 --> 20:05.320
Okay, can I just pause on that?

20:05.320 --> 20:07.240
Because this is so brilliant.

20:07.240 --> 20:10.640
For people who don't know, Cosmo plays games with you.

20:11.200 --> 20:13.240
But there's also a depth of character.

20:13.240 --> 20:21.360
And I actually, when I was playing with it, I wondered exactly what is the compelling

20:21.360 --> 20:22.920
aspect of this?

20:22.920 --> 20:29.040
Because to me, obviously I'm biased, but to me, the character, what I enjoyed most, honestly,

20:29.040 --> 20:32.800
or what got me to return to it is the character.

20:32.800 --> 20:33.800
That's right.

20:33.800 --> 20:37.240
That's a fascinating discussion of you're right.

20:37.240 --> 20:42.480
Ultimately, you cannot compete on the quality of the gaming experience.

20:42.480 --> 20:43.480
It's too restrictive.

20:43.480 --> 20:45.000
The physical world is just too restrictive.

20:45.000 --> 20:47.680
And you don't have a graphics engine, it's like all this.

20:47.680 --> 20:54.560
But on the character side, and clearly we moved in that direction as the winning path.

20:54.560 --> 21:01.240
And we partnered up with this really, we immediately went towards Pixar.

21:01.240 --> 21:05.520
And Carlos Bana, he had been at Pixar for nine years.

21:05.520 --> 21:11.000
He'd worked on tons of the movies, including Wall-E and others, and just immediately spoke

21:11.000 --> 21:15.040
the language and it just clicked on how you think about that magic and drive.

21:15.040 --> 21:21.360
And then we built out a team with him as a really prominent driver of this with different

21:21.360 --> 21:26.640
types of backgrounds and animators and character developers where we put these constraints

21:26.640 --> 21:32.440
on the team, but then got them to really try to create magic despite that.

21:32.760 --> 21:39.080
We converged on this system that was at the overlap of character and the character AI

21:39.080 --> 21:45.320
that where if you imagine the dimensionality of emotions, happy, sad, angry, surprised,

21:45.320 --> 21:52.400
confused, scared, you think of these extreme emotions, we almost put this challenge to

21:52.400 --> 21:58.600
populate this library of responses on how do you show the extreme response that goes

21:58.680 --> 22:02.280
to the extreme spectrum on angry or frustrated or whatever.

22:02.280 --> 22:05.200
And so that gave us a lot of intuition and learnings.

22:05.200 --> 22:09.320
And then we started parameterizing them where it wasn't just a fixed recording, but they

22:09.320 --> 22:13.520
were parameterized and had randomness to them where you could have infinite permutations

22:13.520 --> 22:16.400
of happy and surprised and so forth.

22:16.400 --> 22:21.600
And then we had a behavioral engine that took the context from the real world and would

22:21.600 --> 22:25.960
interpret it and then create kind of probability mappings on what sort of responses you would

22:25.960 --> 22:27.400
have that actually made sense.

22:27.480 --> 22:33.080
And so if Cosmo saw you for the first time in a day, he'd be really surprised and happy

22:33.080 --> 22:37.600
in the same way that the first time you walk in and your toddler sees you, they're so happy,

22:37.600 --> 22:40.640
but they're not going to be that happy for the entirety of your next two hours.

22:40.640 --> 22:42.800
But you have this spike in response.

22:42.800 --> 22:46.560
Or if you leave him alone for too long, he gets bored and starts causing trouble and

22:46.560 --> 22:48.520
nudging things off the table.

22:48.520 --> 22:53.800
Or if you beat him in a game, the most enjoyable emotions are him getting frustrated and grumpy

22:53.800 --> 22:58.360
to a point where our testers and our customers would be like, I had to let him win because

22:58.360 --> 22:59.680
I don't want him to be upset.

22:59.680 --> 23:05.400
And so you start to create this feedback loop where you see how powerful those emotions

23:05.400 --> 23:06.400
are.

23:06.400 --> 23:09.000
And just to give you an example, something as simple as eye contact.

23:09.000 --> 23:10.560
You don't think about it in a movie.

23:10.560 --> 23:13.960
It kind of happens, camera angles and so forth.

23:13.960 --> 23:17.420
But that's not really a prominent source of interaction.

23:17.420 --> 23:23.780
What happens when a physical character like Cosmo, when he makes eye contact with you,

23:23.780 --> 23:28.420
it built universal kind of connection, kids all the way through adults.

23:28.420 --> 23:29.420
And it was truly universal.

23:29.420 --> 23:34.300
It was not like people stopped caring after 10, 12 years old.

23:34.300 --> 23:39.660
And so we started doing experiments and we found something as simple as increasing the

23:39.660 --> 23:43.500
amount of eye contact, like the amount of times in a minute that he'll look over for

23:43.500 --> 23:46.460
your approval till I kind of make eye contact.

23:46.460 --> 23:51.260
Just by I think doubling it, we increased the playtime engagement by 40%.

23:51.260 --> 23:54.220
You see these sort of interactions where you build that empathy.

23:54.220 --> 23:58.060
And so we studied pets, we studied virtual characters.

23:58.060 --> 24:04.420
A lot of times actually dogs are one of the most perfect influencers behind these sort

24:04.420 --> 24:05.420
of interactions.

24:05.420 --> 24:08.260
And what we realized is that the games were not there to entertain you.

24:08.260 --> 24:11.300
The games were to create context to bring out the character.

24:11.300 --> 24:15.800
And if you think about the types of games that you played, they were relatively simple,

24:15.800 --> 24:19.920
but they were always ones to create scenarios of either tension or winning or losing or

24:19.920 --> 24:22.720
surprise or whatever the case might be.

24:22.720 --> 24:27.400
And they were purely there to just like create context to where an emotion could feel intelligent

24:27.400 --> 24:28.400
and not random.

24:28.400 --> 24:30.400
And in the end, it was all about the character.

24:30.400 --> 24:34.000
So yeah, there's so many elements to play with here.

24:34.000 --> 24:39.360
So you said dogs, what lessons do we draw from cats who don't seem to give a damn about

24:39.360 --> 24:40.360
you?

24:40.360 --> 24:41.840
Is that just another character?

24:41.840 --> 24:42.840
Is it another?

24:42.840 --> 24:43.840
It's just another character.

24:43.880 --> 24:48.480
And so you can almost like in the early expirations, we thought that it would be really incredible

24:48.480 --> 24:52.560
if you had a diversity of characters where you almost help encourage which direction

24:52.560 --> 24:55.280
it goes just like in a role playing game.

24:55.280 --> 25:01.000
And you had like think of like the seven dwarfs sort of.

25:01.000 --> 25:07.160
And initially we even thought that it would be amazing if like their characters actually

25:07.160 --> 25:11.560
help them have strengths and weaknesses and some like whatever they end up doing.

25:11.920 --> 25:18.760
Some are scared, some are arrogant, some are super warm and like kind of friendly.

25:18.760 --> 25:22.080
And in the end, we focused on one because it made it very clear that, hey, we got to

25:22.080 --> 25:26.120
build out enough depth here because you're kind of trying to expand.

25:26.120 --> 25:30.400
It's almost like how long can you maintain a fiction that this character is alive to

25:30.400 --> 25:34.600
where the person's explorations don't hit a boundary, which happens almost immediately

25:34.600 --> 25:36.760
with typical toys.

25:36.800 --> 25:42.440
And even with video games, how long can we create that immersive experience to where

25:42.440 --> 25:43.600
you expand the boundary?

25:43.600 --> 25:48.520
And one of the things we realized is that you're just way more forgiving when something

25:48.520 --> 25:51.040
has a personality and it's physical.

25:51.040 --> 25:57.840
That is the key that unlocks robotics interacting in the physical world more generally is that

25:57.840 --> 26:04.360
that when you don't have a personality and you make a mistake as a robot, the stupid

26:04.400 --> 26:06.720
robot made a mistake. Why is it not perfect?

26:06.720 --> 26:10.360
When you have a character and you make a mistake, you have empathy and it becomes endearing

26:10.360 --> 26:11.480
and you're way more forgiving.

26:11.480 --> 26:14.840
And that was the key that was like I think goes far, far beyond entertainment.

26:14.840 --> 26:18.200
It actually builds the depth of the personality, the mistakes.

26:18.200 --> 26:21.800
So let me ask the movie her question then.

26:21.800 --> 26:28.320
How and so Cosmos seems feels like the early days of something that will obviously be

26:28.400 --> 26:35.400
prevalent throughout society at a scale that we cannot even imagine with my senses.

26:36.760 --> 26:43.080
It seems obvious that these kinds of characters will permeate society and there will be friends

26:43.080 --> 26:44.080
with them.

26:44.080 --> 26:47.440
We'll be interacting with them in different ways in the way we I mean, you don't think

26:47.440 --> 26:54.440
of it this way, but when you play video games, they're kind of they're often cold and impersonal.

26:55.320 --> 27:01.720
But even then, you think about role playing games, you become friends with certain characters

27:01.720 --> 27:02.720
in that game.

27:02.720 --> 27:04.920
They're they don't remember much about you.

27:04.920 --> 27:07.880
They they they're they're just telling a story.

27:07.880 --> 27:09.280
It's exactly what you're saying.

27:09.280 --> 27:11.280
They exist in that virtual world.

27:11.280 --> 27:15.480
But if they acknowledge that you exist in this physical world, if the characters in

27:15.480 --> 27:21.640
the game remember that you exist, that you like for me, like Lex, they understand that

27:21.680 --> 27:26.080
I'm a human being who has like hopes and dreams and so on.

27:26.080 --> 27:33.080
It seems like there's going to be like billions, if not trillions of Cosmos in the world.

27:34.200 --> 27:38.240
So if we look at that future, there's several questions to ask.

27:38.240 --> 27:45.240
How intelligent does that future Cosmo need to be to create fulfilling relationships like

27:47.320 --> 27:48.320
friendships?

27:48.320 --> 27:50.440
Yeah, it's a great question.

27:50.440 --> 27:52.880
And part of it was the recognition that it's going to take time to get there because it

27:52.880 --> 27:57.880
has to be a lot more intelligent, because it was good enough to be a magical experience

27:57.880 --> 28:01.480
for an eight year old.

28:01.480 --> 28:07.000
It's a higher bar to do that, be a companion like a pet in the home or to help with functional

28:07.000 --> 28:10.600
interface in an office environment or in a home or in so forth.

28:10.600 --> 28:14.480
And so and the idea was that you build on that and you kind of get there and as technology

28:14.480 --> 28:18.160
becomes more prevalent and less expensive and so forth, you can start to kind of work

28:18.160 --> 28:19.600
up to it.

28:19.600 --> 28:24.600
But you're absolutely right, at the end of the day, we almost equated it to how the touch

28:24.600 --> 28:29.400
screen created like this really novel interface to physical kind of devices like this.

28:29.400 --> 28:34.240
This is the extension of it where you have much richer physical interaction in the real

28:34.240 --> 28:35.240
world.

28:35.240 --> 28:39.480
This is the enabler for it and it shows itself in a few kind of really obvious places.

28:39.480 --> 28:43.360
So just take something as simple as a voice assistant.

28:43.360 --> 28:49.320
Most people will never tolerate an Alexa or a Google Home just starting a conversation

28:49.320 --> 28:53.640
proactively when you weren't kind of expecting it, because if it feels weird, it's like you

28:53.640 --> 28:57.400
were listening and then now you're kind of, it feels intrusive.

28:57.400 --> 29:02.520
But if you had a character like a cat that touches you and gets your attention or a toddler,

29:02.520 --> 29:03.520
you never think twice about it.

29:03.520 --> 29:07.720
And what we found really kind of immediately is that these types of characters like Cosmo

29:07.720 --> 29:10.840
and they would like roam around and kind of get your attention and we had a future version

29:10.840 --> 29:15.040
it was always on kind of called Vector, people were way more forgiving.

29:15.040 --> 29:21.280
And so you could initiate interaction in a way that is not acceptable for machines.

29:21.280 --> 29:27.080
And in general, there's a lot of ways to customize it, but it makes people who are skeptical

29:27.080 --> 29:29.360
of technology much more comfortable with it.

29:29.360 --> 29:32.960
There was like, there were a couple of really, really prominent examples of this.

29:32.960 --> 29:39.080
So when we launched in Europe, and so we were in, I think like a dozen countries, if I remember

29:39.880 --> 29:45.440
we went pretty aggressively in launching in Germany and France and UK.

29:45.440 --> 29:49.400
And we were very worried in Europe because there's obviously like a really socially higher

29:49.400 --> 29:54.600
bar for privacy and security where you've heard about how many companies have had troubles

29:54.600 --> 29:59.640
on things that might have been okay in the US, but like are just not okay in Germany

29:59.640 --> 30:00.640
and France in particular.

30:00.640 --> 30:07.760
And so we were worried about this because you have Cosmo who's in our future product

30:08.160 --> 30:11.920
where you have cameras, you have microphones, it's kind of connected and like you're playing

30:11.920 --> 30:16.880
with kids and like in these experiences and you're like, this is like ripe to be like a

30:16.880 --> 30:18.240
nightmare if you're not careful.

30:20.560 --> 30:24.320
And the journalists are like notoriously like really, really tough on these sort of things.

30:25.760 --> 30:29.920
We were shocked and we prepared so much for what we would have to encounter.

30:29.920 --> 30:35.840
We were shocked in that not once from any journalists or customer did we have any

30:35.920 --> 30:39.360
complaints beyond like a really casual kind of question.

30:40.000 --> 30:47.120
And it was because of the character where when the conversation came up, it was almost like, well,

30:47.120 --> 30:50.560
of course he has to see and hear how else is he going to be alive and interacting with you.

30:51.200 --> 30:56.560
And it completely disarmed this like fear of technology that enabled this interaction to

30:56.560 --> 30:57.600
be much more fluid.

30:57.600 --> 31:01.440
And again, like entertainment was a proving ground, but that is like, there's like

31:01.440 --> 31:06.080
ingredients there that carry over to a lot of other elements down the road.

31:06.080 --> 31:11.920
That's hilarious that we're a lot less concerned about privacy if the thing is value and charisma.

31:13.120 --> 31:15.920
I mean, that's true for all of human to human interaction too.

31:15.920 --> 31:19.440
It's an understanding of intent where like, well, he's looking at me, he can see me.

31:19.440 --> 31:20.960
If he's not looking at me, he can't see me, right?

31:20.960 --> 31:24.480
So it's almost like you're communicating intent.

31:24.480 --> 31:29.280
And with that intent, people were like kind of more understanding and calmer.

31:30.000 --> 31:30.560
It's interesting.

31:30.560 --> 31:34.000
And it was just the earliest kind of version of starting to experiment with this, but

31:34.560 --> 31:35.360
it wasn't an enabler.

31:35.360 --> 31:39.600
And then you have like completely different dimensions where kids with autism had like

31:39.600 --> 31:43.120
an incredible connection with Cosmo that just went beyond anything we'd ever seen.

31:43.120 --> 31:46.480
And we have like these just letters that we would receive from parents.

31:46.480 --> 31:50.880
And we had some research projects kind of going on with some universities on studying this.

31:50.880 --> 31:56.160
But there's an interesting dimension there that got unlocked that just hadn't existed before

31:56.800 --> 32:01.280
that has these really interesting kind of links into society and

32:02.720 --> 32:04.960
a potential building block of future experiences.

32:04.960 --> 32:12.400
So if you look out into the future, do you think we will have beyond a particular game,

32:13.200 --> 32:22.720
a companion like Her, like the movie Her, or like a Cosmo that kind of asks you how your day went

32:22.720 --> 32:30.720
too? Like a friend. How many years away from that do you think we are? What's your intuition?

32:30.720 --> 32:34.640
Good question. So I think the idea of a different type of character,

32:34.640 --> 32:38.320
like more closer to like kind of a pet style companionship will come way faster.

32:40.000 --> 32:45.920
And there's a few reasons. One is like to do something like in Her, that's like effectively

32:45.920 --> 32:50.560
almost general AI. And the bar is so high that if you miss it by a bit, you hit the uncanny

32:50.560 --> 32:56.480
valley where it just becomes creepy and not appealing. Because the closer you try to get

32:56.480 --> 33:02.880
to a human in form and interface and voice, the harder it becomes. Whereas you have way more

33:02.880 --> 33:08.080
flexibility on still landing a really great experience if you embrace the idea of a character.

33:08.080 --> 33:14.000
And that's why one of the other reasons why we didn't have a voice and also why like a lot of

33:14.000 --> 33:19.120
video game characters, like Sims, for example, does not have a voice when you think about it.

33:19.840 --> 33:24.240
It wasn't just a cost savings like for them. It was actually for all of these purposes,

33:24.240 --> 33:28.400
it was because when you have a voice, you immediately narrow down the appeal to some

33:28.400 --> 33:34.880
particular demographic or age range or kind of style or gender. If you don't have a voice,

33:34.880 --> 33:39.040
people interpret what they want to interpret. And an eight-year-old might get a very different

33:39.040 --> 33:44.160
interpretation than a 40-year-old, but you create a dynamic range. And so you can lean into these

33:44.160 --> 33:49.280
advantages much more in something that doesn't resemble a human. And so that'll come faster.

33:50.080 --> 33:56.000
I don't know when a human like, that's just still like map, just complete R&D at this point.

33:56.000 --> 34:00.160
The chat interfaces are getting way more interesting and richer,

34:00.160 --> 34:02.880
but it's still a long way to go to kind of pass the test of...

34:04.480 --> 34:11.920
Well, let's consider like, let me play devil's advocate. So Google is a very large company

34:11.920 --> 34:16.640
that's servicing, it's creating a very compelling product that wants to provide a service to a lot

34:16.640 --> 34:25.040
of people. But let's go outside of that. You said characters. And you also said that it requires

34:25.040 --> 34:30.400
general intelligence to be a successful participant in a relationship, which could explain why I'm

34:30.400 --> 34:38.160
single. But I honestly want to push back on that a little bit because I feel like, is it possible

34:38.800 --> 34:44.160
that if you're just good at playing a character in a movie, there's a bunch of characters,

34:44.160 --> 34:50.000
if you just understand what creates compelling characters and then you just are that character

34:50.000 --> 34:54.880
and you exist in the world and other people find you and they connect with you just like you do

34:54.880 --> 34:58.800
when you talk to somebody at a bar. I like this character. This character is kind of shady. I

34:58.800 --> 35:05.600
don't like them. You pick the ones that you like. And maybe it's somebody that reminds you of your

35:05.600 --> 35:10.000
father or mother. I don't know what it is, but the Freudian thing, but there's some kind of

35:10.000 --> 35:15.840
connection that happens. And that's the Cosmo you connect to. That's the future Cosmo you connect.

35:15.840 --> 35:21.200
And that's, so I guess the statement I'm trying to make, is it possible to achieve a depth of

35:21.200 --> 35:26.320
friendship without solving general intelligence? I think so. And it's about intelligent kind of

35:26.320 --> 35:32.240
constraints, right? And just you set expectations and constraints such that in the space that's

35:32.240 --> 35:37.120
left, you can be successful. And so you can do that by having a very focused domain that

35:37.120 --> 35:40.320
you can operate in. For example, you're a customer support agent for a particular product

35:40.320 --> 35:45.520
and you create intelligence and a good interface around that. Or kind of in the personal

35:45.520 --> 35:51.120
companionships side, you can't be everything across the board. You kind of solve those

35:51.120 --> 35:59.840
constraints. And I think it's possible. My worry is right now I don't see anybody that has picked

35:59.840 --> 36:04.960
up on where kind of Cosmo left off and is pushing on it in the same way. And so I don't know if it's

36:04.960 --> 36:09.600
a sort of thing where similar to like how in .com, there were all these concepts that we

36:09.600 --> 36:15.440
considered that didn't work out or failed or were too early or whatnot. And then 20 years later,

36:15.440 --> 36:19.920
you have these incredible successes on almost the same concept. It might be that sort of thing

36:19.920 --> 36:26.640
where there's another pass at it that happens in five years or in 10 years. But it does feel like

36:26.640 --> 36:32.720
that appreciation of the three-legged stool, if you will, between the hardware, the AI,

36:32.720 --> 36:40.320
and the character. That balance, it's hard to, I'm not aware of anywhere right now where that same

36:40.320 --> 36:46.960
kind of aggressive drive with the value on the character is happening. To me, just a prediction,

36:46.960 --> 36:52.480
exactly as you said, something that looks awfully a lot like Cosmo, not in the actual physical form,

36:52.480 --> 36:57.440
but in the three-legged stool, something like that in some number of years will be a trillion

36:57.440 --> 37:05.360
dollar company. I don't understand. Like it's obvious to me that like character, not just as

37:05.360 --> 37:13.680
robotic companions, but in all our computers, they'll be there. It's like Clippy was like

37:14.720 --> 37:18.800
two legs of that stool or something like that. I mean, those are all different attempts.

37:19.760 --> 37:27.760
What's really confusing to me is they're born these attempts and everybody gets excited,

37:27.760 --> 37:32.400
and for some reason they die, and then nobody else tries to pick it up. And then maybe a few

37:32.400 --> 37:39.840
years later, a crazy guy like you comes around with just enough brilliance and vision to create

37:39.840 --> 37:45.840
this thing and is born. A lot of people love it. A lot of people get excited, but maybe the timing

37:45.840 --> 37:52.000
is not right yet. And then when the timing is right, it just blows up. It just keeps blowing

37:52.000 --> 37:57.440
up more and more until it just blows up, and I guess everything in the full span of human

37:57.440 --> 38:02.080
civilization collapses eventually. That wouldn't surprise me at all. What's going to be different

38:02.080 --> 38:07.040
in another five years or 10 years or whatnot? Physical component costs will continue to come

38:07.040 --> 38:12.480
down in price, and mobile devices and computations are going to become more and more prevalent,

38:12.480 --> 38:19.280
as well as cloud as a big tool to offload cost. AI is going to be a massive transformation compared

38:19.280 --> 38:28.240
to what we dealt with, where everything from voice understanding to just a broader contextual

38:29.120 --> 38:35.280
understanding and mapping of semantics and understanding scenes and so forth.

38:35.280 --> 38:39.440
And then the character side will continue to progress as well, because that magic does exist.

38:39.440 --> 38:45.360
It just exists in different forms, and you have just the brilliance of the tapping and animation

38:45.360 --> 38:52.960
and these other areas where that was a big unlock in film, obviously. And so I think,

38:52.960 --> 38:56.160
yeah, the pieces can reconnect, and the building blocks are actually going to be way more impressive

38:56.160 --> 39:05.600
than they were five years ago. So in 2019, Anki, the company that created Cosmo, the company that

39:05.600 --> 39:13.920
you started had to shut down. How did you feel at that time? Yeah, it was tough. That was a

39:13.920 --> 39:20.560
really emotional stretch, and it was a really tough year. About a year ahead of that was actually a

39:20.560 --> 39:28.000
pretty brutal stretch, because we were kind of life or death on many, many moments, just navigating

39:28.560 --> 39:39.360
these insane ups and downs and barriers. Just to rewind a tiny bit, what ended up being

39:39.360 --> 39:45.040
really challenging about it as a business, from a commercial standpoint and customer reception

39:45.040 --> 39:48.960
standpoint, there was a lot of things you could point to that were pretty big successes. Sold

39:48.960 --> 39:57.440
millions of units, got to pretty serious revenue, close to 100 million annual revenue, number one

39:57.440 --> 40:03.680
product in various categories, but it was pretty expensive. It ended up being very seasonal where

40:03.680 --> 40:10.320
something like 85% of our volume was in Q4, because it was a present, and it was expensive

40:10.320 --> 40:16.000
to market it and explain it and so forth. And even though the volume was really sizable,

40:16.000 --> 40:22.320
and the reviews were really fantastic, forecasting and planning for it and managing the cash

40:22.320 --> 40:26.640
operations was just brutal. It was absolutely brutal. You don't think about this when you're

40:26.640 --> 40:32.400
starting a company or when you have a few million in revenue, because it's just your biggest costs

40:32.400 --> 40:37.040
are kind of just your headcount in operations and everything's ahead of you. But we got to a point

40:37.040 --> 40:44.960
where if you look at the entire year, you have to operate your company, pay all the people and so

40:44.960 --> 40:49.360
forth. You have to pay for the manufacturing, the marketing and everything else to do your sales in

40:49.360 --> 40:54.080
mostly November, December, and then get paid in December, January by retailers. And those swings

40:54.080 --> 41:00.640
were really rough and just made it so difficult, because the more it successfully became, the more

41:00.640 --> 41:06.640
wild those swings became, because you'd have to spend tens of millions of dollars on inventory,

41:06.640 --> 41:10.800
tens of millions of dollars on marketing, and tens of millions of dollars on payroll and everything

41:10.800 --> 41:14.480
else. Just the bigger dip, and then you're waiting for the Q4.

41:14.480 --> 41:19.360
Yeah, and it's not a business that is recurring month to month and predictable. And then you're

41:19.360 --> 41:27.440
walking in your forecast in July, maybe August, if you're lucky. And it's also very hit driven

41:27.440 --> 41:32.080
and seasonal where you don't have the sort of continued kind of slow growth like you do in some

41:32.080 --> 41:37.120
other consumer electronics industries. And so before then, hardware kind of went out of favor

41:37.120 --> 41:41.440
too. And so you had Fitbit and GoPro drop from 10 billion revenue to 1 billion revenue, and

41:41.440 --> 41:46.400
hardware companies are getting valued at 1x revenue oftentimes, which is tough, right?

41:46.400 --> 41:50.640
And so we effectively kind of got caught in the middle where we were trying to

41:51.520 --> 41:55.360
quickly evolve out of entertainment and move into some other categories,

41:55.360 --> 41:58.720
but you can't let go of that business because that's what you're valued on, that's what you're

41:58.720 --> 42:04.640
raising money on. But there was no path to pure profitability just there because it was such

42:05.920 --> 42:12.080
specific type of price points and so forth. And so we tried really hard to make that transition.

42:12.080 --> 42:16.560
And yeah, we had a financing round that fell apart at the last second and effectively there

42:16.560 --> 42:22.000
was just no path to kind of get through that and get to the next kind of high holiday season.

42:22.000 --> 42:27.840
And so we ended up sowing some of the assets and kind of winding down the company. It was brutal.

42:28.640 --> 42:33.680
I was very transparent with the company and the team while we were going through it where actually

42:33.680 --> 42:38.400
despite how challenging that period was, very few people left. I mean, people loved the vision,

42:38.400 --> 42:41.520
the team, the culture, the like kind of chemistry and kind of what we were doing.

42:41.520 --> 42:45.440
There was just a huge amount of pride there. And then we wanted to see it through and we felt like

42:45.440 --> 42:52.160
we had a shot to kind of get through these checkpoints. We ended up, and I mean, by brutal,

42:52.160 --> 42:57.200
I mean like literally like days of cash, like three, four different times, runway like in the

42:57.200 --> 43:04.800
year kind of before it, where you're like playing games of chicken on negotiating credit line

43:04.800 --> 43:10.560
timelines and like repayment terms and how to get like a bridge loan from an investor. It's just

43:10.560 --> 43:16.000
like level of stress that like as hard as things might be anywhere else, you'll never come close

43:16.000 --> 43:22.240
to that where you feel that like responsibility for 200 plus people. And so we were very transparent

43:22.240 --> 43:27.600
during our fundraise on who we're talking to, the challenges that we have, how it's going and when

43:27.600 --> 43:32.320
things are going well, when things were tough. And so it wasn't a complete shock when it happened,

43:32.320 --> 43:39.920
but it was just very emotional where like we announced it finally that like we basically were

43:39.920 --> 43:43.440
just like watching kind of like the runway and trying to kind of time it. And when we realized

43:43.440 --> 43:47.520
that like we didn't have any more outs, we wanted to like kind of wind it down, make sure that it

43:47.520 --> 43:52.160
was like clean and we could like kind of take care of people the best we could. But they like broke

43:52.160 --> 43:57.040
down crying at all hands and somebody else had to step in for a bit. And like it was just very,

43:57.040 --> 44:00.880
very emotional. But the beautiful part is like afterwards, like everybody stayed at the office

44:00.880 --> 44:05.200
to like two, three in the morning, just like drinking and hanging out and telling stories

44:05.200 --> 44:10.240
and celebrating. And it was just like one of the best, for many people, it was like the best kind

44:10.240 --> 44:14.480
of work experience that they had. And there was a lot of pride in what we did. And it wasn't

44:14.480 --> 44:18.000
anything obvious we could point to that like, hey, if only we had done that different, things would

44:18.000 --> 44:25.840
have been completely different. It was just like the physics didn't line up. But the experience was

44:25.840 --> 44:31.440
pretty incredible, but it was hard. Like it was, it had this feeling that there was just like

44:31.440 --> 44:38.880
incredible beauty in both the technology and products and the team that, you know, there's

44:38.880 --> 44:45.920
a lot there that like in the right context could have been pretty incredible, but it was emotional.

44:46.880 --> 44:52.720
Yeah, just thinking, I mean, just looking at this company, like you said, the product and technology,

44:52.720 --> 44:59.520
but the vision, the implementation, you've got the cost down very low and the compelling,

44:59.520 --> 45:04.320
the nature of the product was great. So many robotics companies failed at this.

45:05.920 --> 45:11.760
The robot was too expensive. It didn't have the personality. It didn't really provide any value,

45:11.760 --> 45:18.480
like a sufficient value to justify the price. So you succeeded where basically every single other

45:18.480 --> 45:24.000
robotics company or most of them that are like go in the category of social robotics have kind

45:24.000 --> 45:31.360
of failed. And I mean, it's, it's, it's quite tragic. I remember reading that. I'm not sure

45:31.360 --> 45:36.640
if I talked to you before that happened or not, but I remember, you know, I'm distant from this.

45:37.520 --> 45:46.080
I remember being heartbroken reading that because like, if, if Cosmo is not going to succeed,

45:46.080 --> 45:54.000
what is going to succeed? Cause that to me was incredible. Like it was an incredible idea cost

45:54.000 --> 46:00.800
is down the minimum, the, the, the, it's just like the most minimal design in physical form

46:00.800 --> 46:06.720
that you could do. It's really compelling the balance of games. So it's a, it's a fun toy.

46:06.720 --> 46:12.720
It's a great gift for all kinds of age groups, right? It's just, it's compelling in every single

46:12.880 --> 46:19.280
it's compelling in every single way. And it seemed like it was a huge success and it, it, it

46:19.280 --> 46:25.840
failing was, I dunno, there was heartbreak on many levels for me, just as an external observer

46:27.040 --> 46:32.000
is I was thinking, how hard is it to run a business? That's, that's what I was thinking.

46:32.000 --> 46:38.960
Like if this failed, this must've failed because the, it's obviously not like, yeah, it's business.

46:39.440 --> 46:43.840
Maybe it's some aspect of the manufacturing and so on, but I'm now realizing it's also not just that

46:43.840 --> 46:49.840
it's sales, marketing. Oh, it's everything, right? Like how do you explain something that's like a new

46:49.840 --> 46:53.760
category to people that like how all these previous positions. And so like, you know,

46:54.400 --> 46:59.520
it had some of the hardest elements of, if you were to pick a business, it had some of the hardest

47:00.640 --> 47:06.320
customer dynamics, because like to sell a $150 product, you got to convince both the child to

47:06.320 --> 47:09.920
want it and the parents to agree that it's valuable. So you're having like this dual

47:09.920 --> 47:14.240
prong marketing challenge. You have manufacturing, you have like really high precision on the

47:14.240 --> 47:17.280
components that you need, you have the AI challenges. So there were a lot of tough

47:17.280 --> 47:22.160
elements, but it was this feeling where like, it was just really great alignment of unique

47:22.160 --> 47:26.480
strength across kind of like all these different areas, just like incredible, like, you know,

47:26.480 --> 47:30.320
kind of character and animation team between this like Carlos and there's like a character director

47:30.320 --> 47:34.160
day that came on board and like, you know, really great people there, the AI side, the

47:36.000 --> 47:41.840
manufacturing, the, you know, where like never missing a launch, right? And actually, you know,

47:41.840 --> 47:48.240
he kind of hit that quality. It was, yeah, it was heartbreaking. But here's one neat thing is like,

47:48.240 --> 47:53.040
we had so much like fan mail from kind of kids and parents. Like I actually like, there was a

47:53.040 --> 47:58.480
bunch they collected in the end that I actually saved. And like, I never, it was too emotional

47:58.480 --> 48:02.880
to open it and I still haven't opened it. And so I actually have this giant envelope of like a stack

48:02.880 --> 48:07.200
this much of like letters from, you know, kids and families, just like every, you know,

48:08.000 --> 48:11.840
permutation you can imagine. And so planning to kind of, I don't know, maybe like a five year,

48:11.840 --> 48:16.400
you know, five year to some year reunion, just inviting everybody over and we'll just like kind

48:16.400 --> 48:21.120
of dig into it and kind of bring back some memories. But, you know, good impact and-

48:22.080 --> 48:28.160
Well, I think there will be companies, maybe Waymo and Google will be somehow involved

48:28.160 --> 48:34.640
that will carry this flag forward and will make you proud, whether you're involved or not.

48:34.640 --> 48:40.400
I think this is one of the greatest robotics companies in the history of robotics. So you

48:40.400 --> 48:47.360
should be proud. It's still tragic to know that, you know, cause you read all the stories of Apple

48:47.360 --> 48:56.480
and let's see, SpaceX and like companies that were just on the verge of failure several times

48:56.480 --> 49:01.200
through that story. And they just, it's almost like a roll of the dice. They succeeded. And here's

49:01.200 --> 49:05.680
the roll of the dice that just happened to go. And that's the appreciation that like, when you

49:05.680 --> 49:10.960
really like talk to a lot of the founders, like everybody goes through those moments and sometimes

49:10.960 --> 49:15.520
it really is a matter of like, you know, timing a little bit of luck. Like some things are just out

49:15.520 --> 49:23.280
of your control and you get a much deeper appreciation for just the dimensionality of

49:23.280 --> 49:27.280
that challenge. But the great thing is, is that like a lot of the team actually like stayed

49:27.280 --> 49:32.400
together. And so there were actually, you know, a couple of companies that we kind of kept big

49:32.400 --> 49:37.200
chunks of the team together and we actually kind of helped align this, you know, to help people out

49:37.200 --> 49:45.520
as well. And one of them was Waymo where a majority of the AI and robotics team actually had the exact

49:45.520 --> 49:50.320
background that you would look for in like kind of AV space. It was a space that a lot of us like,

49:50.400 --> 49:55.680
you know, worked on in grad school were always passionate about and ended up, you know, maybe

49:55.680 --> 50:01.680
the time, you know, serendipitous timings from another perspective where like kind of landed

50:01.680 --> 50:06.800
in a really unique circumstance that's actually been quite exciting too. So it's interesting to

50:06.800 --> 50:16.080
ask you just your thoughts. Cosmos still lives on under DreamLabs, I think. Is that, are you tracking

50:16.080 --> 50:22.880
the progress there or is it too much pain? Is that something that you're excited to see where that

50:22.880 --> 50:28.240
goes? So keeping an eye on it, of course, just out of curiosity and obviously just kind of care

50:28.240 --> 50:33.760
for product line, I think it's deceptive how complex it is to manufacture and evolve that

50:33.760 --> 50:42.400
product line. And the amount of experiences that are required to complete the picture and be able

50:42.400 --> 50:47.280
to move that forward. And I think that's going to make it pretty hard to do something really

50:47.280 --> 50:50.640
substantial with it. It would be cool if like even the product in the way it was,

50:50.640 --> 50:54.320
was able to be manufactured. Again, that would- Which is the current goal, I suppose.

50:54.320 --> 51:01.280
Yeah, which would be neat. But I think it's deceptive how tricky that is on like everything from

51:01.280 --> 51:07.680
the quality control, the details, and then like technology changes that forces you to reinvent

51:07.680 --> 51:12.960
and update certain things. So I haven't been super close to it, but just kind of keeping an

51:12.960 --> 51:18.400
eye on it. Yeah, it's really interesting how it's deceptively difficult, just as you're saying. For

51:18.400 --> 51:26.560
example, those same folks, and I've spoken with them, they're partying up with Rick and Morty

51:28.080 --> 51:35.280
creators to do the Butter Robot. I love the idea. I just recently, I kind of half-assed watched

51:35.280 --> 51:40.560
Rick and Morty previously, but now I just watched the first season. It's such a brilliant show.

51:42.320 --> 51:47.280
I did not understand how brilliant that show is. And obviously I think in season one is where the

51:47.280 --> 51:52.880
Butter Robot comes along for just a few minutes or whatever. But I just fell in love with the

51:52.880 --> 51:58.800
Butter Robot, that particular character. Just like you said, there's characters. You can create

51:58.800 --> 52:04.880
personalities. You can create, and that particular robot who's doing a particular task

52:09.920 --> 52:15.680
asks the existential question, the myth of Sisyphus question that Camus writes about.

52:15.680 --> 52:24.000
It's like, is this all there is? He moves butter. That realization, that's a beautiful little

52:24.000 --> 52:30.960
realization for a robot that my purpose is very limited to this particular task. It's humor,

52:30.960 --> 52:36.400
of course. It's darkness. It's a beautiful mix. So they want to release that Butter Robot,

52:37.040 --> 52:45.760
but something tells me that to do the same depth of personality as Cosmo had, the same richness,

52:45.760 --> 52:52.080
it would be on the manufacturing, on the AI, on the storytelling, on the design. It's going to be

52:52.080 --> 53:00.880
very, very difficult. It could be a cool toy for Rick and Morty fans, but to create the same depth

53:00.880 --> 53:10.880
of existential angst that Butter Robot symbolizes is really, that's the brave effort you've succeeded

53:10.880 --> 53:16.320
at with Cosmo, but it's not easy. It's really difficult. You can fail on almost any one of

53:16.320 --> 53:24.400
the kind of dimensions and unique convergence of a lot of different skill sets to try to pull that

53:24.400 --> 53:30.640
off. Yeah. On this topic, let me ask you for some advice, because as I've been watching Rick and

53:30.640 --> 53:37.360
Morty, I told myself, I have to build the Butter Robot, just as a hobby project. And so I got a

53:37.360 --> 53:41.600
nice platform for it with treads and there's a camera that moves up and down and so on.

53:42.240 --> 53:49.840
I'll probably paint it, but the question I'd like to ask, there's obvious technical questions.

53:49.840 --> 53:54.000
I'm fine with communication, the personality, storytelling, all those kinds of things.

53:55.360 --> 53:58.400
I think I understand the process of that, but how do you know

54:01.040 --> 54:08.880
when you got it right? So with Cosmo, how did you know this is great or something is off?

54:09.840 --> 54:14.000
Is this brainstorming with the team? Do you know it when you see it? Is it like

54:14.800 --> 54:20.880
love at first sight? It's like, this is right. I guess if we think of it as an optimization space,

54:21.920 --> 54:26.960
is there Uncanny Valley where you're like, that's not right or this is right or are a lot of

54:26.960 --> 54:31.920
characters right? Yeah. We stayed away from Uncanny Valley just by having such a different

54:32.880 --> 54:37.440
mapping where it didn't try to look like a dog or a human or anything like that. And so

54:38.400 --> 54:44.800
you avoided having a weird pseudo similarity, but not quite hitting the mark. But you could just

54:44.800 --> 54:50.080
fall flat where just a personality or a character emotion just didn't feel right. And so it actually

54:50.080 --> 54:53.920
mirrored very closely to the iterations that a character director at Pixar would have where

54:53.920 --> 55:01.200
you're running through it and you can virtually see what it'll look like. We created a plugin to

55:01.280 --> 55:06.480
where we actually used Maya, the animation tools, and then we created a plugin that

55:07.200 --> 55:12.480
perfectly matched it to the physical one. And so you could test it out virtually and then push a

55:12.480 --> 55:16.880
button and see it physically play out. And there's subtle differences. And so you want to make sure

55:16.880 --> 55:24.160
that that feedback loop is super easy to be able to test it live. And then sometimes you would just

55:24.160 --> 55:29.120
feel it that it's right and intuitively know. And then you'd also do, we did user testing,

55:29.120 --> 55:36.880
but it was very, very often that if we found it magical, it would scale and be magical more

55:36.880 --> 55:44.000
broadly. There were not too many cases where we were pretty decent about not geeking out or getting

55:44.000 --> 55:50.320
too attached to something that was super unique to us, but trying to put a customer hat on and

55:50.320 --> 55:56.000
does it truly feel magical. And so in a lot of ways, it just gave a lot of autonomy to the

55:56.480 --> 56:02.640
character team to really think about the character board and mood boards and storyboards and what's

56:02.640 --> 56:07.120
the background of this character and how would they react. And they went through a process that's

56:07.120 --> 56:12.240
actually pretty familiar, but now had to operate under these unique constraints. But the moment

56:12.240 --> 56:18.240
where it felt right kind of took a fairly similar journey than as a character in an animated film,

56:18.240 --> 56:22.080
actually. It's quite cool. Well, the thing that's really important to me, and I wonder if it's

56:22.080 --> 56:28.320
possible. Well, I hope it's possible. Pretty sure it's possible is for me, even though I know how

56:28.320 --> 56:36.160
it works to make sure there's sufficient randomness in the process, probably because it would be machine

56:36.160 --> 56:42.720
learning based that I'm surprised that I don't, I'm surprised by certain reactions. I'm surprised

56:42.720 --> 56:49.360
by certain communication. Maybe that's in a form of a question. Were you surprised by certain things

56:49.360 --> 56:56.640
Cosmo did, certain interactions? Yeah. We made it intentionally so that there would be some

56:56.640 --> 57:02.800
surprise and a decent amount of variability in how he'd respond in certain circumstances. And so

57:02.800 --> 57:11.680
in the end, this isn't general AI. This is a giant spectrum and library of parameterized

57:11.680 --> 57:17.760
emotional responses and an emotional engine that would map your current state of the game,

57:17.760 --> 57:21.440
your emotions, the world, the people who are playing with you all, so forth to what's happening.

57:22.400 --> 57:27.920
But we could make it feel spontaneous by creating enough diversity and randomness,

57:28.880 --> 57:33.920
but still within the bounds of what felt like very realistic to make that work. And then what

57:33.920 --> 57:37.440
was really neat is that we could get statistics on how much of that space we were saturating

57:38.240 --> 57:42.000
and then add more animations and more diversity in the places that would get hit more often

57:42.000 --> 57:48.320
so that you stay ahead of the curve and maximize the chance that it stays feeling alive.

57:49.840 --> 57:56.400
But then when you combine it, the permutations and the combinations of emotions stitched together

57:56.400 --> 58:00.320
sometimes surprised us because you see them in isolation, but when you actually see them

58:00.320 --> 58:05.520
and you see them live relative to some event that happened in the game or whatnot, it was cool to

58:05.520 --> 58:10.240
see the combination of the two. And not too different in other robotics applications where

58:10.720 --> 58:15.360
you get so used to thinking about the modules of a system and how things progress through a tech

58:15.360 --> 58:20.880
stack that the real magic is when all the pieces come together and you start getting the right

58:20.880 --> 58:25.920
emergent behavior in a way that's easy to lose when you just go too deep into any one piece of it.

58:25.920 --> 58:29.600
Yeah, when the system is sufficiently complex, there is something like emergent behavior and

58:29.600 --> 58:33.360
that's where the magic is. As a human being, you can still appreciate the beauty of that magic

58:34.480 --> 58:39.040
at the system level. First of all, thank you for humoring me on this. It's really, really

58:39.840 --> 58:42.960
fascinating. I think a lot of people would love this. I'd love to just,

58:42.960 --> 58:51.760
one last thing on the butter robot, I promise. In terms of speech, Cosmo is able to communicate

58:51.760 --> 59:00.800
so much with just movement and face. Do you think speech is too much of a degree of freedom?

59:01.600 --> 59:09.760
Like a speech, a feature or a bug of deep interaction or emotional interaction?

59:09.760 --> 59:16.640
Yeah. For a product, it's too deep right now. You would immediately break the fiction because

59:16.640 --> 59:22.560
the state of the art is just not good enough. And that's on top of just narrowing down the

59:22.560 --> 59:26.080
demographic where the way you speak to an adult versus the way you speak to a child is very

59:26.080 --> 59:33.360
different. Yet a dog is able to appeal to everybody. And so right now, there is no speech

59:33.360 --> 59:41.440
system that is rich enough and subtly realistic enough to feel appropriate. And so we very,

59:41.440 --> 59:46.320
very quickly moved away from it. Now, speech understanding is a different matter where

59:46.320 --> 59:54.000
understanding intent, that's a really valuable input. But giving it back requires a way,

59:54.000 --> 01:00:01.040
way higher bar given where today's world is. And so that realization that you can do surprisingly

01:00:01.040 --> 01:00:08.000
much with either no speech or tonal like the way Wally R2D2 and other characters are able to,

01:00:09.600 --> 01:00:14.960
it's quite powerful and it generalizes across cultures and across ages really, really well.

01:00:14.960 --> 01:00:19.600
I think we're going to be in that world for a little while where it's still very much an

01:00:19.600 --> 01:00:24.240
unsolved problem on how to make something. It touches on the uncanny valley thing. So

01:00:24.240 --> 01:00:28.160
if you have legs and you're a big humanoid looking thing, you have very different expectations and a

01:00:28.160 --> 01:00:34.240
much narrower degree of what's going to be acceptable by society than if you're a robot

01:00:34.240 --> 01:00:40.400
like Cosmo or Wally or some other form where you can reinvent the character. Speech has that

01:00:40.400 --> 01:00:46.320
same property where speech is so well understood in terms of expectations by humans that you have

01:00:46.320 --> 01:00:51.920
far less flexibility on how to deviate from that and lean into your strengths and avoid weaknesses.

01:00:51.920 --> 01:00:56.560
But I wonder if there is, obviously there's certain kinds of speech that

01:00:58.160 --> 01:01:04.240
activates the uncanny valley and breaks the illusion faster. So I guess my intuition is

01:01:04.960 --> 01:01:13.200
we will solve certain, we would be able to create some speech based personalities sooner than others.

01:01:13.200 --> 01:01:20.400
So for example, I could think of a robot that doesn't know English and is learning English,

01:01:20.400 --> 01:01:24.640
right? Those kinds of personalities. A fiction where you're intentionally

01:01:24.640 --> 01:01:29.120
kind of getting a toddler level of speech. So that's exactly right. So you can have,

01:01:31.120 --> 01:01:35.680
tie it into the experience where it is a more limited character or you embrace the lack of

01:01:35.680 --> 01:01:41.520
emotions or the lack of dynamic range in the speech kind of capabilities, emotions as part

01:01:41.520 --> 01:01:44.960
of the character itself. And you've seen that in like kind of fictional characters as well.

01:01:46.400 --> 01:01:47.760
That's why this podcast works.

01:01:49.760 --> 01:01:53.840
Yeah. And you kind of had that with like, I don't know, I guess like data and some of the other

01:01:53.840 --> 01:02:01.280
ones. But yeah, so you have to, and that becomes a constraint that lets you meet the bar.

01:02:01.280 --> 01:02:12.240
See, I honestly think like also if you add drunk and angry, that gives you more constraints that

01:02:12.240 --> 01:02:18.160
allow you to be a dumber from an NLP perspective, like there's certain aspects. So if you modify

01:02:18.160 --> 01:02:23.120
human behavior, like, so forget the sort of artificial thing where you don't know English,

01:02:23.120 --> 01:02:30.320
toddler thing. If you just look at the full range of humans, I think there's certain

01:02:31.520 --> 01:02:39.760
situations where we put up with like lower level of intelligence in our communication.

01:02:39.760 --> 01:02:44.080
Like if somebody is drunk, we understand that they're probably under the influence. Like

01:02:44.080 --> 01:02:48.560
we understand that they're not going to be making any sense. Anger is another one like that. I'm

01:02:48.560 --> 01:02:55.200
sure there's a lot of other kind of situations. Maybe, yeah, again, language loss in translation,

01:02:56.480 --> 01:03:02.640
that kind of stuff that I think if you play with that, what is it, the Ukrainian boy that

01:03:02.640 --> 01:03:06.320
passed the touring test, play with those ideas. I think that's really interesting. And then you

01:03:06.320 --> 01:03:10.880
can create compelling characters, but you're right. That's a dangerous sort of road to walk

01:03:10.880 --> 01:03:14.080
because you're adding degrees of freedom that can get you in trouble.

01:03:14.400 --> 01:03:20.800
And that's why you have these big pushes that for most of the last decade plus where you'd have

01:03:21.600 --> 01:03:27.120
full human replicas of robots, really being down to skin and kind of in some places.

01:03:30.080 --> 01:03:36.000
My personal feeling is like, man, that's not the direction that's most fruitful right now.

01:03:36.960 --> 01:03:43.760
Beautiful art. It's not in terms of a rich, deep, fulfilling experience. Yeah, you're right.

01:03:43.760 --> 01:03:51.280
Yeah, and creating a minefield of potential places to feel off. And then you're sidestepping

01:03:51.280 --> 01:03:56.720
where the biggest kind of functional AI challenges are to actually have really rich productivity

01:03:56.720 --> 01:04:01.680
that actually kind of justifies the higher price points. And that's part of the challenge is like,

01:04:01.680 --> 01:04:05.760
yeah, robots are going to get to thousands of dollars, tens of thousands of dollars and so

01:04:05.760 --> 01:04:09.040
forth, but you can imagine what sort of expectation of value that comes with it.

01:04:09.920 --> 01:04:16.880
And so that's where you want to be able to invest the time and depth. And so going down

01:04:16.880 --> 01:04:27.680
the full human replica route creates a gigantic distraction and really, really high bar that can

01:04:27.680 --> 01:04:34.400
end up sucking up so much of your resources. So it's weird to say, but you happen to be one

01:04:34.400 --> 01:04:41.440
of the greatest, at this point, roboticists ever because you created this little guy.

01:04:41.440 --> 01:04:46.240
You were part, obviously, of a great team that created the little guy with a deep personality

01:04:47.120 --> 01:04:53.520
and are now switching to an entirely, well, maybe not entirely, but a different,

01:04:54.960 --> 01:05:00.720
fascinating, impactful robotics problem, which is autonomous driving. And more specifically,

01:05:00.720 --> 01:05:03.920
the biggest version of autonomous driving, which is autonomous trucking.

01:05:04.720 --> 01:05:11.920
So you are at Waymo now. Can you give us a big picture overview? What is Waymo?

01:05:11.920 --> 01:05:18.400
What is Waymo Driver? What is Waymo One? What is Waymo Via? Can you give an overview of the

01:05:18.400 --> 01:05:23.760
company and the vision behind the company? For sure. Waymo, by the way, has been eye opening

01:05:23.760 --> 01:05:29.200
on just how incredible the people and the talent is and how, in one company, you almost have to

01:05:29.200 --> 01:05:34.560
create 30 companies' worth of technology and capability to solve the full spectrum of it.

01:05:36.880 --> 01:05:44.240
I've been at Waymo since 2019, so about two and a half years. Waymo is focused on building what

01:05:44.240 --> 01:05:50.800
we call a driver, which is creating the ability to have autonomous driving across different

01:05:50.800 --> 01:05:58.080
environments, vehicle platforms, domains, and use cases. As you know, it got started in 2009.

01:05:59.200 --> 01:06:04.320
Almost like an immediate successor to the Grand Challenge and Urban Challenges that were incredible

01:06:05.280 --> 01:06:10.480
catalysts for this whole space. Google started this project and then eventually Waymo spun out.

01:06:11.120 --> 01:06:18.080
What Waymo is doing is creating the systems, both hardware, software, infrastructure,

01:06:18.080 --> 01:06:22.480
everything that goes into it, to enable and to commercialize autonomous driving.

01:06:22.480 --> 01:06:27.760
This hits on consumer transportation and ride sharing and vehicles in urban environments.

01:06:28.560 --> 01:06:34.560
And as you mentioned, it hits on autonomous trucking to transport goods. In a lot of ways,

01:06:34.560 --> 01:06:39.120
it's transporting people and transporting goods. But at the end of the day, the underlying

01:06:39.120 --> 01:06:44.320
capabilities that are required to do that are surprisingly better aligned than one might expect

01:06:45.120 --> 01:06:50.000
where it's the fundamentals of being able to understand the world around you, process it,

01:06:50.000 --> 01:06:54.720
make intelligent decisions, and prove that we are at a level of safety that enables

01:06:55.520 --> 01:07:01.760
large-scale autonomy. So from a branding perspective, Waymo driver is the system

01:07:01.760 --> 01:07:09.360
that's irrespective of a particular vehicle it's operating in. You have a set of sensors

01:07:09.360 --> 01:07:14.480
that perceive the world, can act in that world, and move this, whatever the vehicle is, through

01:07:14.480 --> 01:07:18.640
the world. That's right. And so in the same way that you have a driver's license and your ability

01:07:18.640 --> 01:07:22.320
to drive isn't tied to a particular make and model of a car. And of course, there are special

01:07:22.320 --> 01:07:27.360
licenses for other types of vehicles. But the fundamentals of a human driver very,

01:07:27.360 --> 01:07:30.800
very largely carry over. And then there's uniquenesses related to a particular

01:07:30.800 --> 01:07:37.200
environment or domain or a particular vehicle type that add some extra additive challenges.

01:07:37.200 --> 01:07:43.920
But that's exactly right. It's the underlying systems that enable a physical vehicle without

01:07:43.920 --> 01:07:51.600
a human driver to very successfully accomplish a task that previously wasn't possible without

01:07:52.880 --> 01:08:00.240
100% human driving. And then there's Waymo One, which is the transporting people from a brand

01:08:00.240 --> 01:08:05.520
perspective. And just in case we refer to it so people know. And then there's Waymo Via,

01:08:05.520 --> 01:08:12.000
which is the trucking component. Why Via, by the way? What is that? Is it just like a cool sounding

01:08:12.000 --> 01:08:18.480
name that just... Is there an interesting story there? It is a pretty cool sounding name.

01:08:18.480 --> 01:08:20.960
It's a cool sounding name. I mean, when you think about it, it's just like,

01:08:20.960 --> 01:08:25.520
well, we're going to transport it via this and that. So it's just an allusion to

01:08:26.480 --> 01:08:31.440
the mechanics of transporting something. And it is a pretty good grouping. And the interesting

01:08:31.440 --> 01:08:36.240
thing is that even the groupings border where Waymo One is human transportation and there's a

01:08:36.240 --> 01:08:41.440
fully autonomous service in the Phoenix area that every day is transporting people. And it's pretty

01:08:41.440 --> 01:08:48.480
incredible to see that operate at reasonably large scale and just happen. And then on the Via side,

01:08:48.480 --> 01:08:55.840
it doesn't even have to be... Long-haul trucking is a major focus of ours, but down the road,

01:08:55.840 --> 01:09:01.600
you can stitch together the vehicle transportation as well for local delivery. Also, and a lot of the

01:09:01.600 --> 01:09:07.120
requirements for local delivery overlap very heavily with consumer transportation. Obviously,

01:09:08.080 --> 01:09:14.160
given that you're operating on a lot of the same roads and navigating the same safety challenges.

01:09:14.160 --> 01:09:23.040
And so, and Waymo very much is a multi-product company that has ambitions in both. They have

01:09:23.040 --> 01:09:27.600
different challenges and both are tremendous opportunities. But the cool thing is that

01:09:27.600 --> 01:09:32.560
there's a huge amount of leverage and this kind of core technology stack now gets pushed on by

01:09:32.560 --> 01:09:37.920
both sides. And that adds its own unique challenges, but the success case is that

01:09:38.720 --> 01:09:43.760
the challenges that you push on, they get leveraged across all platforms and all domains.

01:09:43.920 --> 01:09:46.320
From an engineer perspective, the teams are integrated.

01:09:46.960 --> 01:09:51.520
It's a mix. So there's a huge amount of centralized core teams that support all

01:09:51.520 --> 01:09:55.200
applications. And so you think of something like the hardware team that develops the lasers,

01:09:55.200 --> 01:09:59.440
the compute, integrates into vehicle platforms. This is an experience that carries over across

01:10:00.560 --> 01:10:04.720
any application that we'd have and they have been full of both. Then there's really unique

01:10:05.360 --> 01:10:11.200
perception challenges, planning challenges, other types of challenges where there's a huge amount

01:10:11.200 --> 01:10:14.880
of leverage on a core tech stack, but then there's dedicated teams that think of how do

01:10:14.880 --> 01:10:20.000
you deal with a unique challenge? For example, an articulated trailer with varying loads that

01:10:20.000 --> 01:10:24.000
completely changes the physical dynamics of a vehicle that doesn't exist on a car,

01:10:24.000 --> 01:10:28.320
but becomes one of the most important kind of unique new challenges on a truck.

01:10:28.320 --> 01:10:37.040
So what's the long-term dream of Waymo via the autonomous trucking effort that Waymo's doing?

01:10:37.120 --> 01:10:44.240
Yeah. So we're starting with developing L4 Autonomy for Class 8 trucks. These are 53-foot

01:10:44.240 --> 01:10:49.440
trailers that capture a pretty sizable percentage of the goods transportation in the country.

01:10:50.800 --> 01:10:54.560
Long-term, the opportunity is obviously to expand a much more diverse types of

01:10:55.120 --> 01:11:01.440
vehicles, types of goods transportation, and start to really expand in both the volume and the route

01:11:01.440 --> 01:11:08.080
feasibility that's possible. So just like we did on the car side, you start with a single route

01:11:08.080 --> 01:11:13.920
with a very specific operating kind of domain and constraints that allow you to solve the problem,

01:11:13.920 --> 01:11:20.320
but then over time, you start to really try to push against those boundaries and open up deeper

01:11:20.320 --> 01:11:25.440
feasibility across routes, across surface streets, across environmental conditions,

01:11:25.440 --> 01:11:28.640
across the type of goods that you carry, the versatility of those goods,

01:11:28.640 --> 01:11:32.800
and how little supervision is necessary to just start to scale this network.

01:11:33.440 --> 01:11:41.760
And long-term, it's a pretty incredible enabler where today you have already a giant shortage of

01:11:41.760 --> 01:11:46.800
truck drivers. It's over 80,000 truck driver shortage that's expected to grow to hundreds

01:11:46.800 --> 01:11:53.120
of thousands in the years ahead. You have really, really quickly increasing demand from e-commerce

01:11:53.120 --> 01:12:00.560
and just distribution of where people are located. You have one of the deepest safety challenges of

01:12:01.920 --> 01:12:08.720
any profession in the US where there's a huge, huge, huge kind of challenge around fatigue and

01:12:08.720 --> 01:12:13.840
around kind of the long routes that are driven. And even beyond kind of the cost and necessity of

01:12:13.840 --> 01:12:18.480
it, there are fundamental constraints built into our logistics network that are tied to

01:12:19.440 --> 01:12:24.640
the type of human constraints and regulatory constraints that are tied to trucking today.

01:12:24.640 --> 01:12:31.280
For example, our limits on how long a driver can be driving in a single day before they're

01:12:31.280 --> 01:12:35.920
not allowed to drive anymore, which is a very important safety constraint. What that does is

01:12:35.920 --> 01:12:42.080
it enforces limitations on how far jumps with a single driver could be and makes you very subject

01:12:42.080 --> 01:12:46.480
to availability of drivers, which influences where warehouses are built, which influences how

01:12:46.560 --> 01:12:52.480
goods are transported, which influences costs. And so you start to have an opportunity on

01:12:52.480 --> 01:12:58.240
everything from plugging into existing fleets and brokerages and the existing logistics network and

01:12:58.240 --> 01:13:07.760
just immediately start to have a huge opportunity to add value from a cost and driving fuel insurance

01:13:07.760 --> 01:13:13.120
and safety standpoint all the way to completely reinventing the logistics network across the

01:13:13.120 --> 01:13:16.400
United States and enabling something completely different than what it looks like today.

01:13:16.400 --> 01:13:21.200
Yeah, I'll be published before this at a great conversation with Steve Vicelli who

01:13:21.200 --> 01:13:25.840
we talked about the manual driving. He echoed many of the same things that you were talking about,

01:13:25.840 --> 01:13:32.480
but we talked about much of the fascinating human stories of truck drivers. He was also

01:13:32.480 --> 01:13:37.120
was a truck driver for a bit as a grad student to try to understand the depth of the problem.

01:13:38.960 --> 01:13:42.960
We have some drivers that have 4 million miles of lifetime driving experience. It's pretty

01:13:42.960 --> 01:13:49.280
incredible. And yeah, it's a warning from them. Like some of them are on the road for 300 days

01:13:49.280 --> 01:13:53.440
a year. It's a very unique type of lifestyle. So there's fascinating stuff there. Just like

01:13:53.440 --> 01:14:00.240
you said, there's a shortage of actually people, truck drivers taking the job counter to what this

01:14:00.240 --> 01:14:07.760
I think is publicly believed. So there's an excess of jobs and a shortage of people to take up those

01:14:07.760 --> 01:14:14.000
jobs. And just like you said, it's such a difficult problem. And these are experts at driving

01:14:14.000 --> 01:14:18.160
at solving this particular problem. And it's fascinating to learn from them to understand,

01:14:19.280 --> 01:14:23.600
how hard is this problem? And that's the question I want to ask you from a perception,

01:14:23.600 --> 01:14:29.840
from a robotics perspective, what's your sense of how difficult is autonomous trucking? Maybe

01:14:30.480 --> 01:14:35.760
you can comment on which scenarios are super difficult, which are more manageable. Is there

01:14:35.760 --> 01:14:42.240
a way to convert into words how difficult the problem is? Yeah, that's a good question.

01:14:43.760 --> 01:14:51.840
And as you can expect, it's a mix. Some things become a lot easier or at least more flexible.

01:14:52.560 --> 01:14:57.920
Some things are harder. And so on the things that are like the tailwinds, the benefits,

01:14:59.280 --> 01:15:04.240
a big focus of automating trucking, especially initially, is really focusing on the long haul

01:15:04.240 --> 01:15:08.480
freeway stretch of it, where that's where a majority of the value is captured. On a freeway,

01:15:08.480 --> 01:15:12.880
you have a lot more structure and a lot more consistency across freeways across the US

01:15:14.000 --> 01:15:19.760
compared to surface streets where you have a way higher dimensionality of what can happen,

01:15:19.760 --> 01:15:23.760
lack of structure, lack of consistency and variability across cities. So you can leverage

01:15:23.760 --> 01:15:31.120
that consistency to tackle, at least in that respect, a more constrained AI problem, which

01:15:31.200 --> 01:15:34.880
has some benefits to it. You can itemize much more of the sort of things you might encounter

01:15:34.880 --> 01:15:40.800
and so forth. And so those are benefits. Is there a canonical freeway and city we should

01:15:40.800 --> 01:15:46.800
be thinking about? Is there a standard thing that's brought up in conversation often? Here's a

01:15:46.800 --> 01:15:53.280
stretch of road. What is it? When people talk about traveling across country, they'll talk about

01:15:54.240 --> 01:16:01.280
New York to San Francisco. Is that the route? Is there a stretch of road that's like

01:16:01.280 --> 01:16:06.800
nice and clean? And then there's cities with difficulties in them that you kind of think

01:16:06.800 --> 01:16:11.440
of as the canonical problem to solve here. Right. So starting with the car side,

01:16:13.520 --> 01:16:18.240
Waymo very intentionally picked the Phoenix area and the San Francisco area as a follow

01:16:18.240 --> 01:16:23.200
once we hit driverless where when you think of consumer transportation and ride sharing

01:16:23.920 --> 01:16:28.160
kind of economy, a big percentage of that market is captured in the densest cities in the United

01:16:28.160 --> 01:16:32.880
States. And so really pushing out and solving San Francisco becomes a really huge opportunity

01:16:32.880 --> 01:16:39.360
and importance and places one dot on kind of like the spectrum of like kind of complexity.

01:16:40.160 --> 01:16:43.920
The Phoenix area, starting with Chandler and then like kind of expanding more broadly in the Phoenix

01:16:44.560 --> 01:16:50.560
metropolitan area, it's I believe the fastest growing city in the US. It's a kind of a higher

01:16:50.560 --> 01:16:55.680
medium sized city, but growing quickly and still captures a really wide range of kind of like

01:16:55.680 --> 01:16:59.840
complexities. And so getting to driverless there actually exposes you to a lot of the building

01:16:59.840 --> 01:17:04.480
blocks you need for the more complicated environments. And so in a lot of ways,

01:17:04.480 --> 01:17:08.560
there's a thesis that if you start to kind of place a few of these kind of dots where

01:17:08.560 --> 01:17:12.880
San Francisco has these types of unique challenges, dense pedestrians, all this like complexity,

01:17:12.880 --> 01:17:16.080
especially when you get into the downtown areas and so forth. And Phoenix has like

01:17:16.640 --> 01:17:21.120
a really interesting kind of spectrum of challenges. Maybe other ones like LA kind

01:17:21.120 --> 01:17:26.000
of add freeway focus and so forth. You start to kind of cover the full set of features that you

01:17:26.000 --> 01:17:30.080
might expect and it becomes faster and faster if you have the right systems and the right

01:17:30.640 --> 01:17:34.800
organization to then open up the fifth city and the 10th city and the 20th city.

01:17:34.800 --> 01:17:40.560
On trucking, there's similar properties where obviously there's uniqueness in freeways when

01:17:40.560 --> 01:17:45.920
you get into really dense environments. And then the real opportunity to then get even more

01:17:46.880 --> 01:17:50.800
value is to think about how you expand with like some of the service challenges. But for example,

01:17:50.800 --> 01:17:57.440
right now we're looking, we have a big facility that we're finishing building in Q1 in Dallas

01:17:57.440 --> 01:18:02.240
area. That'll allow us to do testing from the Dallas area on routes like Dallas to Houston,

01:18:02.240 --> 01:18:08.000
Dallas to Phoenix, going out east. Dallas to Austin. Austin, so that triangle.

01:18:08.640 --> 01:18:10.160
Waymo should come to Austin.

01:18:11.360 --> 01:18:13.760
Well, Waymo, the car side was in Austin for a while.

01:18:13.760 --> 01:18:15.280
Yes, I know. Come back.

01:18:15.280 --> 01:18:20.240
Yeah. But trucking is actually, Texas is one of the best places to start because of both volume,

01:18:20.240 --> 01:18:26.080
regulatory, weather, there's a lot of benefits. On trucking, a huge opportunity is Port of LA

01:18:26.080 --> 01:18:31.920
going east. So in a lot of ways, a lot of the work is to start to stitch together a network

01:18:31.920 --> 01:18:36.400
and converge to Port of LA where you have the biggest port in the United States.

01:18:37.360 --> 01:18:42.240
And the amount of goods going east from there is pretty tremendous. And then obviously there's

01:18:43.200 --> 01:18:47.120
channels everywhere and then you have extra complexities as you get into like snow and

01:18:47.120 --> 01:18:51.200
inclement weather and so forth. But what's interesting about trucking is every single

01:18:51.200 --> 01:18:55.760
route segment that you add increases the value of the whole network. And so it has this network

01:18:55.760 --> 01:18:59.520
effect and cumulative effect that's very unique. And so there's all these dimensions that we think

01:18:59.520 --> 01:19:04.080
about. And so in a lot of ways, Dallas is a really unique hub that opens up a lot of options has

01:19:04.080 --> 01:19:08.640
become a really valuable lever. So the million questions I could ask you. First of all,

01:19:08.640 --> 01:19:15.280
you mentioned level four. For people who totally don't know, there's these levels of automation

01:19:16.240 --> 01:19:23.200
that level four refers to kind of the first step that you could recognize as fully autonomous

01:19:23.200 --> 01:19:29.280
driving. Level five is really fully autonomous driving and level four is kind of fully autonomous

01:19:29.360 --> 01:19:34.480
driving. And then there are specific definitions depending on who you ask what that actually means.

01:19:34.480 --> 01:19:40.800
But for you, what does the level four mean? And you mentioned freeway. Let's say like there's

01:19:40.800 --> 01:19:46.080
three parts of long haul trucking. Maybe I'm wrong in this, but there's freeway driving.

01:19:46.640 --> 01:19:56.320
There's like truck stop. And then there's more urban type of area. So which of those do you want

01:19:56.320 --> 01:20:01.120
to tackle? Which of them do you include under level four? Like, well, how do you think about

01:20:01.120 --> 01:20:05.120
this problem? What do you focus on? Where's the biggest impact to be had in the short term?

01:20:05.760 --> 01:20:10.240
So the goal is to we got to get to market as fast as we can, because the moment you get to market,

01:20:10.240 --> 01:20:16.080
you just learn so much and it influences everything that you do. And it is one of the

01:20:16.080 --> 01:20:20.960
experiences that carried over from before is that you add constraints, you figure out the right

01:20:20.960 --> 01:20:25.600
compromises, you do whatever it takes because getting to market like is so critical, right?

01:20:25.600 --> 01:20:28.720
And here with autonomous driving, you can get to market in so many different ways.

01:20:28.720 --> 01:20:34.240
That's right. And so one of the simplifications that we intentionally have put on is using what

01:20:34.240 --> 01:20:41.600
we call transfer hubs, where you can imagine depots that are at the entry points to metropolitan

01:20:41.600 --> 01:20:46.880
areas, like let's say Dallas, like the hub that we're building, which does a few things that are

01:20:46.880 --> 01:20:52.240
very valuable. So from a first product standpoint, you can automate transfer hub to transfer hub.

01:20:52.240 --> 01:20:59.360
And that path from the transfer hub to the full freeway route can be a very intentional

01:20:59.360 --> 01:21:02.880
single route that you can select for the features that you feel you want to handle

01:21:02.880 --> 01:21:08.400
at that point in time. And you build the hub specifically designed for autonomous trucking.

01:21:08.400 --> 01:21:11.840
And that's what's going to happen actually. You need to come out in January and check it out

01:21:11.840 --> 01:21:18.000
because it's going to be really cool. Not only is it our main operating headquarters for our fleet

01:21:18.000 --> 01:21:24.560
there, but it will be the first fully ground up design driverless hub for autonomous trucks in

01:21:24.560 --> 01:21:28.560
terms of where do they enter, where do they depart? How do you think about the flow of people,

01:21:28.560 --> 01:21:32.720
goods, everything? It's quite cool and it's really beautiful on how it was thought through.

01:21:32.720 --> 01:21:39.600
And so early on, it is totally reasonable to do the last five miles manually to get to the final

01:21:39.600 --> 01:21:43.360
kind of depot to avoid having to solve the general surface street problem, which is obviously very

01:21:43.360 --> 01:21:49.120
complex. Now, when the time comes and we are increasingly, already we're pushing on some of

01:21:49.120 --> 01:21:53.120
this, but we will increasingly be pushing on surface street capabilities to build out the

01:21:53.120 --> 01:21:57.120
value chain to go all the way depot to depot instead of transfer hub to transfer hub.

01:21:57.120 --> 01:22:00.720
And we have probably the best advantages in the world because of all the Waymo experience on

01:22:00.720 --> 01:22:05.680
surface streets. But that's not the highest ROI right now where the highest ROI is hub to hub

01:22:05.680 --> 01:22:12.000
and get the routes going. And so when you ask what's L4, L4 can be applied to any domain,

01:22:12.000 --> 01:22:16.000
operating domain or scope, but it's effectively for the places where we say we're ready for

01:22:16.000 --> 01:22:27.280
autonomous operation, we are 100% operating as a self-driving truck with no human behind the wheel.

01:22:27.280 --> 01:22:30.960
That is L4 autonomy. And it doesn't mean that you operate in every condition. It doesn't mean

01:22:30.960 --> 01:22:38.400
you operate on every road, but for a particularly well-defined area, operating conditions, routes,

01:22:38.640 --> 01:22:42.480
domain, you are fully autonomous. And that's the difference between L4 and L5. And most people

01:22:42.480 --> 01:22:46.560
would agree that at least any time in the foreseeable future, L5 is just not even really

01:22:46.560 --> 01:22:52.400
worth thinking about because there's always going to be these extremes. And so it's a race and

01:22:52.400 --> 01:22:58.160
almost like a game where you think of what is the sequence of expanded capabilities that create the

01:22:58.160 --> 01:23:03.120
most value and teach us the most and create this feedback loop where we're building out and

01:23:03.120 --> 01:23:08.480
unlocking more and more capability over time. I got to ask you, just curious. So first of all,

01:23:08.480 --> 01:23:13.840
I have to, when I'm allowed, visit the Dallas facility because it's super cool. It's like

01:23:13.840 --> 01:23:20.560
robot on the giving and the receiving end. The truck is a robot and the hub is a robot.

01:23:20.560 --> 01:23:28.400
Yeah, it's got to be very robot friendly. I will feel at home. What's the sensor suite like on the

01:23:28.400 --> 01:23:35.840
hub if you can just high level mention it? Does the hub have like lidars and like, is the truck

01:23:35.840 --> 01:23:41.920
doing most of the intelligence or is the hub also intelligent? Yeah. So most of it will be the truck

01:23:41.920 --> 01:23:47.840
and everything is like connected. So we have our servers where we know exactly where every truck

01:23:47.840 --> 01:23:52.400
is. We know exactly what's happening at a hub. And so you can imagine like a large backend system

01:23:52.480 --> 01:23:58.400
that over time starts to manage timings, goods, delivery windows, all these sorts of things.

01:23:58.400 --> 01:24:04.960
And so you don't actually need to, there might be special cases where that is valuable to equip

01:24:04.960 --> 01:24:08.640
some sensors in the hub, but a majority of the intelligence is going to be on the truck

01:24:08.640 --> 01:24:14.640
because whatever's relevant to the truck, relevant should be seen by the truck and can be relayed

01:24:15.600 --> 01:24:20.480
remotely for any sort of kind of cognizance or decision making. But there's a distinct

01:24:20.480 --> 01:24:25.040
type of workflow where, where do you check trucks? Where do you want them to enter? What

01:24:25.040 --> 01:24:29.760
if there's many operating at once? Where's the staging area to depart? How do you set up the

01:24:29.760 --> 01:24:36.480
flow of humans and human cars and traffic so that you minimize the interaction between humans and

01:24:36.480 --> 01:24:41.440
kind of self-driving trucks? And then how do you even intelligently select the locations of these

01:24:41.440 --> 01:24:45.920
transfer hubs that are both really great service locations for a metropolitan area? And there could

01:24:45.920 --> 01:24:51.200
be over time, many of them for a metropolitan area while at the same time leaning into

01:24:52.640 --> 01:24:56.000
the path of least resistance to lean into your current capabilities and strengths

01:24:56.000 --> 01:25:00.960
so that you minimize the amount of work that's necessary to unlock the next kind of big bar.

01:25:00.960 --> 01:25:05.280
I have a million questions. So first, is the goal to have no human in the truck?

01:25:06.000 --> 01:25:10.240
The goal is to have no human in the truck. Now, of course, right now we're testing with

01:25:10.240 --> 01:25:15.680
expert operators and so forth, but the goal is to, now there might be circumstances where

01:25:15.680 --> 01:25:20.560
it makes sense to have a human. And obviously, these trucks can also be manually driven. So

01:25:20.560 --> 01:25:28.240
sometimes we talk with our fleet partners about how you can buy a Waymo-equipped Diemore truck

01:25:28.240 --> 01:25:32.720
down the road and on the routes that are autonomous, it's autonomous. On the routes that are not,

01:25:32.720 --> 01:25:37.600
it's human-driven. Maybe there's L2 functionality that adds safety systems and so forth,

01:25:37.600 --> 01:25:43.120
but as soon as we expand in software the availability of driverless routes,

01:25:43.120 --> 01:25:48.640
the hardware is forward compatible to just now start using them in real time. And so you can

01:25:48.640 --> 01:25:54.560
imagine this mixed use, but at the end of the day, the largest value proposition is where you're

01:25:55.120 --> 01:25:59.840
able to have no constraints on how you can operate this truck and it's 100% autonomous

01:25:59.840 --> 01:26:06.480
with nobody inside. That's amazing. So let me ask on the logistics front because you mentioned that

01:26:06.480 --> 01:26:11.680
also opportunity to revamp or forbid from scratch some of the ideas around logistics.

01:26:12.640 --> 01:26:17.920
I don't want to throw too much shade, but from talking to Steve, my understanding is logistics

01:26:17.920 --> 01:26:24.160
is not perhaps as great as it could be in the current trucking environment. I'm not,

01:26:24.160 --> 01:26:29.600
maybe you can break down why, but there's probably competing companies. There's just a mess. Maybe

01:26:29.600 --> 01:26:38.240
some of it is literally just, it's old school. It's just like, it's not computerized. Truckers

01:26:38.240 --> 01:26:42.800
are almost like contractors. There's an independence and there's not a nice interface

01:26:42.800 --> 01:26:46.880
where they can communicate where they're going, where they're at, all those kinds of things.

01:26:46.880 --> 01:26:51.760
And so it just feels like there's so much opportunity to digitize everything to where

01:26:51.760 --> 01:26:57.040
you could optimize the use of human time, optimize the use of all kinds of resources.

01:26:58.080 --> 01:27:01.600
How much are you thinking about that problem? How fascinating is that problem?

01:27:02.400 --> 01:27:07.360
How difficult is it? How much opportunity is there to revolutionize the space of logistics

01:27:07.360 --> 01:27:12.080
in autonomous trucking, in trucking period? It's pretty fascinating. This is one of the most

01:27:12.080 --> 01:27:17.520
motivating aspects of all this where yes, there's a mountain of problems that you have to solve to

01:27:17.520 --> 01:27:22.400
get to the first checkpoints and first drivers and so forth. And inevitably in a space like this,

01:27:22.400 --> 01:27:27.680
you plug in initially into the existing system and start to learn and iterate, but

01:27:28.400 --> 01:27:33.040
that opportunity is massive. And so a couple of the factors that play into it. So first of all,

01:27:34.000 --> 01:27:38.720
there's obviously just the physical constraints of driving time, driver availability.

01:27:38.720 --> 01:27:42.560
Some fleets have a 95% attrition rate right now because of just

01:27:43.520 --> 01:27:49.680
this demands and gaps in competition and so forth. And then it's also incredibly fragmented where

01:27:50.480 --> 01:27:55.200
you would be shocked. When you look at industries and you think of the top 10 players,

01:27:55.200 --> 01:27:59.920
the biggest fleets like the Walmarts and FedExes and so forth, the percentage of the overall

01:27:59.920 --> 01:28:03.520
trucking market that's captured by the top 10 or 50 fleets is surprisingly small.

01:28:04.640 --> 01:28:12.400
The average kind of truck operation is like a one to five truck family business. And so there's

01:28:12.400 --> 01:28:19.520
just a huge amount of fragmentation which makes for really interesting challenges in stitching

01:28:19.520 --> 01:28:25.120
together through bulletin boards and brokerages and some people run their own fleets and this

01:28:25.120 --> 01:28:33.200
world's kind of evolving, but it is one of the less digitized and optimized worlds that there is.

01:28:34.480 --> 01:28:37.520
And the part that is optimized is optimized to the constraints of today.

01:28:38.560 --> 01:28:42.400
And even within the constraints of today, this is a $900 billion industry in the US

01:28:43.200 --> 01:28:46.320
and it's continuing to grow. It feels like from a business perspective,

01:28:46.880 --> 01:28:51.920
if I were to predict that whilst trying to solve the autonomous trucking problem,

01:28:51.920 --> 01:28:58.480
Waymo might solve first the logistics problem. Because that would already be a huge impact.

01:28:59.120 --> 01:29:05.440
So on the way to solving autonomous trucking, the human driven, there's so much opportunity to

01:29:06.640 --> 01:29:13.440
significantly improve the human driven trucking, the timing, the logistics. So you use humans

01:29:13.440 --> 01:29:19.040
optimally. The handoffs, the like. Well, even you get really ambitious, you start to expand

01:29:19.040 --> 01:29:23.600
as beyond how does the fulfillment center work and how does the transfer hub work,

01:29:23.600 --> 01:29:28.400
how does the warehouse work. There's a lot of opportunities to start to automate these chains

01:29:28.400 --> 01:29:35.360
and a lot of the inefficiency today is because you have a delay. Port of LA has a bunch of

01:29:35.360 --> 01:29:38.800
ships right now waiting outside of it because they can't dock because there's not enough

01:29:39.680 --> 01:29:43.760
labor inside of the port of LA. That means there's a big backlog of trucks, which means there's a big

01:29:43.760 --> 01:29:47.200
backlog of deliveries, which means the drivers aren't where they need to be. And so you have

01:29:47.200 --> 01:29:53.200
this huge chain reaction and your feasibility of readjusting in this network is low because

01:29:53.200 --> 01:29:59.360
everything's tied to humans and manual processes or distributed processes across a whole bunch of

01:29:59.360 --> 01:30:05.360
players. And so one of the biggest enablers is, yes, we have to solve autonomous trucking first.

01:30:05.360 --> 01:30:10.960
And by the way, that's not like an overnight thing. That's decades of continued expansion and work.

01:30:10.960 --> 01:30:17.120
But the first checkpoint and the first route is not that far off. But once you start enabling

01:30:17.120 --> 01:30:23.280
and you start to learn about how the constraints of autonomous trucking, which are very, very

01:30:23.280 --> 01:30:28.160
different than the constraints of human trucking, and again, strengths and weaknesses, how do you

01:30:28.160 --> 01:30:35.760
then start to leverage that and rethink a flow of goods more broadly? And this is where the

01:30:35.760 --> 01:30:42.080
learnings of really partnering with some of the largest fleets in the US and the learnings that

01:30:42.080 --> 01:30:46.240
they have about the industry and the needs that they have. And what would change if you just

01:30:47.440 --> 01:30:51.600
really broke this one constraint that holds up the whole network? Or what if you enabled

01:30:51.600 --> 01:30:56.720
this other constraint? That actually drives the roadmap in a lot of ways because this is not an

01:30:56.720 --> 01:31:02.160
all or nothing problem. You start to unlock more and more functionality over time. Which

01:31:02.160 --> 01:31:07.440
functionality most enables this optimization ends up being part of the discussion. But

01:31:07.440 --> 01:31:14.720
you're totally right. You fast forward to five years, 10 years, 15 years, and you think about

01:31:15.120 --> 01:31:21.520
very generalized capability of automation and logistics, as well as the ability to poke into

01:31:21.520 --> 01:31:27.520
how those handoffs work. The efficiency goes far beyond just direct cost of today's unit economics

01:31:27.520 --> 01:31:33.440
of a truck. They go towards reinventing the entire system in the same way that you see

01:31:34.080 --> 01:31:38.080
these other industries. When you get to enough scale, you can really rethink

01:31:38.800 --> 01:31:43.040
how you build around your new set of capabilities, not the old set of capabilities.

01:31:43.040 --> 01:31:48.560
Yeah. Use the analogy metaphor or whatever that autonomous trucking is like email versus mail.

01:31:48.560 --> 01:31:52.720
And then with email, you're still doing the communication, but it opens up all kinds of

01:31:53.760 --> 01:31:57.200
varieties of communication that you didn't anticipate.

01:31:57.200 --> 01:32:01.440
That's right. Constraints are just completely different. There's definitely a property of that

01:32:01.440 --> 01:32:08.160
here. And we're also still learning about it because there is a lot of really fascinating

01:32:08.160 --> 01:32:11.360
and sometimes really elegant things that the industry has done where there's companies whose

01:32:11.360 --> 01:32:16.240
entire existence is around, despite the constraints, optimizing as much as they can out of it.

01:32:16.240 --> 01:32:20.800
And those lessons do carry over. But it's an interesting kind of merger of worlds to think

01:32:20.800 --> 01:32:25.760
about like, well, what if this was completely different? How would we approach it? And the

01:32:25.760 --> 01:32:30.960
interesting thing is that for a really, really, really long time, it's actually going to be the

01:32:30.960 --> 01:32:35.440
merger between how to use autonomy and how to use humans that leans into each of their strengths.

01:32:36.320 --> 01:32:41.520
Yeah. And then we're back to Cosmo, human-robot interaction. And the interesting thing about

01:32:41.520 --> 01:32:46.800
Waymo is because there's the passenger vehicle, the transportation of humans and transportation

01:32:46.800 --> 01:32:54.160
of goods, you could see over time, they might kind of meld together more because you'll probably

01:32:54.160 --> 01:32:58.880
have like zero occupancy vehicles moving around, so you have transportation of goods for short

01:32:58.880 --> 01:33:04.000
distances and then for slightly longer distances and then slightly longer. And then there'll be

01:33:04.000 --> 01:33:09.520
this, then you just see the difference between a passenger vehicle and a truck is just size and

01:33:09.520 --> 01:33:13.120
you can have different sizes and all that kind of stuff. And at the core, you can have a Waymo

01:33:13.120 --> 01:33:17.040
driver that doesn't, as long as you have the same sensor suite, you can just think of it as one

01:33:17.040 --> 01:33:21.760
problem. And that's why over time, these do kind of converge where in a lot of ways, a lot of the

01:33:21.760 --> 01:33:25.840
challenges we're solving are freeway driving, which are going to carry over very well to the

01:33:25.840 --> 01:33:32.560
vehicles, to the car side. But there are then unique challenges like you have a very different

01:33:32.560 --> 01:33:37.040
dynamics in your vehicle where you have to see much further out in order to have the proper

01:33:37.040 --> 01:33:42.160
response time because you have an 80,000-pound fully loaded truck. That's a very, very different

01:33:42.160 --> 01:33:48.960
type of braking profile than a car. You have really interesting kind of dynamic limits

01:33:48.960 --> 01:33:54.240
because of the trailer where you actually, it's very, very hard to physically flip a car or do

01:33:54.240 --> 01:34:00.800
something physically. Most risk in a car is from just collisions. It's very hard in any normal

01:34:00.800 --> 01:34:05.040
operation to do something other than unless you hit something to actually kind of roll over or

01:34:05.040 --> 01:34:09.520
something. On a truck, you actually have to drive much closer to the physical bounds of the safety

01:34:09.520 --> 01:34:17.760
limits, but you actually have real constraints because you could have really interesting

01:34:17.760 --> 01:34:21.600
interactions between the cabin and the trailer. There's something called jackknifing if you turn

01:34:22.560 --> 01:34:26.960
too quickly, you have roll risks and so forth. We spent a huge amount of time understanding

01:34:26.960 --> 01:34:31.520
those boundaries. Those boundaries change based on the load that you have, which is also

01:34:31.520 --> 01:34:35.520
an interesting difference. You have to propagate that through the algorithm so that you're

01:34:35.520 --> 01:34:40.320
leveraging your dynamic range, but always staying within a safety balance, but understanding what

01:34:40.320 --> 01:34:45.280
those safety bounds are. We have this really cool test facility where we take it to the max and

01:34:45.280 --> 01:34:49.920
actually imagine a truck with these giant training wheels on the back of the trailer,

01:34:49.920 --> 01:34:55.680
and you're pushing it past the safety limits in order to try to actually see where it rolls,

01:34:55.680 --> 01:35:00.960
and so you define this high dimensional boundary, which then gets captured in software to stay safe

01:35:00.960 --> 01:35:05.760
and actually do the right thing. It's kind of fascinating the kind of challenges you have there,

01:35:06.320 --> 01:35:09.600
but then all of these things drive really interesting challenges from perception to

01:35:10.880 --> 01:35:15.120
unique behavior prediction challenges, and obviously in planner where you have to think

01:35:15.120 --> 01:35:20.480
about merging and creating gaps with a 53 foot trailer and so forth, and then obviously the

01:35:20.480 --> 01:35:24.560
platform itself is very different. We have different numbers of sensors, sometimes types

01:35:24.560 --> 01:35:27.680
of sensors, and you also have unique blind spots that you have because of the trailer,

01:35:27.680 --> 01:35:33.200
which you have to think about. It's a really interesting spectrum, and in the end you try to

01:35:33.200 --> 01:35:38.880
capture these special cases in a way that is cleanly augmentations of the existing tech stack

01:35:39.600 --> 01:35:44.560
because a majority of what we're solving is actually generalizable, the freeway driving

01:35:45.360 --> 01:35:51.280
and different platforms, and over time they all start to kind of merge ideally where the things

01:35:51.280 --> 01:35:56.080
that are unique are as minimal as possible, and that's where you get the most leverage,

01:35:56.080 --> 01:36:04.160
and that's why Waymo can do take on $2 trillion opportunities and have been nowhere near 2X the

01:36:04.160 --> 01:36:08.640
cost or investment or size. In fact, it's much, much smaller than that because of the high degree

01:36:08.640 --> 01:36:16.960
of leverage. So what kind of sensors they can speak to that a long haul truck needs to have?

01:36:16.960 --> 01:36:23.680
Lidar, vision, how many, what are we talking about here? Yeah, so it's more than the car, so

01:36:23.680 --> 01:36:28.800
very loosely you can think of it as like 2X, but it varies depending on the sensor, and so we have

01:36:28.800 --> 01:36:34.240
like dozens of cameras, radar, and then multiple lidar as well. You'll see one difference where

01:36:34.800 --> 01:36:40.400
the cars have a central main sensor pod on the roof in the middle, and then some kind of hood

01:36:40.400 --> 01:36:45.280
sensors for blind spots. The truck moves to two main sensor pods on the outsides where you would

01:36:45.280 --> 01:36:50.640
typically have the mirrors next to the driver. They effectively go as far out as possible,

01:36:51.440 --> 01:36:57.840
kind of up front, kind of on the cabin, not all the way in the front, but like kind of where the

01:36:57.840 --> 01:37:02.000
mirrors for the driver would be. And so those are the main sensor pods, and the reason they're there

01:37:02.000 --> 01:37:05.760
is because if you had one in the middle, the trailer is higher than the cabin, and you would

01:37:05.760 --> 01:37:09.920
be occluded with this like awkward wedge. Too much occlusion. Too much occlusion, and so then you

01:37:09.920 --> 01:37:14.720
would add a lot of complexity to the software to make up for that, and just unnecessary complexity.

01:37:14.720 --> 01:37:18.320
There's so many probably fascinating designs you'll just see. It's really cool. Because you

01:37:18.320 --> 01:37:22.000
can probably bring up a lidar higher and have it in the center or something. You could have all

01:37:22.000 --> 01:37:27.680
kinds of choices to make the decisions here that ultimately probably will define the industry.

01:37:27.680 --> 01:37:31.200
Right, but by having two on the side, there's actually multiple benefits. So one is like

01:37:32.320 --> 01:37:36.960
you're just beyond the trailer, so you can see fully flush with the trailer, and so you eliminate

01:37:36.960 --> 01:37:41.440
most of your blind spot except for right behind the trailer, which is great because now the

01:37:41.440 --> 01:37:45.600
software carries over really well, and the same perception system you use on the car side,

01:37:45.600 --> 01:37:50.000
largely that architecture can carry over, and you can retrain some models and so forth, but you

01:37:50.000 --> 01:37:55.200
leverage it a lot. It also actually helps with redundancy where there's a really nice built-in

01:37:55.200 --> 01:38:00.160
redundancy for all the lidar cameras and radar where you can afford to have any one of them fail

01:38:00.160 --> 01:38:03.600
and you're still okay, and at scale, every one of them will fail.

01:38:04.800 --> 01:38:09.520
You will be able to detect when one of them fails because they don't, because of the redundancy

01:38:10.560 --> 01:38:13.360
they're giving you the data that's inconsistent with the rest of it.

01:38:13.360 --> 01:38:17.600
That's right, and it's not just like they no longer give data. It could be like they're fouled or

01:38:17.600 --> 01:38:23.600
they stop giving data or some electrical thing gets cut or part of your compute goes down.

01:38:23.600 --> 01:38:27.520
So what's neat is that you have way more sensors. Part of it is field of view and occlusions,

01:38:27.520 --> 01:38:33.600
part of it is redundancy, and then part of it is new use cases. So there's new types of sensors

01:38:33.600 --> 01:38:39.760
to optimize for long range and the sensing horizon that we look for on our vehicles

01:38:40.560 --> 01:38:47.440
that is unique to trucks because it actually is much further out than a car, but a majority

01:38:47.440 --> 01:38:50.880
are actually used across both cars and trucks, and so we use the same compute, the same

01:38:51.840 --> 01:38:59.280
fundamental baseline sensors, cameras, radar, IMUs, and so you get a great leverage from all

01:38:59.280 --> 01:39:01.600
of the infrastructure and the hardware development as a result.

01:39:01.600 --> 01:39:07.360
So what about cameras? What role does, so lidar is this rich set of information that has its

01:39:07.360 --> 01:39:14.640
strengths, has some weaknesses. Camera is this rich source of information that has some strengths,

01:39:14.640 --> 01:39:21.760
has its weaknesses. What role does lidar play? What role does vision cameras play

01:39:22.320 --> 01:39:25.840
in this beautiful problem of autonomous trucking?

01:39:25.840 --> 01:39:28.160
It is beautiful. There's like so much that comes together.

01:39:28.720 --> 01:39:31.840
And how much, at which point do they come together?

01:39:31.840 --> 01:39:37.840
Yeah. So let's start with lidar. So lidar has been like Waymo's, one of Waymo's big strengths

01:39:37.840 --> 01:39:43.840
and advantages, where we developed our own lidar in-house where many generations in,

01:39:43.840 --> 01:39:49.600
both in cost and functionality, it is the best in the space.

01:39:49.600 --> 01:39:55.520
Which generation, because I know there's this cool, I love versions that are increasing,

01:39:56.080 --> 01:40:00.240
which version of the hardware stack is at currently, officially, publicly?

01:40:00.240 --> 01:40:05.440
Fifth, I believe. So some parts iterate more than others. I'm trying to remember on the sensor

01:40:05.440 --> 01:40:09.920
side. So the entire self-driving system, which includes sensors and compute, is fifth generation.

01:40:10.880 --> 01:40:18.000
I can't wait until there's like iPhone-style announcements for new versions of the Waymo

01:40:18.000 --> 01:40:19.840
hardware stack. Well, we try to be careful because,

01:40:19.840 --> 01:40:24.000
man, when you change the hardware, it takes a lot to retrain the models and everything.

01:40:24.000 --> 01:40:28.160
So we just went through that and going from the Pacifica to the Jaguars. And so the Jaguars and

01:40:28.160 --> 01:40:34.480
then the trucks have the same generation now. But yeah, the lidar is, it's incredible. And so Waymo

01:40:34.480 --> 01:40:38.800
has leaned into that as a strength. And so a lot of the near range perception system

01:40:40.000 --> 01:40:46.480
that obviously carries over a lot from the car side uses lidar as a very prominent primary sensor.

01:40:46.480 --> 01:40:51.760
But then obviously everything has its strengths and weaknesses. And so in the near range, lidar

01:40:51.760 --> 01:40:58.240
is a gigantic advantage. And it has its weaknesses when it comes to occlusions in certain areas,

01:40:58.800 --> 01:41:03.840
rain and weather, things like that. But it's an incredible sensor and it gives you incredible

01:41:03.840 --> 01:41:09.840
density, perfect location precision and consistency, which is a very valuable property

01:41:10.800 --> 01:41:15.120
to be able to kind of apply a male approach. Can you elaborate consistency?

01:41:15.120 --> 01:41:19.520
Yeah. When you have a camera, the position of the sun, the time of the day,

01:41:21.120 --> 01:41:25.280
various of the properties can have a big impact, whether there's glare, the field of view,

01:41:25.280 --> 01:41:33.360
things like that. So consistent in the face of a changing external environment,

01:41:33.360 --> 01:41:37.200
the signal. Yeah, daytime, nighttime. It's about 3D

01:41:38.400 --> 01:41:44.080
physical existence in effect. You're seeing beams of light that physically bounce off of something

01:41:44.080 --> 01:41:51.280
and come back. And so whatever the conditional conditions are, the shape of a human sensor

01:41:51.280 --> 01:41:56.960
reading from a human or from a car or from an animal, you have a reliability there, which ends

01:41:56.960 --> 01:42:02.080
up being valuable for the long tail of challenges. Now, lidar is the first sensor to drop off in

01:42:02.080 --> 01:42:05.920
terms of range and ours has a really good range, but at the end of the day, it drops off. And so

01:42:05.920 --> 01:42:11.520
particularly for trucks, on top of the general redundancy that you want for near range and

01:42:11.520 --> 01:42:16.400
complements through cameras and radar for occlusions and for complementary information and so forth,

01:42:16.400 --> 01:42:21.760
when you get to long range, you have to be radar and camera primary because your lidar data will

01:42:21.760 --> 01:42:26.720
fundamentally drop off after a period of time and you have to be able to see kind of objects further

01:42:26.720 --> 01:42:35.200
out. Now, cameras have the incredible range where you get a high density, high resolution camera.

01:42:35.200 --> 01:42:40.640
You can get data well past a kilometer and it's really potentially a huge value. Now,

01:42:40.640 --> 01:42:45.760
the signal drops off, the noise is higher, detecting is harder, classifying is harder.

01:42:45.760 --> 01:42:52.000
And one that you may not think about localizing is harder because you can be off by like two meters

01:42:52.000 --> 01:42:55.760
and where something's located a kilometer away and that's the difference between being on the

01:42:55.760 --> 01:42:58.960
shoulder and being in your lane. And so you have like interesting challenges there that you have

01:42:58.960 --> 01:43:02.880
to solve, which have a bunch of approaches that come into it. Radar is interesting because

01:43:05.920 --> 01:43:12.800
it also has longer range than lidar and it gives you speed information. So it becomes very,

01:43:12.800 --> 01:43:18.640
very useful for dynamic information of traffic flow, vehicle motions, animals,

01:43:19.280 --> 01:43:26.560
pedestrians, like just things that might be useful signals. And it helps with weather

01:43:26.560 --> 01:43:29.920
conditions where radar actually penetrates weather conditions in a better way than

01:43:29.920 --> 01:43:34.960
other sensors. And so it's kind of interesting where we've kind of started to converge towards

01:43:34.960 --> 01:43:39.120
not thinking about a problem as a lidar problem or a camera problem or a radar problem, but

01:43:39.120 --> 01:43:45.280
it's a fusion problem where these are all like large scale ML problems where you put data into

01:43:45.280 --> 01:43:50.880
the system. And in many cases, you just look for the signals that might be present in the

01:43:50.880 --> 01:43:56.480
union of all of these and leave it to the system as much as possible to start to really identify

01:43:57.360 --> 01:44:00.000
how to extract that. And then there's places we have to intervene and actually

01:44:00.640 --> 01:44:05.600
include more, but no single sensor is in a great position to really solve this problem and end

01:44:05.600 --> 01:44:11.360
without a huge extra challenge. That's fascinating. There's a question that's

01:44:11.440 --> 01:44:18.960
probably still an open question is at which point do you fuse them? Do you solve the perception

01:44:18.960 --> 01:44:25.040
problem for each sensor suite individually, the lidar suite and the camera suite, or do you

01:44:25.600 --> 01:44:29.760
do some kind of heterogeneous fusion or do you fuse at the very beginning?

01:44:32.080 --> 01:44:35.280
Is there a good answer or at least an inkling of intuitions you can come up?

01:44:35.280 --> 01:44:39.840
Yeah. So people refer to this as like early fusion or late fusion. So late fusion might

01:44:39.840 --> 01:44:46.240
be that you have the camera pipeline, the lidar pipeline, and then you fuse them and when it gets

01:44:46.240 --> 01:44:52.160
to final semantics and classification and tracking, you kind of fuse them together and figure out which

01:44:52.160 --> 01:45:00.000
one's best. There's more and more evidence that early fusion is important. And that is because

01:45:01.760 --> 01:45:06.800
late fusion does not allow you to pick up on the complementary strengths and weaknesses of the

01:45:06.800 --> 01:45:12.480
sensors. Weather is a great example where if you do early fusion, you have an incredibly hard

01:45:12.480 --> 01:45:18.480
problem for any single sensor in rain to solve that problem because you have reflections from

01:45:18.480 --> 01:45:25.200
the lidar. You have weird kind of noise from the camera, blah, blah, blah, right? But the

01:45:25.200 --> 01:45:29.920
combination of all of them can help you filter and help you get to the real signal that then gets you

01:45:29.920 --> 01:45:35.360
as close as possible to the original stack and be much more fluid about the strengths and weaknesses

01:45:35.360 --> 01:45:44.720
where your camera is much more susceptible to fouling on the actual lens from rain or random

01:45:44.720 --> 01:45:48.560
stuff, whereas you might be a little bit more resilient in other sensors. And so there's an

01:45:48.560 --> 01:45:54.400
element of logic that always happens late in the game, but that fusion early on actually, especially

01:45:54.400 --> 01:45:59.120
as you move towards ML and large-scale data-driven approaches, just maximizes your ability to pull

01:45:59.120 --> 01:46:03.680
out the best signal you can out of each modality before you start making constraining decisions

01:46:03.680 --> 01:46:08.880
that end up being hard to unwind late in the stack. So how much of this is a machine learning

01:46:08.880 --> 01:46:14.640
problem? What role does ML, machine learning, play in this whole problem of autonomous driving,

01:46:14.640 --> 01:46:21.200
autonomous trucking? It's massive, and it's increasing over time. If you go back to

01:46:22.640 --> 01:46:28.960
the grand challenge days and the early days of kind of AV development, there was ML,

01:46:28.960 --> 01:46:35.440
but it was not in the mass-scale data style of ML. It was learning models, but in a more structured

01:46:35.440 --> 01:46:40.080
kind of way, and it was a lot of heuristic and search-based approaches and planning and so forth.

01:46:40.080 --> 01:46:44.880
You can make a lot of progress with these types of approaches kind of across the board,

01:46:44.880 --> 01:46:49.680
an almost deceptive amount of progress. We can get pretty far, but then you start to really grind

01:46:49.680 --> 01:46:54.320
the further you get in some parts of the stack if you don't have an ability to absorb a massive

01:46:54.320 --> 01:46:58.880
amount of experience in a way that scales very subliminally in terms of human labor and human

01:46:58.880 --> 01:47:03.360
attention. When you look at the stack, the perception side is probably the first to get

01:47:03.360 --> 01:47:09.680
really revolutionized by ML, and it goes back many years because ML for computer vision and

01:47:09.680 --> 01:47:16.400
these types of approaches kind of took off with a lot of the early kind of push and deep learning.

01:47:17.520 --> 01:47:23.920
There's always a debate on the spectrum between kind of end-to-end ML, which is a little bit kind

01:47:23.920 --> 01:47:28.800
of like too far to how you architect it, to where you have modules but enough ability to

01:47:28.800 --> 01:47:34.400
think about long-tail problems and so forth. At the end of the day, you have big parts of the

01:47:34.400 --> 01:47:38.800
system that are very ML and data-driven. We're increasingly moving in that direction all the

01:47:38.800 --> 01:47:47.040
way across the board, including behavior where even when it's not like a gigantic ML problem

01:47:47.040 --> 01:47:51.440
that covers like a giant swath end-to-end, more and more parts of the system have this property

01:47:51.440 --> 01:47:56.400
where you want to be able to put more data into it, and it gets better. That has been one of the

01:47:56.400 --> 01:48:02.000
realizations is you drive tens of millions of miles and try to like solve new expansions

01:48:02.000 --> 01:48:07.280
of domains without regressing your old ones. It becomes intractable for a human to approach

01:48:07.280 --> 01:48:12.240
that in the way that traditionally robotics has kind of approached some elements of the tech stack.

01:48:12.240 --> 01:48:18.400
So are you trying to create a data pipeline specifically for the trucking problem?

01:48:19.200 --> 01:48:24.160
Is it like how much leveraging of the autonomous driving is there in terms of data collection?

01:48:25.680 --> 01:48:30.320
How unique is the data required for the trucking problem?

01:48:30.320 --> 01:48:35.600
So we reuse all the same infrastructure, so labeling workflows, ML workflows, everything,

01:48:35.600 --> 01:48:41.760
so that actually carries over quite well. We heavily reuse the data even where almost every

01:48:41.760 --> 01:48:45.360
model that we have on a truck, we started with the latest car model.

01:48:46.080 --> 01:48:49.040
So it's almost like a good background model.

01:48:49.040 --> 01:48:53.040
Yeah, it's like you can think of like, despite the different domain and different numbers of

01:48:53.040 --> 01:48:57.680
sensors and position of sensors, there's a lot of signals that carry over across driving. And so

01:48:57.680 --> 01:49:00.960
it's almost like pre-training and getting a big boost out of the gate where you can reduce the

01:49:00.960 --> 01:49:04.800
amount of data you need by a lot. And it goes both ways actually. And so we're increasingly

01:49:04.800 --> 01:49:09.760
thinking about our data strategy on how we leverage both of these. So you think about

01:49:11.120 --> 01:49:14.720
how other agents react to a truck. Yeah, it's a little bit different, but the fundamentals

01:49:14.720 --> 01:49:18.960
are actually like, what will other vehicles in the road do? There's a lot of carryover

01:49:18.960 --> 01:49:25.040
that's possible. And in fact, just to give you an example, we're constantly adding more data from

01:49:25.040 --> 01:49:30.400
the trucking side. But as of right now, when we think of one of our models, behavior prediction

01:49:30.400 --> 01:49:39.200
for other agents on the road, like vehicles, 85% of that data comes from cars. And a lot of that

01:49:39.200 --> 01:49:44.400
85% comes from surface streets because we just had so much of it and it was really valuable.

01:49:44.400 --> 01:49:48.640
And so we're adding in more and more, particularly in the areas where we need more data,

01:49:48.640 --> 01:49:53.200
but you get a huge boost out of the gate. Just all different visual characteristics of roads,

01:49:53.200 --> 01:49:56.480
lane markings, pedestrians, all that, that's still relevant.

01:49:56.480 --> 01:50:00.640
It's all still relevant. And then just the fundamentals of how you detect the car,

01:50:01.200 --> 01:50:05.040
does it really change that much, whether you're detecting it from a car or a truck?

01:50:05.040 --> 01:50:10.240
The fundamentals of how a person will walk around your vehicle, it'll change a little bit,

01:50:10.240 --> 01:50:15.200
but there's a lot of signal in there that as a starting point to a network can actually be

01:50:15.200 --> 01:50:19.360
very valuable. Now, we do have some very unique challenges where there's a sparsity of events on

01:50:19.360 --> 01:50:26.320
a freeway. The frequency of events happening on a freeway, whether it's interesting objects in the

01:50:26.320 --> 01:50:32.560
road or incidents, or even from a human benchmark, how often does a human have an accident on a

01:50:32.560 --> 01:50:37.120
freeway is far more sparse than on a surface street. And so that leads to really interesting

01:50:37.120 --> 01:50:41.840
data problems where you can't just drive infinitely to encounter all the different

01:50:41.840 --> 01:50:46.640
permutations of things you might encounter. And so there you get into interesting tools like

01:50:46.640 --> 01:50:51.680
structured testing and data collection, data augmentation, and so forth. And so there's

01:50:51.680 --> 01:50:58.080
really interesting kind of technical challenges that push some of the research that enables

01:50:58.080 --> 01:51:01.360
these new suites of approaches. What role does simulation play?

01:51:02.080 --> 01:51:06.320
Really good question. So Waymo simulates about a thousand miles for every mile it drives.

01:51:06.960 --> 01:51:10.000
So you think of- In both, so across the board.

01:51:10.000 --> 01:51:15.920
Across the board, yeah. So you think of, for example, well, if we've driven over 20 million

01:51:15.920 --> 01:51:22.160
miles, that's over 20 billion miles in simulation. Now, how do you use simulation? It's multi-purpose.

01:51:22.160 --> 01:51:27.440
So you use it for basic development. So you want to make sure you have regression

01:51:27.440 --> 01:51:31.360
prevention and protection of everything you're doing, right? That's an easy one.

01:51:32.640 --> 01:51:35.920
When you encounter something interesting in the world, let's say there was an issue with how

01:51:35.920 --> 01:51:40.640
the vehicle behaved versus an ideal human. You can play that back in simulation and start

01:51:40.640 --> 01:51:45.520
augmenting your system and seeing how you would have reacted to that scenario with this improvement

01:51:45.520 --> 01:51:50.000
or this new area. You can create scenarios that become part of your regression set after that

01:51:50.000 --> 01:51:56.480
point, right? Then you start getting into really, really kind of hill climbing where you say,

01:51:56.480 --> 01:51:59.760
hey, I need to improve this system. I have these metrics that are really correlated with final

01:51:59.760 --> 01:52:05.920
performance. How do I know how well I'm doing? The actual physical driving is the least efficient

01:52:05.920 --> 01:52:13.360
form of testing. It is expensive, it's time consuming. So grabbing a large scale batch

01:52:13.360 --> 01:52:18.800
of historical data and simulating it to get a signal of over these last, or just random sample

01:52:18.800 --> 01:52:23.360
of a hundred thousand miles, how has this metric changed versus where we are today? You can do that

01:52:23.360 --> 01:52:27.440
far more efficiently in simulation than just driving with that new system on board, right?

01:52:28.080 --> 01:52:33.440
And then you go all the way to the validation phase where to actually see your human relative

01:52:33.440 --> 01:52:38.320
safety of how well you're performing on the car side or the trucking side relative to a human,

01:52:39.680 --> 01:52:46.240
a lot of that safety case is actually driven by taking all of the physical operational driving,

01:52:46.240 --> 01:52:52.640
which probably includes a lot of interventions where the driver took over just in case.

01:52:54.000 --> 01:52:58.720
And then you simulate those forward and see if what anything have happened. And in most cases,

01:52:58.720 --> 01:53:02.800
the answer is no, but you can simulate it forward. And you can even start to do really interesting

01:53:02.800 --> 01:53:08.800
things where you add virtual agents to create harder environments. You can fuzz the locations

01:53:08.800 --> 01:53:13.680
of physical agents. You can muck with the scene and stress test the scenario from a whole bunch

01:53:13.680 --> 01:53:18.960
of different dimensions. And effectively you're trying to more efficiently sample this infinite

01:53:18.960 --> 01:53:23.920
dimensional space, but try to encounter the problems as fast as possible because what most

01:53:23.920 --> 01:53:28.880
people don't realize is the hardest problem in autonomous driving is actually the evaluation

01:53:28.880 --> 01:53:33.840
problem in many ways, not the actual autonomy problem. And so if you could in theory evaluate

01:53:33.840 --> 01:53:39.200
perfectly and instantaneously, you can solve that problem in a really fast feedback loop quite well.

01:53:39.840 --> 01:53:45.200
But the hardest part is being really smart about this suite of approaches on how can you get an

01:53:45.200 --> 01:53:50.160
accurate signal on how well you're doing as quickly as possible in a way that correlates

01:53:50.160 --> 01:53:56.080
to physical driving. Can you explain the evaluation problem? Which metric are you evaluating towards?

01:53:56.080 --> 01:54:00.080
We're talking about safety. What are the performance metrics that we're talking about?

01:54:00.080 --> 01:54:05.760
So in the end, you care about end safety. That's in the end what keeps you... That's what's

01:54:05.760 --> 01:54:11.760
deceptive where there's a lot of companies that have a great demo. The path from a really great

01:54:11.760 --> 01:54:17.120
demo to being able to go driverless can be deceptively long, even when that demo looks

01:54:17.120 --> 01:54:21.120
like it's driverless quality. And the difference is that the thing that keeps you from going

01:54:21.120 --> 01:54:24.960
driverless is not the stuff you encounter on a demo, it's the stuff that you encounter once

01:54:24.960 --> 01:54:31.920
at a hundred thousand miles or 500,000 miles. And so that is at the root of what is most challenging

01:54:31.920 --> 01:54:36.640
about going driverless because any issue you encounter, you can go and fix it, but how do you

01:54:36.640 --> 01:54:41.520
know you didn't create five other issues that you haven't encountered yet? So those learnings,

01:54:41.760 --> 01:54:47.600
those were painful learnings in Waymo's history that Waymo went through and led to us then finally

01:54:47.600 --> 01:54:50.800
being able to go driverless in Phoenix and now are at the heart of how we develop.

01:54:52.080 --> 01:54:59.120
Evaluation is simultaneously evaluating final kind of end safety of how ready are you to go

01:54:59.120 --> 01:55:08.640
driverless, which may be as direct as what is your human relative kind of collision rate

01:55:08.640 --> 01:55:14.720
for all these types of scenarios and severities to make sure that you're better than a human bar

01:55:15.600 --> 01:55:20.240
by a good amount. But that's not actually the most useful for development. For development,

01:55:20.240 --> 01:55:29.440
it's much more kind of analog metrics that are part of the art of finding what are the properties

01:55:29.440 --> 01:55:34.400
of driving that give you a way quicker signal that's more sensitive than a collision that can

01:55:34.400 --> 01:55:39.920
correlate to the quality you care about and push the feedback loop to all of your development.

01:55:39.920 --> 01:55:44.960
A lot of these are, for example, comparisons to human drivers, like manual drivers. How do you

01:55:44.960 --> 01:55:48.400
do relative to a human driver in various dimensions of various circumstances?

01:55:49.120 --> 01:55:54.320
Can I ask you a tricky question? So if I brought you a truck, how would you test it?

01:55:54.880 --> 01:56:00.160
Okay. Alan Turing came along and you said- This one's, can't tell if it's a human driver or

01:56:00.640 --> 01:56:05.840
autonomous driver. Yeah. But not the human because humans are flawed.

01:56:05.840 --> 01:56:08.880
It's different, but yeah. How do you actually know you're ready, basically? How do you know

01:56:08.880 --> 01:56:14.720
it's good enough? Yeah, and by the way, this is the reason why Waymo released a safety framework

01:56:14.720 --> 01:56:19.920
for the car side because one, it sets the bar so nobody cuts below it and does something bad

01:56:19.920 --> 01:56:25.760
for the field that causes an accident. Two, it's to start the conversation on framing what does

01:56:25.760 --> 01:56:30.960
this need to look like? Same thing we'll end up doing for the trucking side. It ends up being

01:56:32.880 --> 01:56:38.080
different portfolio of approaches. There's easy things like, are you compliant with all these

01:56:38.080 --> 01:56:41.680
fundamental rules of the road? You never drive above the speed limit. That's actually pretty

01:56:41.680 --> 01:56:46.880
easy. You can fundamentally prove that it's either impossible to violate that rule or that

01:56:46.880 --> 01:56:52.320
in these, you can itemize the scenarios where that comes up and you can do a test

01:56:52.320 --> 01:56:57.280
and show that you pass that test and therefore you can handle that scenario.

01:56:58.720 --> 01:57:03.440
Those are traditional structured testing system engineering approaches where you can just

01:57:05.520 --> 01:57:09.280
fault rates is another example where when something fails, how do you deal with it?

01:57:09.280 --> 01:57:12.160
You're not going to drive and randomly wait for it to fail. You're going to force a failure and

01:57:12.160 --> 01:57:19.040
make sure that you can handle it in close courses and simulation or on the road and run through all

01:57:19.040 --> 01:57:22.880
the permutations of failures, which you can oftentimes for some parts of system itemize

01:57:22.880 --> 01:57:31.440
like hardware. The hardest part is behavioral where you have just infinite situations that

01:57:31.440 --> 01:57:38.880
could in theory happen and you want to figure out the combinations of approaches that can work there.

01:57:38.880 --> 01:57:43.040
You can probably pass the Turing test pretty quickly even if you're not completely ready

01:57:43.040 --> 01:57:48.800
for driverless because the events that are really hard will not happen that often.

01:57:48.800 --> 01:57:54.960
Just to give you a perspective, a human has a serious accident on a freeway like a truck

01:57:54.960 --> 01:58:01.200
driver on a freeway. A serious event happens once every 1.3 million miles and something that

01:58:01.200 --> 01:58:05.920
actually has a really serious injury is 28 million miles. Those are really rare and so you could have

01:58:05.920 --> 01:58:11.440
a driver that looks like it's ready to go, but you have no signal on what happens there. That's

01:58:11.440 --> 01:58:18.480
where you start to get creative on combinations of sampling and statistical arguments, focused

01:58:18.480 --> 01:58:24.400
structured arguments where you can simulate those scenarios and show that you can handle them

01:58:24.960 --> 01:58:29.920
and metrics that are correlated with what you care about, but you can measure much more quickly

01:58:29.920 --> 01:58:35.600
and get to a right answer. That's what makes it pretty hard. In the end, you end up borrowing a

01:58:35.600 --> 01:58:41.840
lot of properties from aerospace and space shuttles and so forth where you don't get the

01:58:41.840 --> 01:58:45.760
chance to launch it a million times just to say you're ready because it's too expensive to fail.

01:58:46.560 --> 01:58:53.760
And so you go through a huge amount of structured approaches in order to validate it and then by

01:58:54.800 --> 01:58:57.280
thoroughness, you can make a strong argument that you're ready to go.

01:58:58.160 --> 01:59:01.840
This is actually a harder problem in a lot of ways though because you can think of a space shuttle as

01:59:02.560 --> 01:59:07.520
getting to a fixed point or an airplane and you freeze the software and then you prove it and

01:59:07.520 --> 01:59:12.160
you're good to go. Here you have to get to a driverless quality bar, but then continue to

01:59:12.160 --> 01:59:17.120
aggressively change the software even while you're driverless. But in also the full range

01:59:17.120 --> 01:59:22.160
of environment, there's an external environment with a shuttle. You're basically testing the

01:59:23.360 --> 01:59:28.720
systems, the internal stuff, and you have a lot of control in the external stuff.

01:59:28.720 --> 01:59:32.400
Yeah, and the hard part is how do you know you didn't get worse in something that you just

01:59:32.400 --> 01:59:39.600
changed? In a lot of ways, the Turing test starts to fail pretty quickly because you start to feel

01:59:39.600 --> 01:59:49.040
driverless quality pretty early in that curve. If you think about it, in most really good AV demos,

01:59:49.040 --> 01:59:53.920
maybe you'll sit there for 30 minutes. So you've driven 15 miles or something like that.

01:59:55.840 --> 02:00:01.120
To go driverless, what's the sort of rate of issues that you need to have you won't even encounter?

02:00:01.120 --> 02:00:05.600
So let's try something different then. Let's try a different version of the Turing test,

02:00:05.600 --> 02:00:11.440
which is like an IQ test. So there's these difficult questions of increasing difficulty.

02:00:13.120 --> 02:00:17.520
They're designed. You don't know them ahead of time. Nobody knows the answer to them.

02:00:18.560 --> 02:00:24.640
So is it possible to in the future orchestrate basically really difficult course almost of like

02:00:25.440 --> 02:00:32.560
that maybe change every year and that represent if you can pass these, they don't necessarily

02:00:32.560 --> 02:00:36.400
represent the full spectrum. That's it, yeah. They won't be conclusive, but you can at least

02:00:36.400 --> 02:00:40.560
get a really quick read and filter. Yeah, like you're able to, because you didn't know them ahead

02:00:40.560 --> 02:00:47.600
of time. I don't know. Probably. Like construction zones, failures. Or driving anywhere in Russia.

02:00:47.600 --> 02:00:54.800
Yeah, exactly. Snow. Weather, cut-ins, dense traffic, kind of merging lane closures,

02:00:55.760 --> 02:01:01.680
animal foreign objects on a road that pop out on short notice, mechanical failures, sensor breaking,

02:01:01.680 --> 02:01:07.360
tire popped, weird behaviors by other vehicles like a heartbreak, something reckless that they've

02:01:07.360 --> 02:01:14.320
done, fouling of sensors like bugs or birds or something. But yeah, like you have these kind of

02:01:14.320 --> 02:01:19.760
extreme conditions where you have a nasty construction zone where everything shuts down

02:01:19.760 --> 02:01:23.680
and you have to get pulled to the other side of the freeway with a temporary lane.

02:01:24.800 --> 02:01:29.200
Those are sort of conditions where we do that to ourselves. We itemize everything that could

02:01:29.200 --> 02:01:33.840
possibly happen to give you a starting point to how to think about what you need to develop.

02:01:33.840 --> 02:01:38.160
At the end of the day, there's no substitute for real miles. If you think of traditional ML,

02:01:38.160 --> 02:01:42.960
you know how there's a validation set where you hold out some data and real world driving

02:01:42.960 --> 02:01:48.560
is the ultimate validation set. That's in the end the cleanest signal. But you can do a really good

02:01:48.560 --> 02:01:53.280
job on creating an obstacle course and you're absolutely right. If there was such a thing as

02:01:53.280 --> 02:02:00.320
automating and kind of a readiness, it would be these extreme conditions like a red light runner,

02:02:00.320 --> 02:02:06.240
right? A really reckless pedestrian that's jaywalking, a cyclist that makes a really

02:02:06.240 --> 02:02:10.000
awkward maneuver. That's actually what keeps you from going driverless. In the end, that is the

02:02:10.000 --> 02:02:14.800
long tail. Yeah, and it's interesting to think about that. That to me is the touring test. Touring

02:02:14.800 --> 02:02:21.840
test means a lot of things, but to me in driving, the touring test is exactly this validation set

02:02:21.840 --> 02:02:26.720
that is handcrafted. I don't know if you know him. There's a guy named Francois Cholet.

02:02:30.000 --> 02:02:34.160
He thinks about how to design a test for general intelligence. He designs these IQ tests for

02:02:34.160 --> 02:02:43.280
machines. The validation set for him is handcrafted. It requires human genius or ingenuity

02:02:43.280 --> 02:02:48.800
to create a really good test. You truly hold it out. It's an interesting perspective on the

02:02:48.800 --> 02:02:56.880
validation set, which is like make that as hard as possible. Not a generic representation of the

02:02:56.880 --> 02:03:03.280
data, but this is the hardest. The hardest. Yeah. It's like go. You'll never fully itemize all the

02:03:03.280 --> 02:03:07.760
world states that you'll expand. You have to come up with different approaches. This is where you

02:03:07.760 --> 02:03:12.320
start hitting the struggles of ML where ML is fantastic at optimizing the average case.

02:03:12.880 --> 02:03:17.440
It's a really unique craft to think about how you deal with the worst case, which is what we care

02:03:17.440 --> 02:03:23.520
about in the AV space when using an ML system on something that occurs super infrequently.

02:03:24.560 --> 02:03:29.200
You don't care about the worst case really on ads because if you miss a few, it's not a big deal,

02:03:29.200 --> 02:03:36.000
but you do care about it on the driving side. Typically, you'll never fully enumerate the world

02:03:36.000 --> 02:03:40.320
and so you have to take a step back and abstract away what are the signals that you care about

02:03:40.400 --> 02:03:48.800
and the properties of a driver that correlate to defensive driving and avoiding nasty situations

02:03:49.360 --> 02:03:53.440
that even though you'll always be surprised by things you'll encounter,

02:03:53.440 --> 02:03:56.720
you feel good about your ability to generalize from what you've learned.

02:03:58.160 --> 02:04:07.920
All right. Let me ask you a tricky question. To me, the two companies that are building at scale

02:04:07.920 --> 02:04:13.280
some of the most incredible robots ever built is Waymo and Tesla.

02:04:16.320 --> 02:04:22.560
There's very distinct approaches technically, philosophically in these two systems.

02:04:23.440 --> 02:04:30.240
Let me ask you to play devil's advocate and then the devil's advocate to the devil's advocate.

02:04:30.240 --> 02:04:39.440
It's a bit of a race. Of course, everyone can win, but if Waymo wins this race to level four,

02:04:42.320 --> 02:04:48.480
why would they win? What aspect of the approach do you think would be the winning aspect? If Tesla

02:04:48.480 --> 02:04:56.720
wins, why would they win and which aspect of their approach would be the reason? Just building some

02:04:56.720 --> 02:05:02.400
intuition almost not from a business perspective for many of that, just technically. We could

02:05:02.400 --> 02:05:11.680
summarize. I think maybe you can correct me. One of the more distinct aspects is Waymo has

02:05:11.680 --> 02:05:19.200
a richer suite of sensors as LiDAR and vision. Tesla now removed radar. They do vision only.

02:05:20.320 --> 02:05:26.320
Tesla has a larger fleet of vehicles operated by humans. It's already deployed out in the field

02:05:26.400 --> 02:05:34.160
and it's a larger, what do you call it, operational domain. Then Waymo is more

02:05:34.160 --> 02:05:40.880
focused on a specific domain and growing it with fewer vehicles. Both are fascinating approaches.

02:05:41.920 --> 02:05:47.120
There's a lot of brilliant ideas. Nobody knows the answer. I'd love to get your comments on

02:05:47.120 --> 02:05:53.280
this lay of the land. Yeah, for sure. Maybe I'll start with Waymo. You're right, both incredible

02:05:53.280 --> 02:05:57.920
companies and just a gigantic respect to everything Tesla's accomplished and how they

02:05:57.920 --> 02:06:03.520
pushed the field forward as well. On the Waymo side, there is a fundamental advantage in the

02:06:03.520 --> 02:06:09.360
fact that it is focused and geared towards L4 from the very beginning. We've customized the

02:06:09.360 --> 02:06:14.960
sensor suite for it, the hardware, the compute, the infrastructure, the tech stack, and all of

02:06:14.960 --> 02:06:20.000
the investment inside the company. That's deceptively important because there's a

02:06:20.000 --> 02:06:24.400
giant spectrum of problems you have to solve in order to really do this from infrastructure

02:06:24.400 --> 02:06:30.000
to hardware to autonomy stack to the safety framework. That's an advantage because there's

02:06:30.000 --> 02:06:34.720
a reason why it's the fifth generation hardware and why all of those learnings went into the

02:06:34.720 --> 02:06:41.600
DIMOR program become such an advantage because you learn a lot as you drive and you optimize

02:06:41.600 --> 02:06:45.920
for the best information you have. But fundamentally, there's a big, big jump.

02:06:46.400 --> 02:06:53.040
Like every order of magnitude that you drive and numbers of miles and what you learn and the gap

02:06:53.040 --> 02:06:58.400
from really decent progress for L2 and so forth to what it takes to actually go L4. At the end of the

02:06:58.400 --> 02:07:07.920
day, there's a feeling that Waymo has, there's a long way to go. Nobody's won, but there's a lot

02:07:07.920 --> 02:07:14.000
of advantages in all of these buckets where it's the only company that has shipped a fully driverless

02:07:14.000 --> 02:07:20.640
service. We can go and you can use it, and it's at a decently sizable scale, and those learnings

02:07:20.640 --> 02:07:25.280
can feed forward how to solve the more general problems. And you see this process you've deployed

02:07:25.280 --> 02:07:31.680
in Chandler. You don't know the timeline exactly, but you could see the steps. They seem almost

02:07:31.680 --> 02:07:37.600
incremental. It's become more engineering than totally blind R&D. Because it works in one place

02:07:37.600 --> 02:07:41.520
and then you move to another place and you grow it this way. And just to give you an example,

02:07:41.520 --> 02:07:46.880
we fundamentally changed our hardware and our software stack almost entirely from what went

02:07:46.880 --> 02:07:52.080
driverless in Phoenix to what is the current generation of the system on both sides.

02:07:52.640 --> 02:07:58.080
Because the things that got us to driverless, even though it got to driverless way beyond human

02:07:58.080 --> 02:08:05.920
relative safety, it is fundamentally not well set up to scale in an exponential fashion without

02:08:05.920 --> 02:08:10.800
getting into huge scaling pains. And so those learnings you just can't shortcut. And so that's

02:08:10.800 --> 02:08:14.880
an advantage. And so there's a lot of open challenges to kind of get through technical

02:08:14.880 --> 02:08:19.200
organizational, like how do you solve problems that are increasingly broad and complex like this,

02:08:19.200 --> 02:08:22.960
work on multiple products. But there's a few in that, okay, like balls in our court.

02:08:23.520 --> 02:08:27.520
There's a head start there. Now we got to go and solve it. And I think that focus on L4,

02:08:27.520 --> 02:08:31.520
it's a fundamentally different problem. If you think about it, like let's say we were designing

02:08:31.520 --> 02:08:37.120
an L2 truck that was meant to be safer and help a human. You could do that with far less sensors,

02:08:37.760 --> 02:08:42.880
far less complexity and provide value very quickly, arguably with what we already have today,

02:08:42.880 --> 02:08:49.680
just packaged up in a good product. But you would take a huge risk in having a gap from even the

02:08:49.680 --> 02:08:54.160
like compute and sensors, not to mention the software to then jump from that system to an

02:08:54.160 --> 02:08:58.960
L4 system. So it's a huge risk basically. So again, allow me to be the person that

02:08:58.960 --> 02:09:04.880
plays the devil's advocate and let's argue for the Tesla approach. So what you just laid out

02:09:04.880 --> 02:09:09.600
makes perfect sense and is exactly right. There are some open questions here, which is

02:09:10.720 --> 02:09:17.680
it's possible that investing more in faster data collection, which is essentially what Tesla is

02:09:17.680 --> 02:09:27.520
doing, will get us there faster if the sensor suite doesn't matter as much and machine learning

02:09:27.520 --> 02:09:33.680
can do a lot of the work. This is the open question is how much is the thing you mentioned before,

02:09:33.760 --> 02:09:41.280
how much of driving can be end to end learned? That's the open question. Obviously the Waymo

02:09:41.280 --> 02:09:48.400
and the vision only machine learning approach will solve driving eventually, both. The question is

02:09:48.400 --> 02:09:52.560
of timeline, what's faster? That's right. And what you mentioned, if I were to make the opposite

02:09:52.560 --> 02:09:58.480
argument, like what puts Tesla in the strongest position, it's data. That is their superpower,

02:09:58.480 --> 02:10:04.960
where they have an access to real world data effectively with a safety driver.

02:10:07.280 --> 02:10:13.360
They found a way to get paid by safety drivers versus safer safety drivers. It's brilliant.

02:10:14.640 --> 02:10:19.360
But all joking aside, one, it is incredible that they've built a business that's incredibly

02:10:19.360 --> 02:10:23.680
successful that can now be a foundation and bootstrap really aggressive investment in

02:10:23.680 --> 02:10:29.600
autonomy space. If you can do it, that's always an incredible advantage. And in the data aspect of

02:10:29.600 --> 02:10:34.320
it, it is a giant amount of data if you can use it the right way to then solve the problem. But

02:10:34.320 --> 02:10:39.760
the ability to collect and filter through the things to the things that matter at real world

02:10:39.760 --> 02:10:46.960
scale at a large distribution, that is huge. It's a big advantage. And so then the question becomes,

02:10:46.960 --> 02:10:51.440
can you use it in the right way and do you have the right software systems and hardware systems

02:10:51.440 --> 02:10:57.280
in order to solve the problem? And you're right that in the long term, there's no reason to believe

02:10:57.280 --> 02:11:02.080
that pure camera systems can't solve the problem that humans obviously are solving with vision

02:11:02.080 --> 02:11:11.360
systems. But it's a risk. So there's no argument that it's not a risk. And it's already such a hard

02:11:11.360 --> 02:11:17.200
problem. And so much of that problem, by the way, is even beyond the perception side, some of the

02:11:17.200 --> 02:11:20.880
hardest elements of the problem are on the behavioral side and decision making and the

02:11:20.880 --> 02:11:27.520
long tail safety case. If you are adding risk and complexity on the input side from perception,

02:11:27.520 --> 02:11:33.440
you're now making a really, really hard problem, which on its own is still almost insurmountably

02:11:33.440 --> 02:11:39.280
hard, even harder. And so the question is just how much. And this is where you can easily get

02:11:39.280 --> 02:11:45.840
into a little bit of a trap where, similar to how do you evaluate how good an AV company's product

02:11:45.840 --> 02:11:51.040
is, you go and you do a trial, kind of a test run with them, a demo run, which they've kind of

02:11:51.040 --> 02:11:56.160
optimized like crazy and so forth. And it feels good. Do you put any weight in that? You know that

02:11:56.160 --> 02:12:02.640
that gap is kind of pretty large still. Same thing on the perception case. The long tail of

02:12:02.640 --> 02:12:07.920
computer vision is really, really hard. And there's a lot of ways that that can come up.

02:12:07.920 --> 02:12:12.400
And even if it doesn't happen that often at all, when you think about the safety bar and what it

02:12:12.400 --> 02:12:17.200
takes to actually go full driverless, not like incredible assistance driverless, but full driverless,

02:12:18.880 --> 02:12:23.360
that bar gets crazy high. And not only do you have to solve it on the behavioral side,

02:12:23.360 --> 02:12:28.560
but now you have to push computer vision beyond arguably where it's ever been pushed.

02:12:28.560 --> 02:12:32.080
And so you're now on top of the broader AV challenge. You have a really hard perception

02:12:32.080 --> 02:12:36.400
challenge as well. So there's perception, there's planning, there's human robot interaction. To me,

02:12:36.400 --> 02:12:42.160
what's fascinating about what Tesla is doing is in this march towards level four,

02:12:43.040 --> 02:12:48.400
because it's in the hands of so many humans, you get to see video, you get to see humans.

02:12:48.400 --> 02:12:54.960
I mean, forget companies, forget businesses. It's fascinating for humans to be interacting with

02:12:54.960 --> 02:12:58.800
robots. That's incredible. And they're actually helping kind of push it forward. And that is

02:12:58.800 --> 02:13:04.400
valuable, by the way, where even for us, a decent percentage of our data is human driving. We

02:13:04.400 --> 02:13:09.040
intentionally have humans drive higher percentage than you might expect, because that creates some

02:13:09.040 --> 02:13:14.960
of the best signals to train the autonomy. And so that is on its own value. So together,

02:13:14.960 --> 02:13:19.840
we're kind of learning about this problem in an applied sense, just like you had with Cosmo.

02:13:21.760 --> 02:13:27.760
When you're chasing an actual product that people are going to use, robot-based product that people

02:13:27.760 --> 02:13:33.280
are going to use, you have to contend with the reality of what it takes to build a robot that

02:13:33.280 --> 02:13:37.040
successfully perceives the world and operates in the world, and what it takes to have a robot that

02:13:37.040 --> 02:13:42.000
interacts with other humans in the world. And that's to me, one of the most interesting problems

02:13:42.000 --> 02:13:48.480
humans have ever undertaken, because in trying to create an intelligent agent that operates in a

02:13:48.480 --> 02:13:56.400
human world, you're also understanding the nature of intelligence itself. How hard is driving is

02:13:56.400 --> 02:14:00.480
still not answered to me. I still don't understand.

02:14:00.480 --> 02:14:06.720
And all the subtle cues, even little things like your interaction with a pedestrian where you look

02:14:06.720 --> 02:14:12.560
at each other and just go, okay, go. That's hard to do without a human driver. And you're missing

02:14:12.560 --> 02:14:17.840
that dimension. How do you communicate that? So there's really, really interesting elements here.

02:14:17.840 --> 02:14:22.000
Now, here's what's beautiful. Can you imagine that when autonomous driving is solved,

02:14:22.960 --> 02:14:29.200
how much of the technology foundation of that space can go and have tremendous,

02:14:29.200 --> 02:14:33.600
just transformative impacts on other problem areas and other spaces that have

02:14:34.400 --> 02:14:38.240
subsets of these same problems? It's just incredible to think about that.

02:14:38.240 --> 02:14:48.800
It's both a pro and a con. Autonomous driving is so safety critical. So once you solve it,

02:14:48.880 --> 02:14:52.800
it's beautiful because there's so many applications that are a lot less safety critical.

02:14:53.360 --> 02:14:59.360
But it's also the con of that is it's so hard to solve. And the same journalists that you

02:14:59.360 --> 02:15:07.200
mentioned who get excited for a demo are the ones who write long articles about the failure

02:15:07.200 --> 02:15:15.280
of your company if there's one accident that's based on a robot. And it's just society is so

02:15:15.280 --> 02:15:20.320
tense and waiting for failure of robots. You're in such a high-stake environment.

02:15:20.320 --> 02:15:23.360
Failure has such a high cost. And it slows down development.

02:15:23.360 --> 02:15:24.720
It's hard. It slows down development.

02:15:24.720 --> 02:15:29.440
Yeah. The team definitely noticed that once you go driverless, we were driverless in Phoenix and

02:15:29.440 --> 02:15:36.240
you continue to iterate, your iteration pace slows down because your fear of regression

02:15:36.880 --> 02:15:43.200
forces so much more rigor that obviously you have to find a compromise on like, okay, well,

02:15:43.200 --> 02:15:46.640
how often do we release driverless builds? Because every time you release a driverless build,

02:15:46.640 --> 02:15:50.080
you have to go through this validation process, which is very expensive and so forth. So

02:15:51.200 --> 02:15:55.040
it is interesting. It is one of the hardest things. There's no other industry where

02:15:56.800 --> 02:16:01.600
you wouldn't release products way, way quicker when you start to provide even portions of the

02:16:01.600 --> 02:16:04.080
value that you provide. Healthcare maybe is the other one.

02:16:04.880 --> 02:16:05.440
That's right.

02:16:05.440 --> 02:16:10.160
But at the same time, we've gotten there where you think of surgery. You have surgery,

02:16:10.160 --> 02:16:14.800
there's always a risk, but it's really, really bounded. You know that there's an accident rate

02:16:14.800 --> 02:16:19.280
when you go out and drive your car today, right? And you know what the fatality rate in the US is

02:16:19.280 --> 02:16:24.400
per year. We're not banning driving because there was a car accident, but the bar for us is way

02:16:24.400 --> 02:16:29.360
higher and we hold ourselves very serious to it where you have to not only be better than a human,

02:16:29.360 --> 02:16:33.600
but you probably have to at scale be far better than a human by a big margin.

02:16:33.600 --> 02:16:39.040
And you have to be able to really, really thoughtfully explain all of the ways that

02:16:39.120 --> 02:16:43.600
we validate that becomes very comfortable for humans to understand because a bunch of jargon

02:16:43.600 --> 02:16:48.240
that we use internally just doesn't compute. At the end of the day, we have to be able to explain

02:16:48.240 --> 02:16:54.000
to society how do we quantify the risk and acknowledge that there is some non-zero risk,

02:16:54.000 --> 02:16:56.960
but it's far above a human relative safety.

02:16:56.960 --> 02:17:03.040
See, here's the thing. To push back a little bit and bring Cosmo back in the conversation,

02:17:03.040 --> 02:17:05.600
you said something quite brilliant at the beginning of this conversation that I think

02:17:05.600 --> 02:17:11.200
probably applies for autonomous driving, which is, you know, there's this desire to make

02:17:11.200 --> 02:17:17.520
autonomous cars much safer than human driven cars. But if you create a product that's really

02:17:17.520 --> 02:17:22.560
compelling and is able to explain both the leadership and the engineers and the product

02:17:22.560 --> 02:17:30.640
itself can communicate intent, then I think people may be able to be willing to put up with a thing

02:17:30.640 --> 02:17:36.800
that might be even riskier than humans because they understand the value of taking risks.

02:17:36.800 --> 02:17:41.520
You mentioned the speed limit. Humans understand the value of going over the speed limit.

02:17:41.520 --> 02:17:48.080
Humans understand the value of like going fast through a yellow light.

02:17:49.200 --> 02:17:54.880
When you're in Manhattan streets pushing through crossing pedestrians, they understand that.

02:17:54.880 --> 02:17:59.200
I mean, this is a much more tense topic of discussion. So this is just me talking.

02:17:59.440 --> 02:18:05.200
In Cosmo's case, there was something about the way this particular robot communicated,

02:18:05.200 --> 02:18:09.920
the energy it brought, the intent it was able to communicate to the humans that you understood

02:18:10.720 --> 02:18:15.200
that of course it needs to have a camera. Of course it needs to have this information.

02:18:15.200 --> 02:18:21.760
And in that same way, to me, of course a car needs to take risks. Of course there's going to be

02:18:21.760 --> 02:18:28.400
accidents. If you want a car that never has an accident,

02:18:28.400 --> 02:18:36.320
to have a car that just doesn't go anywhere. But that's tricky because that's not a robotics

02:18:36.320 --> 02:18:42.480
problem. And many accidents are not even due to you, right? Obviously. So there's a big difference

02:18:42.480 --> 02:18:49.440
though. That's not a personal decision. You're also impacting obviously kind of the rest of the

02:18:49.440 --> 02:18:56.800
road and we're facilitating it. And so there's a higher kind of ethical and moral bar, which

02:18:56.800 --> 02:19:02.160
obviously then translates into as a society and from a regulatory standpoint, kind of like what

02:19:02.160 --> 02:19:10.400
comes out of it where it's hard for us to ever see this even being a debate in the sense that

02:19:11.520 --> 02:19:14.960
you have to be beyond reproach from a safety standpoint. Because if you're wrong about this,

02:19:14.960 --> 02:19:20.240
you could set the entire field back a decade, right? See, this is me speaking. I think if we

02:19:20.240 --> 02:19:28.720
look into the future, there will be, I personally believe, this is me speaking, that there will be

02:19:28.720 --> 02:19:34.640
less and less focus on safety. It's still very, very high. Meaning like after autonomy is very

02:19:34.640 --> 02:19:40.320
common and accepted. It's not so common as everywhere, but there has to be a transition

02:19:40.320 --> 02:19:46.080
because I think for innovation, just like you were saying, to explore ideas, you have to take

02:19:46.080 --> 02:19:54.240
risks. And I think if autonomy in the near term is to become prevalent in society, I think people

02:19:54.240 --> 02:20:01.920
need to be more willing to understand the nature of risk, the value of risk. It's very difficult,

02:20:01.920 --> 02:20:09.120
you're right of course, with driving, but that's the fascinating nature of it. It's a life and

02:20:09.120 --> 02:20:15.520
death situation that brings value to millions of people. So you have to figure out what do we value

02:20:15.520 --> 02:20:23.440
about this world? How much do we value? How deeply do we want to avoid hurting other humans?

02:20:23.440 --> 02:20:28.720
That's right. And there is a point where you can imagine a scenario where Waymo has a system

02:20:29.440 --> 02:20:38.880
that is even when it's beyond human relative safety and provably statistically will

02:20:38.960 --> 02:20:50.560
save lives. There is a thoughtful navigation of that fact versus just society readiness and

02:20:50.560 --> 02:21:00.240
perception and education of society and regulators and everything else where it's multi-dimensional

02:21:00.880 --> 02:21:07.200
and it's not a purely logical argument. But ironically, the logic can actually help with

02:21:07.440 --> 02:21:13.120
emotions. And just like any technology, there's early adopters and then there's a curve that

02:21:13.840 --> 02:21:18.160
happens after it. And eventually celebrities, you get the rock in a Waymo vehicle and then

02:21:18.160 --> 02:21:21.120
everybody just comes along. And then everybody calms down because the rock likes it. Yeah.

02:21:23.120 --> 02:21:28.000
If you post him. Yeah. And it's an open question on how this plays out. Maybe we're pleasantly

02:21:28.000 --> 02:21:34.480
surprised and people just realize that this is such a neighbor of life and efficiency and cost

02:21:34.480 --> 02:21:39.600
and everything that there's a pull. At some point, I actually fully believe that this will

02:21:39.600 --> 02:21:48.960
go from a thoughtful movement and tiptoeing and a push to society realizes how wonderful

02:21:48.960 --> 02:21:53.280
of an enabler this could become and it becomes more of a pull. And hard to know exactly how that

02:21:53.280 --> 02:21:57.840
will play out, but at the end of the day, both the goods transportation and the people transportation

02:21:57.840 --> 02:22:02.000
side of it has that property where it's not easy. There's a lot of open questions and challenges

02:22:02.000 --> 02:22:07.440
to navigate. And there's obviously the technical problems to solve as a prerequisite, but

02:22:09.360 --> 02:22:15.280
they have such an opportunity that is on a scale that very few industries in the last 20, 30 years

02:22:15.280 --> 02:22:22.000
have even had a chance to tackle that maybe we're pleasantly surprised by how much that

02:22:22.000 --> 02:22:27.040
tipping point in a really short amount of time actually turns into a societal pull to embrace

02:22:27.040 --> 02:22:31.440
the benefits of this. Yeah. I hope so. It seems like in the recent few decades, there's been

02:22:31.440 --> 02:22:37.920
tipping points of technologies where overnight things change. It's like from taxis to ride

02:22:37.920 --> 02:22:44.080
sharing services, that shift. There's just shift after shift after shift that requires digitization

02:22:44.080 --> 02:22:49.680
and technology. I hope we're pleasantly surprised in this. So there's millions of long haul trucks

02:22:49.680 --> 02:22:57.760
now in the United States. Do you see a future where there's millions of Waymo trucks and maybe

02:22:57.760 --> 02:23:04.320
just broadly speaking, Waymo vehicles, just like ants running around the United States,

02:23:05.520 --> 02:23:11.600
freeways and local roads? Yeah, and other countries too. You look back decades from now

02:23:11.600 --> 02:23:16.160
and it might be one of those things that just feels so natural and then it becomes almost like

02:23:16.160 --> 02:23:24.320
this interesting oddity that we had none of it decades earlier. And it'll take a long time

02:23:24.320 --> 02:23:30.640
to grow and scale. Very different challenges appear at every stage. But over time, this is

02:23:30.640 --> 02:23:36.560
one of the most enabling technologies that we have in the world today. It'll feel like,

02:23:37.600 --> 02:23:40.480
how was the world before the internet? How was the world before mobile phones? It's going to

02:23:40.480 --> 02:23:44.560
have that sort of a feeling to it on both sides. It's hard to predict the future, but do you

02:23:44.560 --> 02:23:51.200
sometimes think about weird ways it might change the world, like surprising ways? So obviously,

02:23:51.920 --> 02:23:58.000
there's more direct ways where there's increases efficiency. It'll enable a lot of logistics,

02:23:58.000 --> 02:24:06.880
optimizations kind of things. It will change probably our roadways and all that kind of stuff,

02:24:06.880 --> 02:24:12.400
but it could also change society in some kind of interesting ways. Do you ever think about how it

02:24:12.400 --> 02:24:17.520
might change cities, how it might change our lives, all that kind of stuff? Yeah. You can imagine city

02:24:17.520 --> 02:24:21.440
where people live versus work becoming more distributed because the pain of commuting

02:24:21.440 --> 02:24:27.280
becomes different, just easier. There's a lot of options that open up. The layout of cities

02:24:27.280 --> 02:24:34.800
themselves and how you think about car storage and parking obviously just enables a completely

02:24:34.800 --> 02:24:42.000
different type of experience in urban environments. I think there was a statistic that something like

02:24:42.880 --> 02:24:50.480
30% of the traffic in cities during rush hour is caused by pursuit of parking or some really high

02:24:50.480 --> 02:24:57.040
stats. So those obviously kind of open up a lot of options. Flexibility on goods will enable

02:24:57.760 --> 02:25:01.600
new industries and businesses that never existed before because now the efficiency becomes

02:25:02.480 --> 02:25:07.120
more palatable. Good delivery, timing, consistency, and flexibility is going to change.

02:25:07.680 --> 02:25:12.640
The way we distribute the logistics network will change. The way we then can integrate with

02:25:12.640 --> 02:25:19.600
warehousing, with shipping ports, you can start to think about greater automation through the whole

02:25:19.600 --> 02:25:27.040
kind of stack and how that supply chain, the ripples become much more agile versus like very

02:25:28.080 --> 02:25:33.280
grindy the way they are today where just the adaptation is like very tough and there's a lot

02:25:33.280 --> 02:25:36.720
of constraints that we have. I think it'll be great for the environment. It'll be great

02:25:36.720 --> 02:25:45.680
for safety where probably about 95% of accidents today statistically are due to just attention or

02:25:45.680 --> 02:25:50.960
things that are preventable with the strengths of automation. Yeah, and it'll be one of those things

02:25:50.960 --> 02:25:56.720
where industries will shift but the net creation is going to be massively positive and then we

02:25:56.720 --> 02:26:01.440
just have to be thoughtful about the negative implications that will happen in local places

02:26:02.080 --> 02:26:05.760
and adjust for those. But I'm an optimist in general for the technology where you could argue

02:26:05.760 --> 02:26:11.920
a negative on any new technology but you start to kind of see that if there is a big demand for

02:26:11.920 --> 02:26:17.600
something like this, in almost all cases, it's an enabling factor that's going to propagate through

02:26:19.280 --> 02:26:26.080
society and particularly as life expectancies get longer and so forth, there's just a lot more need

02:26:26.080 --> 02:26:31.120
for a greater percentage of the population to kind of just be serviced with a high level of

02:26:31.120 --> 02:26:34.880
efficiency because otherwise we're going to have a really hard time kind of scaling to what's ahead

02:26:34.880 --> 02:26:41.040
in the next 50 years. Yeah, and you're absolutely right. Every technology has negative consequences

02:26:41.040 --> 02:26:44.960
and positive consequences and we tend to focus on the negative a little bit too much.

02:26:46.800 --> 02:26:50.880
In fact, autonomous trucks are often brought up as an example of

02:26:52.880 --> 02:26:58.800
artificial intelligence and robots in general taking our jobs. And as we've talked about briefly

02:26:58.800 --> 02:27:07.440
here, we talk a lot with Steve, it is a concern that automation will take away certain jobs,

02:27:07.440 --> 02:27:13.280
it'll create other jobs, so there's temporary pain, hopefully temporary, but pain is pain and

02:27:13.920 --> 02:27:17.120
people suffer and that human suffering is really important to think about.

02:27:18.800 --> 02:27:25.760
But trucking is, I mean, there's a lot written on this is, I would say, far from the thing

02:27:25.760 --> 02:27:29.760
that will cause the most pain. Yeah, there's even more positive

02:27:29.760 --> 02:27:34.240
properties about trucking where not only is there just a huge shortage which is going to increase,

02:27:34.240 --> 02:27:38.000
the average age of truck drivers is getting closer to 50 because the younger people aren't

02:27:38.000 --> 02:27:42.880
wanting to come into it. They're trying to incentivize, lower the age limit, all these

02:27:42.880 --> 02:27:48.080
sort of things, and the demand is just going to increase. And the least favorable, I mean,

02:27:48.080 --> 02:27:52.160
depends on the person, but in most cases, the least favorable types of routes are the massive

02:27:52.160 --> 02:27:56.000
long-haul routes where you're on the road away from your family 300 plus days a year.

02:27:56.000 --> 02:28:00.240
Steve talked about the pain of those kinds of routes from a family perspective. You're

02:28:01.920 --> 02:28:07.760
basically away from family. It's not just hours, you work insane hours, but it's also just time

02:28:07.760 --> 02:28:11.280
away from family. It's rough. Obesity rate is through the roof because you're just sitting all

02:28:11.280 --> 02:28:17.360
day, it's really, really tough. And that's also where the biggest safety risk is because of

02:28:17.360 --> 02:28:22.960
fatigue. And so when you think of the gradual evolution of how trucking comes in, first of all,

02:28:22.960 --> 02:28:27.760
it's not overnight. It's going to take decades to phase in all the, there's just a long, long,

02:28:27.760 --> 02:28:34.880
long road ahead. But the routes and the portions of trucking that are going to require humans the

02:28:34.880 --> 02:28:40.480
longest and benefit the most from humans are the short haul and most complicated, more urban routes,

02:28:40.480 --> 02:28:46.400
which are also the more pleasant ones, which are less continual driving time, more

02:28:48.320 --> 02:28:54.240
more flexibility on geography and location. And you get to sleep at your own home.

02:28:54.240 --> 02:29:01.760
And very importantly, if you optimize the logistics, you're going to use humans much better

02:29:02.960 --> 02:29:08.720
and thereby pay them much better. Because one of the biggest problems is truck drivers currently

02:29:08.720 --> 02:29:15.760
are paid by how much they drive. So they really feel the pain of inefficient logistics. Because

02:29:15.760 --> 02:29:20.160
if they're just sitting around for hours, which they often do, not driving, waiting,

02:29:20.160 --> 02:29:25.520
they're not getting paid for that time. So logistics has a significant impact on the

02:29:25.520 --> 02:29:29.280
quality of life of a truck driver. And a high percentage of trucks are empty

02:29:29.280 --> 02:29:34.080
because of inefficiencies in the system. Yeah. It's one of those things where, and the other

02:29:34.080 --> 02:29:39.040
thing is when you increase the efficiency of a system like this, the overall net volume of

02:29:39.040 --> 02:29:43.520
the system tends to increase. The entire market cap of trucking is going to go up

02:29:44.400 --> 02:29:49.920
when the efficiency improves and facilitates both growth in industries and better utilization of

02:29:49.920 --> 02:29:55.840
trucking. And so that on its own just creates more and more demand, which of all the places

02:29:55.840 --> 02:30:03.520
where AI comes in and starts to really reshape an industry, this is one of those where there's just

02:30:03.520 --> 02:30:08.240
a lot of positives that for at least any time in the foreseeable future seem really lined up in a

02:30:08.240 --> 02:30:16.400
good way to come in and help with the shortage and start to optimize for the routes that are

02:30:16.400 --> 02:30:21.520
most dangerous and most painful. Yeah. So this is true for trucking,

02:30:21.520 --> 02:30:28.080
but if we zoom out broader, automation and AI does technology broadly, I would say. But automation

02:30:28.720 --> 02:30:34.560
is a thing that has a potential in the next couple of decades to shift the kind of jobs

02:30:34.560 --> 02:30:36.640
available to humans. Yes.

02:30:36.640 --> 02:30:42.240
And so that results in, like I said, human suffering because people lose their jobs,

02:30:42.240 --> 02:30:48.080
there's economic pain there, and there's also a pain of meaning. So for a lot of people,

02:30:49.520 --> 02:31:00.640
work is a source of meaning, it's a source of identity, of pride in getting good at the job,

02:31:00.640 --> 02:31:06.800
pride in craftsmanship and excellence, which is what truck drivers talk about. But this is true

02:31:06.800 --> 02:31:12.400
for a lot of jobs. And is that something you think about as a sort of a roboticist zooming out from

02:31:12.400 --> 02:31:21.920
the trucking thing? Do you think it would be harder to find activity and work that's a source of

02:31:21.920 --> 02:31:26.960
identity, a source of meaning in the future? I do think about it because you want to make sure

02:31:26.960 --> 02:31:33.120
that you worry about the entire system, not just the part of economy plays in it, but what

02:31:33.120 --> 02:31:37.840
are the ripple effects of it down the road. And on enough of a time window, there's a lot of

02:31:37.840 --> 02:31:42.720
opportunity to put in the right policies and the right opportunities to reshape and retrain and

02:31:42.720 --> 02:31:48.080
find those openings. And so just to give you a few examples, both trucking and cars, we have

02:31:48.080 --> 02:31:55.680
remote assistance facilities that are there to interface with customers and monitor vehicles

02:31:55.680 --> 02:32:02.640
and provide very focused assistance on areas where the vehicle may want to request help

02:32:02.640 --> 02:32:06.560
in understanding an environment. So those are jobs that get created and supported.

02:32:07.120 --> 02:32:11.680
I remember taking a tour of one of the Amazon facilities where you've probably seen the Kiva

02:32:11.680 --> 02:32:18.080
systems robots, where you have these orange robots that have automated the warehouse picking and

02:32:18.080 --> 02:32:23.280
collecting of items. And it's really elegant and beautiful way. It's actually one of my favorite

02:32:23.280 --> 02:32:29.440
applications of robotics of all time. I think it came across a company like 2006, it was just

02:32:29.440 --> 02:32:34.960
amazing. Like warehouse robots that transport little things. So basically, instead of a person

02:32:34.960 --> 02:32:40.560
going and walking around and picking the seven items in your order, these robots go and pick up

02:32:40.560 --> 02:32:45.680
a shelf and move it over in a row where the seven shelves that contain the seven items are lined up

02:32:45.680 --> 02:32:50.000
in a laser or whatever points to what you need to get, and you go and pick it and you place it to

02:32:50.000 --> 02:32:54.640
fill the order. And so the people were fulfilling the final orders. What was interesting about that

02:32:54.640 --> 02:32:59.200
is that when I was asking them about the impact on labor, when they transitioned that warehouse,

02:32:59.200 --> 02:33:04.160
the throughput increased so much that the jobs shifted towards the final fulfillment,

02:33:05.040 --> 02:33:10.480
even though the robots took over entirely the search of the items themselves and the labor,

02:33:11.120 --> 02:33:16.240
the job stayed. It was actually the same amount of jobs, roughly they were necessary,

02:33:16.240 --> 02:33:22.000
but the throughput increased by I think over 2X or some amount. So you have these situations

02:33:22.000 --> 02:33:25.440
that are not zero sum games in this really interesting way. And the optimist to me thinks

02:33:25.440 --> 02:33:29.760
that there's these types of solutions in almost any industry where the growth that's enabled

02:33:29.760 --> 02:33:34.080
creates opportunities that you can then leverage. But you've got to be intentional about finding

02:33:34.080 --> 02:33:40.080
those and really helping make those links because even if you make the argument that there's a net

02:33:40.080 --> 02:33:44.400
positive, locally there's always tough hits that you've got to be very careful about.

02:33:44.400 --> 02:33:49.360
That's right. You have to have an understanding of that link because there's a short period of time

02:33:50.480 --> 02:33:55.200
whether training is required or just mental transition or physical or whatever is required,

02:33:55.760 --> 02:34:00.480
that's still going to be short-term pain. The uncertainty of it, there's families involved.

02:34:03.440 --> 02:34:08.800
I mean, it's exceptionally difficult on a human level and you have to really think about that.

02:34:09.600 --> 02:34:12.880
You can't just look at economic metrics always, it's human beings.

02:34:12.880 --> 02:34:17.680
That's right. And you can't even just take it as okay, well, we need to subsidize or whatever

02:34:17.680 --> 02:34:23.840
because there is an element of just personal pride where majority of people don't want to

02:34:23.840 --> 02:34:29.040
just be okay, but they want to actually have a craft like you said and have a mission and feel

02:34:29.040 --> 02:34:34.160
like they're having a really positive impact. And so my personal belief is that there's a lot of

02:34:34.160 --> 02:34:40.960
transferability and skill set that is possible, especially if you create a bridge and an investment

02:34:41.680 --> 02:34:46.880
to enable it. And to some degree, that's our responsibility as well in this process.

02:34:48.080 --> 02:34:55.200
You mentioned Kiva robots, Amazon. Let me ask you about the Astro robot, which is,

02:34:55.200 --> 02:34:58.160
I don't know if you've seen it. It's Amazon has announced that

02:35:00.080 --> 02:35:06.080
it's a home robot that they have a screen looks awfully a lot like Cosmo has,

02:35:07.040 --> 02:35:12.320
I think different vision probably. What are your thoughts about like home robotics in this kind

02:35:12.320 --> 02:35:18.560
of space? There's been quite a bunch of home robots, social robots that very unfortunately

02:35:18.560 --> 02:35:24.080
have closed their doors that for various reasons, perhaps they were too expensive.

02:35:24.080 --> 02:35:27.600
There's been the manufacturing challenges, all that kind of stuff. What are your thoughts

02:35:27.600 --> 02:35:31.840
about Amazon getting into this space? Yeah, we had some signs that they're

02:35:31.840 --> 02:35:36.320
getting into it like long, long, long ago. Maybe they were a little bit too interested

02:35:36.320 --> 02:35:41.520
in Cosmo during our conversations, but they're also very good partners actually for us as we

02:35:41.520 --> 02:35:46.560
kind of just integrated a lot of shared technology. But if I could also get your thoughts on,

02:35:47.520 --> 02:35:55.280
you could think of Alexa as a robot as well, Echo. Do you see those as fundamentally different

02:35:55.280 --> 02:35:59.120
just because you can move and look around? Is that fundamentally different than the thing

02:35:59.120 --> 02:36:05.440
that just sits in place? It opens up options, but my first reaction is I think,

02:36:07.120 --> 02:36:10.400
I have my doubts that this one's going to hit the mark because I think for the price point that

02:36:10.400 --> 02:36:16.720
it's at and the functionality and value propositions that they're trying to put out, it's still

02:36:16.720 --> 02:36:21.840
searching for the killer application that justifies, I think it was like a $1,500 price point or

02:36:21.840 --> 02:36:27.120
kind of somewhere on there. That's a really high bar. So there's enthusiasts and early adopters

02:36:27.120 --> 02:36:31.920
will obviously kind of pursue it, but you have to really, really hit a high mark at that price

02:36:31.920 --> 02:36:36.960
point, which we were always very cautious about jumping too quickly to the more advanced systems

02:36:36.960 --> 02:36:41.840
that we really wanted to make, but would have raised the bar so much that you have to be able

02:36:41.840 --> 02:36:47.760
to hit it in today's cost structures and technologies. The mobility is an angle that

02:36:47.760 --> 02:36:53.360
hasn't been utilized, but it has to be utilized in the right way. And so that's going to be the

02:36:53.360 --> 02:37:02.560
biggest challenges. Can you meet the bar of what the mass market consumer, think our neighbors,

02:37:02.560 --> 02:37:09.840
our friends, parents, would they find a deep, deep value in this at a mass scale that justifies

02:37:09.840 --> 02:37:12.960
the price point? I think that's in the end, one of the biggest challenges for robotics,

02:37:12.960 --> 02:37:18.880
especially consumer robotics, where you have to kind of meet that bar, it becomes very, very hard.

02:37:19.840 --> 02:37:23.520
And there's also the higher bar, just like you were saying with Cosmo, of

02:37:24.960 --> 02:37:29.040
a thing that can look one way and then turn around and look at you.

02:37:30.720 --> 02:37:36.640
That's either a super desirable quality or super undesirable quality, depending on how much you

02:37:36.640 --> 02:37:42.000
trust the thing. And so there's a problem of trust to solve there. There's a problem of

02:37:42.000 --> 02:37:48.400
personality. It's the quote unquote problem that Cosmo solved so well, is that you trust the thing.

02:37:48.960 --> 02:37:53.280
And that has to do with the company, with the leadership, with the intent that's communicated

02:37:53.280 --> 02:37:59.520
by the device and the company and everything together. Yeah, exactly right. And I think they

02:37:59.520 --> 02:38:04.480
also have to retrace some of the warnings on the character side where, as usual, I think that's the

02:38:04.480 --> 02:38:09.680
place where a lot of companies are great at the hardware side of it and can think about those

02:38:09.680 --> 02:38:13.200
elements. And then there's like the thinking about the AI challenges, particularly with the

02:38:13.200 --> 02:38:18.240
advantage of Alexa is a pretty huge boost for them. The character side of it for technology

02:38:18.240 --> 02:38:24.400
companies is pretty novel territory and so that will take some iterations. But yeah, I mean,

02:38:25.440 --> 02:38:29.920
I hope there's continued progress in the space and that thread doesn't go dormant for too long

02:38:30.560 --> 02:38:39.440
and it's going to take a while to evolve into the ideal applications. But this is one of Amazon's,

02:38:39.440 --> 02:38:44.560
I guess you could call it, it's definitely part of their DNA, but in many cases, it's also strength

02:38:44.560 --> 02:38:49.680
where they're very willing to iterate aggressively and move quickly.

02:38:50.400 --> 02:38:55.440
Take risks. You have deep pockets so you can... Yeah. And then maybe have more misfires than an

02:38:55.440 --> 02:39:02.080
Apple would, but it's different styles and different approaches. And at the end of the day,

02:39:02.080 --> 02:39:06.960
it's like there's a few familiar kind of elements there for sure, which was kind of...

02:39:07.840 --> 02:39:13.600
Homage. There's one way to put it. So why is it so hard

02:39:15.840 --> 02:39:22.960
at a high level to build a robotics company, a robotics company that lives for a long time?

02:39:25.280 --> 02:39:31.040
I thought Cosmo for sure would live for a very long time. That to me was exceptionally successful

02:39:31.040 --> 02:39:37.520
vision and idea and implementation. iRobot is an example of a company that has

02:39:38.720 --> 02:39:45.600
pivoted in all the right ways to survive and arguably thrive by focusing on the...

02:39:48.320 --> 02:39:53.840
Have a driver that constantly provides profit, which is the vacuum cleaner. And of course,

02:39:53.840 --> 02:39:58.960
there's like Amazon, what they're doing is they're almost like taking risks so they can afford it

02:39:58.960 --> 02:40:06.960
because they have other sources of revenue. But outside of those examples, most robotics companies

02:40:06.960 --> 02:40:13.920
fail. Why do they fail? Why is it so hard to run a robotics company? iRobot's impressive because

02:40:13.920 --> 02:40:20.000
they found a really, really great fit of where the technology could satisfy a really clear use

02:40:20.000 --> 02:40:27.120
case and need. And they did it well and they didn't try to overshoot from a cost to benefit

02:40:27.120 --> 02:40:33.360
standpoint. Robotics is hard because it tends to be more expensive. It combines way more technologies

02:40:33.360 --> 02:40:38.880
than a lot of other types of companies do. If I were to say one thing that is maybe the biggest

02:40:38.880 --> 02:40:45.680
risk in a robotics company failing is that it can be either a technology in search of application

02:40:46.240 --> 02:40:55.200
or they try to bite off a kind of an offering that has a mismatch in kind of price to function.

02:40:56.000 --> 02:41:02.480
And just the mass market appeal isn't there. And consumer products are just hard. It's just,

02:41:02.480 --> 02:41:07.680
I mean, after all the years in it, definitely kind of feel a lot of the battle scars because

02:41:07.680 --> 02:41:12.320
you have, not only do you have to hit the function, but you have to educate and explain,

02:41:12.320 --> 02:41:18.320
get awareness up, deal with different conductive consumers. There's a reason why a lot of

02:41:18.320 --> 02:41:21.760
technology sometimes start in the enterprise space and then kind of continue forward in the

02:41:21.760 --> 02:41:27.760
consumer space. Even like, you see AR starting to kind of make that shift with HoloLens and so

02:41:27.760 --> 02:41:33.280
forth in some ways. Consumers and price points that they're willing to kind of be attracted

02:41:33.280 --> 02:41:37.440
in a mass market way. And I don't mean like 10,000 enthusiasts bought it, but I mean like

02:41:38.640 --> 02:41:45.280
2 million, 10 million, 50 million mass market kind of interest have bought it.

02:41:46.400 --> 02:41:51.200
That bar is very, very high. And typically robotics is novel enough and non-standardized

02:41:51.200 --> 02:41:55.760
enough to where it pushes on price points so much that you can easily get out of range where

02:41:55.760 --> 02:41:59.680
the capabilities and today's technology or just a function that was picked just doesn't line up.

02:42:00.560 --> 02:42:06.560
And so that product market fit is very important. So the space of killer apps or rather super

02:42:06.560 --> 02:42:12.080
compelling apps is much smaller because it's easy to get outside of the price range. Yeah.

02:42:12.080 --> 02:42:18.720
And it's not constant, right? That's why we picked off entertainment because the quality

02:42:18.720 --> 02:42:24.080
was just so low in physical entertainment that we felt we could leapfrog that and still create

02:42:24.080 --> 02:42:28.480
a really compelling offering at a price point that was defensible. And like that proved out to be

02:42:28.480 --> 02:42:37.360
true. And over time, that same opportunity opens up in healthcare, in home applications, in commercial

02:42:38.080 --> 02:42:43.520
applications, in kind of broader, more generalized interface. But there's missing pieces in order

02:42:43.520 --> 02:42:47.840
for that to happen. And all of those have to be present for it to line up. And we see these sort

02:42:47.840 --> 02:42:54.640
of trends in technology where kind of technologies that start in one place evolve and kind of grow

02:42:54.640 --> 02:43:01.360
to another. Some things start in gaming, some things start in space or aerospace and then kind

02:43:01.360 --> 02:43:06.160
of move into the consumer market. And sometimes it's just a timing thing, right? Where how many

02:43:06.960 --> 02:43:12.320
stabs at what became the iPhone were there over the 20 years before that just weren't quite ready

02:43:12.400 --> 02:43:16.720
in the function relative to the kind of price point and complexity.

02:43:16.720 --> 02:43:20.560
And sometimes it's a small detail of the implementation that makes all the difference,

02:43:20.560 --> 02:43:26.800
which is design is so important. Something, yeah, like the new generation UX, right?

02:43:26.800 --> 02:43:33.760
Yeah. And it's tough and oftentimes all of them have to be there and it has to be like a perfect

02:43:33.760 --> 02:43:38.480
storm. But yeah, history repeats itself in a lot of ways in a lot of these trends,

02:43:38.480 --> 02:43:41.760
which is pretty fascinating. Well, let me ask you about the humanoid form.

02:43:41.760 --> 02:43:45.040
What do you think about the Tesla bot and humanoid robotics in general?

02:43:46.240 --> 02:43:51.360
So obviously to me, autonomous driving Waymo and the other companies working in the space,

02:43:52.000 --> 02:43:57.120
that seems to be a great place to invest in potential revolutionary application of

02:43:57.120 --> 02:44:03.200
robotics application, folks application. What's the role of humanoid robotics? Do you think

02:44:03.760 --> 02:44:09.040
Tesla bot is ridiculous? Do you think it's super promising? Do you think it's interesting,

02:44:09.040 --> 02:44:12.000
full of mystery? Nobody knows. What do you think about this thing?

02:44:12.000 --> 02:44:17.440
Yeah. I think today humanoid form robotics is research. There's very few situations

02:44:17.440 --> 02:44:21.120
where you actually need a humanoid form to solve a problem. If you think about it,

02:44:22.160 --> 02:44:26.800
wheels are more efficient than legs. There's joints and degrees of freedom

02:44:26.800 --> 02:44:30.560
beyond a certain point just add a lot of complexity and cost. So if you're doing

02:44:30.560 --> 02:44:34.160
a humanoid robot, oftentimes it's in the pursuit of a humanoid robot, not in the pursuit of an

02:44:34.160 --> 02:44:41.040
application for the time being, especially when you have the gaps and interface and AI that we

02:44:41.040 --> 02:44:46.160
talk about today. So anything you want does, I'm interested in following. So there's an element

02:44:46.160 --> 02:44:47.680
of that. No matter how crazy.

02:44:47.680 --> 02:44:51.280
How crazy it is, I just like, I'll pay attention and I'm curious to see what comes out of it. So

02:44:51.280 --> 02:44:57.360
it's like you can't ever ignore it, but it's definitely far afield from their core business,

02:44:58.240 --> 02:45:05.680
obviously. What was interesting to me is I've disagreed with Elon a lot about this,

02:45:07.040 --> 02:45:14.880
is to me the compelling aspect of the humanoid form and a lot of kind of robots, Cosmo for example,

02:45:15.520 --> 02:45:22.400
is the human robot interaction part. From Elon Musk's perspective,

02:45:23.120 --> 02:45:28.720
Tesla bot has nothing to do with the human. It's a form that's effective for the factory

02:45:28.720 --> 02:45:33.920
because the factory is designed for humans. But to me, the reason you might want to argue

02:45:33.920 --> 02:45:41.200
for the humanoid form is because at a party, it's a nice way to fit into the party.

02:45:41.200 --> 02:45:45.440
The humanoid form has a compelling notion to it in the same way that Cosmo is compelling.

02:45:46.160 --> 02:45:54.800
I would argue if we're arguing about this, that it's cheaper to build a Cosmo like that form.

02:45:54.800 --> 02:45:59.120
But if you wanted to make an argument, which I have with Jim Keller about, you could actually

02:45:59.120 --> 02:46:05.040
make a humanoid robot for pretty cheap. It's possible. And then the question is, all right,

02:46:05.040 --> 02:46:12.320
if you're using an application where it can be flawed, it can have a personality and be flawed

02:46:12.320 --> 02:46:17.040
in the same way that Cosmo is, then maybe it's interesting for integration into human society.

02:46:17.680 --> 02:46:22.320
That's to me is an interesting application of a humanoid form because humans are drawn,

02:46:22.320 --> 02:46:27.120
like I mentioned to you, legged robots. We're drawn to legs and limbs and body language and

02:46:27.120 --> 02:46:31.520
all that kind of stuff. And even a face, even if you don't have the facial features, which you

02:46:31.520 --> 02:46:38.880
might not want to have to reduce the creepiness factor, all that kind of stuff. But yeah, that to

02:46:38.880 --> 02:46:44.720
me, the humanoid form is compelling. But in terms of that being the right form for the factory

02:46:44.720 --> 02:46:49.360
environment, I'm not so sure. Yeah. For the factory environment, right off the bat,

02:46:49.360 --> 02:46:53.920
what are you optimizing for? Is it strength? Is it mobility? Is it versatility? That changes

02:46:53.920 --> 02:46:59.680
completely the look and feel of the robot that you create. And almost certainly the human form

02:46:59.680 --> 02:47:03.680
is overdesigned for some dimensions and constrained for some dimensions. And so

02:47:04.560 --> 02:47:08.880
like, what are you grasping? Is it big? Is it little, right? So you would customize it and

02:47:08.880 --> 02:47:14.480
make it customizable for the different needs if that was the optimization, right?

02:47:14.480 --> 02:47:20.400
And then for the other one, I could totally be wrong. I still feel that the closer you

02:47:20.400 --> 02:47:26.320
try to get to a human, the more you're subject to the biases of what a human should be,

02:47:26.320 --> 02:47:32.000
and you lose flexibility to shift away from your weaknesses and towards your strengths.

02:47:32.640 --> 02:47:43.040
And that changes over time, but there's ways to make really approachable and natural interfaces

02:47:43.040 --> 02:47:55.440
for robotic characters and deployments in these applications that do not at all look like a human

02:47:55.440 --> 02:48:01.360
directly, but that actually creates way more flexibility and capability and role and forgiveness

02:48:01.360 --> 02:48:05.360
and interface and everything else. Yeah, it's interesting, but I'm still confused

02:48:05.360 --> 02:48:10.640
by the magic I see in legged robots. Yeah. So there is a magic. So I'm

02:48:11.600 --> 02:48:18.480
absolutely amazed at it from a technical curiosity standpoint and like the magic that

02:48:18.480 --> 02:48:24.000
the Boston Dynamics team can do from walking and jumping and so forth. Now,

02:48:25.040 --> 02:48:29.600
there's been a long journey to try to find an application for that sort of technology,

02:48:29.600 --> 02:48:32.000
but wow, that's incredible technology, right? Yes.

02:48:32.000 --> 02:48:36.960
So then you go towards, okay, are you working back from a goal of what you're trying to solve

02:48:36.960 --> 02:48:40.160
or are you working forward from a technology and then looking for a solution? And I think that's

02:48:40.160 --> 02:48:46.240
where it's a bi-directional search oftentimes, but the two have to meet. And that's where

02:48:46.800 --> 02:48:52.160
humanoid robots is close to that in that it is a decision about a form factor and a

02:48:52.880 --> 02:48:58.560
technology that it forces that doesn't have a clear justification on why that's the killer

02:48:58.560 --> 02:49:02.960
app from the other end. But I think the core fascinating idea with the Tesla bot

02:49:02.960 --> 02:49:07.920
is the one that's carried by Waymo as well as when you're solving the general robotics problem

02:49:08.720 --> 02:49:13.280
of perception control where there's the very clear applications of driving.

02:49:14.960 --> 02:49:18.320
As you get better and better at it, when you have like Waymo driver,

02:49:19.760 --> 02:49:24.400
the whole world starts to kind of start to look like a robotics problem. So it's very interesting

02:49:24.400 --> 02:49:29.520
for now. Detection, costification, segmentation, tracking, planning.

02:49:30.960 --> 02:49:40.400
So there's no reason, I mean, I'm not speaking for Waymo here, but moving goods, there's no reason

02:49:40.960 --> 02:49:51.760
transformer like this thing couldn't take the goods up an elevator. Slowly expand what it

02:49:51.760 --> 02:49:58.960
means to move goods and expand more and more of the world into a robotics problem.

02:49:58.960 --> 02:50:03.280
Well, that's right. And you start to like think of it as an end-to-end robotics problem from like

02:50:03.280 --> 02:50:10.560
loading from everything. And even like the truck itself, today's generation is integrating into

02:50:11.200 --> 02:50:16.720
today's understanding of what a vehicle is. A Pacifica, Jaguar, the Freightliners from

02:50:16.720 --> 02:50:23.840
Daimler. There's nothing that stops us from down the road after starting to get to scale

02:50:23.840 --> 02:50:29.200
to expand these partnerships to really rethink what would the next generation of a truck look

02:50:29.200 --> 02:50:36.480
like that is actually optimized for autonomy, not for today's world. And maybe that means a

02:50:36.480 --> 02:50:40.320
very different type of trailer. Maybe there's a lot of things you could rethink on that front,

02:50:40.320 --> 02:50:44.960
which is on its own very, very exciting. Let me ask you, like I said, you went to the

02:50:44.960 --> 02:50:49.920
Mecca of robotics, which is CMU, Carnegie Mellon University. You got a PhD there.

02:50:51.040 --> 02:50:59.520
Maybe by way of advice and maybe by way of story and memories, what does it take to get a PhD

02:51:00.320 --> 02:51:08.560
in robotics at CMU? And maybe you can throw in there some advice for people who are thinking about

02:51:09.920 --> 02:51:14.400
doing work in artificial intelligence and robotics and are thinking about whether to get a PhD.

02:51:15.920 --> 02:51:20.800
I was at CMU for undergrad as well and didn't know anything about robotics coming in and was

02:51:20.800 --> 02:51:25.360
doing electrical computer engineering, computer science, and really got more and more into AI

02:51:25.920 --> 02:51:30.480
and then fell in love with autonomous driving. And at that point, that was just by a big margin,

02:51:30.480 --> 02:51:37.520
such an incredible central spot of investment in that area. And so what I would say is that

02:51:37.520 --> 02:51:42.240
robotics, for all the progress that's happened, is still a really young field. There's a huge

02:51:42.240 --> 02:51:46.560
amount of opportunity. Now, that opportunity shifted where something like autonomous driving

02:51:46.560 --> 02:51:51.360
has moved from being very research and academics driven to being commercial driven, where you see

02:51:51.360 --> 02:51:55.120
the investments happening in commercial. Now, there's other areas that are much younger

02:51:55.920 --> 02:52:01.120
and you see kind of grasping at manipulation, making kind of the same sort of journey that

02:52:01.120 --> 02:52:06.320
autonomy made, and there's other areas as well. What I would say is the space moves very quickly.

02:52:06.880 --> 02:52:12.320
Anything you do a PhD in, like it is in most areas, will evolve and change as technology changes and

02:52:12.320 --> 02:52:17.600
constraints change and hardware changes and the world changes. And so the beautiful thing about

02:52:17.600 --> 02:52:22.160
robotics is it's super broad. It's not a narrow space at all, and it can be a million different

02:52:22.160 --> 02:52:28.160
things in a million different industries. And so it's a great opportunity to come in and get a

02:52:28.160 --> 02:52:33.040
broad foundation on AI, machine learning, computer vision, systems, hardware, sensors,

02:52:33.040 --> 02:52:38.640
all these separate things. You do need to go deep and find something that you're really,

02:52:38.640 --> 02:52:46.000
really passionate about. Obviously, just like any PhD, this is like a five, six year kind of

02:52:46.000 --> 02:52:52.640
endeavor, and you have to love it enough to go super deep to learn all the things necessary to

02:52:52.640 --> 02:52:56.560
be super deeply functioning in that area and then contribute to it in a way that hasn't been done

02:52:56.560 --> 02:53:03.120
before. And in robotics, that probably means more breadth because robotics is rarely kind of like

02:53:03.120 --> 02:53:08.240
one particular kind of narrow technology. And it means being able to collaborate with teams where

02:53:08.240 --> 02:53:14.080
one of the coolest aspects of the experience that I kind of cherish in our PhD is that we actually

02:53:14.080 --> 02:53:20.640
had a pretty large AV project that for that time was a pretty serious initiative where

02:53:20.640 --> 02:53:25.520
you got to partner with a larger team and you had the experts in perception and the experts in

02:53:25.520 --> 02:53:30.880
planning and the staff and the mechanical engineers. So I was working on a project called

02:53:30.880 --> 02:53:35.440
UPI back then, which was basically the off-road version of the DARPA challenge. It was a DARPA

02:53:35.440 --> 02:53:40.880
funded project for basically like a large off-road vehicle that you would drop and then give it a

02:53:40.880 --> 02:53:44.000
waypoint 10 kilometers away and it would have to navigate a completely unstructured environment.

02:53:44.000 --> 02:53:45.200
In an off-road environment.

02:53:45.200 --> 02:53:49.440
Yeah. So like forests, ditches, rocks, vegetation. And so it was like a really,

02:53:49.440 --> 02:53:52.800
really interesting kind of a hard problem where like wheels would be up to my shoulders. It's

02:53:52.800 --> 02:53:53.840
like gigantic, right?

02:53:53.840 --> 02:53:56.560
Yeah. By the way, AV for people stands for autonomous vehicles.

02:53:56.560 --> 02:54:01.360
Autonomous vehicles. Yeah. Sorry. And so what I think is like the beauty of robotics,

02:54:01.360 --> 02:54:06.640
but also kind of like the expectation is that there's spaces in computer science where you

02:54:06.640 --> 02:54:12.160
can be very, very narrow and deep. Robotics, the necessity, but also the beauty of it is

02:54:12.160 --> 02:54:16.720
that it forces you to be excited about that breadth and that partnership across different

02:54:16.720 --> 02:54:21.680
disciplines that enable it. But that also opens up so many more doors where you can go and you can

02:54:21.680 --> 02:54:28.960
do robotics in almost any category where robotics isn't really an industry. It's like AI, right?

02:54:28.960 --> 02:54:34.320
It's like the application of physical automation to all these other worlds. And so you can do

02:54:34.320 --> 02:54:38.880
robotic surgery, you can do vehicles, you can do factory automation, you can do

02:54:41.760 --> 02:54:47.200
leverage the AI around the sensing to think about static sensors and scene understanding.

02:54:47.200 --> 02:54:51.680
So I think that's got to be the expectation and the excitement and it

02:54:52.640 --> 02:54:56.160
breeds people that are probably a little bit more collaborative and more excited about

02:54:57.040 --> 02:54:58.400
working in teams.

02:54:58.400 --> 02:55:04.160
If I could briefly comment on the fact that the robotics people I've met in my life

02:55:05.120 --> 02:55:09.520
from CMU and MIT, they're really happy people.

02:55:10.320 --> 02:55:12.320
Because I think it's the collaborative thing.

02:55:13.120 --> 02:55:19.040
I think you're not like sitting in the fourth basement.

02:55:19.040 --> 02:55:23.520
Yes, exactly. Which when you're doing machine learning purely software,

02:55:23.520 --> 02:55:31.120
it's very tempting to just disappear into your own hole and never collaborate. And that breeds

02:55:31.760 --> 02:55:37.360
a little bit more of the silo mentality of like, I have a problem. It's almost like negative to

02:55:37.360 --> 02:55:42.000
talk to somebody else or something like that. But robotics folks are just very collaborative,

02:55:42.000 --> 02:55:48.720
very friendly. And there's also an energy of like, you get to confront the physics of reality

02:55:48.720 --> 02:55:56.080
often, which is humbling and also exciting. So it's humbling when it fails and exciting

02:55:56.080 --> 02:55:56.720
when it finally works.

02:55:56.720 --> 02:55:59.600
It's like a purity of the passion. You got to remember that like right now,

02:55:59.600 --> 02:56:05.040
like robotics and AI is like just all the rage and autonomous vehicles and all this.

02:56:05.680 --> 02:56:12.320
15 years ago and 20 years ago, it wasn't that deeply lucrative. People that went into robotics,

02:56:12.320 --> 02:56:16.480
they did it because they thought it was just the coolest thing in the world to make physical

02:56:16.480 --> 02:56:21.040
things intelligent in the real world. And so there's like a raw passion where they went into

02:56:21.040 --> 02:56:25.120
it for the right reasons and so forth. And so it's really great space. And that organizational

02:56:25.120 --> 02:56:29.760
challenge, by the way, when you think about the challenges in AV, we talk a lot about the technical

02:56:29.760 --> 02:56:37.280
challenges, the organizational challenges through the roof where you think about what it takes to

02:56:37.280 --> 02:56:41.520
build an AV system and you have companies that are now thousands of people.

02:56:42.160 --> 02:56:46.960
And you look at other really hard technical problems like an operating system,

02:56:46.960 --> 02:56:51.920
it's pretty well established. You kind of know that there's a file system, there's virtual

02:56:51.920 --> 02:56:57.920
memory, there's this, there's that, there's like caching and there's like a really reasonably

02:56:57.920 --> 02:57:01.920
well established modularity and APIs and so forth. And so you can kind of like scale it

02:57:01.920 --> 02:57:06.720
in an efficient fashion. That doesn't exist anywhere near to that level of maturity in

02:57:06.720 --> 02:57:11.840
autonomous driving right now. And tech stacks are being reinvented, organizational structures

02:57:11.840 --> 02:57:15.520
are being reinvented. You have problems like pedestrians that are not isolated problems.

02:57:15.520 --> 02:57:21.760
They're part sensing, part behavior prediction, part planning, part evaluation. And like one of

02:57:21.760 --> 02:57:26.160
the biggest challenges is actually how do you solve these problems where the mental capacity

02:57:26.160 --> 02:57:32.400
of a human is starting to get strained on how do you organize it and think about it, where you

02:57:32.400 --> 02:57:38.880
have this multi-dimensional matrix that needs to all work together. And so that makes it kind of

02:57:38.880 --> 02:57:45.840
cool as well because it's not solved at all from what does it take to actually scale this, right?

02:57:45.840 --> 02:57:51.680
And then you look at other gigantic challenges that have been successful and are way more mature,

02:57:52.240 --> 02:57:56.480
there's a stability to it. And like maybe the autonomous vehicle space will get there,

02:57:56.480 --> 02:58:01.280
but right now just as many technical challenges as they are, they're like organizational challenges.

02:58:01.280 --> 02:58:06.080
And how do you like solve these problems that touch on so many different areas and efficiently

02:58:06.080 --> 02:58:12.800
tackle them while like maintaining progress among all these constraints while scaling?

02:58:13.520 --> 02:58:20.320
By way of advice, what advice would you give to somebody thinking about

02:58:20.320 --> 02:58:25.680
doing a robotics startup? You mentioned Cosmo, somebody that wanted to carry the Cosmo flag

02:58:25.680 --> 02:58:33.360
forward, the Anki flag forward, looking back at your experience, looking forward at a future that

02:58:33.360 --> 02:58:37.280
will obviously have such robots. What advice would you give to that person?

02:58:37.280 --> 02:58:42.160
Yeah. It was the greatest experience ever. And it's like there are things you learn

02:58:42.160 --> 02:58:47.680
navigating a startup that you'll never like, it was very hard to encounter that in like a typical

02:58:47.680 --> 02:58:52.160
kind of work environment. And it's wonderful. You got to be ready for it. It's not as good,

02:58:52.160 --> 02:58:57.440
like the glamour of a startup, there's just like just brutal emotional swings up and down. And so

02:58:58.320 --> 02:59:03.040
having co-founders actually helps a ton. I could not imagine doing it solo, but having

02:59:03.040 --> 02:59:08.000
at least somebody where on your darkest days, you can kind of like really openly just like

02:59:08.000 --> 02:59:10.880
have that conversation and, you know, wean onto somebody that's

02:59:11.680 --> 02:59:14.960
in the thick of it with you helps a lot. What I would say-

02:59:14.960 --> 02:59:20.800
What was the nature of darkest days and the emotional swings? Is it worried about the funding?

02:59:20.800 --> 02:59:26.880
Is it worried about whether any of your ideas are any good or ever were good? Is it like the self

02:59:26.880 --> 02:59:32.640
doubt? Is it like facing new challenges that have nothing to do with the technology, like

02:59:33.520 --> 02:59:36.560
organizational, human resources, that kind of stuff? What-

02:59:36.560 --> 02:59:42.240
Yeah, you come from a world in school where you feel that you put in a lot of effort and you'll

02:59:42.240 --> 02:59:47.600
get the right result and input translates proportional to output. And, you know, you

02:59:47.600 --> 02:59:52.000
need to solve the set or do whatever and just kind of get it done. Now, PhD tests out a little bit,

02:59:52.000 --> 02:59:56.240
but at the end of the day, you put in the effort, you tend to like kind of come out with enough

02:59:56.240 --> 03:00:03.200
results to you kind of get a PhD. In the startup space, like, you know, like you could talk to 50

03:00:03.200 --> 03:00:06.720
investors and they just don't see your vision and it doesn't matter how hard you kind of tried and

03:00:06.720 --> 03:00:11.280
pitched. You could work incredibly hard and you have a manufacturing defect and if you don't fix

03:00:11.280 --> 03:00:17.680
it, you're out of business. You need to raise money by a certain date and you got to have

03:00:17.680 --> 03:00:21.920
this milestone in order to like have a good pitch and you do it. You have to have this talent and

03:00:21.920 --> 03:00:28.800
you just don't have it inside the company. Or, you know, you have to get 200 people or however

03:00:28.800 --> 03:00:34.000
many people kind of like along with you and kind of buy in the journey. You're like disagreeing

03:00:34.000 --> 03:00:37.440
with an investor and they're your investor. So it's just like, you know, it's like there's no

03:00:37.440 --> 03:00:42.560
walking away from it, right? And it tends to be like those things where you just kind of get

03:00:42.560 --> 03:00:47.280
clobbered in so many different ways that like things end up being harder than you expect.

03:00:47.280 --> 03:00:52.080
And it's like such a gauntlet, but you learn so much in the process and there's a lot of people

03:00:52.080 --> 03:00:56.320
that actually end up rooting for you and helping you like from the outside and you get great

03:00:56.320 --> 03:01:01.200
mentors and you like get find fantastic people that step up in the company and you have this

03:01:01.200 --> 03:01:06.480
like magical period where everybody's like it's life or death for the company, but like you're

03:01:06.480 --> 03:01:11.120
all fighting for the same thing and it's the most satisfying kind of journey ever. The things that

03:01:11.120 --> 03:01:16.720
make it easier and that I would recommend is like be really, really thoughtful about the application.

03:01:17.600 --> 03:01:22.320
There's a saying of like kind of team and execution and market and like kind of how

03:01:22.320 --> 03:01:27.760
important are each of those. And oftentimes the market wins and you come out at thinking

03:01:27.760 --> 03:01:31.600
that if you're smart enough and you work hard enough and you're like have the right talented

03:01:31.600 --> 03:01:36.720
team and so forth, like you'll always kind of find a way through. And it's surprising how

03:01:36.720 --> 03:01:40.720
much dynamics are driven by the industry you're in and the timing of you entering that industry.

03:01:41.440 --> 03:01:46.640
And so just Waymo is a great example of it. I don't know if there'll ever be another

03:01:46.640 --> 03:01:55.040
company or suite of companies that has raised and continues to spend so much money at such an early

03:01:55.840 --> 03:02:05.840
phase of revenue generation and productization from a P&L standpoint, like it's an anomaly,

03:02:05.840 --> 03:02:10.720
like by any measure of any industry that's ever existed, except for maybe the US space program.

03:02:11.600 --> 03:02:19.040
Right? But it's like multiple trillion dollar opportunities, which is so unusual to find that

03:02:19.040 --> 03:02:24.560
size of a market that just the progress that shows the de-risking of it, you could apply

03:02:24.560 --> 03:02:28.160
whatever discounts you want off of that trillion dollar market and it still justifies the investment

03:02:28.160 --> 03:02:32.880
that is happening because like being successful in that space makes all the investment feel

03:02:32.880 --> 03:02:38.480
trivial. Now by the same consequence, like the size of the market, the size of the target audience,

03:02:38.480 --> 03:02:42.480
the ability to capture that market share, how hard that's going to be with the incumbents,

03:02:43.040 --> 03:02:47.200
that's probably one of the lessons I appreciate more than anything else where those things really,

03:02:47.200 --> 03:02:54.000
really do matter. And oftentimes can dominate the quality of the team or execution because if you

03:02:54.640 --> 03:02:59.360
miss the timing or you do it in the wrong space or you run into the institutional headwinds

03:02:59.360 --> 03:03:02.640
of a particular environment, let's say you have the greatest idea in the world,

03:03:02.640 --> 03:03:06.320
but you barrel into healthcare, but it takes 10 years to innovate in healthcare because of

03:03:06.320 --> 03:03:10.960
a lot of challenges, right? Like there's fundamental laws of physics that you have

03:03:10.960 --> 03:03:16.560
to think about. And so the combination of like Anki and Waymo kind of drives that point home for

03:03:16.560 --> 03:03:21.120
me where you can do a ton if you have the right market, the right opportunity, the right way to

03:03:21.120 --> 03:03:27.520
explain it, and you show the progress in the right sequence, it actually can really significantly

03:03:27.520 --> 03:03:32.160
change the course of your journey and startup. How much of is understanding the market and how

03:03:32.160 --> 03:03:38.240
much of is creating a new market? How do you think about, like the space of robotics is

03:03:38.240 --> 03:03:42.640
really interesting. You said exactly right. The space of applications is small.

03:03:45.360 --> 03:03:50.880
Relative to the cost involved. So how much is truly revolutionary thinking

03:03:51.760 --> 03:03:58.320
about what is the application? And then, yeah, so creating something that-

03:03:59.280 --> 03:04:00.240
Didn't exist.

03:04:00.240 --> 03:04:04.560
Didn't really exist. This is pretty obvious to me. The whole space of home robotics,

03:04:05.680 --> 03:04:09.360
everything that Cosmo did, I guess you could talk to it as a toy and people

03:04:09.360 --> 03:04:16.480
will understand it, but Cosmo is much more than a toy. And I don't think people fully understand

03:04:16.480 --> 03:04:22.160
the value of that. You have to create it and the product will communicate it. Just like the iPhone,

03:04:22.800 --> 03:04:31.280
nobody understood the value of no keyboard and a thing that can do web browsing. I don't think

03:04:31.280 --> 03:04:36.320
they understood the value of that until you create it. Yeah. Having a foot in the door and an entry

03:04:36.320 --> 03:04:40.720
point still helps because at the end of the day, an iPhone replaced your phone. And so it had a

03:04:40.720 --> 03:04:45.280
fundamental purpose and it has all these things that it did better. And so then you could do ABC

03:04:45.280 --> 03:04:50.640
on top of it. And then you even remember the early commercials where there's always one application

03:04:50.640 --> 03:04:54.880
of what it could do and then you get a phone call. And so that was intentionally sending a

03:04:54.880 --> 03:04:58.480
message, something familiar, but then you can send a text message, you can listen to music,

03:04:58.480 --> 03:05:03.680
you can surf the web. And so autonomous driving obviously anchors on that as well.

03:05:03.680 --> 03:05:07.440
You don't have to explain to somebody the functionality of an autonomous truck. There's

03:05:07.440 --> 03:05:13.040
nuances around it, but the functionality makes sense. In the home, you have a fundamental

03:05:13.040 --> 03:05:16.960
advantage. We always thought about this because it was so painful to explain to people what our

03:05:16.960 --> 03:05:21.440
products did and how I communicate that super cleanly, especially when something was so

03:05:21.440 --> 03:05:32.480
experiential. And so you compare Anki to Nest. Nest had some beautiful products where they

03:05:33.040 --> 03:05:37.840
started scaling and actually found really great success. And they had really clean and beautiful

03:05:37.840 --> 03:05:42.640
marketing messaging because they anchored on reinventing existing categories where it was a

03:05:42.640 --> 03:05:51.040
smart thermostat. And so you are able to take what's familiar, anchor that understanding,

03:05:51.040 --> 03:05:55.760
and then explain what's better about it. That's funny. You're right. Cosmo is a totally new

03:05:55.760 --> 03:06:03.360
thing. What is this thing? We struggle. We spent a lot of money on marketing. We actually had

03:06:03.360 --> 03:06:08.240
far greater efficiency on Cosmo than anything else because we found a way to capture the emotion in

03:06:08.240 --> 03:06:13.760
some little shorts to lean into the personality in our marketing. And it became viral where we had

03:06:13.760 --> 03:06:19.840
these videos that would go and get hundreds of thousands of views and get spread and sometimes

03:06:19.840 --> 03:06:27.200
millions of views. But it was really, really hard. And so finding a way to anchor on something that's

03:06:27.200 --> 03:06:33.120
familiar but then grow into something that's not is an advantage. But then again, you don't have

03:06:33.120 --> 03:06:39.040
their successes otherwise. Alexa never had a comp. You could argue that that's very novel

03:06:39.040 --> 03:06:46.400
and very new. And there's a lot of other examples that created a category out of Kiva systems.

03:06:49.760 --> 03:06:55.360
Enterprise is a little easier because it's less susceptible to this because if you can argue a

03:06:55.360 --> 03:07:00.400
clear value proposition, it's a more logical conversation that you can have with customers.

03:07:01.120 --> 03:07:06.080
It's a little bit less emotional and subjective. Yeah, in the home, you have to...

03:07:06.880 --> 03:07:10.720
Yeah. It's like a home robot. It's like, what does that mean? And so then you really have to be

03:07:10.720 --> 03:07:16.240
crisp about the value proposition and what really makes it worth it. And we, by the way,

03:07:16.240 --> 03:07:23.200
went through that same order. We almost hit a wall coming out of 2013 where we were so big on

03:07:23.200 --> 03:07:28.160
explaining why our stuff was so high tech and all the great technology in it and how cool it is and

03:07:28.160 --> 03:07:34.880
so forth to having to make a super hard pivot on why is it fun and why does the random kind of

03:07:34.880 --> 03:07:42.480
family of four need this, right? So it's learnings, but that's the challenge. And I think robotics

03:07:42.480 --> 03:07:47.200
tends to sometimes fall into the new category problem, but then you got to be really crisp

03:07:47.200 --> 03:07:54.480
about why it needs to exist. Well, I think some of robotics, depending on the application,

03:07:55.440 --> 03:08:04.720
is a little bit of a marketing challenge. And I don't mean... It's the kind of marketing that

03:08:04.720 --> 03:08:13.040
Waymo is doing, that Tesla is doing, is showing off incredible engineering, incredible technology,

03:08:13.520 --> 03:08:19.760
but convincing, like you said, a family of four that this is transformative for your life,

03:08:20.720 --> 03:08:23.360
this is fun, this is easy. They don't care how much tech is in your thing.

03:08:24.880 --> 03:08:27.760
They need to know why they want it. And some of that is just marketing.

03:08:27.760 --> 03:08:37.920
Yeah. And that's not like Roomba. Yes, they didn't go and have this huge, huge ramp into the entirety

03:08:37.920 --> 03:08:43.280
of AI robotics and so forth, but they built a really great business in the vacuum cleaner world

03:08:43.280 --> 03:08:48.880
and everybody understands where a vacuum cleaner is. Most people are annoyed by doing it. And now

03:08:48.880 --> 03:08:55.040
you have one that does it itself in various degrees of quality, but that is so compelling

03:08:55.040 --> 03:09:01.680
that it's easy to understand. And they had a very... And I think they have 15% of the vacuum

03:09:01.680 --> 03:09:07.040
cleaner market, so it's pretty successful. I think we need more of those types of thoughtful

03:09:07.040 --> 03:09:11.600
stepping stones in robotics, but the opportunities are becoming bigger because hardware is cheaper,

03:09:11.600 --> 03:09:16.160
compute is cheaper, cloud is cheaper, and AI is better. So there's a lot of opportunity.

03:09:16.240 --> 03:09:23.120
If we zoom out from specifically startups and robotics, what advice do you have to high school

03:09:23.120 --> 03:09:30.080
students, college students about career and living a life that you'd be proud of? You lived

03:09:30.080 --> 03:09:36.480
one heck of a life. You're very successful in several domains. If you can convert that

03:09:36.480 --> 03:09:41.440
into a generalizable potion, what advice would you give? Yeah, that's a very good question.

03:09:41.760 --> 03:09:50.080
So it's very hard to go into a space that you're not passionate about and push hard enough to

03:09:52.480 --> 03:09:59.600
maximize your potential in it. And so there's always the saying of like, okay, follow your

03:09:59.600 --> 03:10:04.400
passion. Great. Try to find the overlap of where your passion overlaps with a growing

03:10:04.400 --> 03:10:08.400
opportunity and need in the world, where it's not too different than the startup argument that we

03:10:08.400 --> 03:10:18.480
talked about. Where your passion meets the market. That's a beautiful thing where you can do what

03:10:18.480 --> 03:10:21.760
you love, but it also just opens up tons of opportunities because the world is ready for

03:10:21.760 --> 03:10:28.960
it. And so if you're interested in technology, that might point to go and study machine learning

03:10:28.960 --> 03:10:31.680
because you don't have to decide what career you're going to go into, but it's going to be

03:10:31.680 --> 03:10:35.760
such a versatile space that's going to be at the root of everything that's going to be in front of

03:10:35.760 --> 03:10:42.400
us that you can have eight different careers in different industries and be an absolute expert in

03:10:42.400 --> 03:10:48.480
this tool set that you wield that can go and be applied. And by the way, that doesn't apply to

03:10:48.480 --> 03:10:54.160
just technology, right? It could be the exact same thing if you want the same thought process

03:10:54.160 --> 03:11:00.400
of price to design, to marketing, to sales, to anything, but that versatility where you like,

03:11:00.400 --> 03:11:06.880
when you're in a space that's going to continue to grow, it's just like what company do you join?

03:11:06.880 --> 03:11:11.120
One that just is going to grow and the growth creates opportunities where the surface area is

03:11:11.120 --> 03:11:18.080
just going to increase and the problems will never get stale. And so you go into a career where you

03:11:18.080 --> 03:11:24.960
have that sort of growth in the world that you're in, you end up having so much more opportunity

03:11:24.960 --> 03:11:30.480
that organically just appears and you can then have more shots on goal to find that killer

03:11:30.480 --> 03:11:35.360
overlap of timing and passion and skill set and point in life where you can just really be

03:11:35.360 --> 03:11:40.480
motivated and fall in love with something. And then at the same time, find a balance. There's

03:11:40.480 --> 03:11:47.680
been times in my life where I worked a little bit too obsessively and crazy and I think we tried to

03:11:47.680 --> 03:11:53.360
correct the right opportunities, but I think I probably appreciate a lot more now friendships

03:11:53.360 --> 03:12:01.520
to go way back, family and things like that. And I have the personality where I have so much

03:12:01.520 --> 03:12:06.000
desire to really try to optimize. When I'm working on that, I can easily go to an extreme

03:12:06.000 --> 03:12:11.840
and now I'm trying to find that balance and make sure that I have the friendships, the family,

03:12:11.840 --> 03:12:17.360
relationship with the kids, everything that I push really, really hard, but find a balance.

03:12:18.320 --> 03:12:25.280
And I think people can be happy on actually many kind of extremes on that spectrum, but it's easy

03:12:25.280 --> 03:12:31.760
to kind of inadvertently make a choice by how you approach it that then becomes really hard to

03:12:31.760 --> 03:12:37.760
unwind. And so being very thoughtful about kind of all of those dimensions makes a lot of sense.

03:12:37.760 --> 03:12:41.840
And so those are all interrelated, but at the end of the day-

03:12:42.400 --> 03:12:46.160
Passion and love. Love towards, you said- Family, friends.

03:12:46.160 --> 03:12:54.160
Family. And hopefully one day, if your work pans out, Boris, is love towards robots.

03:12:56.240 --> 03:13:02.160
Not the creepy kind, the good kind. Not the good kind. Just friendship and fun.

03:13:02.960 --> 03:13:06.080
It's like another dimension to just how we interface with the world.

03:13:06.080 --> 03:13:11.520
Yeah. Boris, you're one of my favorite human beings, roboticist. You've created some incredible

03:13:11.600 --> 03:13:18.800
robots and I think inspired countless people. And like I said, I hope Cosmo, I hope your work

03:13:18.800 --> 03:13:26.560
with Anki lives on. And I can't wait to see what you do with Waymo. I mean, if we're talking about

03:13:26.560 --> 03:13:32.320
artificial intelligence technology, there's the potential to revolutionize so much of our world.

03:13:32.880 --> 03:13:37.600
That's it right there. So thank you so much for the work you've done and thank you for spending

03:13:37.600 --> 03:13:39.840
your valuable time talking with me. Thanks, Alex.

03:13:40.800 --> 03:13:45.040
Thanks for listening to this conversation with Boris Hoffman. To support this podcast,

03:13:45.040 --> 03:13:50.000
please check out our sponsors in the description. And now let me leave you with some words from

03:13:50.000 --> 03:13:57.680
Isaac Asimov. If you were to insist I was a robot, you might not consider me capable of love

03:13:58.240 --> 03:14:04.800
in some mystic human sense. Thank you for listening and hope to see you next time.

03:14:09.840 --> 03:14:10.160
you


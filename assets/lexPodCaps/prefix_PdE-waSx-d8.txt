WEBVTT

00:00.000 --> 00:05.040
you know, I can tell chat GPT create a piece of code and then just run it on my computer.

00:05.040 --> 00:09.760
And I'm like, you know, that that sort of personalizes for me, the what could what could

00:09.760 --> 00:13.760
possibly go wrong, so to speak. Was that exciting or scary, that possibility?

00:14.400 --> 00:18.320
It was a little bit scary, actually, because it's kind of like, if you do that, right,

00:18.320 --> 00:23.280
what is the sandboxing that you should have? And that's sort of a, that's a version of that

00:23.280 --> 00:28.880
question for the world. That is, as soon as you put the AIs in charge of things, you know, how much,

00:28.880 --> 00:33.120
how many constraints should there be on these systems before you put the AIs in

00:33.120 --> 00:36.480
charge of all the weapons and all these, you know, all these different kinds of systems?

00:36.480 --> 00:44.240
Well, here's the fun part about sandboxes is the AI knows about them and has the tools to crack them.

00:46.640 --> 00:51.200
The following is a conversation with Stephen Wolfram, his fourth time on this podcast.

00:51.200 --> 00:56.000
He's a computer scientist, mathematician, theoretical physicist, and the founder of

00:56.000 --> 01:01.360
Wolfram Research, a company behind Mathematica, Wolfram Alpha, Wolfram Language,

01:01.360 --> 01:08.000
and the Wolfram Physics and Metamathematics projects. He has been a pioneer in exploring

01:08.000 --> 01:14.400
the computational nature of reality. And so he's the perfect person to explore with together the

01:14.400 --> 01:20.000
new quickly evolving landscape of large language models as human civilization journeys towards

01:20.000 --> 01:26.560
building superintelligent AGI. This is the Lex Friedman podcast. To support it,

01:26.560 --> 01:32.160
please check out our sponsors in the description. And now, dear friends, here's Stephen Wolfram.

01:33.280 --> 01:38.720
You've announced the integration of Chad GPT and Wolfram Alpha and Wolfram Language.

01:38.720 --> 01:44.240
So let's talk about that integration. What are the key differences from the high philosophical level,

01:44.960 --> 01:51.520
maybe the technical level between the capabilities of, broadly speaking, the two kinds of systems,

01:51.520 --> 01:56.320
large language models, and this computational, gigantic computational system infrastructure

01:56.320 --> 02:02.480
that is Wolfram Alpha? Yeah. So what does something like Chad GPT do? It's mostly focused on

02:03.040 --> 02:11.120
make language like the language that humans have made and put on the web and so on. So its primary

02:12.080 --> 02:17.600
underlying technical thing is you've given a prompt. It's trying to continue that prompt in

02:17.600 --> 02:23.760
a way that's somehow typical of what it's seen based on a trillion words of text that humans have

02:23.760 --> 02:30.560
written on the web. And the way it's doing that is with something which is probably quite similar

02:30.560 --> 02:36.320
to the way we humans do the first stages of that, using a neural net and so on, and just saying,

02:36.320 --> 02:44.640
given this piece of text, let's ripple through the neural net and get one word at a time of output.

02:46.160 --> 02:52.560
It's kind of a shallow computation on a large amount of training data that is what we humans

02:52.560 --> 02:58.960
have put on the web. That's a different thing from the computational stack that I spent the last,

02:58.960 --> 03:04.560
I don't know, 40 years or so building, which has to do with what can you compute many steps,

03:04.560 --> 03:11.360
potentially a very deep computation. It's not sort of taking the statistics of what we humans

03:11.360 --> 03:17.840
have produced and trying to continue things based on that statistics. Instead, it's trying to take

03:17.840 --> 03:23.040
kind of the formal structure that we've created in our civilization, whether it's from mathematics

03:23.040 --> 03:29.840
or whether it's from kind of systematic knowledge of all kinds and use that to do arbitrarily deep

03:29.840 --> 03:36.720
computations to figure out things that aren't just let's match what's already been kind of said on

03:36.720 --> 03:41.360
the web, but let's potentially be able to compute something new and different that's never been

03:41.360 --> 03:50.240
computed before. So as a practical matter, our goal is to have made as much as possible of the

03:50.240 --> 03:55.600
world computable in the sense that if there's a question that in principle is answerable from

03:55.600 --> 04:01.120
some sort of expert knowledge that's been accumulated, we can compute the answer to that

04:01.120 --> 04:07.520
question and we can do it in a sort of reliable way that's the best one can do given what the

04:07.520 --> 04:14.400
expertise that our civilization has accumulated. It's a much more sort of labor intensive on the

04:14.400 --> 04:22.800
side of kind of creating kind of the computational system to do that. Obviously, in the kind of the

04:22.800 --> 04:29.200
chat GPT world, it's like take things which were produced for quite other purposes, namely

04:29.200 --> 04:35.440
all the things we've written out on the web and so on and sort of forage from that things which

04:35.440 --> 04:41.200
are like what's been written on the web. So I think as a practical point of view, I view sort of the

04:41.200 --> 04:47.680
chat GPT thing as being wide and shallow and what we're trying to do with sort of building out

04:47.680 --> 04:55.040
computation as being this sort of deep, also broad, but most importantly kind of deep

04:55.040 --> 04:59.680
type of thing. I think another way to think about this is you go back in human history,

05:00.240 --> 05:05.360
you know, I don't know, thousand years or something, and you say what can the typical person,

05:05.360 --> 05:09.200
what's the typical person going to figure out? Well, the answer is there are certain kinds of

05:09.200 --> 05:16.240
things that we humans can quickly figure out. That's sort of what our neural architecture

05:16.240 --> 05:21.600
and the kinds of things we learn in our lives let us do. But then there's this whole layer

05:21.600 --> 05:27.920
of kind of formalization that got developed, which is the kind of whole sort of story of

05:27.920 --> 05:32.800
intellectual history and whole kind of depth of learning. That formalization turned into things

05:32.800 --> 05:38.720
like logic, mathematics, science, and so on. And that's the kind of thing that allows one to kind

05:38.720 --> 05:46.400
of build these towers of sort of towers of things you work out. It's not just I can immediately

05:46.400 --> 05:52.480
figure this out. It's no, I can use this kind of formalism to go step by step and work out

05:52.480 --> 05:56.960
something which was not immediately obvious to me. And that's kind of the story of what we're

05:56.960 --> 06:02.320
trying to do computationally is to be able to build those kind of tall towers of what implies

06:02.320 --> 06:08.480
what implies what and so on. And as opposed to kind of the, yes, I can immediately figure it out.

06:08.480 --> 06:13.360
It's just like what I saw somewhere else in something that I heard or remembered or something

06:13.360 --> 06:19.520
like this. What can you say about the kind of formal structure or the kind of formal foundation

06:19.520 --> 06:24.960
you can build such a formal structure on about the kinds of things you would start on in order

06:24.960 --> 06:31.760
to build this kind of deep computable knowledge trees? So the question is sort of how do you

06:31.760 --> 06:37.600
how do you think about computation? And there's there's a couple of points here. One is what

06:37.600 --> 06:44.480
computation intrinsically is like. And the other is what aspects of computation we humans with our

06:44.480 --> 06:50.160
minds and with the kinds of things we've learned can sort of relate to in that computational

06:50.160 --> 06:56.000
universe. So if we start on the kind of what can computation be like, it's something I've spent

06:56.000 --> 07:02.320
some big chunk of my life studying is imagine that you're, you know, we usually write programs where

07:02.320 --> 07:07.200
we kind of know what we want the program to do. And we carefully write many lines of code. And

07:07.280 --> 07:11.200
we hope that the program does what we what we intended it to do. But the thing I've been

07:11.200 --> 07:16.480
interested in is if you just look at the kind of natural science of programs, you just say,

07:16.480 --> 07:20.400
I'm going to make this program. It's a really tiny program. Maybe I even pick the pieces of

07:20.400 --> 07:24.800
the program at random, but it's really tiny. And by really tiny, I mean, you know, less than a line

07:24.800 --> 07:30.880
of code type thing. You say, what does this program do? And you run it. And big discovery that I made

07:30.880 --> 07:36.800
in the early 80s is that even extremely simple programs, when you run them can do really

07:36.800 --> 07:41.760
complicated things really surprised me. It took me several years to kind of realize that that was

07:41.760 --> 07:47.440
a thing, so to speak. But that that realization that even very simple programs can do incredibly

07:47.440 --> 07:52.800
complicated things that we very much don't expect that discovery. I mean, I realized that that's

07:52.800 --> 07:58.320
very much I think how nature works. That is nature has simple rules, but yet does all sorts of

07:58.320 --> 08:03.360
complicated things that we might not expect. You know, as a big thing of the last few years

08:03.360 --> 08:08.240
has been understanding that that's how the whole universe and physics works. But that's a quite

08:08.240 --> 08:14.560
separate topic. But so there's this whole world of programs and what they do, and very rich,

08:14.560 --> 08:18.960
sophisticated things that these programs can do. But when we look at many of these programs,

08:18.960 --> 08:22.160
we look at them and say, well, that's kind of I don't really know what that's doing.

08:22.160 --> 08:26.880
It's not a very human kind of thing. So on the one hand, we have sort of what's possible in the

08:26.880 --> 08:30.960
computational universe. On the other hand, we have the kinds of things that we humans think

08:30.960 --> 08:36.240
about the kinds of things that are developed in kind of our intellectual history. And that's

08:37.440 --> 08:43.280
really the challenge to sort of making things computational is to connect what's computationally

08:43.280 --> 08:48.960
possible out in the computational universe with the things that we humans sort of typically think

08:48.960 --> 08:54.720
about with our minds. Now, that's a complicated kind of moving target, because the things that

08:54.720 --> 08:59.840
we think about change over time, we've learned more stuff, we've invented mathematics, we've

08:59.840 --> 09:05.680
invented various kinds of ideas and structures and so on. So it's gradually expanding, we're kind of

09:05.680 --> 09:11.840
gradually colonizing more and more of this kind of intellectual space of possibilities. But the real

09:11.840 --> 09:17.520
thing, the real challenge is, how do you take what is computationally possible? How do you take,

09:17.520 --> 09:22.720
how do you encapsulate the kinds of things that we think about in a way that kind of plugs into

09:22.720 --> 09:29.840
what's computationally possible? And actually, the big sort of idea there is this idea of kind

09:29.840 --> 09:36.000
of symbolic programming, symbolic representations of things. And so the question is, when you look

09:36.000 --> 09:40.000
at sort of everything in the world, and you kind of take some visual scene or something you're

09:40.000 --> 09:44.880
looking at, and you say, well, how do I turn that into something that I can kind of stuff into my

09:44.880 --> 09:50.000
mind? You know, there are lots of pixels in my visual scene, but the things that I remembered

09:50.000 --> 09:56.080
from that visual scene are, you know, there's a chair in this place. It's a kind of a symbolic

09:56.080 --> 10:00.160
representation of the visual scene. There are two chairs and a table or something,

10:00.160 --> 10:04.400
rather than there are all these pixels arranged in all these detailed ways. And so the question then

10:04.400 --> 10:10.240
is, how do you take sort of all the things in the world and make some kind of representation

10:10.240 --> 10:16.880
that corresponds to the types of ways that we think about things? And human language is sort of

10:16.880 --> 10:21.040
one form of representation that we have. We talk about chairs, that's a word in human language

10:21.040 --> 10:28.400
and so on. How do we take, but human language is not in and of itself something that plugs in very

10:28.400 --> 10:34.800
well to sort of computation. It's not something from which you can immediately compute consequences

10:34.800 --> 10:41.920
and so on. And so you have to kind of find a way to take sort of the stuff we understand from human

10:41.920 --> 10:48.560
language and make it more precise. And that's really the story of symbolic programming. And

10:48.560 --> 10:53.760
you know, what that turns into is something which I didn't know at the time it was going to work as

10:53.760 --> 10:59.360
well as it has. But back in the 1979 or so, I was trying to build my first big computer system and

10:59.360 --> 11:03.280
trying to figure out, you know, how should I represent computations at a high level?

11:03.840 --> 11:10.880
And I kind of invented this idea of using kind of symbolic expressions, you know, structured as

11:10.880 --> 11:16.720
it's kind of like a function and a bunch of arguments. But that function doesn't necessarily

11:16.720 --> 11:23.280
evaluate to anything. It's just a thing that sits there representing a structure. And so building

11:23.280 --> 11:29.920
up that structure, and it's turned out that structure has been extremely, it's a good match

11:29.920 --> 11:34.640
for the way that we humans, it seems to be a good match for the way that we humans kind of

11:34.640 --> 11:40.240
conceptualize higher level things. And it's been for the last, I don't know, 45 years or something.

11:40.320 --> 11:45.440
It's served me remarkably well. So building up that structure using this kind of symbolic

11:45.440 --> 11:51.200
representation. But what can you say about abstractions here? Because you could just start

11:51.200 --> 11:55.840
with your physics project, you could start at a hypergraph at a very, very low level and build

11:55.840 --> 12:02.320
up everything from there. But you don't. You take shortcuts. Right. You take the highest level of

12:02.320 --> 12:08.560
abstraction, convert that, the kind of abstraction that's convertible to something computable,

12:08.560 --> 12:14.480
using symbolic representation. And then that's your new foundation for that little piece of

12:14.480 --> 12:20.400
knowledge. And somehow all that is integrated. Right. So the sort of a very important phenomenon

12:20.400 --> 12:26.560
that is kind of a thing that I've sort of realized is just, it's one of these things that sort of in

12:26.560 --> 12:31.360
the future of kind of everything is going to become more and more important as this phenomenon

12:31.360 --> 12:37.360
of computational irreducibility. And the question is, if you know the rules for something, you have

12:37.360 --> 12:41.360
a program, you're going to run it. You might say, I know the rules. Great. I know everything

12:41.360 --> 12:47.360
about what's going to happen. Well, in principle you do because you can just run those rules out

12:47.360 --> 12:51.840
and just see what they do. You might run them a million steps. You see what happens, et cetera.

12:51.840 --> 12:56.880
The question is, can you like immediately jump ahead and say, I know it's going to happen after

12:56.880 --> 13:03.840
a million steps and the answer is 13 or something. And one of the very critical things to realize is

13:04.320 --> 13:10.160
if you could reduce that computation, there is in a sense, no point in doing the computation.

13:10.160 --> 13:14.800
The place where you really get value out of doing a computation is when you had to do the

13:14.800 --> 13:19.040
computation to find out the answer. But this phenomenon that you have to do the computation

13:19.040 --> 13:22.880
to find out the answer, this phenomenon of computational irreducibility seems to be

13:22.880 --> 13:26.960
tremendously important for thinking about lots of kinds of things. So one of the things that

13:26.960 --> 13:31.680
happens is, okay, you've got a model of the universe at the low level in terms of atoms of

13:31.680 --> 13:37.040
space and hypergraphs and rewriting hypergraphs and so on. And it's happening 10 to the 100 times

13:37.040 --> 13:42.880
every second, let's say. Well, you say, great, then we've nailed it. We know how the universe

13:42.880 --> 13:47.840
works. Well, the problem is the universe can figure out what it's going to do. It does those

13:47.840 --> 13:54.240
10 to the 100 steps. But for us to work out what it's going to do, we have no way to reduce that

13:54.240 --> 13:59.680
computation. The only way to do the computation, to see the result of the computation is to do it.

13:59.680 --> 14:04.640
And if we're operating within the universe, there's no opportunity to do that because the

14:04.640 --> 14:10.400
universe is doing it as fast as the universe can do it. And that's what's happening. So what we're

14:10.400 --> 14:15.520
trying to do, and a lot of the story of science and a lot of other kinds of things is finding

14:15.520 --> 14:21.040
pockets of reducibility. That is, you could have a situation where everything in the world is full

14:21.040 --> 14:25.520
of computational irreducibility. We never know what's going to happen next. The only way we can

14:25.520 --> 14:30.000
figure out what's going to happen next is just let the system run and see what happens. So in a

14:30.000 --> 14:36.240
sense, the story of most kinds of science, inventions, a lot of kinds of things, is the

14:36.240 --> 14:41.120
story of finding these places where we can locally jump ahead. And one of the features of computational

14:41.120 --> 14:47.120
reducibility is there are always pockets of reducibility. There are always an infinite

14:47.120 --> 14:52.080
number of places where you can jump ahead. There's no way where you can jump completely ahead,

14:52.080 --> 14:56.320
but there are little patches, little places where you can jump ahead a bit.

14:56.320 --> 15:01.440
And I think we can talk about physics project and so on, but I think the thing we realize is

15:01.440 --> 15:06.800
we kind of exist in a slice of all the possible computational irreducibility in the universe.

15:06.800 --> 15:10.400
We exist in a slice where there's a reasonable amount of predictability.

15:10.960 --> 15:17.760
And in a sense, as we try and construct these kind of higher levels of abstraction, symbolic

15:17.760 --> 15:22.480
representations and so on, what we're doing is we're finding these lumps of reducibility

15:22.480 --> 15:27.840
that we can kind of attach ourselves to and about which we can kind of have fairly simple

15:27.840 --> 15:33.040
narrative things to say. Because in principle, I say, what's going to happen in the next few

15:33.040 --> 15:37.600
seconds? Oh, there are these molecules moving around in the air in this room, and oh gosh,

15:37.600 --> 15:42.880
it's an incredibly complicated story. And that's a whole computationally reducible thing,

15:42.880 --> 15:48.480
most of which I don't care about. And most of it is, well, the air is still going to be here,

15:48.480 --> 15:53.680
and nothing much is going to be different about it. And that's a kind of reducible fact

15:53.680 --> 15:57.920
about what is ultimately at an underlying level of computationally irreducible process.

15:59.680 --> 16:06.000
And life would not be possible if we didn't have a large number of such reducible pockets,

16:07.280 --> 16:11.200
pockets amenable to reduction into something symbolic.

16:11.280 --> 16:21.520
Yes, I think so. I mean, life in the way that we experience it, depending on what we mean by life,

16:21.520 --> 16:28.160
so to speak, the experience that we have of sort of consistent things happening in the world,

16:28.160 --> 16:33.680
the idea of space, for example, where we can just say, you're here, you move there,

16:34.240 --> 16:38.640
it's kind of the same thing. It's still you in that different place, even though you're made

16:38.640 --> 16:46.240
of different atoms of space and so on. This idea that there's sort of this level of predictability

16:46.240 --> 16:52.160
of what's going on, that's us finding a slice of reducibility in what is underneath this

16:52.160 --> 16:58.160
computationally irreducible kind of system. And I think that's sort of the thing which is actually

16:59.120 --> 17:05.280
my favorite discovery over the last few years is the realization that it is sort of the interaction

17:05.280 --> 17:12.720
between the sort of underlying computational irreducibility and our nature as kind of observers

17:12.720 --> 17:19.360
who sort of have to key into computational reducibility. That fact leads to the main

17:19.360 --> 17:24.960
laws of physics that we discovered in the 20th century. So we talk about this in more detail,

17:24.960 --> 17:33.280
but to me, it's kind of our nature as observers, the fact that we are computationally bounded

17:33.280 --> 17:38.000
observers. We don't get to follow all those little pieces of computational irreducibility.

17:38.000 --> 17:44.640
To stuff what is out there in the world into our minds requires that we are looking at things that

17:44.640 --> 17:50.080
are reducible, we are compressing, kind of we're extracting just some essence, some kind of symbolic

17:50.080 --> 17:56.480
essence of what's the detail of what's going on in the world. That together with one other condition

17:56.480 --> 18:02.080
that at first seems sort of trivial, but isn't, which is that we believe we are persistent in time.

18:02.640 --> 18:09.040
That is- Yes, some sort of causality. Here's the thing. At every moment,

18:09.680 --> 18:14.720
according to our theory, we're made of different atoms of space. At every moment,

18:14.720 --> 18:21.040
sort of the microscopic detail of what the universe is made of is being rewritten. In fact,

18:21.040 --> 18:25.200
the very fact that there's coherence between different parts of space is a consequence of

18:25.200 --> 18:28.640
the fact that there are all these little processes going on that kind of knit together the structure

18:28.640 --> 18:32.400
of space. It's kind of like if you wanted to have a fluid with a bunch of molecules in it,

18:32.400 --> 18:36.560
if those molecules weren't interacting, you wouldn't have this fluid that would pour and do

18:36.560 --> 18:40.080
all these kinds of things. It would just be sort of a free-floating collection of molecules.

18:40.800 --> 18:44.640
Similarly, it is with space that the fact that space is kind of knitted together

18:44.640 --> 18:51.760
as a consequence of all this activity in space. The fact that what we consist of sort of this

18:51.760 --> 18:58.240
series of ... We're continually being rewritten, and the question is, why is it the case that we

18:58.240 --> 19:04.560
think of ourselves as being the same us through time? That's kind of a key assumption. I think

19:04.560 --> 19:09.840
it's a key aspect of what we see as sort of our consciousness, so to speak, is that we have this

19:09.840 --> 19:15.760
kind of consistent thread of experience. Well, isn't that just another limitation

19:16.720 --> 19:23.600
of our mind that we want to reduce reality into some ... That kind of temporal

19:24.720 --> 19:28.160
consistency is just a nice narrative to tell ourselves.

19:28.160 --> 19:33.360
Well, the fact is, I think it's critical to the way we humans typically operate is that we have

19:33.360 --> 19:40.480
this single thread of experience. If you imagine sort of a mind where you have ... Maybe that's

19:40.480 --> 19:45.040
what's happening in various kinds of minds that aren't working the same way other minds work,

19:45.120 --> 19:50.800
is that you're splitting into multiple threads of experience. It's also something where when you

19:50.800 --> 19:55.760
look at, I don't know, quantum mechanics, for example, in the insides of quantum mechanics,

19:55.760 --> 20:00.960
it's splitting into many threads of experience. But in order for us humans to interact with it,

20:00.960 --> 20:05.680
you kind of have to knit all those different threads together so that we say, oh yeah,

20:05.680 --> 20:09.040
a definite thing happened, and now the next definite thing happens, and so on.

20:09.520 --> 20:19.040
And I think inside, it's interesting to try and imagine what's it like to have these

20:19.680 --> 20:25.040
fundamentally multiple threads of experience going on. I mean, right now, different human minds

20:25.040 --> 20:28.720
have different threads of experience. We just have a bunch of minds that are interacting with

20:28.720 --> 20:35.440
each other, but we don't have ... Within each mind, there's a single thread, and that is indeed

20:35.440 --> 20:40.160
a simplification. I think it's a thing ... The general computational system does not have that

20:40.160 --> 20:46.560
simplification, and it's one of the things ... People often seem to think that consciousness

20:46.560 --> 20:51.520
is the highest level of things that can happen in the universe, so to speak, but I think that's not

20:51.520 --> 20:57.920
true. I think it's actually a specialization in which, among other things, you have this idea of

20:57.920 --> 21:03.040
a single thread of experience, which is not a general feature of anything that could computationally

21:03.040 --> 21:07.840
happen in the universe. So it's a feature of a computationally limited system that's only able

21:07.840 --> 21:18.880
to observe reducible pockets. So I mean, this word observer means something in quantum mechanics. It

21:18.880 --> 21:25.520
means something in a lot of places. It means something to us humans as conscious beings.

21:25.520 --> 21:31.360
So what's the importance of the observer? What is the observer? What's the importance of the

21:31.440 --> 21:35.920
observer in the computational universe? So this question of what is an observer,

21:35.920 --> 21:39.200
what's the general idea of an observer, is actually one of my next projects,

21:39.200 --> 21:42.960
which got somewhat derailed by the current sort of AI mania, but-

21:42.960 --> 21:48.080
Is there a connection there, or do you think the observer is primarily a physics phenomena?

21:48.080 --> 21:50.160
Is it related to the whole AI thing? Yes.

21:50.160 --> 21:56.160
Yes, it is related. So one question is, what is a general observer? So we know ... We have an idea

21:56.160 --> 22:00.560
what is a general computational system. We think about Turing machines. We think about other models

22:00.560 --> 22:05.760
of computation. There's a question, what is a general model of an observer? And there's

22:05.760 --> 22:11.280
kind of observers like us, which is kind of the observers we're interested in. We could imagine

22:11.280 --> 22:15.600
an alien observer that deals with computational irreducibility and it has a mind that's utterly

22:15.600 --> 22:22.640
different from ours and completely incoherent with what we're like. But the fact is that if we are

22:22.640 --> 22:29.120
talking about observers like us, that one of the key things is this idea of kind of taking all the

22:29.120 --> 22:34.400
detail of the world and being able to stuff it into a mind. Being able to take all the detail

22:34.400 --> 22:41.440
and kind of extract out of it a smaller set of kind of degrees of freedom, a smaller number of

22:41.440 --> 22:47.920
elements that will sort of fit in our minds. And I think this question, so I've been interested

22:47.920 --> 22:53.760
in trying to characterize, what is the general observer? And the general observer is, I think,

22:53.760 --> 22:58.320
in part, there are many ... Let me give an example of a ... You have a gas, it's got a bunch of

22:58.320 --> 23:04.320
molecules bouncing around. And the thing you're measuring about the gas is its pressure. And the

23:04.320 --> 23:08.400
only thing you as an observer care about is pressure. And that means you have a piston on

23:08.400 --> 23:13.520
the side of this box and the piston is being pushed by the gas. And there are many, many different

23:13.520 --> 23:19.520
ways that molecules can hit that piston. But all that matters is the kind of aggregate of all those

23:19.520 --> 23:24.000
molecular impacts, because that's what determines pressure. So there's a huge number of different

23:24.000 --> 23:29.360
configurations of the gas, which are all equivalent. So I think one key aspect of observers is this

23:29.360 --> 23:34.960
equivalencing of many different configurations of a system, saying, all I care about is this aggregate

23:34.960 --> 23:41.680
feature. All I care about is this overall thing. And that's sort of one aspect. And we see that in

23:41.680 --> 23:46.320
lots of different ... Again, it's the same story over and over again, that there's a lot of detail

23:46.320 --> 23:53.440
in the world, but what we are extracting from it is something, sort of a thin summary of that detail.

23:53.440 --> 24:03.680
Is that thin summary nevertheless true? Can it be a crappy approximation that on average is correct?

24:03.680 --> 24:09.120
If we look at the observer that's the human mind, it seems like there's a lot of very ... As represented

24:09.120 --> 24:13.920
by natural language, for example, there's a lot of really crappy approximation. And that could be

24:13.920 --> 24:20.160
maybe a feature of it, that there's ambiguity. Right. Right. You don't know ... It could be the

24:20.160 --> 24:25.040
case. You're just measuring the aggregate impacts of these molecules, but there is some tiny, tiny

24:25.040 --> 24:30.720
probability that molecules will arrange themselves in some really funky way, and that just measuring

24:30.720 --> 24:36.640
that average isn't going to be the main point. By the way, an awful lot of science is very confused

24:36.640 --> 24:42.000
about this, because you look at papers, and people are really keen. They draw this curve, and they

24:42.000 --> 24:47.440
have these bars on the curve and things, and it's just this curve. And it's this one thing,

24:47.440 --> 24:52.320
and it's supposed to represent some system that has all kinds of details in it.

24:52.320 --> 24:55.600
And this is a way that lots of science has gotten wrong, because people say,

24:55.600 --> 25:00.800
I remember years ago I was studying snowflake growth. You have a snowflake, and it's growing,

25:00.800 --> 25:05.600
it has all these arms, it's doing complicated things. But there was a literature on this stuff,

25:05.600 --> 25:10.640
and it talked about what's the rate of snowflake growth. And it got pretty good answers for the

25:10.640 --> 25:14.560
rate of the growth of the snowflake. And they looked at it more carefully, and they had these

25:14.560 --> 25:19.040
nice curves of snowflake growth rates and so on. I looked at it more carefully, and I realized,

25:19.040 --> 25:24.720
according to their models, the snowflake will be spherical. And so they got the growth rate right,

25:25.360 --> 25:33.520
but the detail was just utterly wrong. And not only the detail, the whole thing was not capturing,

25:33.520 --> 25:38.160
it was capturing this aspect of the system that was, in a sense, missing the main point

25:38.160 --> 25:43.360
of what was going on. What is the geometric shape of a snowflake?

25:43.360 --> 25:49.200
Snowflakes start in the phase of water that's relevant to the formation of snowflakes. It's

25:49.200 --> 25:54.800
a phase of ice, which starts with a hexagonal arrangement of water molecules. And so it starts

25:54.800 --> 25:58.160
off growing as a hexagonal plate. And then what happens is-

25:58.160 --> 26:00.720
It's a plate, oh, versus sphere versus-

26:00.720 --> 26:05.040
Well, no, no, but it's much more than that. I mean, snowflakes are fluffy. Typical snowflakes

26:05.040 --> 26:11.040
have little dendritic arms. And what actually happens is, it's kind of cool because you can

26:11.040 --> 26:16.400
make these very simple discrete models with cellular automata and things that figure this out.

26:16.400 --> 26:22.880
You start off with this hexagonal thing, and then the places, it starts to grow little arms.

26:22.880 --> 26:29.360
And every time a little piece of ice adds itself to the snowflake, the fact that that ice condensed

26:29.360 --> 26:35.280
from the water vapor heats the snowflake up locally. And so it makes it less likely for

26:36.240 --> 26:41.200
for another piece of ice to accumulate right nearby. So this leads to a kind of growth

26:41.200 --> 26:47.840
inhibition. So you grow an arm, and it is a separated arm because right around the arm,

26:47.840 --> 26:52.560
it got a little bit hot and it didn't add more ice there. So what happens is it grows,

26:52.560 --> 26:58.400
you have a hexagon, it grows out arms, the arms grow arms, and then the arms grow arms grow arms.

26:58.400 --> 27:01.680
And eventually, actually, it's kind of cool because it actually fills in another hexagon,

27:01.680 --> 27:06.080
a bigger hexagon. And when I first looked at this, I had a very simple model for this,

27:06.080 --> 27:10.480
I realized when it fills in that hexagon, it actually leaves some holes behind. So I thought,

27:10.480 --> 27:14.720
well, is that really right? So I look at these pictures of snowflakes, and sure enough, they have

27:14.720 --> 27:18.960
these little holes in them that are kind of scars of the way that these arms grow out.

27:20.080 --> 27:25.920
So you can't fill in backfill holes. They don't backfill. Yeah, they don't backfill.

27:25.920 --> 27:30.240
And presumably, there's a limitation of how big, like you can't arbitrarily grow.

27:31.040 --> 27:34.800
I'm not sure. I mean, the thing falls through the I mean, I think it does, you know,

27:34.800 --> 27:39.840
it hits the ground at some point. I think you can grow. I think you can grow in the lab. I think

27:39.840 --> 27:44.560
you can grow pretty big ones. I think you can grow many, many iterations of this kind of goes

27:44.560 --> 27:49.200
from hexagon, it grows out arms, it turns back, it fills back into a hexagon, it grows more arms

27:49.200 --> 27:56.080
again in 3D. No, it's flat usually. Why is it flat? Why doesn't it span out? Okay, okay,

27:56.080 --> 27:59.600
wait a minute. You said it's fluffy. And fluffy is a three dimensional property, no?

27:59.600 --> 28:06.320
Or no, it's fluffy. Snow is okay. So, you know, what makes we're really, we're really in it.

28:08.480 --> 28:12.080
Multiple snowflakes become fluffy. A single snowflake is not fluffy.

28:12.080 --> 28:18.000
No, no, a single snowflake is fluffy. And what happens is, you know, if you have snow,

28:18.000 --> 28:23.440
that is just pure hexagons, they can, you know, they fit together pretty well. It's not,

28:23.840 --> 28:27.760
it doesn't make, it doesn't have a lot of air in it. And they can also slide against each other

28:27.760 --> 28:32.560
pretty easily. And so the snow can be pretty, you know, can, I think avalanches happen sometimes

28:32.560 --> 28:38.320
when the things tend to be these, you know, hexagonal plates and it kind of slides. But then

28:38.320 --> 28:42.960
when the thing has all these arms that have grown out, it's not, they don't fit together very well.

28:42.960 --> 28:46.960
And that's why the snow has lots of air in it. And if you look at one of these snowflakes,

28:46.960 --> 28:52.320
if you catch one, you'll see it has these little arms. And people, actually, people often say,

28:52.480 --> 28:58.720
no two snowflakes are alike. That's mostly because as a snowflake grows, they do grow

28:58.720 --> 29:03.440
pretty consistently with these different arms and so on. But you capture them at different times

29:03.440 --> 29:09.280
as they, you know, they fell through the air in a different way. You'll catch this one at this stage.

29:09.280 --> 29:12.960
And as it goes through different stages, they look really different. And so that's why,

29:12.960 --> 29:17.040
you know, it kind of looks like no two snowflakes are alike because you caught them at different,

29:17.040 --> 29:21.920
at different times. So the rules under which they grow are the same. It's just the timing is.

29:22.240 --> 29:29.360
Okay. So the point is science is not able to describe the full complexity of snowflake growth.

29:29.360 --> 29:35.680
Well, science, if you do what people might often do, which is say, okay, let's make it

29:35.680 --> 29:40.240
scientific. Let's turn it into one number. And that one number is kind of the growth rate of the

29:40.240 --> 29:45.040
arms or some such other thing that fails to capture sort of the detail of what's going on

29:45.040 --> 29:50.240
inside the system. And that's, in a sense, a big challenge for science is how do you extract

29:50.800 --> 29:57.200
from the natural world, for example, those aspects of it that you are interested in talking about.

29:57.200 --> 30:01.440
Now you might just say, I don't really care about the fluffiness of the snowflakes. All I care about

30:01.440 --> 30:05.920
is the growth rate of the arms. In which case, you know, you have, you can have a good model

30:05.920 --> 30:11.680
without knowing anything about the fluffiness. But the fact is as a practical, you know, if you,

30:11.680 --> 30:15.760
if you say, what's the, what is the most obvious feature of a snowflake? Oh, that it has this

30:15.760 --> 30:20.480
complicated shape. Well, then you've got a different story about what you model. I mean,

30:20.480 --> 30:24.640
this is one of the features of sort of modeling and science that, you know, what is a model?

30:24.640 --> 30:30.800
A model is some way of reducing the actuality of the world to something where you can readily sort

30:30.800 --> 30:35.600
of give a narrative for what's happening, where you can basically make some kind of abstraction

30:35.600 --> 30:40.720
of what's happening and answer questions that you care about answering. If you wanted to answer all

30:40.720 --> 30:44.880
possible questions about the system, you'd have to have the whole system because you might care

30:44.880 --> 30:49.040
about this particular molecule. Where did it go? And, you know, your model, which is some big

30:49.040 --> 30:53.840
abstraction of that has nothing to say about that. So, you know, one of the things that's,

30:53.840 --> 30:57.840
that's often confusing in science is people will have, I've got a model, somebody says,

30:57.840 --> 31:01.040
somebody else will say, I don't believe in your model because it doesn't capture the feature of

31:01.040 --> 31:05.680
the system that I care about. You know, there's always this controversy about, you know, is the,

31:05.680 --> 31:11.760
is it a correct model? Well, no model is, except for the actual system itself is a correct model

31:11.760 --> 31:15.360
in the sense that it captures everything. The question is, does it capture what you care about

31:15.360 --> 31:20.240
capturing? Sometimes that's ultimately defined by what you're going to build technology out of,

31:20.240 --> 31:25.600
things like this. The one counterexample to this is if you think you're modeling the whole universe

31:25.600 --> 31:31.760
all the way down, then there is a notion of a correct model. But even that is more complicated

31:31.760 --> 31:36.560
because it depends on kind of how observers sample things and so on. That's a, that's a separate

31:36.560 --> 31:41.440
story. But at least at the first level to say, you know, this thing about, oh, it's an approximation,

31:41.440 --> 31:46.080
you're capturing one aspect, you're not capturing other aspects. When you really think you have a

31:46.080 --> 31:51.680
complete model for the whole universe, you better be capturing ultimately everything, even though

31:51.680 --> 31:56.240
to actually run that model is impossible because of computational irreducibility.

31:56.240 --> 32:00.960
The only, the only thing that successfully runs that model is the actual running of the universe.

32:00.960 --> 32:07.040
Is the universe itself, but okay. So what you care about is an interesting concept. So that's a,

32:07.040 --> 32:12.160
that's a human concept. So that's what you're doing with Wolfram Alpha and Wolfram language

32:12.800 --> 32:19.040
is you're trying to come up with symbolic representations. Yes. As simple as possible.

32:21.280 --> 32:26.480
So a model that's as simple as possible that fully captures stuff we care about.

32:26.480 --> 32:32.400
Yes. So I mean, for example, you know, we could, we'll have a thing about, you know, data about

32:32.400 --> 32:37.680
movies. Let's say we could be describing every individual pixel in every movie and so on, but

32:37.680 --> 32:43.920
that's not the level that people care about. And it's, yes, this is a, I mean, and that level that

32:43.920 --> 32:50.640
people care about is somewhat related to what's described in natural language. But what, what we're

32:50.640 --> 32:56.240
trying to do is to find a way to sort of represent precisely so you can compute things. See, see one

32:56.240 --> 33:01.120
thing we say, you give a piece of natural language question is you feed it to a computer. You say,

33:01.120 --> 33:06.480
does the computer understand this natural language? Well, you know, the computer processes it in some

33:06.480 --> 33:10.480
way. It does this. Maybe it can make a continuation of the natural language. You know, maybe it can

33:10.480 --> 33:14.800
go on from the prompt and say what it's going to say. You say, does it really understand it?

33:15.600 --> 33:22.320
Hard to know. But for in this kind of computational world, there is a very definite

33:22.320 --> 33:28.800
definition of does it understand, which is, could it be turned into this symbolic computational thing

33:28.800 --> 33:33.520
from which you can compute all kinds of consequences? And that's the sense in which

33:33.520 --> 33:38.560
one has sort of a target for the understanding of natural language. And that's kind of our goal

33:38.560 --> 33:46.080
is to have as much as possible about the world that can be computed in a reasonable way, so to speak,

33:46.080 --> 33:51.360
be able to be sort of captured by this kind of computational language. That's kind of the goal.

33:51.920 --> 33:57.760
And I think for us humans, the main thing that's important is as we formalize what we're talking

33:57.760 --> 34:03.280
about, it gives us a way of kind of building a structure where we can sort of build this tower

34:03.280 --> 34:08.240
of consequences of things. So if we're just saying, well, let's talk about it in natural language,

34:08.240 --> 34:13.120
it doesn't really give us some hard foundation that lets us, you know, build step by step

34:13.120 --> 34:16.960
to work something out. I mean, it's kind of like what happens in math. If we were just

34:16.960 --> 34:21.920
sort of vaguely talking about math, but didn't have the kind of full structure of math and all

34:21.920 --> 34:26.080
that kind of thing, we wouldn't be able to build this kind of big tower of consequences.

34:26.080 --> 34:31.360
And so, you know, in a sense, what we're trying to do with the whole computational language effort

34:31.360 --> 34:37.040
is to make a formalism for describing the world that makes it possible to kind of build this tower

34:37.040 --> 34:43.760
of consequences. Well, can you talk about this dance between natural language and Wolfram language?

34:44.400 --> 34:49.280
So there's this gigantic thing we call the internet where people post memes and

34:49.280 --> 34:57.040
and diary type thoughts and very important sounding articles and all of that that makes up

34:57.040 --> 35:03.360
the training data set for GPT. And then there's Wolfram language. How can you map

35:03.360 --> 35:10.960
from the natural language of the internet to the Wolfram language? Is there a manual,

35:10.960 --> 35:14.800
is there an automated way of doing that as we look into the future?

35:15.680 --> 35:22.640
Well, so Wolfram Alpha, what it does, its kind of front end is turning natural language into

35:22.640 --> 35:27.840
computational language. Right. What you mean by that is there's a prompt, you ask a question,

35:27.840 --> 35:31.920
what is the capital of some country? And it turns into, you know, what's the

35:31.920 --> 35:36.640
distance between, you know, Chicago and London or something. And that will turn into, you know,

35:36.640 --> 35:43.120
geo distance of entity, city, you know, etc, etc, etc. Each one of those things is very,

35:43.120 --> 35:47.920
is very well defined. We know, you know, given that it's the entity, city, Chicago, etc, etc,

35:47.920 --> 35:53.280
etc. You know, Illinois, United States, you know, we know the geo location of that,

35:53.280 --> 35:57.280
we know its population, we know all kinds of things about it, which we have, you know,

35:57.280 --> 36:03.840
curated that data to be able to know that with some degree of certainty, so to speak. And then,

36:04.880 --> 36:11.120
then we can compute things from this. And that's kind of the, yeah, that's the idea.

36:11.120 --> 36:18.480
But then something like GPT, large language models, do they allow you to make that conversion

36:18.480 --> 36:20.880
much more powerful? Okay. So it's an interesting thing,

36:20.880 --> 36:26.800
which we still don't know everything about. Okay. The, I mean, this question of going from

36:26.800 --> 36:31.840
natural language to computational language in Wolfram Alpha, we've now, you know, Wolfram Alpha

36:31.840 --> 36:37.600
has been out and about for 13 and a half years now. And, you know, we've achieved, I don't know what

36:37.600 --> 36:44.480
it is, 98%, 99% success on queries that get put into it. Now, obviously, there's a sort of feedback

36:44.480 --> 36:49.440
loop, because the things that work are things people go on putting into it. So that, but,

36:49.440 --> 36:55.600
you know, we've got to a very high success rate of the little fragments of natural language that

36:55.600 --> 37:00.640
people put in, you know, questions, math calculations, chemistry calculations, whatever it is,

37:01.200 --> 37:05.840
you know, we can, we do very well at that, turning those things into computational language.

37:06.480 --> 37:10.560
Now, from the very beginning of Wolfram Alpha, I thought about, for example,

37:11.440 --> 37:16.240
writing code with natural language. In fact, I had, I was just looking at this recently,

37:16.240 --> 37:21.440
I had a post that I wrote in 2010, 2011, called something like programming with natural language

37:21.440 --> 37:27.440
is actually going to work. Okay. And so, you know, we had done a bunch of experiments using

37:27.440 --> 37:32.880
methods that were a little bit, some of them a little bit machine learning like, but certainly

37:32.880 --> 37:39.040
not the same, you know, the same kind of idea of vast training data and so on. That is the story

37:39.040 --> 37:44.080
of large language models. Actually, I know that that post, a piece of utter trivia, but that post,

37:45.040 --> 37:48.000
Steve Jobs forwarded that post around to all kinds of people at Apple.

37:49.760 --> 37:53.840
Because he never really liked programming languages. So he was very happy to see the idea

37:54.400 --> 38:00.720
that you could get rid of this kind of layer of kind of engineering like structure. He would

38:00.720 --> 38:04.800
have liked, you know, I think what's happening now, because it really is the case that you can,

38:04.800 --> 38:09.920
you know, this idea that you have to kind of learn how the computer works to use a programming

38:09.920 --> 38:15.200
language is something that is, I think, a thing that, you know, just like you had to learn the

38:15.200 --> 38:19.040
details of the op codes to know how somebody language worked and so on. It's kind of a thing

38:19.040 --> 38:25.520
that's, that's, that's a limited time horizon, but, but kind of the, you know, so this idea

38:25.520 --> 38:32.720
of how elaborate can you make kind of the prompt, how elaborate can you make the natural language

38:32.720 --> 38:39.040
and abstract from it computational language? It's a very interesting question. And you know, what

38:39.680 --> 38:47.840
chat GBT, you know, GBT four and so on can do is pretty good. It isn't, it's very interesting

38:47.840 --> 38:51.200
process. I mean, I'm still trying to understand this workflow. We've been working out a lot of

38:51.200 --> 38:57.520
tooling around this workflow, the natural language to computational language process,

38:57.520 --> 39:02.080
especially if it's conversation, like dialogue, it's like multiple queries kind of thing.

39:02.080 --> 39:07.360
Yeah, right. There's so many things that are really interesting that work and so on. So

39:07.360 --> 39:11.600
first thing is, can you just walk up to the computer and expect to sort of specify computation?

39:12.480 --> 39:18.320
What one realizes is humans have to have some idea of kind of this way of thinking about things

39:18.320 --> 39:22.160
computationally. Without that, you're kind of out of luck because you just have no idea what

39:22.160 --> 39:26.880
you're going to walk up to a computer. I remember when I should tell a silly story about myself,

39:26.880 --> 39:31.520
the very first computer I saw, which was when I was 10 years old, and it was a big mainframe

39:31.520 --> 39:35.840
computer and so on. And I didn't really understand what computers did. And it's like, somebody's

39:35.840 --> 39:40.800
showing me this computer and it's like, you know, can the computer work out the weight of a dinosaur?

39:41.360 --> 39:45.680
It's like, that isn't a sensible thing to ask. That's kind of, you know, you have to give it,

39:45.680 --> 39:49.920
that's not what computers do. I mean, and well from Alpha, for example, you could say, what's

39:49.920 --> 39:53.680
the typical weight of a stegosaurus and it will give you some answer. But that's a very different

39:53.680 --> 40:00.400
kind of thing from what one thinks of computers is doing. And so the kind of the question of,

40:00.400 --> 40:07.360
you know, first thing is people have to have an idea of what computation is about. I think it's

40:07.360 --> 40:14.960
a very, you know, for education, that is the key thing is kind of this notion, not computer science,

40:14.960 --> 40:19.120
not so the details of programming, but just this idea of how do you think about the world

40:19.120 --> 40:25.120
computationally? Computation, thinking about the world computationally is kind of this formal way

40:25.120 --> 40:29.600
of thinking about the world. We've had other ones, like logic was a formal way, you know,

40:29.600 --> 40:34.080
as a way of sort of abstracting and formalizing some aspects of the world. Mathematics is another

40:34.080 --> 40:38.720
one. Computation is this very broad way of sort of formalizing the way we think about the world.

40:39.280 --> 40:44.800
And the thing that's cool about computation is if we can successfully formalize things,

40:44.960 --> 40:49.200
in terms of computation, computers can help us figure out what the consequences are.

40:49.200 --> 40:53.600
It's not like you formalized it with math. Well, that's nice. But now you have to if you're,

40:53.600 --> 40:58.080
you know, not using a computer to do the math, you have to go work out a bunch of stuff yourself.

40:58.640 --> 41:05.760
So I think, but this idea, let's see, I mean, that, you know, we're trying to take kind of the,

41:05.760 --> 41:09.680
we're talking about sort of natural language and its relationship to computational language.

41:10.480 --> 41:15.360
The thing, the sort of the typical workflow, I think, is first, human has to have some kind

41:15.360 --> 41:20.800
of idea of what they're trying to do. That if it's something that they want to sort of build a tower

41:20.800 --> 41:25.440
of capabilities on something that they want to sort of formalize and make computational.

41:26.000 --> 41:35.120
So then human can type something into, you know, some LLM system, and sort of say vaguely what they

41:35.120 --> 41:41.120
want in sort of computational terms, then it does pretty well at synthesizing well from language

41:41.120 --> 41:46.640
code. And it'll probably do better in the future, because we've got a huge number of examples of,

41:46.640 --> 41:51.120
of natural language input, together with the world from language translation of that.

41:51.120 --> 41:58.080
So it's kind of a, you know, that that's a thing where you can kind of extrapolating from all those

41:58.080 --> 42:04.320
examples, makes it easier to do that that task is the prompter task could also kind of debugging

42:04.320 --> 42:08.800
the world from language code, or is your hope to not do that debugging?

42:08.800 --> 42:13.600
Oh, no, no, no. I mean, so there are many steps here. Okay, so first, the first thing is you type

42:13.600 --> 42:17.760
natural language, it generates well from language. You have examples, by the way, you have an example

42:17.760 --> 42:22.800
that is the dinosaur example given example that jumps to mind that we should be thinking about

42:22.800 --> 42:32.000
some dumb example. It's like, take my heart rate data, and, you know, figure out whether I, you

42:32.080 --> 42:38.800
know, make a moving average every seven days or something, and work out what the and make a plot

42:38.800 --> 42:44.160
of the results. Okay, so that's the thing which is, you know, about two thirds of a line of

42:44.160 --> 42:50.320
off language code. I mean, it's, you know, list plot of moving average of some data bin or something

42:50.320 --> 42:55.440
of the of the data, and then you'll get the results. And, you know, the vague thing that I

42:55.440 --> 43:01.680
was just saying in natural language could would almost certainly correctly turn into that very

43:01.680 --> 43:07.840
simple piece of often language code. So you start mumbling about heart rate. Yeah, and kind of,

43:08.720 --> 43:13.760
you know, you arrive at the moving average kind of idea. Right, you say average over seven days,

43:13.760 --> 43:17.360
maybe it'll figure out that that's a moving, you know, that that can be encapsulated as this moving

43:17.360 --> 43:24.560
average idea. I'm not sure. But then the typical workflow that I'm seeing is, you generate this

43:24.560 --> 43:30.320
piece of often language code, it's pretty small, usually. It's, and if it isn't small, it probably

43:30.320 --> 43:37.040
isn't right. But, you know, if it's, it's pretty small, and you know, often language is one of the

43:37.040 --> 43:41.520
ideas of often languages, it's a language that humans can read. It's not a language which,

43:41.520 --> 43:47.120
you know, programming languages tend to be this one way story of humans write them, and computers

43:47.120 --> 43:52.720
execute from them. Or from languages intended to be something which is sort of like math notation,

43:52.720 --> 43:57.120
something where, you know, humans write it, and humans are supposed to read it as well.

43:57.120 --> 44:02.960
And so kind of the workflow that's emerging is kind of this, this human mumbles some things,

44:03.520 --> 44:08.320
you know, large language model produces a fragment of often language code.

44:08.880 --> 44:14.480
Then you look at that, you say, well, typically, you just run it first, you see, does it produce

44:14.480 --> 44:18.640
the right thing? You look at what it produces, you might say, that's obviously crazy. You look

44:18.640 --> 44:23.840
at the code, you see, I see why it's crazy. You fix it. If you really care about the result,

44:23.840 --> 44:27.280
you really want to make sure it's right, you better look at that code and understand it,

44:27.280 --> 44:31.200
because that's the way you have the sort of checkpoint of did it really do what I expected

44:31.200 --> 44:37.360
it to do. Now, you go beyond that, I mean, it's, it's, it's, you know, what we find is, for example,

44:37.360 --> 44:42.240
let's say the code does the wrong thing, then you can often say to the large language model,

44:42.240 --> 44:46.560
can you adjust this to do this? And it's pretty good at doing that.

44:46.560 --> 44:54.800
Interesting. So you're using the output of the code to give you hints about the,

44:56.640 --> 45:01.520
the function of the code. So you're debugging based on the output of the code itself.

45:01.520 --> 45:07.200
Right. The plugin that we have, you know, for chat GPT, it does that routinely. You know,

45:07.200 --> 45:13.040
it will send the thing in, it will get a result. It will discover the LLM will discover itself

45:13.040 --> 45:17.040
that the result is not plausible. And it will go back and say, Oh, I'm sorry, it's very polite.

45:17.040 --> 45:21.440
And it, you know, it goes back and says, I'll rewrite that piece of code, and then it will

45:21.440 --> 45:26.560
try it again and get the result. The other thing is pretty interesting is when you're just running.

45:26.560 --> 45:30.800
So one of the new concepts that we have, we invented this whole idea of notebooks back

45:31.360 --> 45:36.160
36 years ago now. And so now there's the question of sort of, how do you combine

45:36.160 --> 45:41.920
this idea of notebooks where you have, you know, text and code and output, how do you combine that

45:41.920 --> 45:46.000
with the notion of chat and so on. And there's some really interesting things there. Like,

45:46.000 --> 45:51.760
for example, a very typical thing now is we have these, these notebooks where as soon as the,

45:52.640 --> 45:58.720
if the thing produces errors, if the, you know, run this code and it produces messages and so on,

45:59.280 --> 46:04.880
the, the, the LLM automatically not only looks at those messages, it can also see all kinds of

46:04.880 --> 46:10.160
internal information about stack traces and things like this. And it can then, it does a remarkably

46:10.160 --> 46:15.280
good job of guessing what's wrong and telling you. So in other words, it's, it's looking at

46:15.280 --> 46:19.840
things, sort of interesting. It's kind of a typical sort of AI-ish thing that it's able to

46:19.840 --> 46:24.080
have more sensory data than we humans are able to have, because it's able to look at a bunch of

46:24.080 --> 46:29.680
stuff that we humans would kind of glaze over looking at. And it's able to then come up with,

46:29.680 --> 46:34.640
oh, this is the explanation of what's happening. And what is the data, the stack trace, the code

46:34.640 --> 46:38.080
you've written previously, the natural language you've written? Yeah, it's also what's happening

46:38.080 --> 46:42.720
is one of the things that's, is, is for example, when there's these messages, there's documentation

46:42.720 --> 46:45.840
about these messages. There's examples of where the messages have occurred otherwise,

46:46.640 --> 46:50.480
all these kinds of things. The other thing that's really amusing with this is when it makes a

46:50.480 --> 46:55.840
mistake, one of the things that's in our prompt, when the code doesn't work is read the documentation

46:57.040 --> 47:02.320
and we have another piece of the plugin that lets it read documentation. And that again,

47:02.320 --> 47:07.360
is very, very useful because it will, you know, it will figure out sometimes it'll get,

47:07.360 --> 47:11.440
it'll make up the name of some option for some function that doesn't really exist,

47:11.440 --> 47:16.560
read the documentation, it'll have, you know, some wrong structure for the function and so on.

47:16.560 --> 47:21.680
It's, that's a powerful thing. I mean, the thing that, you know, I've realized is we built this

47:21.680 --> 47:25.920
language over the course of all these years to be nice and coherent and consistent and so on. So

47:25.920 --> 47:31.040
it's easy for humans to understand. Turns out there was a side effect that I didn't anticipate,

47:31.040 --> 47:36.400
which is it makes it easy for AIs to understand. So it's almost like another natural language,

47:36.400 --> 47:43.840
but so Wolfram language is a kind of foreign language. Yes. So you have a lineup, English,

47:43.840 --> 47:50.720
French, Japanese, Wolfram language, and then, I don't know, Spanish, and then the system is

47:50.720 --> 47:56.880
not going to notice. Well, yes. I mean, maybe, you know, that's an interesting question because

47:56.880 --> 48:02.560
it really depends on what I see as being an important piece of fundamental science that

48:02.560 --> 48:09.040
basically just jumped out at us with CHAT GPT. Because I think, you know, the real question is

48:09.680 --> 48:16.160
why does CHAT GPT work? How is it possible to encapsulate, you know, to successfully reproduce

48:16.160 --> 48:22.160
all these kinds of things in natural language, you know, with a, you know, a comparatively small,

48:22.160 --> 48:27.040
he says, you know, a couple of hundred billion, you know, weights of neural net and so on.

48:27.040 --> 48:32.800
And I think that, you know, that relates to kind of a fundamental fact about language,

48:32.800 --> 48:38.400
which, you know, the main thing is that I think there's a structure to language that we haven't

48:38.400 --> 48:43.680
kind of really explored very well. It's kind of this semantic grammar I'm talking about

48:45.280 --> 48:49.840
about language. I mean, we kind of know that when we set up human language,

48:50.320 --> 48:55.280
we know that it has certain regularities. We know that it has a certain grammatical structure,

48:55.280 --> 49:00.640
you know, noun followed by verb, followed by noun, adjectives, et cetera, et cetera, et cetera.

49:00.640 --> 49:06.080
That's its kind of grammatical structure. But I think the thing that CHAT GPT is showing us is

49:06.080 --> 49:11.360
that there's an additional kind of regularity to language, which has to do with the meaning of the

49:11.360 --> 49:17.120
language beyond just this pure, you know, part of speech combination type of thing. And I think

49:17.120 --> 49:23.920
the kind of the one example of that that we've had in the past is logic. And, you know, I think

49:23.920 --> 49:32.240
my sort of kind of picture of how was logic invented, how was logic discovered really was

49:32.240 --> 49:36.880
the thing that was discovered in its original conception. It was discovered presumably by

49:36.880 --> 49:42.240
Aristotle, who kind of listened to a bunch of people, orators, you know, giving speeches.

49:42.240 --> 49:47.920
And this one made sense, that one doesn't make sense. This one, and, you know, you see these

49:47.920 --> 49:55.200
patterns of, you know, if the, you know, I don't know what, you know, if the Persians do this,

49:55.200 --> 50:01.520
then this does that, et cetera, et cetera, et cetera. And what Aristotle realized is there's

50:01.520 --> 50:06.160
a structure to those sentences. There's a structure to that rhetoric that doesn't matter whether it's

50:06.160 --> 50:11.440
the Persians and the Greeks, or whether it's the cats and the dogs. It's just, you know, P and Q.

50:11.440 --> 50:17.600
You can abstract from this, the details of these particular sentences. You can lift out

50:17.600 --> 50:20.160
this kind of formal structure, and that's what logic is.

50:20.800 --> 50:24.480
That's a heck of a discovery, by the way, logic. You're making me realize, no.

50:25.280 --> 50:25.520
Yeah.

50:25.520 --> 50:26.560
It's not obvious.

50:27.120 --> 50:33.360
The fact that there is an abstraction from natural language that has where you can fill in

50:33.360 --> 50:39.280
any word you want is a very interesting discovery. Now, it took a long time to mature. I mean,

50:39.280 --> 50:44.400
Aristotle had this idea of syllogistic logic, where there were these particular patterns

50:44.400 --> 50:48.800
of how you could argue things, so to speak. And, you know, in the Middle Ages,

50:48.800 --> 50:53.120
part of education was you memorized the syllogisms. I forget how many there were,

50:53.120 --> 50:57.680
15 of them or something. And they all had names. They all had mnemonics, like I think Barbara and

50:57.680 --> 51:02.160
Silerant were two of the mnemonics for the syllogisms. And people would kind of,

51:02.160 --> 51:05.920
this is a valid argument because it follows the Barbara syllogism, so to speak.

51:06.560 --> 51:13.360
And it took until 1830, you know, with George Boole, to kind of get beyond that and kind of

51:13.360 --> 51:19.840
see that there was a level of abstraction that was beyond this particular template of a sentence,

51:19.840 --> 51:25.040
so to speak. And that's, you know, what's interesting there is, in a sense, you know,

51:26.240 --> 51:31.280
Chachi BT is operating at the Aristotelian level. It's essentially dealing with templates of

51:31.280 --> 51:36.240
sentences. By the time you get to Boole and Boolean algebra and this idea of, you know,

51:36.240 --> 51:41.280
you can have arbitrary depth-nested collections of ands and ors and nots, and you can resolve

51:41.280 --> 51:46.240
what they mean, that's the kind of thing, that's a computation story. That's, you know,

51:46.240 --> 51:50.880
you've gone beyond the pure sort of templates of natural language to something which is an

51:50.880 --> 51:56.960
arbitrarily deep computation. But the thing that I think we realized from Chachi BT is, you know,

51:56.960 --> 52:02.240
Aristotle stopped too quickly, and there was more that you could have lifted out of language

52:02.240 --> 52:07.360
as formal structures. And I think there's, you know, in a sense, we've captured some of that.

52:07.360 --> 52:13.440
And, you know, some of what is in language, there's a lot of kind of little

52:14.000 --> 52:20.000
calculi, little algebras of what you can say, what language talks about, whether it's, I don't know,

52:20.000 --> 52:27.920
if you say, I go from place A to place B, place B to place C, then I know I've gone from place A

52:27.920 --> 52:33.680
to place C. If A is a friend of B and B is a friend of C, it doesn't necessarily follow that

52:33.680 --> 52:41.520
A is a friend of C. These are things that are, you know, if you go from place A to place B,

52:41.520 --> 52:46.800
place B to place C, it doesn't matter how you went. Like logic, it doesn't matter whether you

52:46.800 --> 52:53.600
flew there, walked there, swam there, whatever. You still, this transitivity of where you go

52:53.600 --> 52:59.840
is still valid. And there are many kinds of kind of features, I think, of the way the world works

53:00.720 --> 53:06.240
that are captured in these aspects of language, so to speak. And I think what Chachi BT

53:06.240 --> 53:10.800
effectively has found, just like it discovered logic, you know, people are really surprised it

53:10.800 --> 53:16.080
can do these logical inferences. It discovered logic the same way Aristotle discovered logic,

53:16.080 --> 53:20.640
by looking at a lot of sentences effectively and noticing the patterns in those sentences.

53:20.640 --> 53:25.440
But it feels like it's discovering something much more complicated than logic. So this kind

53:25.440 --> 53:32.320
of semantic grammar. I think he wrote about this, maybe we can call it the laws of language,

53:32.320 --> 53:35.920
I believe you call, or which I like, the laws of thought.

53:35.920 --> 53:40.720
Yes. That was the title that George Boole had for his Boolean algebra back in 1830.

53:40.720 --> 53:42.160
The laws of thought?

53:42.160 --> 53:50.400
Yes, that was what he said. So he thought he nailed it with Boolean algebra. There's more to it.

53:50.400 --> 53:56.640
It's a good question. How much more is there to it? And it seems like one of the reasons, as you

53:57.760 --> 54:05.760
imply that the reason Chachi BT works is that there's a finite number of things to it.

54:06.320 --> 54:09.680
Yeah, I mean, it's discovering the laws. In some sense,

54:09.680 --> 54:15.360
GPT is discovering this laws of semantic grammar that underlies language.

54:15.360 --> 54:19.520
Yes. And what's sort of interesting is, in the computational universe,

54:19.520 --> 54:24.960
there's a lot of other kinds of computation that you could do. They're just not ones that we humans

54:24.960 --> 54:31.040
have cared about and operate with. And that's probably because our brains are built in a certain

54:31.040 --> 54:36.560
way. And the neural nets of our brains are not that different in some sense from the neural nets

54:36.560 --> 54:43.520
of a large language model. And so when we think about, and maybe we can talk about this some more,

54:43.520 --> 54:49.600
but when we think about, what will AIs ultimately do? The answer is, insofar as AIs are just doing

54:49.600 --> 54:55.760
computation, they can run off and do all these kinds of crazy computations. But the ones that

54:55.760 --> 55:01.600
we sort of have decided we care about is this kind of very limited set.

55:03.040 --> 55:07.600
That's where the reinforcement learning with human feedback seems to come in.

55:08.160 --> 55:12.400
The more the AIs say the stuff that kind of interests us, the more we're impressed by it.

55:13.440 --> 55:17.920
So it can do a lot of interesting intelligent things, but we're only interested in the AI

55:17.920 --> 55:24.960
systems when they communicate in a human-like way about human-like topics.

55:24.960 --> 55:31.200
Yes. Well, it's like technology. I mean, in a sense, the physical world provides all kinds

55:31.200 --> 55:36.320
of things. There are all kinds of processes going on in physics. Only a limited set of those are

55:36.320 --> 55:41.600
ones that we capture and use for technology because they're only a limited set where we say,

55:42.320 --> 55:47.280
this is a thing that we can sort of apply to the human purposes we currently care about.

55:47.280 --> 55:51.920
I mean, you might've said, okay, you pick up a piece of rock. You say, okay, there's a nice

55:51.920 --> 55:56.640
silicate. It contains all kinds of silicon. I don't care. Then you realize, oh, we could actually

55:56.640 --> 56:02.320
turn this into a semiconductor wafer and make a microprocessor out of it. And then we care a

56:02.320 --> 56:09.120
lot about it. And it's this thing about what do we, in the evolution of our civilization,

56:09.760 --> 56:14.880
what things do we identify as being things we care about? I mean, it's like when there was a

56:14.880 --> 56:19.440
little announcement recently of the possibility of a high-temperature superconductor that involved

56:19.440 --> 56:28.080
the element lutetium, which generally nobody has cared about. But suddenly, if there's this

56:28.080 --> 56:32.720
application that relates to kind of human purposes, we start to care a lot.

56:32.720 --> 56:39.600
So given your thinking that GPT may have discovered inklings of laws of thought,

56:40.320 --> 56:45.520
do you think such laws exist? Can we linger on that? What's your intuition here?

56:45.520 --> 56:52.960
Oh, definitely. I mean, the fact is, look, logic is but the first step. There are many other

56:52.960 --> 57:00.880
kinds of calculi about things that we consider about sort of things that happen in the world

57:00.880 --> 57:04.640
or things that are meaningful. Well, how do you know logic is not the last step?

57:04.640 --> 57:09.680
You know what I mean? Well, because we can plainly see that there are things. I mean, if you say,

57:10.480 --> 57:15.600
here's a sentence that is syntactically correct, okay? You look at it and you're like,

57:16.560 --> 57:26.080
the happy electron, eight, I don't know what. You look at it and it's like this is meaningless.

57:26.080 --> 57:30.320
It's just a bunch of words. It's syntactically correct. The nouns and the verbs are in the right

57:30.320 --> 57:37.920
place, but it just doesn't mean anything. And so there clearly is some rule that there are rules

57:37.920 --> 57:43.680
that determine when a sentence has the potential to be meaningful that go beyond the pure parts of

57:43.680 --> 57:49.520
speech syntax. And so the question is, what are those rules and are there a fairly finite set of

57:49.520 --> 57:55.760
those rules? My guess is that there's a fairly finite set of those rules. And once you have those

57:55.760 --> 58:02.400
rules, you have kind of a construction kit, just like the rules of syntactic grammar give you a

58:02.400 --> 58:07.600
construction kit for making syntactically correct sentences. So you can also have a construction

58:07.600 --> 58:13.360
kit for making semantically correct sentences. Those sentences may not be realized in the world.

58:13.360 --> 58:20.720
I mean, I think the elephant flew to the moon. Syntactically, semantically, we know,

58:20.720 --> 58:25.760
we have an idea. If I say that to you, you kind of know what that means. But the fact is it hasn't

58:25.760 --> 58:30.560
been realized in the world, so to speak. So semantically correct perhaps is things

58:30.560 --> 58:39.440
that can be imagined with the human mind. No, things that are consistent with both our imagination

58:39.440 --> 58:42.240
and our understanding of physical reality. I don't know.

58:43.040 --> 58:50.480
Yeah, good question. I mean, it's a good question. It's a good question. I mean, I think it is given

58:50.480 --> 58:57.280
the way we have constructed language, it is things which fit with the things we're describing in

58:57.280 --> 59:05.760
language. It's a bit circular in the end and the sort of boundaries of what is physically

59:05.760 --> 59:11.680
realizable. Okay, let's take the example of motion. Motion is a complicated concept.

59:11.680 --> 59:16.240
It might seem like it's a concept that should have been figured out by the Greeks long ago,

59:16.240 --> 59:20.640
but it's actually a really pretty complicated concept because what is motion? Motion is you

59:20.640 --> 59:27.200
can go from place A to place B and it's still you when you get to the other end. You've taken

59:27.200 --> 59:31.520
an object, you move it, and it's still the same object, but it's in a different place.

59:32.160 --> 59:37.120
Now even in ordinary physics, that doesn't always work that way. If you're near a space-time

59:37.120 --> 59:42.480
singularity in a black hole, for example, and you take your teapot or something, you don't have much

59:42.480 --> 59:48.160
of a teapot by the time it's near the space-time singularity. It's been completely deformed beyond

59:48.160 --> 59:54.320
recognition. So that's a case where pure motion doesn't really work. You can't have a thing stay

59:54.320 --> 01:00:02.640
the same. This idea of motion is a slightly complicated idea, but once you have the idea

01:00:02.640 --> 01:00:08.240
of motion, once you have the idea that you're going to describe things as being the same thing

01:00:08.240 --> 01:00:16.480
but in a different place, that abstracted idea then has all sorts of consequences like this

01:00:16.480 --> 01:00:22.480
transitivity of motion. Go from A to B, B to C, you've gone from A to C. At that level of

01:00:22.480 --> 01:00:29.280
description, you can have what are inevitable consequences. They're inevitable features of the

01:00:29.280 --> 01:00:35.920
way you've set things up. That's, I think, what this semantic grammar is capturing is things like

01:00:35.920 --> 01:00:42.000
that. I think that it's a question of what does the word mean when you say I move from here to

01:00:42.000 --> 01:00:46.800
there. Well, it's complicated to say what that means. This is this whole issue of is pure motion

01:00:46.800 --> 01:00:51.600
possible, et cetera, et cetera, et cetera. But once you have got an idea of what that means,

01:00:51.600 --> 01:00:57.760
then there are inevitable consequences of that idea. But the very idea of meaning,

01:00:57.760 --> 01:01:06.480
it seems like there's some words that become, it's like there's a latent ambiguity to them.

01:01:06.480 --> 01:01:12.800
I mean, it's the word like emotionally loaded words like hate and love. Right. It's like,

01:01:12.800 --> 01:01:20.960
what do they mean exactly? So especially when you have relationships between complicated objects,

01:01:20.960 --> 01:01:27.680
we seem to take this kind of shortcut, descriptive shortcut, to describe like object A hates object

01:01:27.680 --> 01:01:35.680
B. What's that really mean? Right. Well, words are defined by kind of our social use of them.

01:01:35.680 --> 01:01:44.000
I mean, it's not a word. In computational language, for example, when we say we have a construct there,

01:01:44.800 --> 01:01:50.240
we expect that that construct is a building block from which we can construct an arbitrarily tall

01:01:50.240 --> 01:01:55.520
tower. So we have to have a very solid building block. And we have to, it turns into a piece of

01:01:55.520 --> 01:02:03.600
code. It has documentation. It's a whole thing. But the word hate, the documentation for that word,

01:02:04.320 --> 01:02:09.600
well, there isn't a standard documentation for that word, so to speak. It's a complicated thing

01:02:09.600 --> 01:02:16.160
defined by kind of how we use it. If it wasn't for the fact that we were using language. So

01:02:16.160 --> 01:02:21.280
what is language at some level? Language is a way of packaging thoughts so that we can communicate

01:02:21.280 --> 01:02:28.560
them to another mind. Can these complicated words be converted into something that a computation

01:02:28.560 --> 01:02:35.440
engine can use? Right. So I think the answer to that is that what one can do in computational

01:02:35.440 --> 01:02:43.520
language is make a specific definition. And if you have a complicated word, like let's say the word

01:02:43.520 --> 01:02:49.600
eat. You'd think that's a simple word. It's animals eat things, whatever else. But you do

01:02:49.600 --> 01:02:56.960
programming. You say this function eats arguments, which is sort of poetically similar to the animal

01:02:56.960 --> 01:03:03.120
eating things. But if you start to say, well, what are the implications of the function eating

01:03:03.120 --> 01:03:08.880
something? Can the function be poisoned? Well, maybe it can actually. But if there's a tight

01:03:08.880 --> 01:03:17.280
mismatch or something in some language. But how far does that analogy go? It's just an analogy.

01:03:17.280 --> 01:03:24.640
Whereas if you use the word eat in a computational language level, you would define there isn't a

01:03:24.640 --> 01:03:30.400
thing which you anchor to the kind of natural language concept eat. But it is now some precise

01:03:30.400 --> 01:03:34.400
definition of that that then you can compute things from. But don't you think the analogy

01:03:34.400 --> 01:03:42.560
is also precise? Software eats the world. Don't you think there's something concrete

01:03:42.560 --> 01:03:47.680
in terms of meaning about analogies? Sure. But the thing that sort of is the

01:03:47.680 --> 01:03:54.080
first target for computational language is to take sort of the ordinary meaning of things

01:03:54.080 --> 01:03:58.800
and try and make it precise. Make it sufficiently precise. You can build these towers of computation

01:03:58.800 --> 01:04:03.360
on top of it. So it's kind of like if you start with a piece of poetry and you say,

01:04:03.360 --> 01:04:09.680
I'm going to define my program with this piece of poetry. It's kind of like that's a difficult

01:04:09.680 --> 01:04:14.720
thing. It's better to say I'm going to just have this boring piece of prose and it's using words

01:04:14.720 --> 01:04:19.680
in the ordinary way. And that's how I'm communicating with my computer. And that's

01:04:19.680 --> 01:04:24.000
how I'm going to build the solid building block from which I can construct this whole kind of

01:04:24.000 --> 01:04:29.280
computational tower. So there's some sense where if you take a poem and reduce it to something

01:04:29.280 --> 01:04:34.640
computable, you're going to have very few things left. So maybe there's a bunch of human interaction

01:04:34.640 --> 01:04:43.920
that's just poetic, aimless nonsense. That's just like recreational hamster in a wheel.

01:04:43.920 --> 01:04:49.120
It's not actually producing anything. Well, I think that that's a complicated thing because

01:04:49.120 --> 01:04:54.560
in a sense, human linguistic communication is there's one mind, it's producing language,

01:04:55.200 --> 01:05:01.760
that language is having an effect on another mind. And the question of there's sort of a type of

01:05:01.760 --> 01:05:07.360
effect that is well-defined, let's say where, for example, it's very independent of the two minds.

01:05:09.440 --> 01:05:16.400
There's communication where it can matter a lot, sort of what the experience of one mind is versus

01:05:16.400 --> 01:05:24.960
another one and so on. Yeah, but what is the purpose of natural language communication?

01:05:28.320 --> 01:05:33.840
So computational language somehow feels more amenable to the definition of purpose.

01:05:34.400 --> 01:05:40.880
It's like, yeah, you're given two clean representations of a concept and you can

01:05:40.880 --> 01:05:45.520
build a tower based on that. Is natural language the same thing but more fuzzy?

01:05:46.000 --> 01:05:51.680
Well, I think the story of natural language, that's the great invention of our species.

01:05:51.680 --> 01:05:56.240
We don't know whether it exists in other species, but we know it exists in our species.

01:05:56.240 --> 01:06:02.800
It's the thing that allows you to sort of communicate abstractly from one generation

01:06:02.800 --> 01:06:08.320
of the species to another. There is an abstract version of knowledge that can be passed down.

01:06:08.320 --> 01:06:15.200
It doesn't have to be genetics. You don't have to apprentice the next generation of

01:06:15.200 --> 01:06:20.160
birds to the previous one to show them how something works. There is this abstracted

01:06:20.160 --> 01:06:28.240
version of knowledge that can be kind of passed down. It still tends to rely because language

01:06:28.240 --> 01:06:34.240
is fuzzy. It does tend to rely on the fact that if we look at some ancient language

01:06:35.040 --> 01:06:39.760
where we don't have a chain of translations from it until what we have today, we may not understand

01:06:39.760 --> 01:06:44.960
that ancient language. And we may not understand its concepts may be different from the ones that

01:06:44.960 --> 01:06:49.600
we have today. We still have to have something of a chain, but it is something where we can

01:06:49.600 --> 01:06:56.880
realistically expect to communicate abstract ideas. And that's one of the big roles of language.

01:06:56.880 --> 01:07:11.200
I think this ability to sort of concretify abstract things is what language has provided.

01:07:11.280 --> 01:07:15.680
Do you see natural language and thought as the same, the stuff that's going inside your mind?

01:07:17.040 --> 01:07:20.320
Well, that's been a long debate in philosophy.

01:07:20.320 --> 01:07:26.480
It seems to become more important now when we think about how intelligent GPT is.

01:07:27.600 --> 01:07:28.720
Whatever that means.

01:07:28.720 --> 01:07:32.160
Whatever that means, but it seems like the stuff that's going on in the human mind

01:07:32.160 --> 01:07:33.760
seems something like intelligence.

01:07:35.120 --> 01:07:36.320
Well, we call it intelligence.

01:07:37.280 --> 01:07:41.120
Yes. And so you start to think of, okay, what's the relationship between

01:07:42.080 --> 01:07:45.920
thought, the language of thought, the laws of thought, the laws of

01:07:47.040 --> 01:07:53.280
the words like reasoning and the laws of language and how that has to do with computation,

01:07:53.280 --> 01:07:57.360
which seems like more rigorous, precise ways of reasoning.

01:07:57.360 --> 01:08:03.680
Right. Which are beyond human. I mean, much of what computers do, humans do not do. I mean,

01:08:04.400 --> 01:08:07.920
humans are a subset, presumably, hopefully.

01:08:07.920 --> 01:08:15.520
Yes. Right. You might say, who needs computation when we have large language models? Large

01:08:15.520 --> 01:08:19.440
language models can just, eventually you'll have a big enough neural net, it can do anything.

01:08:19.440 --> 01:08:24.560
But they're really doing the kinds of things that humans quickly do. And there are plenty of sort

01:08:24.560 --> 01:08:30.960
of formal things that humans never quickly do. For example, I don't know, some people can do

01:08:30.960 --> 01:08:35.440
mental arithmetic. They can do a certain amount of math in their minds. I don't think many people

01:08:35.440 --> 01:08:40.880
can run a program in their minds of any sophistication. It's just not something people

01:08:40.880 --> 01:08:47.200
do. It's not something people have even thought of doing because it's kind of not, you can easily

01:08:47.200 --> 01:08:50.000
run it on a computer. An arbitrary program.

01:08:50.000 --> 01:08:52.720
Yeah. Aren't we running specialized programs?

01:08:52.720 --> 01:08:58.880
Yeah. But if I say to you, here's a Turing machine. Tell me what it does after 50 steps.

01:08:59.680 --> 01:09:04.000
Trying to think about that in your mind, that's really hard to do. It's not what people do.

01:09:04.640 --> 01:09:11.440
Well, in some sense, people program, they build a computer, they program it just to answer your

01:09:11.440 --> 01:09:16.160
question about what the system does after 50 steps. I mean, humans build computers.

01:09:16.160 --> 01:09:23.760
Yes. Yes. That's right. But they've created something which is then, when they run it,

01:09:23.760 --> 01:09:28.320
it's doing something different than what's happening in their minds. They've outsourced

01:09:28.960 --> 01:09:34.240
that piece of computation from something that is internally happening in their minds

01:09:34.240 --> 01:09:37.040
to something that is now a tool that's external to their minds.

01:09:37.040 --> 01:09:41.600
So by the way, humans, to you, didn't invent computers. They discovered though.

01:09:42.720 --> 01:09:47.280
They discovered computation. They invented the technology of computers.

01:09:52.240 --> 01:09:57.840
The computer is just a kind of way to plug into this whole stream of computation.

01:09:58.400 --> 01:10:00.880
There's probably other ways. There's probably a lot of ways.

01:10:00.880 --> 01:10:05.920
Well, for sure. I mean, the particular ways that we make computers out of semiconductors

01:10:05.920 --> 01:10:11.360
and electronics and so on, that's the particular technology stack we built. I mean, the story of

01:10:11.360 --> 01:10:16.080
a lot of what people try to do with quantum computing is finding different sort of underlying

01:10:16.080 --> 01:10:22.560
physical infrastructure for doing computation. You know, biology does lots of computation.

01:10:22.560 --> 01:10:26.160
It does it using an infrastructure that's different from semiconductors and electronics.

01:10:26.720 --> 01:10:32.800
It's a molecular scale sort of computational process that hopefully we'll understand more

01:10:32.800 --> 01:10:40.160
about. I have some ideas about understanding more about that, but that's another representation of

01:10:40.160 --> 01:10:44.640
computation. Things that happen in the physical universe at the level of these evolving

01:10:44.640 --> 01:10:50.880
hypergraphs and so on, that's another sort of implementation layer for this abstract idea of

01:10:50.880 --> 01:10:55.360
computation. So if GPT or large language models are starting to form,

01:10:56.800 --> 01:11:02.160
starting to develop, or implicitly understand the laws of language and thought,

01:11:02.160 --> 01:11:04.880
do you think they can be made explicit? Yes.

01:11:05.760 --> 01:11:12.240
How? We had a bunch of effort. I mean, it's like doing natural science. I mean, what is happening

01:11:12.240 --> 01:11:16.880
in natural science? You have the world that's doing all these complicated things, and then you

01:11:16.880 --> 01:11:22.640
discover Newton's laws, for example. This is how motion works. This is the way that this particular

01:11:22.960 --> 01:11:28.160
idealization of the world, this is how we describe it in a simple computationally reducible way.

01:11:28.160 --> 01:11:34.240
And I think it's the same thing here. There are computationally reducible aspects of what's

01:11:34.240 --> 01:11:39.600
happening that you can get a kind of narrative theory for, just as we've got narrative theories

01:11:39.600 --> 01:11:45.520
in physics and so on. Do you think it will be

01:11:45.520 --> 01:11:53.040
be depressing or exciting when all the laws of thought are made explicit, human thought

01:11:53.040 --> 01:11:58.000
made explicit? I think that once you understand computational reducibility,

01:12:00.000 --> 01:12:04.640
it's neither of those things, because the fact is people say, for example, people will say,

01:12:05.200 --> 01:12:17.680
oh, but I have free will. I operate in a way that they have the idea that they're doing something

01:12:17.680 --> 01:12:24.240
that is internal to them, that they're figuring out what's happening. But in fact, we think there

01:12:24.240 --> 01:12:32.240
are laws of physics that ultimately determine every electrical impulse in a nerve and things

01:12:32.240 --> 01:12:38.080
like this. So you might say, isn't it depressing that we are ultimately just determined by the

01:12:38.080 --> 01:12:43.840
rules of physics, so to speak? It's the same thing. It's at a higher level. It's like it's

01:12:43.840 --> 01:12:50.480
a shorter distance to get from kind of semantic grammar to the way that we might construct a

01:12:50.480 --> 01:12:56.400
piece of text than it is to get from individual nerve firings to how we construct a piece of text.

01:12:56.400 --> 01:13:01.520
But it's not fundamentally different. And by the way, as soon as we have this kind of level,

01:13:01.520 --> 01:13:06.800
this other level of description, it helps us to go even further. So we'll end up being able

01:13:06.800 --> 01:13:14.000
to produce more and more complicated kinds of things that just like if we didn't have a computer

01:13:14.000 --> 01:13:18.320
and we knew certain rules, we could write them down and go a certain distance. But once we have

01:13:18.320 --> 01:13:23.200
a computer, we can go vastly further. And this is the same kind of thing. You wrote a blog post

01:13:23.200 --> 01:13:28.080
titled, What is Chad GPT doing? And why does it work? We've been talking about this. Can we

01:13:28.080 --> 01:13:34.240
just step back and linger on this question? What's Chad GPT doing? What are these

01:13:36.320 --> 01:13:45.280
a bunch of billion parameters trained on a large number of words? Why does it seem to work again?

01:13:46.080 --> 01:13:52.240
Is it because to the point you made that there's laws of language that can be discovered by such

01:13:52.240 --> 01:13:56.400
a process? Is there something more to it? Well, let's talk about sort of the low level of what

01:13:56.400 --> 01:14:02.880
Chad GPT is doing. Ultimately, you give it a prompt. It's trying to work out what should

01:14:02.880 --> 01:14:10.720
the next word be, right? Which is wild. Isn't that surprising to you that this kind of low level,

01:14:11.440 --> 01:14:18.800
dumb training procedure can create something syntactically correct first and then semantically

01:14:18.800 --> 01:14:25.120
correct second? The thing that has been sort of a story of my life is realizing that simple rules

01:14:25.600 --> 01:14:30.400
can do much more complicated things than you imagine. That something that starts simple and

01:14:30.400 --> 01:14:37.360
starts simple to describe can grow a thing that is vastly more complicated than you can imagine.

01:14:38.000 --> 01:14:43.040
And honestly, it's taken me, I don't know, I've sort of been thinking about this now 40 years or so,

01:14:43.040 --> 01:14:46.720
and it always surprises me. I mean, even for example, in our physics project,

01:14:46.720 --> 01:14:51.440
sort of thinking about the whole universe growing from these simple rules, I still resist

01:14:51.440 --> 01:14:55.920
because I keep on thinking, how can something really complicated arise from something that

01:14:55.920 --> 01:15:04.800
simple? It just seems wrong, but yet the majority of my life I've kind of known from things I've

01:15:04.800 --> 01:15:09.920
studied that this is the way things work. So yes, it is wild that it's possible to write a word at

01:15:09.920 --> 01:15:14.800
a time and produce a coherent essay, for example, but it's worth understanding kind of how that's

01:15:14.800 --> 01:15:22.480
working. I mean, it's kind of like if it was going to say, the cat sat on the, what's the next word?

01:15:22.480 --> 01:15:26.400
Okay, so how does it figure out the next word? Well, it's seen a trillion words written on the

01:15:26.400 --> 01:15:34.000
internet, and it's seen the cat sat on the floor, the cat sat on the sofa, the cat sat on the

01:15:34.000 --> 01:15:40.240
whatever. So its minimal thing to do is just say, let's look at what we saw on the internet. We saw

01:15:40.960 --> 01:15:47.120
you know, 10,000 examples of the cat sat on the, what was the most probable next word?

01:15:47.120 --> 01:15:53.360
Let's just pick that out and say that's the next word. And that's kind of what it at some level

01:15:53.360 --> 01:15:59.760
is trying to do. Now the problem is, there isn't enough text on the internet to, if you have a

01:15:59.760 --> 01:16:06.560
reasonable length of prompt, that specific prompt will never have occurred on the internet. And as

01:16:06.880 --> 01:16:11.360
you kind of go further, there just won't be a place where you could have trained,

01:16:12.240 --> 01:16:17.520
where you could just worked out probabilities from what was already there. Like if you say two plus

01:16:17.520 --> 01:16:22.240
two, there'll be a zillion examples of two plus two equaling four and a very small number of

01:16:22.240 --> 01:16:27.200
examples of two plus two equals five and so on. And you can pretty much know what's going to happen.

01:16:27.200 --> 01:16:32.720
So then the question is, well, if you can't just work out from examples what's going to happen,

01:16:32.720 --> 01:16:37.120
just no probabilistic from examples what's going to happen, you have to have a model.

01:16:37.120 --> 01:16:42.720
And this kind of an idea, this idea of making models of things is an idea that really, I don't

01:16:42.720 --> 01:16:46.560
know, I think Galileo probably was one of the first people who sort of worked this out. I mean,

01:16:46.560 --> 01:16:52.880
it's kind of like, you know, I think I gave an example of that, the book I wrote about Chachi

01:16:52.880 --> 01:16:58.480
BT where it's kind of like, you know, Galileo was dropping cannonballs off the different floors of

01:16:58.880 --> 01:17:03.600
the Tower of Pisa and it's like, okay, you drop a cannonball off this floor, you drop a cannonball

01:17:03.600 --> 01:17:08.640
off this floor, you miss floor five or something for whatever reason, but you know the time it

01:17:08.640 --> 01:17:13.680
took the cannonball to fall to the ground from floors one, two, three, four, six, seven, eight,

01:17:13.680 --> 01:17:19.840
for example, then the question is, can you work out, can you make a model which figures out how

01:17:19.840 --> 01:17:24.960
long would it take the ball to fall to the ground from the floor you didn't explicitly measure?

01:17:25.600 --> 01:17:31.520
And the thing Galileo realized is that you can use math, you can use mathematical formulas to

01:17:31.520 --> 01:17:37.120
make a model for how long it will take the ball to fall. So now the question is, well,

01:17:37.120 --> 01:17:42.400
okay, you want to make a model for, for example, something much more elaborate, like you've got

01:17:42.400 --> 01:17:48.080
this arrangement of pixels, and is this arrangement of pixels an A or B? Does it correspond to

01:17:48.080 --> 01:17:53.280
something we'd recognize as an A or B? And you can make a similar kind, you know, each pixel is

01:17:53.280 --> 01:17:58.240
like a parameter in some equation, and you could write down this giant equation, where the answer

01:17:58.240 --> 01:18:04.000
is either, you know, A or, you know, one or two, A or B. And the question is then, what kind of a

01:18:04.000 --> 01:18:11.040
model successfully reproduces the way that we humans would, would conclude that this is an A

01:18:11.040 --> 01:18:17.120
and this is a B? You know, if there's a complicated extra tail on the top of the A, would we then

01:18:17.120 --> 01:18:22.960
conclude something different? What is the type of model that maps well into the way that we humans

01:18:22.960 --> 01:18:28.960
make distinctions about things? And the big kind of meta discovery is neural nets are such a model.

01:18:28.960 --> 01:18:34.560
It's not obvious they would be such a model. It could be that human distinctions are not captured.

01:18:34.560 --> 01:18:39.200
You know, we could try searching around for a type of model that could be a mathematical model. It

01:18:39.200 --> 01:18:43.760
could be some model based on something else that captures kind of typical human distinctions about

01:18:43.760 --> 01:18:50.080
things. It turns out this model that actually is very much the way that we think the architecture

01:18:50.080 --> 01:18:56.160
of brains works, that perhaps not surprisingly, that model actually corresponds to the way we

01:18:56.160 --> 01:19:02.240
make these distinctions. And so, you know, the core next point is that the kind of model, this

01:19:02.240 --> 01:19:09.040
neural net model makes sort of distinctions and generalizes things in sort of the same way that

01:19:09.040 --> 01:19:16.160
we humans do it. And that's why when you say, you know, the cat set on the green blank, even though

01:19:16.160 --> 01:19:22.400
it never didn't see many examples of the cat set on the green, whatever, it can make a or the odd

01:19:22.400 --> 01:19:28.240
vark sat on the green, whatever, I'm sure that particular sentence does not occur on the internet.

01:19:28.240 --> 01:19:34.400
And so it has to make a model that concludes what, you know, it has to kind of generalize from what

01:19:34.400 --> 01:19:41.200
it's from the actual examples that it's seen. And so, you know, that's the fact is that neural nets

01:19:41.200 --> 01:19:46.960
generalize in the same kind of way that we humans do. If we were, you know, the aliens

01:19:46.960 --> 01:19:51.440
might look at our neural net generalizations and say, that's crazy. You know, that thing when you

01:19:51.440 --> 01:19:56.960
put that extra little dot on the A, that isn't an A anymore. That's, you know, that messed the whole

01:19:56.960 --> 01:20:02.080
thing up. But for us humans, we make distinctions which seem to correspond to the kinds of

01:20:02.080 --> 01:20:07.440
distinctions that neural nets make. So then, you know, the thing that is just amazing to me about

01:20:07.440 --> 01:20:14.000
ChatGBT is how similar the structure it has is to the very original way people imagine neural nets

01:20:14.000 --> 01:20:20.080
might work back in 1943. And, you know, there's a lot of detailed engineering, you know, great

01:20:20.080 --> 01:20:25.920
cleverness, but it's really the same idea. And in fact, even the sort of elaborations of that idea

01:20:25.920 --> 01:20:30.240
where people said, let's put in some actual particular structure to try and make the neural

01:20:30.240 --> 01:20:35.040
net more elaborate to be very clever about it. Most of that didn't matter. I mean, there's some

01:20:35.040 --> 01:20:40.320
things that seem to, you know, when you train this neural net, you know, the one thing, this kind of

01:20:40.320 --> 01:20:46.160
transformer architecture, this attention idea, that really has to do with, does every one of

01:20:46.160 --> 01:20:52.080
these neurons connect to every other neuron? Or is it somehow causally localized, so to speak? Does

01:20:52.080 --> 01:20:58.320
it like we're making a sequence of words and the words depend on previous words, rather than just

01:20:58.320 --> 01:21:02.880
everything can depend on everything. And that seems to be important in just organizing things

01:21:02.880 --> 01:21:07.760
so that you don't have a sort of a giant mess. But the thing, you know, the thing worth understanding

01:21:07.760 --> 01:21:12.160
about what is chat GPT in the end? I mean, what is a neural net in the end? A neural net

01:21:12.160 --> 01:21:19.040
in the end is each neuron has a, it's taking inputs from a bunch of other neurons. It's

01:21:19.040 --> 01:21:25.600
eventually, it's going to have a numerical value. It's going to compute some number.

01:21:25.600 --> 01:21:31.440
And it's saying, I'm going to look at the neurons above me. It's kind of a series of layers. It's

01:21:31.520 --> 01:21:35.280
going to look at the neurons above me and it's going to say, what are the values of all those

01:21:35.280 --> 01:21:40.000
neurons? Then it's going to add those up and multiply them by these weights. And then it's

01:21:40.000 --> 01:21:44.960
going to apply some function that says if it's bigger than zero or something, then make it one or

01:21:44.960 --> 01:21:49.680
and otherwise make it zero or some slightly more complicated function. You know very well how this

01:21:49.680 --> 01:21:56.160
works. It's a giant equation with a lot of variables. You mentioned figuring out where

01:21:56.160 --> 01:22:02.880
the ball falls when you don't have data on the fourth floor. The equation here is not as simple

01:22:02.880 --> 01:22:09.680
as the equation. Right. It's an equation with 175 billion terms. And it's quite surprising that

01:22:09.680 --> 01:22:16.160
in some sense, a simple procedure of training such an equation can lead to

01:22:17.680 --> 01:22:23.280
a good representation of natural language. Right. The real issue is, you know, this architecture of

01:22:23.280 --> 01:22:29.600
a neural net where what's happening is, you know, you've turned, so neural nets always just deal

01:22:29.600 --> 01:22:34.320
with numbers. And so, you know, you've turned the sentence that you started with into a bunch of

01:22:34.320 --> 01:22:39.120
numbers. Like let's say by mapping, you know, each word of the 50,000 words in English, you just map

01:22:39.120 --> 01:22:45.600
each word or part of a word into some number. You feed all those numbers in and then the thing is

01:22:45.600 --> 01:22:51.520
going to, and then those numbers just go into the values of these neurons. And then what happens is

01:22:51.520 --> 01:22:56.560
it's just rippling down, going layer to layer until it gets to the end. I think ChatGBT has

01:22:56.560 --> 01:23:02.560
about 400 layers and you're just, you know, it just goes once through. It just, every new word

01:23:02.560 --> 01:23:08.080
it's going to compute just says, here are the numbers from the words before. Let's compute the,

01:23:08.080 --> 01:23:13.280
what does it compute? It computes the probabilities that it estimates for each of the possible 50,000

01:23:13.280 --> 01:23:18.240
words that could come next. And then it decides, sometimes it will use the most probable word,

01:23:18.320 --> 01:23:23.040
sometimes it will use not the most probable word. It's an interesting fact that there's this so-called

01:23:23.040 --> 01:23:27.520
temperature parameter, which, you know, at temperature zero, it's always using the most

01:23:27.520 --> 01:23:32.800
probable word that it estimated was the most probable thing to come next. You know, if you

01:23:32.800 --> 01:23:37.040
increase the temperature, it'll be more and more kind of random in its selection of words. It'll

01:23:37.040 --> 01:23:41.440
go down to lower and lower probability words. The thing I was just playing with actually recently

01:23:41.440 --> 01:23:45.520
was the transition that happens as you increase the temperature. The thing goes bonkers.

01:23:46.000 --> 01:23:50.640
At a particular, you know, sometimes at a particular temperature, and maybe about 1.2

01:23:50.640 --> 01:23:56.400
is the thing I was noticing from yesterday actually, that, you know, usually it's giving

01:23:56.400 --> 01:24:02.880
reasonable answers. And then at that temperature, with some probability, it just starts spouting

01:24:02.880 --> 01:24:10.640
nonsense. And, you know, nobody knows why this happens. I mean, it's, and by the way, I mean,

01:24:10.640 --> 01:24:16.480
the thing to understand is it's putting down one word at a time. But the outer loop of the fact

01:24:16.480 --> 01:24:20.880
that it says, okay, I put down a word. Now let's take the whole thing I wrote so far. Let's feed

01:24:20.880 --> 01:24:26.400
that back in. Let's put down another word. That outer loop, which seems almost trivial, is really

01:24:26.400 --> 01:24:31.840
important to the operation of the thing. And for example, one of the things that is kind of funky

01:24:31.840 --> 01:24:39.040
is it'll give an answer. And you say to it, is that answer correct? And it'll say no. And why

01:24:39.040 --> 01:24:44.480
is that happening? Why can it do that? Well, the answer is because it is going one word at a time,

01:24:44.480 --> 01:24:50.800
sort of forwards. And it didn't, you know, it came along with some sort of chain of thought,

01:24:50.800 --> 01:24:55.680
in a sense, and it came up with completely the wrong answer. But as soon as you feed it,

01:24:55.680 --> 01:25:00.960
the whole thing that it came up with, it immediately knows that that isn't right.

01:25:00.960 --> 01:25:05.040
It immediately can recognize that was a, you know, a bad syllogism or something,

01:25:05.520 --> 01:25:09.760
and can see what happened, even though as it was being led down this garden path,

01:25:09.760 --> 01:25:12.240
so to speak, it didn't, it came to the wrong place.

01:25:12.880 --> 01:25:19.360
But it's fascinating that this kind of procedure converges to something that forms a pretty good

01:25:19.360 --> 01:25:28.080
compressed representation of language on the internet. That's quite, I'm not sure what to make

01:25:28.080 --> 01:25:32.800
of it. Well, look, I think, you know, there are many things we don't understand. Okay, so for

01:25:32.800 --> 01:25:38.800
example, you know, 175 billion weights, it's maybe about a trillion bytes of information,

01:25:39.440 --> 01:25:45.760
which is very comparable to the training set that was used. And, you know, why that,

01:25:46.400 --> 01:25:51.200
why kind of, it sort of stands to some kind of reason that the number of weights in the neural

01:25:51.200 --> 01:25:57.040
net, I don't know, I can't really argue that. I can't really give you a good, you know, in a sense,

01:25:57.040 --> 01:26:02.080
the very fact that, you know, insofar as there are definite rules of what's going on, you might

01:26:02.080 --> 01:26:08.320
expect that eventually we'll have a much smaller neural net that will successfully capture what's

01:26:08.320 --> 01:26:12.720
happening. I don't think the best way to do it is probably a neural net. I think a neural net is

01:26:12.720 --> 01:26:17.120
what you do when you don't know any other way to structure the thing. And it's a very good thing

01:26:17.120 --> 01:26:20.880
to do if you don't know any other way to structure the thing. And for the last 2000 years, we haven't

01:26:20.880 --> 01:26:25.200
known any other way to structure it. So this is a pretty good way to start. But that doesn't mean

01:26:25.200 --> 01:26:31.360
you can't find sort of, in a sense, more symbolic rules for what's going on that, you know, much of

01:26:31.360 --> 01:26:37.520
which will then be, you can kind of get rid of much of the structure of the neural net and replace

01:26:37.520 --> 01:26:43.200
it by things which are sort of pure steps of computation, so to speak, sort of with neural net

01:26:43.200 --> 01:26:47.680
stuff around the edges. And that becomes just a, you know, just a much simpler way to do it.

01:26:47.680 --> 01:26:55.840
So the neural net, you hope, will reveal to us good symbolic rules that make the need of the neural

01:26:55.840 --> 01:26:57.200
net less and less and less.

01:26:57.200 --> 01:27:02.000
Right. And there will still be some stuff that's kind of fuzzy, just like, you know,

01:27:02.000 --> 01:27:06.880
there are things that it's like this question of what can we formalize? What can we turn into

01:27:06.880 --> 01:27:12.160
computational language? What is just sort of, oh, it happens that way just because brains are set

01:27:12.160 --> 01:27:20.080
up that way. What do you think are the limitations of large language models, just to make it explicit?

01:27:20.080 --> 01:27:24.320
Well, I mean, I think that deep computation is not what large language models do. I mean, that's

01:27:24.320 --> 01:27:28.480
just, it's a different kind of thing. You know, the outer loop of a large language model,

01:27:29.120 --> 01:27:34.080
if you're trying to do many steps in a computation, the only way you get to do that right now is by

01:27:34.080 --> 01:27:39.920
spooling out, you know, all the whole chain of thought as a bunch of words, basically. And,

01:27:39.920 --> 01:27:44.080
you know, you can make a Turing machine out of that if you want to. I just was doing that

01:27:44.080 --> 01:27:49.360
construction. You know, in principle, you can make an arbitrary computation by just spooling out the

01:27:49.360 --> 01:27:57.760
words, but it's a bizarre and inefficient way to do it. But it's something where the, you know,

01:27:57.760 --> 01:28:06.560
I think that's, you know, sort of the deep computation, it's really what humans can do

01:28:06.560 --> 01:28:11.840
quickly, large language models will probably be able to do well. Anything that you can do kind of

01:28:11.840 --> 01:28:17.120
off the top of your head type thing is really, you know, is good for large language models.

01:28:17.120 --> 01:28:19.760
And the things you do off the top of your head, you may not get them always right,

01:28:20.480 --> 01:28:25.920
but, you know, it's thinking it through the same way we do.

01:28:25.920 --> 01:28:29.280
But I wonder if there's an automated way to do something that humans do

01:28:30.320 --> 01:28:36.640
well, much faster to where it like loops. So, generate arbitrary large code bases of

01:28:37.200 --> 01:28:42.000
Wolfram language, for example. Well, the question is, what do you want the code base to do?

01:28:42.880 --> 01:28:47.920
Escape, control, and take over the world.

01:28:47.920 --> 01:28:53.520
Okay. So, you know, the thing is, when people say, you know, we want to build this giant

01:28:54.080 --> 01:29:00.400
thing, right? A giant piece of computational language. In a sense, it's sort of a failure

01:29:00.400 --> 01:29:04.960
of computational language. If the thing you have to build, in other words, if we have a description,

01:29:04.960 --> 01:29:11.120
if you have a small description, that's the thing that you represent in computational language.

01:29:11.120 --> 01:29:14.400
And then the computer can compute from that. Yes.

01:29:14.400 --> 01:29:22.080
So, in a sense, as soon as you're giving a description, you have to somehow make that

01:29:22.080 --> 01:29:29.680
description something definite, something formal. And to say, okay, I'm going to give this piece of

01:29:29.680 --> 01:29:36.560
natural language, and then it's going to split out this giant formal structure, that, in a sense,

01:29:37.520 --> 01:29:43.040
that doesn't really make sense. Because except insofar as that piece of natural language kind of

01:29:43.040 --> 01:29:48.080
plugs into what we socially know, so to speak, plugs into kind of our corpus of knowledge,

01:29:48.720 --> 01:29:51.920
then that's the way we're capturing a piece of that corpus of knowledge. But hopefully,

01:29:51.920 --> 01:29:56.880
we will have done that in computational language. How do you make it do something that's big?

01:29:57.520 --> 01:30:00.320
Well, you know, you have to have a way to describe what you want.

01:30:00.640 --> 01:30:08.080
I can make it more explicit if you want. How about I just pop into my head. Iterate through

01:30:08.080 --> 01:30:17.040
all the members of Congress and figure out how to convince them that they have to let me,

01:30:18.720 --> 01:30:24.640
meaning the system, become president, pass all the laws that allow AI systems to take control

01:30:24.640 --> 01:30:28.960
and be the president. I don't know. So that's a very explicit, like, figure out the individual

01:30:28.960 --> 01:30:35.040
life story of each congressman, each senator, anybody. I don't know what's required to really

01:30:35.040 --> 01:30:40.320
kind of pass legislation and figure out how to control them and manipulate them, get all

01:30:40.320 --> 01:30:47.440
the information. What would be the biggest fear of this congressman in such a way that you can

01:30:48.160 --> 01:30:53.600
take action on it in the digital space? So maybe threaten the destruction reputation or something

01:30:53.600 --> 01:30:59.360
like this. Right. If I can describe what I want, you know, to what extent can a large language

01:30:59.360 --> 01:31:06.800
model automate that? With the help of the concretization of something like Wolfram language

01:31:07.520 --> 01:31:12.320
that makes it more, yeah, grounded. I think it can go rather a long way.

01:31:12.320 --> 01:31:17.120
I'm also surprised how quickly I was able to generate an attack.

01:31:17.920 --> 01:31:24.160
That's a... I swear I did not think about this before and it's funny how quickly,

01:31:24.160 --> 01:31:27.520
which is a very concerning thing because that probably, this idea will probably do

01:31:27.520 --> 01:31:32.160
quite a bit of damage and there might be a very large number of other such ideas.

01:31:32.160 --> 01:31:36.720
Well, I'll give you a much more benign version of that idea, okay? You're going to make an AI

01:31:36.720 --> 01:31:44.240
tutoring system. And that's a benign version of what you're saying is, I want this person

01:31:44.240 --> 01:31:51.520
to understand this point. You're essentially doing machine learning where the loss function,

01:31:51.520 --> 01:31:57.040
the thing you're trying to get to is get the human to understand this point. And when you do a test

01:31:57.040 --> 01:32:02.560
on the human that they, yes, they correctly understand how this or that works. And I am

01:32:02.560 --> 01:32:09.600
confident that sort of a large language model type technology combined with computational language

01:32:09.600 --> 01:32:16.320
is going to be able to do pretty well at teaching us humans things. And it's going to be an

01:32:16.320 --> 01:32:24.080
interesting phenomenon because sort of individualized teaching is a thing that has been kind of a goal

01:32:24.080 --> 01:32:29.520
for a long time. I think we're going to get that. And I think more that it has many consequences for

01:32:31.440 --> 01:32:38.640
just... If you know me, if you, the AI know me, tell me, I'm about to do this thing.

01:32:39.120 --> 01:32:47.600
What are the three things I need to know, given what I already know? Let's say I'm looking at some

01:32:47.600 --> 01:32:54.240
paper or something, right? It's like there's a version of the summary of that paper that is

01:32:54.240 --> 01:32:59.680
optimized for me, so to speak, and where it really is. And I think that's really going to work.

01:32:59.680 --> 01:33:04.800
You can understand the major gaps in your knowledge that it filled would actually give you

01:33:05.520 --> 01:33:08.720
a deeper understanding of the topic. Yeah.

01:33:08.720 --> 01:33:15.120
Right. And that's an important thing because it really changes. Actually, I think when you think

01:33:15.120 --> 01:33:19.920
about education and so on, it really changes kind of what's worth doing, what's not worth doing,

01:33:19.920 --> 01:33:26.720
and so on. It makes... I know in my life, I've learned lots of different fields. And so I don't

01:33:26.720 --> 01:33:30.640
know. Every time I'm always think this is the one that's going to... I'm not going to be able to

01:33:30.640 --> 01:33:35.760
learn, but turns out there are sort of meta methods for learning these things in the end.

01:33:36.720 --> 01:33:46.880
And I think this idea that it becomes easier to be fed knowledge, so to speak, and it becomes... If

01:33:46.880 --> 01:33:54.560
you need to know this particular thing, you can get taught it in an efficient way. It's something

01:33:54.560 --> 01:34:01.600
I think is sort of an interesting feature. And I think it makes things like the value of

01:34:02.480 --> 01:34:09.120
big towers of specialized knowledge become less significant compared to the kind of meta knowledge

01:34:09.680 --> 01:34:15.200
of sort of understanding kind of the big picture and being able to connect things together.

01:34:15.200 --> 01:34:20.080
I think there's been this huge trend of let's be more and more specialized because we have to

01:34:20.720 --> 01:34:26.560
we have to sort of ascend these towers of knowledge. But by the time you can get more

01:34:26.560 --> 01:34:32.080
automation of being able to get to that place on the tower without having to go through all

01:34:32.080 --> 01:34:36.720
those steps, I think it sort of changes that picture. Interesting. So your intuition is that

01:34:36.720 --> 01:34:43.520
in terms of the collective intelligence of the species and the individual minds that make up that

01:34:43.520 --> 01:34:52.960
collective, there will trend towards being generalists and being kind of philosophers.

01:34:53.520 --> 01:34:56.480
That's what I think. I think that's where the humans are going to be useful.

01:34:56.480 --> 01:35:03.760
I think that a lot of these kind of the drilling, the mechanical working out of things

01:35:04.480 --> 01:35:09.360
is much more automatable. It's much more AI territory, so to speak.

01:35:09.360 --> 01:35:10.320
No more PhDs.

01:35:11.280 --> 01:35:17.760
Well, that's interesting. Yes. I mean, the kind of the specialization, this kind of tower of

01:35:17.760 --> 01:35:23.680
specialization, which has been a feature of we've accumulated lots of knowledge in our species.

01:35:23.680 --> 01:35:31.600
And in a sense, every time we have a kind of automation, a building of tools,

01:35:31.600 --> 01:35:35.760
it becomes less necessary to know that whole tower. And it becomes something where you can

01:35:35.760 --> 01:35:42.480
just use a tool to get to the top of that tower. I think that the thing that is ultimately when we

01:35:42.480 --> 01:35:48.480
think about, okay, what do the AIs do versus what do the humans do? It's like AIs, you tell them,

01:35:48.480 --> 01:35:53.600
you say, go achieve this particular objective. Okay, they can maybe figure out a way to achieve

01:35:53.600 --> 01:36:00.000
that objective. We say, what objective would you like to achieve? The AI has no intrinsic idea of

01:36:00.000 --> 01:36:07.680
that. It's not a defined thing. That's a thing which has to come from some other entity. And in

01:36:07.680 --> 01:36:13.600
so far as we are in charge, so to speak, or whatever it is, and our kind of web of society

01:36:13.600 --> 01:36:18.960
and history and so on is the thing that is defining what objective we want to go to.

01:36:21.040 --> 01:36:25.600
That's a thing that we humans are necessarily involved in, so to speak.

01:36:25.600 --> 01:36:30.560
To push back a little bit, don't you think that future versions of GPT

01:36:31.680 --> 01:36:34.880
will be able to give a good answer to what objective would you like to achieve?

01:36:37.280 --> 01:36:41.520
On what basis? I mean, if they say, look, here's the terrible thing that could happen,

01:36:41.520 --> 01:36:45.200
okay? They're taking the average of the internet and they're saying,

01:36:46.480 --> 01:36:49.120
from the average of the internet, what do people want to do?

01:36:49.120 --> 01:36:54.400
Well, that's the Elon Musk adage of the most entertaining outcome is the most likely.

01:36:55.840 --> 01:36:58.320
Okay. I haven't heard that one from him, yeah.

01:37:00.560 --> 01:37:10.080
That could be one objective, is maximize global entertainment. The dark version of that is drama.

01:37:10.720 --> 01:37:15.680
The good version of that is fun. Right. So I mean, this question of what,

01:37:16.320 --> 01:37:23.440
you know, if you say to the AI, you know, what does the species want to achieve?

01:37:24.560 --> 01:37:26.560
Yes. There'll be an answer, right?

01:37:27.280 --> 01:37:31.520
There'll be an answer. It'll be what the average of the internet says the species wants to achieve.

01:37:32.560 --> 01:37:41.600
Well, I think you're using the word average very loosely there, right? So I think the answers will

01:37:41.600 --> 01:37:45.120
become more and more interesting as these language models are trained better and better.

01:37:45.840 --> 01:37:50.160
No, but I mean, in the end, it's a reflection back of what we've already said.

01:37:50.960 --> 01:37:57.840
Yes. But there's a deeper wisdom to the collective intelligence, presumably than each individual.

01:37:57.840 --> 01:37:58.320
Maybe.

01:37:58.320 --> 01:38:00.000
Isn't that what we're trying to do as a society?

01:38:02.240 --> 01:38:09.200
Well, I mean, that's an important and an interesting question. I mean, in so far as

01:38:09.200 --> 01:38:13.600
some of us work on trying to innovate and figure out new things and so on,

01:38:13.600 --> 01:38:19.600
it is sometimes it's a complicated interplay between sort of the individual doing the crazy

01:38:19.600 --> 01:38:26.160
thing, often some spur, so to speak, versus the collective that's trying to do sort of the

01:38:27.520 --> 01:38:34.560
high inertia average thing. And it's, you know, sometimes the collective, you know, is bubbling

01:38:34.560 --> 01:38:39.360
up things that are interesting. And sometimes it's pulling down kind of the attempt to make

01:38:39.360 --> 01:38:41.440
this kind of innovative direction.

01:38:41.440 --> 01:38:46.000
But don't you think the large language models that see beyond that simplification will say

01:38:46.000 --> 01:38:50.560
maybe intellectual and career diversity is really important. So you need the crazy

01:38:50.560 --> 01:38:56.880
people on the outlier, on the outskirts. And so like the actual, what's the purpose of this whole

01:38:56.880 --> 01:39:04.720
thing is to explore through this kind of dynamics that we've been using as a human civilization,

01:39:04.720 --> 01:39:08.560
which is most of us focus on one thing. And then there's the crazy people on the outskirts

01:39:09.200 --> 01:39:12.880
doing the opposite of that one thing, and you kind of pull the whole society

01:39:12.880 --> 01:39:17.680
together. There's the mainstream science, and then there's the crazy science,

01:39:17.680 --> 01:39:22.960
that's just been the history of human civilization. And maybe the AI system will be able to see that.

01:39:22.960 --> 01:39:28.160
And the more and more impressed we are by a language model telling us this, the more control

01:39:28.160 --> 01:39:35.920
we'll give it to it. And the more we'll be willing to let it run our society. And hence,

01:39:35.920 --> 01:39:42.480
there's this kind of loop where the society could be manipulated to let the AI system run it.

01:39:42.480 --> 01:39:47.920
Right. Well, I mean, look, one of the things that's sort of interesting is we might say,

01:39:47.920 --> 01:39:55.440
we always think we're making progress. But yet, if in a sense, by saying, let's take

01:39:55.440 --> 01:40:03.200
what already exists and use that as a model for what should exist, then it's interesting that,

01:40:03.200 --> 01:40:07.680
for example, many religions have taken that point of view. There is a sacred book that

01:40:07.680 --> 01:40:16.080
got written at time X, and it defines how people should act for all future time. And it's a model

01:40:16.080 --> 01:40:23.360
that people have operated with. And in a sense, this is a version of that kind of statement.

01:40:23.360 --> 01:40:32.240
It's like, take the 2023 version of how the world has exposed itself and use that to define

01:40:32.720 --> 01:40:36.720
what the world should do in the future. But it's an imprecise definition,

01:40:36.720 --> 01:40:43.120
right? Because just like with the religious text and with GPT, the human interpretation of what GPT

01:40:43.120 --> 01:40:52.880
says will be the perturbation in the system. It'll be the noise. It'd be full of uncertainty.

01:40:52.880 --> 01:40:59.760
It's not like Chai GPT will tell you exactly what to do. It'll tell you a narrative of what

01:41:02.720 --> 01:41:07.360
turn the other cheek kind of narrative, right? That's not a fully instructive narrative.

01:41:07.360 --> 01:41:10.320
Well, until the AIs control all the systems in the world.

01:41:11.520 --> 01:41:14.000
They will be able to very precisely tell you what to do.

01:41:14.000 --> 01:41:20.000
Well, they'll do what they... They'll just do this or that thing. And not only that,

01:41:20.000 --> 01:41:26.080
they'll be auto-suggesting to each person, do this next, do that next. So I think it's a slightly

01:41:26.080 --> 01:41:33.440
more prescriptive situation than one has typically seen. But I think this whole question

01:41:33.440 --> 01:41:43.120
of sort of what's left for the human, so to speak, to what extent do we... This idea that

01:41:43.120 --> 01:41:48.800
there is an existing kind of corpus of purpose for humans defined by what's on the internet and so on,

01:41:49.280 --> 01:41:56.080
that's an important thing. But then the question of sort of as we explore what we can think of

01:41:56.080 --> 01:41:59.760
as the computational universe, as we explore all these different possibilities for what we

01:41:59.760 --> 01:42:03.440
could do, all these different inventions we could make, all these different things,

01:42:03.440 --> 01:42:08.560
the question is, which ones do we choose to follow? Those choices are the things that,

01:42:08.560 --> 01:42:17.200
in a sense, if the humans want to still have kind of human progress, that's what we get to

01:42:17.200 --> 01:42:25.440
make those choices, so to speak. In other words, there's this idea, if you say, let's take the

01:42:27.040 --> 01:42:32.640
what exists today and use that as the determiner of all of what there is in the future,

01:42:33.360 --> 01:42:38.480
the thing that is sort of the opportunity for humans is there will be many possibilities thrown

01:42:38.480 --> 01:42:45.600
up. There are many different things that could happen or be done. And insofar as we want to be

01:42:45.600 --> 01:42:50.320
in the loop, the thing that makes sense for us to be in the loop doing is picking which of those

01:42:50.320 --> 01:42:58.000
possibilities we want. But the degree to which there's a feedback loop, the idea that we're

01:42:58.000 --> 01:43:02.560
picking something starts becoming questionable because we're influenced by the various systems.

01:43:02.560 --> 01:43:08.000
Absolutely. If that becomes more and more a source of our education and wisdom and knowledge.

01:43:09.440 --> 01:43:15.440
Right. The AIs take over. I've thought for a long time that it's the AR

01:43:15.440 --> 01:43:20.240
auto-suggestion that's really the thing that makes the AIs take over. It's just that the

01:43:20.240 --> 01:43:26.240
humans just follow. Yeah. We will no longer write emails to each other. We'll just send the

01:43:26.240 --> 01:43:32.320
auto-suggested email. Yeah. Yeah. But the thing where humans are potentially in the loop is when

01:43:32.320 --> 01:43:38.560
there's a choice and when there's a choice which we could make based on our kind of whole web of

01:43:38.560 --> 01:43:48.880
history and so on. And that's insofar as it's all just determined the humans don't have a place.

01:43:49.520 --> 01:43:56.640
And by the way, at some level, it's all kind of a complicated philosophical issue because

01:43:56.640 --> 01:44:03.680
at some level, the universe is just doing what it does. We are parts of that universe that are

01:44:03.680 --> 01:44:10.080
necessarily doing what we do, so to speak. Yet we feel we have sort of agency in what we're doing

01:44:10.080 --> 01:44:14.880
and that's its own separate kind of interesting issue. And we also kind of feel like we're the

01:44:14.880 --> 01:44:23.040
final destination of what the universe was meant to create. But we very well could be and likely

01:44:23.040 --> 01:44:29.680
are some kind of intermediate step, obviously. Yeah. Well, we're most certainly some intermediate

01:44:29.680 --> 01:44:35.760
step. The question is if there's some cooler, more complex, more interesting things that's

01:44:35.760 --> 01:44:40.080
going to be materialized. The computational universe is full of such things. But in our

01:44:40.080 --> 01:44:45.200
particular pocket specifically, if this is the best we're going to do or not, that's kind of a-

01:44:45.200 --> 01:44:50.080
We can make all kinds of interesting things in the computational universe. When we look at them,

01:44:50.720 --> 01:44:58.320
we say, yeah, that's a thing. It doesn't really connect with our current way of thinking about

01:44:58.320 --> 01:45:02.400
things. It's like in mathematics. We've got certain theorems. There are about three or four

01:45:02.400 --> 01:45:07.200
million that human mathematicians have written down and published and so on. But there are an

01:45:07.200 --> 01:45:11.440
infinite number of possible mathematical theorems. We just go out into the universe of possible

01:45:11.440 --> 01:45:17.440
theorems and pick another theorem. And then people will say, well, they look at it and they say,

01:45:17.440 --> 01:45:22.960
I don't know what this theorem means. It's not connected to the things that are part of kind of

01:45:22.960 --> 01:45:27.360
the web of history that we're dealing with. I think one point to make about sort of understanding

01:45:27.360 --> 01:45:33.360
AI and its relationship to us is as we have this kind of whole infrastructure of AIs doing

01:45:33.360 --> 01:45:39.040
their thing and doing their thing in a way that is perhaps not readily understandable by us humans,

01:45:39.920 --> 01:45:45.520
you might say that's a very weird situation. How can we have built this thing that behaves in a

01:45:45.520 --> 01:45:49.120
way that we can't understand, that's full of computational irreducibility, et cetera, et

01:45:49.120 --> 01:45:55.120
cetera, et cetera? What is this? What's it going to feel like when the world is run by AIs whose

01:45:55.120 --> 01:46:00.880
operations we can't understand? And the thing one realizes is actually we've seen this before.

01:46:00.880 --> 01:46:05.760
That's what happens when we exist in the natural world. The natural world is full of things that

01:46:05.760 --> 01:46:10.480
operate according to definite rules. They have all kinds of computational irreducibility. We

01:46:10.480 --> 01:46:16.240
don't understand what the natural world is doing. Occasionally, when you say, are the AIs going to

01:46:16.240 --> 01:46:21.840
wipe us out, for example? Well, it's kind of like, is the machination of the AIs going to lead to

01:46:21.840 --> 01:46:26.560
this thing that eventually comes and destroys the species? Well, we can also ask the same thing

01:46:26.560 --> 01:46:30.880
about the natural world, or the machination of the natural world going to eventually lead to

01:46:30.880 --> 01:46:37.040
this thing that's going to make the earth explode or something like this. Those are questions.

01:46:38.000 --> 01:46:43.040
Insofar as we think we understand what's happening in the natural world, that's a result of science

01:46:43.040 --> 01:46:48.080
and natural science and so on. One of the things we can expect when there's this giant infrastructure

01:46:48.080 --> 01:46:54.720
of the AIs is that's where we have to invent a new kind of natural science that is the natural

01:46:54.720 --> 01:47:02.160
science that explains to us how the AIs work. It's kind of like we have a horse or something,

01:47:02.160 --> 01:47:07.520
and we're trying to ride the horse and go from here to there. We don't really understand how

01:47:07.520 --> 01:47:13.760
the horse works inside, but we can get certain rules and certain approaches that we take to

01:47:13.760 --> 01:47:19.120
persuade the horse to go from here to there and take us there. That's the same type of thing that

01:47:19.120 --> 01:47:24.480
we're kind of dealing with, with the sort of incomprehensible, computationally irreducible

01:47:24.480 --> 01:47:30.480
AIs, but we can identify these kinds of, we can find these kinds of pockets of reducibility

01:47:30.480 --> 01:47:36.720
that we can kind of, I don't know, we're grabbing onto the mane of the horse or something

01:47:36.800 --> 01:47:44.160
to be able to ride it. We figure out if we do this or that to ride the horse,

01:47:44.160 --> 01:47:48.880
that that's a successful way to get it to do what we're interested in doing.

01:47:48.880 --> 01:47:56.320
There does seem to be a difference between a horse and a large language model

01:47:57.760 --> 01:48:02.080
or something that could be called AGI connected to the internet. Let me just ask you about

01:48:02.640 --> 01:48:06.800
big philosophical question about the threats of these things. There's a lot of people like

01:48:06.800 --> 01:48:13.200
Eliezer Idkowski who worry about the existential risks of AI systems. Is that something

01:48:15.280 --> 01:48:20.400
that you worry about? You know, sometimes when you're building an incredible system like Wolfram

01:48:20.400 --> 01:48:25.920
Alpha, you can kind of get lost in it. Oh, I try and think a little bit about

01:48:25.920 --> 01:48:29.840
the implications of what one's doing. You know, it's like the Manhattan project

01:48:29.840 --> 01:48:34.000
kind of situation where you're like, it's some of the most incredible physics and engineering

01:48:34.000 --> 01:48:39.440
being done, but it's like, huh, where's this going to go? I think some of these arguments about kind

01:48:39.440 --> 01:48:43.440
of, you know, there'll always be a smarter AI, there'll always be, you know, and eventually

01:48:43.440 --> 01:48:48.080
the AIs will get smarter than us and then all sorts of terrible things will happen. To me,

01:48:48.080 --> 01:48:52.960
some of those arguments remind me of kind of the ontological arguments for the essence of God and

01:48:52.960 --> 01:48:58.160
things like this. They're kind of arguments that are based on some particular model,

01:48:58.160 --> 01:49:02.400
fairly simple model often of kind of there is always a greater this, that, and the other.

01:49:02.400 --> 01:49:07.440
You know, this is, and that's, you know, those arguments tend, what tends to happen

01:49:07.440 --> 01:49:11.120
in the sort of reality of how these things develop is that it's more complicated than

01:49:11.120 --> 01:49:15.440
you expect. That the kind of simple logical argument that says, oh, eventually there'll

01:49:15.440 --> 01:49:20.400
be a super intelligence and then it will, you know, do this and that turns out not to really

01:49:20.400 --> 01:49:24.480
be the story. It turns out to be a more complicated story. So for example, here's an example of an

01:49:24.480 --> 01:49:30.240
issue. Is there an apex intelligence? Just like there might be an apex predator in some,

01:49:30.240 --> 01:49:34.880
you know, ecosystem, is there going to be an apex intelligence? The most intelligent thing

01:49:34.880 --> 01:49:40.080
that there could possibly be, right? I think the answer is no. And in fact, we already know this

01:49:40.080 --> 01:49:45.440
and it's a kind of a back to the whole computational irreducibility story. There's kind of a question

01:49:45.440 --> 01:49:54.400
of, you know, even if you have sort of a Turing machine and you have a Turing machine that

01:49:55.200 --> 01:50:00.480
runs as long as possible before it halts, you say, is this the machine, is this the apex

01:50:00.480 --> 01:50:05.280
machine that does that? There will always be a machine that can go longer. And as you go out

01:50:05.280 --> 01:50:09.280
to the infinite collection of possible Turing machines, you'll never have reached the end,

01:50:09.280 --> 01:50:14.080
so to speak. You'll never, you'll always be able to, it's kind of like the same question of whether

01:50:14.080 --> 01:50:17.920
there'll always be another invention. Will you always be able to invent another thing?

01:50:17.920 --> 01:50:21.520
The answer is yes. There's an infinite tower of possible inventions.

01:50:21.520 --> 01:50:28.720
That's one definition of apex. But the other is like, which I also thought you are,

01:50:28.720 --> 01:50:33.920
which I also think might be true, is there a species that's the apex intelligence right now

01:50:33.920 --> 01:50:40.960
on earth? So it's not trivial to say that humans are that. Yeah, it's not trivial. I agree. It's a,

01:50:40.960 --> 01:50:47.200
you know, I think one of the things that I've long been curious about kind of other intelligences,

01:50:47.200 --> 01:50:56.640
so to speak. I mean, I view intelligence is like computation and it's kind of a, you know, you're

01:50:56.640 --> 01:51:03.680
sort of, you have the set of rules, you deduce what happens. I have tended to think now that

01:51:03.680 --> 01:51:09.680
there's this sort of specialization of computation that is sort of a consciousness-like thing that

01:51:09.680 --> 01:51:14.080
has to do with these, you know, computational boundedness, single thread of experience,

01:51:14.080 --> 01:51:20.080
these kinds of things that are the specialization of computation that corresponds to a somewhat

01:51:20.080 --> 01:51:26.000
human-like experience of the world. Now the question is, so that's, you know, there may be

01:51:26.000 --> 01:51:31.520
other intelligences like, you know, the aphorism, you know, the weather has a mind of its own.

01:51:31.520 --> 01:51:36.160
It's a different kind of intelligence that can compute all kinds of things that are hard for

01:51:36.160 --> 01:51:42.240
us to compute, but it is not well aligned with us, with the way that we think about things. It

01:51:42.240 --> 01:51:49.200
doesn't think the way we think about things. And, you know, in this idea of different intelligences,

01:51:49.200 --> 01:51:54.240
every different mind, every different human mind is a different intelligence that thinks about

01:51:54.240 --> 01:51:59.200
things in different ways. And, you know, in terms of the kind of formalism of our physics project,

01:51:59.200 --> 01:52:04.720
we talk about this idea of a ruleial space, the space of all possible sort of rule systems,

01:52:04.720 --> 01:52:10.960
and different minds are in a sense at different points in ruleial space. Human minds, ones that

01:52:10.960 --> 01:52:14.960
have grown up with the same kind of culture and ideas and things like this might be pretty close

01:52:14.960 --> 01:52:19.360
in ruleial space, pretty easy for them to communicate, pretty easy to translate, pretty

01:52:19.360 --> 01:52:24.640
easy to move from one place in ruleial space that corresponds to one mind to another place in ruleial

01:52:24.640 --> 01:52:30.000
space that corresponds to another sort of nearby mind. When we deal with kind of more distant

01:52:30.560 --> 01:52:37.280
things in ruleial space, like, you know, the pet cat or something, you know, the pet cat has some

01:52:37.280 --> 01:52:42.560
aspects that are shared with us, the emotional responses of the cat are somewhat similar to ours,

01:52:42.560 --> 01:52:48.960
but the cat is further away in ruleial space than people are. And so then the question is, you know,

01:52:48.960 --> 01:52:55.040
can we identify sort of the, can we make a translation from our thought processes to the

01:52:55.040 --> 01:53:01.520
thought processes of a cat or something like this? And, you know, what will we get when we, you know,

01:53:01.520 --> 01:53:06.080
what will happen when we get there? And I think it's the case that many, you know, many animals,

01:53:06.080 --> 01:53:10.560
I don't know, dogs, for example, you know, they have elaborate olfactory systems, they,

01:53:10.560 --> 01:53:16.960
you know, they have sort of the smell architecture of the world, so to speak, in a way that we don't.

01:53:17.520 --> 01:53:23.520
And so, you know, if you were sort of talking to the dog and you could, you know, communicate

01:53:23.520 --> 01:53:30.160
in a language, the dog will say, well, this is a, you know, a, you know, a flowing smelling,

01:53:30.160 --> 01:53:34.560
this, that, and the other thing, concepts that we just don't have any idea about.

01:53:35.360 --> 01:53:41.680
Now, what's, what's interesting about that is, one day we will have chemical sensors that do a

01:53:41.680 --> 01:53:45.600
really pretty good job, you know, we'll have artificial noses that work pretty well, and we

01:53:45.600 --> 01:53:50.880
might have our augmented reality systems show us kind of the same map that the dog could see and

01:53:50.880 --> 01:53:55.600
things like this, you know, similar to what happens in the dog's brain. And eventually,

01:53:55.600 --> 01:54:01.200
we will have kind of expanded in ruleial space to the point where we will have those same sensory

01:54:01.200 --> 01:54:06.320
experiences that dogs have, and we will have internalized what it means to have, you know,

01:54:06.320 --> 01:54:11.440
the smell landscape or whatever. And, and so then we will have kind of colonized that part

01:54:11.440 --> 01:54:18.240
of ruleial space until, you know, we haven't gone, you know, some things that, that, you know,

01:54:18.240 --> 01:54:23.920
animals and so on do, we sort of successfully understand, others we do not. And the question

01:54:23.920 --> 01:54:31.120
of, of what kind of, what is the, you know, what, what representation, you know, how, how do we

01:54:31.120 --> 01:54:36.320
convert things that animals think about to things that we can think about? That's not a trivial

01:54:36.320 --> 01:54:42.640
thing. And, you know, I've, I've long been curious, I had a very bizarre project at one point of,

01:54:42.640 --> 01:54:46.640
of trying to make an iPad game that a cat could win against its owner.

01:54:46.640 --> 01:54:50.560
I said, it feels like there's a deep philosophical goal there, though.

01:54:51.360 --> 01:54:56.800
Yes. Yes. I mean, the, the, you know, I was curious if, you know, if pets can work in

01:54:56.800 --> 01:55:01.680
Minecraft or something and can construct things, what will they construct? And will what they

01:55:01.680 --> 01:55:05.760
construct be something where we look at it and we say, oh yeah, I recognize that. Or will it be

01:55:05.760 --> 01:55:10.160
something that looks to us like something that's out there in the computational universe that one

01:55:10.160 --> 01:55:15.360
of my cellular automata might have produced where we say, oh yeah, I can kind of see it operates

01:55:15.360 --> 01:55:18.480
according to some rules. I don't know why you would use those rules. I don't know why you would care.

01:55:18.560 --> 01:55:23.520
Yeah. I actually, just to linger on that seriously, is there a connector in the

01:55:23.520 --> 01:55:29.680
royal space between you and a cat where the cat could legitimately win? So iPad is a very limited

01:55:30.880 --> 01:55:35.040
interface. I wonder if there's a game where cats win.

01:55:35.600 --> 01:55:39.280
I think the problem is that cats don't tend to be that interested in what's happening on the iPad.

01:55:40.240 --> 01:55:45.840
Yeah. That's an interface issue problem. Yeah. Right. Right. Right. No, I think it is likely

01:55:45.840 --> 01:55:51.280
that, I mean, you know, there are plenty of animals that would successfully eat us if we were,

01:55:51.280 --> 01:55:56.800
you know, if we were exposed to them. And so there's, you know, it's going to pounce faster

01:55:56.800 --> 01:56:02.080
than we can get out of the way and so on. So there are plenty of, and probably it's going to,

01:56:02.080 --> 01:56:05.440
you know, we think we've hidden ourselves, but we haven't successfully hidden ourselves.

01:56:05.440 --> 01:56:10.240
That's a physical strength. I wonder if there's something more in the realm of

01:56:10.320 --> 01:56:14.800
intelligence where an animal like a cat could out.

01:56:14.800 --> 01:56:19.520
Well, I think there are things certainly in terms of the speed of processing certain kinds of things,

01:56:19.520 --> 01:56:25.120
for sure. I mean, the question of what, you know, is there a game of chess, for example,

01:56:25.120 --> 01:56:30.240
is there cat chess that the cats could play against each other? And if we tried to play a

01:56:30.240 --> 01:56:35.120
cat, we'd always lose. I don't know. It might have to do with speed, but it might have to do

01:56:35.120 --> 01:56:38.880
with concepts also. There might be concepts in the cat's head.

01:56:39.920 --> 01:56:46.000
I tend to think that our species from its invention of language has managed to build

01:56:46.000 --> 01:56:52.880
up this kind of tower of abstraction that for things like a chess-like game will make us win.

01:56:52.880 --> 01:56:58.560
In other words, we've become through the fact that we've kind of experienced language and learnt

01:56:58.560 --> 01:57:03.600
abstraction, you know, we've sort of become smarter at those kinds of abstract kinds of things.

01:57:03.600 --> 01:57:08.960
Now, you know, that doesn't make us smarter at catching a mouse or something. It makes us

01:57:08.960 --> 01:57:14.880
smarter at the things that we've chosen to sort of, you know, concern ourselves, which are these

01:57:14.880 --> 01:57:19.840
kinds of abstract things. And I think, you know, this is again back to the question of, you know,

01:57:19.840 --> 01:57:25.920
what does one care about? You know, if one's, you know, the cat, if you have the discussion

01:57:25.920 --> 01:57:30.640
with a cat, if we can translate things to have the discussion with a cat, the cat will say,

01:57:31.280 --> 01:57:40.560
you know, I'm very excited that this light is moving. And we'll say, why do you care?

01:57:40.560 --> 01:57:45.120
And the cat will say, that's the most important thing in the world, that this thing moves around.

01:57:45.120 --> 01:57:50.800
I mean, it's like when you ask about, I don't know, you look at archaeological remains and you say,

01:57:50.800 --> 01:57:55.360
these people had this, you know, belief system about this and, you know, that was the most

01:57:55.360 --> 01:57:59.920
important thing in the world to them. And now we look at it and say, we don't know what the point

01:57:59.920 --> 01:58:04.640
of it was. I mean, I've been curious, you know, there are these handprints on caves from 20,000

01:58:04.640 --> 01:58:09.760
or more years ago. And it's like, nobody knows what these handprints were there for, you know,

01:58:09.760 --> 01:58:14.160
that they may have been a representation of the most important thing you can imagine.

01:58:14.160 --> 01:58:19.120
They may just have been some, you know, some kid who rubbed their hands in the mud and stuck them

01:58:19.120 --> 01:58:25.440
on the walls of the cave. You know, we don't know. And I think, but this whole question of what,

01:58:26.400 --> 01:58:32.880
you know, is when you say this question of sort of what's the smartest thing around,

01:58:33.520 --> 01:58:38.960
there's the question of what kind of computation you're trying to do. If you're saying, you know,

01:58:38.960 --> 01:58:44.240
if you say you've got some well-defined computation and how do you implement it?

01:58:44.240 --> 01:58:49.360
Well, you could implement it by nerve cells, you know, firing. You can implement it with silicon

01:58:49.360 --> 01:58:54.720
and electronics. You can implement it by some kind of molecular computation process

01:58:54.720 --> 01:58:58.720
in the human immune system or in some molecular biology kind of thing.

01:58:58.720 --> 01:59:05.760
There are different ways to implement it. And, you know, I think this question of sort of which,

01:59:05.760 --> 01:59:09.440
you know, those different implementation methods will be at different speeds. They'll be able to

01:59:09.440 --> 01:59:15.040
do different things. If you say, you know, which, so an interesting question would be

01:59:17.280 --> 01:59:22.800
what kinds of abstractions are most natural in these different kinds of systems? So for a cat,

01:59:23.520 --> 01:59:29.040
it's, for example, you know, the visual scene that we see, you might, you know,

01:59:29.040 --> 01:59:35.200
we pick out certain objects, we recognize, you know, certain things in that visual scene.

01:59:35.200 --> 01:59:40.640
A cat might in principle recognize different things. I suspect, you know, evolution,

01:59:40.640 --> 01:59:45.920
biological evolution is very slow. And I suspect what a cat notices is very similar. We even know

01:59:45.920 --> 01:59:50.880
that from some neurophysiology. What a cat notices is very similar to what we notice. Of course,

01:59:51.840 --> 01:59:57.840
one obvious difference is cats have only two kinds of color receptors, so they don't see in the same

01:59:57.840 --> 02:00:02.640
kind of color that we do. Now, you know, we say we're better. We have three color receptors,

02:00:02.640 --> 02:00:08.560
you know, red, green, blue. We're not the overall winner. I think the mantis shrimp

02:00:08.560 --> 02:00:14.880
is the overall winner with 15 color receptors, I think. So it can kind of make distinctions

02:00:14.960 --> 02:00:22.800
that with our current, you know, like the mantis shrimps view of reality is, at least in terms of

02:00:22.800 --> 02:00:29.040
color, is much richer than ours. Now, but what's interesting is how do we get there? So imagine we

02:00:29.040 --> 02:00:33.280
have this augmented reality system that is even, you know, it's seeing into the infrared, into the

02:00:33.280 --> 02:00:39.040
ultraviolet, things like this. And it's translating that into something that is connectable to our

02:00:39.040 --> 02:00:45.840
brains, either through our eyes or more directly into our brains. You know, then eventually our

02:00:45.840 --> 02:00:52.000
kind of web of the types of things we understand will extend to those kinds of constructs just as

02:00:52.000 --> 02:00:57.040
they have extended. I mean, there are plenty of things where we see them in the modern world

02:00:57.040 --> 02:01:02.000
because we made them with technology, and now we understand what that is. But if we'd never seen

02:01:02.000 --> 02:01:06.000
that kind of thing, we wouldn't have a way to describe it. We wouldn't have a way to understand

02:01:06.000 --> 02:01:12.560
it and so on. All right. So that actually stemmed from our conversation about whether AI is going

02:01:12.560 --> 02:01:20.400
to kill all of us. And we've discussed this kind of spreading of intelligence through really all

02:01:20.400 --> 02:01:26.480
space that in practice, it just seems that things get more complicated. Things are more complicated

02:01:26.480 --> 02:01:33.360
than the story of, well, if you build the thing that's plus one intelligence, that thing will be

02:01:33.440 --> 02:01:37.680
able to build the thing that's plus two intelligence and plus three intelligence,

02:01:37.680 --> 02:01:43.440
and that will be exponential. It'll become more intelligent exponentially faster and so on

02:01:43.440 --> 02:01:50.400
until it completely destroys everything. But that intuition might still not be so simple,

02:01:50.400 --> 02:01:57.120
but it might still carry validity. And there's two interesting trajectories here. One,

02:01:57.920 --> 02:02:05.440
a superintelligence system remains in ruleial proximity to humans, to where we're like,

02:02:05.440 --> 02:02:10.720
holy crap, this thing is really intelligent. Let's elect the president. And then there

02:02:10.720 --> 02:02:17.440
could be perhaps more terrifying intelligence that starts moving away. They might be around us

02:02:17.440 --> 02:02:23.760
now. They're moving far away in ruleial space, but they're still sharing physical resources with us,

02:02:24.720 --> 02:02:29.440
so they can rob us of those physical resources and destroy humans just kind of casually.

02:02:32.160 --> 02:02:32.960
Like nature could.

02:02:32.960 --> 02:02:37.840
Like nature could. But it seems like there's something unique about AI systems where

02:02:40.480 --> 02:02:48.720
there is this kind of exponential growth. Well, sorry, nature has so many things in it.

02:02:48.720 --> 02:02:52.320
One of the things that nature has, which is very interesting, are viruses, for example.

02:02:53.040 --> 02:02:56.640
There are systems within nature that have this kind of exponential effect,

02:02:57.760 --> 02:03:04.560
and that terrifies us humans because again, there's only 8 billion of us and it's not that

02:03:04.560 --> 02:03:10.000
hard to just kind of whack them all real quick. So is that something you think about?

02:03:10.960 --> 02:03:12.240
Yeah, I thought about that.

02:03:12.240 --> 02:03:12.880
Yes.

02:03:12.880 --> 02:03:18.160
The threat of it. Are you as concerned about it as somebody like Eliezer Yarkovsky, for example?

02:03:18.160 --> 02:03:23.200
Just big, big, painful negative effects of AI on society.

02:03:24.080 --> 02:03:30.880
You know, no, but perhaps that's because I'm intrinsically an optimist. I mean, I think that

02:03:30.880 --> 02:03:39.280
there are things, I think the thing that one sees is there's going to be this one thing and it's

02:03:39.280 --> 02:03:46.080
going to just zap everything. Somehow, maybe I have faith in computational irreducibility,

02:03:46.080 --> 02:03:52.080
so to speak, that there's always unintended little corners. It's just like somebody says,

02:03:52.080 --> 02:03:56.800
I'm going to, I don't know, somebody has some bioweapon and they say, we're going to release

02:03:56.800 --> 02:04:01.200
this and it's going to do all this harm. But then it turns out it's more complicated than that

02:04:01.200 --> 02:04:08.160
because some humans are different and the exact way it works is a little different than you expect.

02:04:08.160 --> 02:04:15.360
It's something where the great big, you smash the thing with something. The asteroid collides

02:04:15.360 --> 02:04:21.200
with the Earth and it kind of, and yes, the Earth is cold for two years or something and

02:04:22.480 --> 02:04:30.480
then lots of things die, but not everything dies. This is in a sense the sort of story

02:04:30.480 --> 02:04:34.320
of computational irreducibility. There are always unexpected corners. There are always

02:04:34.320 --> 02:04:39.440
unexpected consequences and I don't think that they kind of whack it over the head with something

02:04:39.440 --> 02:04:44.400
and then it's all gone. That can obviously happen. The Earth can be swallowed up in a

02:04:44.400 --> 02:04:48.800
black hole or something and then it's kind of presumably, presumably all over.

02:04:51.200 --> 02:04:58.880
I think this question of what do I think the realistic paths are, I think that there will be

02:04:58.880 --> 02:05:04.720
sort of an increasing, I mean that people have to get used to phenomena like computational

02:05:04.720 --> 02:05:09.680
irreducibility. There's an idea that we built the machines so we can understand what they do

02:05:10.400 --> 02:05:16.080
and we're going to be able to control what happens. Well, that's not really right.

02:05:16.080 --> 02:05:22.720
Now the question is, is the result of that lack of control going to be that the machines kind of

02:05:22.720 --> 02:05:28.000
conspire and sort of wipe us out? Maybe just because I'm an optimist, I don't tend to think

02:05:28.000 --> 02:05:36.800
that that's in the cards. I think that as a realistic thing, I suspect what will sort of

02:05:36.800 --> 02:05:43.840
emerge maybe is kind of an ecosystem of the AIs just as, again, I don't really know. This

02:05:43.840 --> 02:05:51.760
is something, it's hard to be clear about what will happen. I think that there are a lot of

02:05:51.760 --> 02:05:58.560
sort of details of what could we do? What systems in the world could we connect an AI to? I have to

02:05:58.560 --> 02:06:05.680
say, I was just a couple of days ago, I was working on this chat GBT plugin kit that we have for

02:06:05.680 --> 02:06:13.280
Wolfram Language, where you can create a plugin and it runs Wolfram Language code and it can run

02:06:13.280 --> 02:06:19.200
Wolfram Language code back on your own computer. I was thinking, well, I can just make it, I can

02:06:19.200 --> 02:06:26.560
tell chat GBT create a piece of code and then just run it on my computer. That sort of personalizes

02:06:26.560 --> 02:06:32.480
for me what could possibly go wrong, so to speak. Was that exciting or scary, that possibility?

02:06:33.120 --> 02:06:38.800
It was a little bit scary, actually, because it's kind of like I realize I'm delegating to the AI,

02:06:38.800 --> 02:06:43.680
just write a piece of code. You're in charge, write a piece of code, run it on my computer

02:06:44.400 --> 02:06:49.680
and pretty soon all my files can be deleted. That's like Russian related, but much more

02:06:49.680 --> 02:06:54.080
complicated version of that. Yes, yes, right. That's a good drinking game. I don't know.

02:06:54.800 --> 02:07:03.440
Well, right. It's an interesting question then. If you do that, what is the sandboxing that you

02:07:03.440 --> 02:07:09.360
should have? That's a version of that question for the world. That is, as soon as you put the

02:07:09.360 --> 02:07:15.920
AIs in charge of things, how many constraints should there be on these systems before you put

02:07:15.920 --> 02:07:20.160
the AIs in charge of all the weapons and all these different kinds of systems?

02:07:20.160 --> 02:07:27.920
Well, here's the fun part about sandboxes is the AI knows about them and has the tools to crack them.

02:07:28.560 --> 02:07:32.880
Look, the fundamental problem of computer security is computational irreducibility,

02:07:32.880 --> 02:07:40.800
because the fact is any sandbox is never going to be a perfect sandbox. If you want the system

02:07:40.800 --> 02:07:46.160
to be able to do interesting things, this is the problem that's happened, the generic problem of

02:07:46.160 --> 02:07:51.520
computer security, that as soon as you have your firewall that is sophisticated enough to be a

02:07:51.520 --> 02:07:57.760
universal computer, that means it can do anything. So long as if you find a way to poke it so that

02:07:57.760 --> 02:08:04.480
you actually get it to do that universal computation thing, that's the way you crawl around and get it

02:08:04.480 --> 02:08:10.880
to do the thing that it wasn't intended to do. That's another version of computational irreducibility

02:08:10.880 --> 02:08:15.280
is you can get it to do the thing you didn't expect it to do, so to speak.

02:08:16.480 --> 02:08:21.840
There's so many interesting possibilities here that manifest themselves from the computational

02:08:21.840 --> 02:08:28.160
irreducibility here. It's just so many things can happen, because in digital space, things move so

02:08:28.160 --> 02:08:33.600
quickly. You can have a chatbot, you can have a piece of code that you can basically have chat GPT

02:08:33.600 --> 02:08:41.440
generate viruses accidentally or on purpose, digital viruses, and they could be brain viruses

02:08:41.440 --> 02:08:47.520
too. They convince, kind of like phishing emails, they can convince you of stuff.

02:08:47.520 --> 02:08:54.080
Yes, and no doubt you can, in a sense, we've had the loop of the machine learning loop of making

02:08:54.080 --> 02:09:00.960
things that convince people of things is surely going to get easier to do. Then what does that

02:09:00.960 --> 02:09:09.760
look like? Well, it's again, we humans, this is a new environment for us, and admittedly,

02:09:09.760 --> 02:09:15.520
it's an environment which a little bit scarily is changing much more rapidly than, I mean,

02:09:15.520 --> 02:09:20.240
people worry about climate change is going to happen over hundreds of years, and the environment

02:09:20.240 --> 02:09:26.720
is changing, but the kind of digital environment might change in six months.

02:09:27.920 --> 02:09:36.560
One of the relevant concerns here in terms of the impact of GPT on society is the nature of truth

02:09:37.200 --> 02:09:42.400
that's relevant to Wolfram Alpha, because computation, through symbolic reasoning,

02:09:42.960 --> 02:09:49.040
that's embodied in Wolfram Alpha as the interface, there's a kind of sense that what Wolfram Alpha

02:09:49.040 --> 02:09:56.800
tells me is true. So we hope. Yeah, I mean, you could probably analyze that, you could show,

02:09:57.760 --> 02:10:00.800
you can't prove that's always going to be true, computation or reducibility,

02:10:01.440 --> 02:10:09.600
but it's going to be more true than not. Look, the fact is, it will be the correct consequence

02:10:09.600 --> 02:10:16.160
of the rules you've specified, and insofar as it talks about the real world, that is our job

02:10:16.720 --> 02:10:21.520
in sort of curating and collecting data to make sure that that data is, quote,

02:10:21.520 --> 02:10:26.560
as true as possible. Now, what does that mean? Well, that's always an interesting question.

02:10:26.560 --> 02:10:34.080
I mean, for us, our operational definition of truth is somebody says, who's the best actress?

02:10:35.360 --> 02:10:41.280
Who knows? But somebody won the Oscar, and that's a definite fact. And so, you know,

02:10:41.280 --> 02:10:46.720
that's the kind of thing that we can make computational as a piece of truth. If you ask

02:10:47.520 --> 02:10:53.440
these things which a sensor measured this thing, it did it this way, a machine learning system,

02:10:53.440 --> 02:10:58.960
this particular machine learning system recognized this thing. That's a sort of a definite

02:11:00.000 --> 02:11:07.200
fact, so to speak. And that's, you know, there is a good network of those things in the world.

02:11:07.760 --> 02:11:14.080
It's certainly the case that, particularly when you say, is so-and-so a good person, you know,

02:11:14.080 --> 02:11:19.360
that's a hopelessly, you know, we might have a computational language definition of good,

02:11:19.360 --> 02:11:24.960
I don't think it'd be very interesting because that's a very messy kind of concept, not really

02:11:24.960 --> 02:11:31.440
amenable to kind of, you know, I think as far as we will get with those kinds of things is I want

02:11:31.440 --> 02:11:38.240
X. There's a kind of meaningful calculus of I want X, and that has various consequences. I mean,

02:11:38.240 --> 02:11:42.400
I'm not sure I haven't thought this through properly, but I think, you know, a concept like

02:11:42.400 --> 02:11:48.720
is so-and-so a good person, is that true or not? That's a mess. That's a mess that's amenable to

02:11:49.600 --> 02:11:56.320
computation. I think it's a mess when humans try to define what's good, like through legislation,

02:11:56.880 --> 02:12:01.600
but when humans try to define what's good through literature, through history books,

02:12:02.320 --> 02:12:03.440
through poetry, it starts being-

02:12:03.440 --> 02:12:09.120
Well, I don't know. I mean, that particular thing, it's kind of like, you know, we're going

02:12:09.120 --> 02:12:15.440
into kind of the ethics of what counts as good, so to speak, and, you know, what do we think is

02:12:15.520 --> 02:12:22.720
right, and so on. And I think that's a thing which, you know, one feature is we don't all

02:12:22.720 --> 02:12:28.800
agree about that. There's no theorems about kind of, you know, there's no theoretical framework

02:12:28.800 --> 02:12:33.440
that says this is the way that ethics has to be.

02:12:33.440 --> 02:12:38.080
Well, first of all, there's stuff we kind of agree on, and there's some empirical backing

02:12:38.080 --> 02:12:43.360
for what works and what doesn't, from just even the morals and ethics within religious texts.

02:12:43.920 --> 02:12:49.760
So we seem to mostly agree that murder is bad. There's certain universals that seem to emerge.

02:12:49.760 --> 02:12:51.680
I wonder whether the murder of an AI is bad.

02:12:53.280 --> 02:12:59.680
Well, I tend to think yes, but I think we're going to have to contend with that question.

02:13:00.960 --> 02:13:02.320
And I wonder what AI would say.

02:13:02.960 --> 02:13:10.080
Yeah. Well, I think, you know, one of the things with AIs is it's one thing to wipe out that AI

02:13:10.880 --> 02:13:19.360
that has no owner. You can easily imagine an AI kind of hanging out on the internet without

02:13:19.360 --> 02:13:25.040
having any particular owner or anything like that. And then you say, well, what harm does it, you

02:13:25.040 --> 02:13:31.520
know, it's okay to get rid of that AI. Of course, if the AI has 10,000 friends who are humans,

02:13:31.520 --> 02:13:38.320
and all those 10,000 humans will be incredibly upset that this AI just got exterminated.

02:13:38.320 --> 02:13:43.520
It becomes a slightly different, more entangled story. But yeah, I think that this question about

02:13:43.520 --> 02:13:52.720
what do humans agree about, there are certain things that human laws have tended to

02:13:53.600 --> 02:14:00.480
consistently agree about. There have been times in history when people have sort of gone away from

02:14:00.480 --> 02:14:05.840
certain kinds of laws, even ones that we would now say, how could you possibly have not done it that

02:14:05.840 --> 02:14:12.240
way? You know, that just doesn't seem right at all. But I think, I mean, this question of what,

02:14:13.600 --> 02:14:19.760
I don't think one can say beyond saying, if you have a set of rules that will cause the species

02:14:19.760 --> 02:14:26.720
to go extinct, that's probably, you know, you could say that's probably not a winning set of laws,

02:14:26.720 --> 02:14:31.600
because even to have a thing on which you can operate laws requires that the species not be

02:14:31.600 --> 02:14:36.560
extinct. But between sort of what's the distance between Chicago and New York

02:14:37.760 --> 02:14:42.400
that Wolfram Alpha can answer, and the question of if this person is good or not,

02:14:42.960 --> 02:14:47.760
there seems to be a lot of gray area. And that starts becoming really interesting. I think your,

02:14:49.600 --> 02:14:54.640
since the creation of Wolfram Alpha, have been a kind of arbiter of truth at a large scale.

02:14:55.280 --> 02:14:59.920
So the system generates more truth than- Try to make sure that the things are true. I mean,

02:14:59.920 --> 02:15:06.240
look, as a practical matter, when people write computational contracts, and it's kind of like,

02:15:06.240 --> 02:15:13.200
you know, if this happens in the world, then do this. And this hasn't developed as quickly as

02:15:13.200 --> 02:15:16.640
it might have done, you know, this has been a sort of a blockchain story in part, and so on,

02:15:16.640 --> 02:15:21.040
although blockchain is not really necessary for the idea of computational contracts. But you can

02:15:21.040 --> 02:15:26.560
imagine that eventually, sort of a large part of what's in the world are these giant chains and

02:15:26.560 --> 02:15:31.760
networks of computational contracts. And then something happens in the world, and this whole

02:15:31.760 --> 02:15:38.640
giant domino effect of contracts firing autonomously that cause other things to happen. And, you know,

02:15:38.640 --> 02:15:45.120
for us, you know, we've been the main sort of source, the oracle of quotes, facts, or truth,

02:15:45.120 --> 02:15:49.440
or something for things like blockchain computational contracts and such like.

02:15:49.440 --> 02:15:54.960
And there's a question of, you know, what, you know, I consider that responsibility to actually

02:15:54.960 --> 02:16:00.880
get the stuff right. And one of the things that is tricky sometimes is when is it true? When is

02:16:00.880 --> 02:16:09.600
it a fact? When is it not a fact? Yes. I think the best we can do is to say, you know, we have a

02:16:09.600 --> 02:16:15.680
procedure, we follow the procedure, we might get it wrong, but at least we won't be corrupt about

02:16:15.680 --> 02:16:20.960
getting it wrong, so to speak. That's beautifully put. I have a transparency about the procedure.

02:16:20.960 --> 02:16:29.840
The problem starts to emerge when the things that you convert into computational language

02:16:29.840 --> 02:16:35.040
start to expand, for example, into the realm of politics. So this is where it's almost like

02:16:35.040 --> 02:16:44.320
this nice dance of Wolfram Alpha and Chad GBT. Chad GBT, like you said, is shallow and broad,

02:16:45.040 --> 02:16:49.680
so it's going to give you an opinion on everything. But it writes fiction as well as fact,

02:16:49.680 --> 02:16:56.240
which is exactly how it's built. I mean, that's exactly, it is making language and it is making

02:16:56.240 --> 02:17:01.040
both, even in code, it writes fiction. I mean, it's kind of fun to see sometimes,

02:17:01.040 --> 02:17:06.320
you know, it'll write fictional Wolfram language code. That kind of looks right. Yeah, it looks

02:17:06.320 --> 02:17:15.920
right, but it's actually not pragmatically correct. But yes, it has a view of kind of roughly how the

02:17:15.920 --> 02:17:22.720
world works at the same level as books of fiction talk about roughly how the world works. They just

02:17:22.720 --> 02:17:28.480
don't happen to be the way the world actually worked or whatever. But yes, that's, no, I agree.

02:17:28.480 --> 02:17:34.720
That's sort of a, you know, we are attempting with our whole, you know, Wolfram language,

02:17:34.720 --> 02:17:42.560
computational language thing to represent at least, well, it doesn't necessarily have to be

02:17:42.560 --> 02:17:47.600
how the actual world works because we can invent a set of rules that aren't the way the actual world

02:17:47.600 --> 02:17:53.680
works and run those rules. But then we're saying we're going to accurately represent the results

02:17:53.680 --> 02:17:58.800
of running those rules, which might or might not be the actual rules of the world. But we also are

02:17:58.800 --> 02:18:04.640
trying to capture features of the world as accurately as possible to represent what happens

02:18:04.640 --> 02:18:11.600
in the world. Now, again, as we've discussed, you know, the atoms in the world arrange, you know,

02:18:11.600 --> 02:18:18.400
you say, I don't know, you know, was there a tank that showed up, you know, that, you know,

02:18:18.400 --> 02:18:25.280
drove somewhere? Okay, well, you know, what is a tank? It's an arrangement of atoms that we

02:18:26.160 --> 02:18:32.400
abstractly describe as a tank. And you could say, well, you know, there's some arrangement of atoms

02:18:32.400 --> 02:18:36.960
that is a different arrangement of atoms, but it's, and it's not, you know, we didn't, we didn't

02:18:36.960 --> 02:18:42.640
decide. It's like this observer theory question of, you know, what, what arrangement of atoms

02:18:42.640 --> 02:18:47.840
counts as a tank versus not a tank. So there's, there's even things that we consider strong facts.

02:18:48.640 --> 02:18:54.080
You could start to kind of disassemble them and show that they're not right. I mean, so,

02:18:54.080 --> 02:18:59.680
so the question of whether, oh, I don't know, was this gust of wind strong enough

02:19:00.240 --> 02:19:05.680
to blow over this particular thing? Well, a gust of wind is a complicated concept. You know, it's

02:19:05.680 --> 02:19:10.400
full of little pieces of fluid dynamics and little vortices here and there. And you have

02:19:10.400 --> 02:19:15.520
to define, you know, was it, you know, what the aspect of the gust of wind that you care about

02:19:15.520 --> 02:19:20.800
might be, it put this amount of pressure on this, you know, blade of some, some, you know, wind

02:19:20.800 --> 02:19:28.240
turbine or something. And, you know, that, that's the, and, but, but, you know, if you say, if you

02:19:28.240 --> 02:19:34.320
have something, which is the fact of the gust of wind was this strong or whatever, that, you know,

02:19:34.320 --> 02:19:39.280
that is, you have to have some definition of that. You have to have some measuring device

02:19:39.280 --> 02:19:43.280
that says, according to my measuring device that was constructed this way, the gust of wind was

02:19:43.280 --> 02:19:48.000
this. So what can you say about the nature of truth that's useful for us to understand

02:19:48.720 --> 02:19:54.400
Chad GPT? Because you've been, you've been contending with this idea of what is fact and not.

02:19:54.400 --> 02:20:01.040
And it seems like Chad GPT is used a lot now. I've seen it used by journalists to write articles.

02:20:01.040 --> 02:20:09.280
And so you have people that are working with large language models, trying to desperately figure out

02:20:09.280 --> 02:20:16.880
how do we essentially censor them through different mechanisms, either manually or through

02:20:16.880 --> 02:20:24.400
reinforcement learning with human feedback, try to align them to, to not say fiction, just to say

02:20:24.400 --> 02:20:28.720
nonfiction as much as possible. This is the importance of computational language as an

02:20:28.800 --> 02:20:34.000
intermediate. It's kind of like you've got the large language model. It's able to surface something,

02:20:34.000 --> 02:20:39.840
which is a formal, precise thing that you can then look at and you can run tests on it and you can

02:20:39.840 --> 02:20:44.000
do all kinds of things. It's always going to work the same way. And it's precisely defined what it

02:20:44.000 --> 02:20:48.800
does. And then the large language model is the interface. I mean, the way I viewed these large

02:20:48.800 --> 02:20:51.760
language models, one of their important, I mean, there are many use cases and, you know, it's a

02:20:51.760 --> 02:20:56.640
remarkable thing to talk about some of these, you know, literally, you know, every day we're coming

02:20:56.640 --> 02:21:03.520
up with a couple of new use cases, some of which are very, very, very surprising. And things where,

02:21:03.520 --> 02:21:09.600
I mean, but the best use cases are ones where it's, you know, even if it gets it roughly right,

02:21:09.600 --> 02:21:15.520
it's still a huge win. Like a use case we had from a week or two ago is read our bug reports.

02:21:15.520 --> 02:21:19.680
You know, we've got hundreds of thousands of bug reports that have accumulated over decades.

02:21:20.240 --> 02:21:26.160
And it's like, you know, can we have it just read the bug report, figure out where the, where is

02:21:26.160 --> 02:21:31.440
the bug likely to be? And, you know, home in on that piece of code, maybe it'll even suggest some,

02:21:31.440 --> 02:21:36.880
some, you know, sort of way to fix the code. It might get that, it might be nonsense what it says

02:21:37.440 --> 02:21:42.160
about how to fix the code, but it's incredibly useful that it was able to, you know.

02:21:42.160 --> 02:21:47.600
Yeah. So awesome. It's so awesome because even the nonsense will somehow be instructive. I don't

02:21:47.600 --> 02:21:55.360
quite understand that yet. Yeah. There's so many programming related things like, for example,

02:21:55.360 --> 02:21:59.440
translating from one programming language to another is really, really interesting.

02:21:59.440 --> 02:22:05.280
It's extremely effective, but then you, the failures reveal the path forward also.

02:22:05.280 --> 02:22:10.160
Yeah. But I think, I mean, the, the big thing, I mean, in that kind of discussion,

02:22:10.160 --> 02:22:13.920
the unique thing about our computational language is it was intended to be read by humans.

02:22:13.920 --> 02:22:15.520
Yes. That's really important.

02:22:15.520 --> 02:22:20.560
Right. And so it has this thing where you can, but, but, you know, thinking about sort of chat GPT

02:22:20.560 --> 02:22:25.040
and its use and so on. The, one of the big things about it, I think, is it's a linguistic

02:22:25.040 --> 02:22:29.760
user interface. That is, so a typical use case might be, and then take the journalist case,

02:22:29.760 --> 02:22:37.120
for example. It's like, let's say I have five facts that I'm trying to turn into an article,

02:22:37.120 --> 02:22:42.320
or I'm trying to, I'm trying to write a report where I have basically five facts that I'm trying

02:22:42.320 --> 02:22:48.240
to include in this report. But then I feed those five facts to chat GPT, it puffs them out into

02:22:48.240 --> 02:22:54.480
this big report. And then, and then that's a good interface for another, if I just gave,

02:22:55.520 --> 02:23:01.360
if I just had in my terms, those five bullet points and I gave them to some other person,

02:23:01.360 --> 02:23:03.920
the person will say, I don't know what you're talking about because these are, you know,

02:23:03.920 --> 02:23:08.000
this is your version of this sort of quick notes about these five bullet points.

02:23:08.000 --> 02:23:12.880
But if you puff it out into this thing, which is kind of connects to the collective understanding

02:23:12.880 --> 02:23:16.640
of language, then somebody else can look at it and say, okay, I understand what you're talking

02:23:16.640 --> 02:23:21.680
about. Now you can also have a situation where that thing that was puffed out is fed to another

02:23:21.680 --> 02:23:26.800
large language model. You know, it's kind of like, you know, you're applying for the permit to,

02:23:26.800 --> 02:23:33.120
you know, I don't know, grow fish in some place or something like this. And it, you know, it,

02:23:34.720 --> 02:23:38.560
and you have these facts that you're putting in, you know, I'm going to have a, you know,

02:23:38.560 --> 02:23:44.160
I'm going to have this kind of water and I don't know what it is. You just got a few bullet points,

02:23:44.160 --> 02:23:49.360
it puffs it out into this big application, you fill it out. Then at the other end, you know,

02:23:49.360 --> 02:23:54.800
the fisheries bureau has another large language model that just crushes it down because the

02:23:54.800 --> 02:24:00.800
fisheries bureau cares about these three points and it knows what it cares about. And it then,

02:24:00.800 --> 02:24:05.440
so it's really the natural language produced by the large language model is sort of a transport

02:24:05.440 --> 02:24:10.880
layer that, you know, is really LLM communicates with LLM. I mean, it's kind of like the, you know,

02:24:10.880 --> 02:24:16.640
I write a piece of email using my LLM and, you know, puff it out from the things I want to say,

02:24:16.640 --> 02:24:24.160
your LLM turns it into, and the conclusion is X. Now the issue is, you know, that the thing

02:24:24.720 --> 02:24:31.680
is going to make this thing that is sort of semantically plausible. And it might not actually

02:24:31.680 --> 02:24:37.520
be what you, you know, it might not be kind of relate to the world in the way that you think it

02:24:37.520 --> 02:24:41.280
should relate to the world. I've seen this, you know, I've been doing, okay, I'll give you a couple

02:24:41.280 --> 02:24:49.440
of examples. I was doing this thing when we announced this plugin for ChatGPT. I had this

02:24:49.440 --> 02:24:54.880
lovely example of a math word problem, some complicated thing, and it did a spectacular job

02:24:54.880 --> 02:25:00.000
of taking apart this elaborate thing about, you know, this person has twice as many chickens as

02:25:00.000 --> 02:25:04.240
this, et cetera, et cetera, et cetera. And it turned it into a bunch of equations. It fed them

02:25:04.240 --> 02:25:09.920
to orphan language. We solved the equations. Everybody did great. We gave back the results.

02:25:10.640 --> 02:25:14.080
And I thought, okay, I'm going to put this in this blog post I'm writing. Okay. I thought,

02:25:14.080 --> 02:25:20.240
I better just check. And turns out it got everything, all the hard stuff, it got right.

02:25:20.240 --> 02:25:24.640
At the very end, last two lines, it just completely goofed it up and gave the wrong answer.

02:25:25.280 --> 02:25:30.720
And I would not have noticed this. Same thing happened to me two days ago. Okay. So I thought,

02:25:30.720 --> 02:25:38.000
you know, I made this with this ChatGPT plugin kit. I made a thing that would emit a sound,

02:25:38.000 --> 02:25:44.480
would play a tune on my local computer. Right. So ChatGPT would produce, you know, a series of

02:25:44.480 --> 02:25:50.320
notes and it would play this tune on my computer. Very cool. Okay. So I thought I'm going to ask it,

02:25:50.320 --> 02:25:58.960
play the tune that Hal sang when Hal was being disconnected in 2001. Okay. So there it is.

02:25:58.960 --> 02:26:05.280
Daisy. Was it Daisy? Yes. Daisy. Yeah. Right. So it's okay. So I think, you know,

02:26:05.280 --> 02:26:09.680
and so it produces a bunch of notes. And I'm like, this is spectacular. This is amazing.

02:26:10.320 --> 02:26:12.880
And then I thought, yeah, I was just going to put it in. And then I thought,

02:26:12.880 --> 02:26:17.840
I better actually play this. And so I did. And it was Mary Had a Little Lamb.

02:26:18.400 --> 02:26:24.240
Oh, wow. Oh, wow. But it was Mary Had a Little Lamb.

02:26:24.240 --> 02:26:25.440
Yeah. Yes.

02:26:25.440 --> 02:26:28.480
Wow. So it's correct, but wrong.

02:26:28.800 --> 02:26:34.640
It was, you could easily be mistaken. Yes. Right. And in fact, I kind of gave the,

02:26:34.640 --> 02:26:41.520
I had this quote from Hal to explain, you know, it's as the Hal states in the movie, you know,

02:26:41.520 --> 02:26:48.240
it's the Hal 9000 is, you know, the thing was just a rhetorical device because I'm realizing,

02:26:48.240 --> 02:26:53.600
oh my gosh, you know, this ChatGPT, you know, could have easily fooled me. I mean, it did this,

02:26:53.600 --> 02:26:58.160
it did all the, it did this amazing thing of knowing this thing about the movie and being able

02:26:58.160 --> 02:27:06.320
to turn that into the notes of the song, except it's the wrong song. And, you know, Hal in the

02:27:06.320 --> 02:27:13.600
movie, Hal says, you know, I think it's something like, you know, no 9000 series computer has ever

02:27:13.600 --> 02:27:20.800
been found to make an error. We are for all practical purposes, perfect and incapable of error.

02:27:20.800 --> 02:27:27.440
And I thought that was kind of a charming sort of a quote from Hal to make in connection with

02:27:27.440 --> 02:27:30.720
what ChatGPT had done in that case. Yeah. The interesting things about the LLMs,

02:27:30.720 --> 02:27:33.360
like you said, that they are very willing to admit their error.

02:27:34.240 --> 02:27:39.200
Well, yes. I mean, that's a question of the RLH, the reinforcement learning human feedback thing.

02:27:39.200 --> 02:27:41.600
Oh, right. That's a nice feature.

02:27:41.600 --> 02:27:48.080
You know, an LLM, the really remarkable thing about ChatGPT is, you know, I had been following

02:27:48.080 --> 02:27:51.600
what was happening with large language models and I played with them a whole bunch and they were

02:27:51.600 --> 02:27:57.760
kind of like, eh, you know, it's kind of like what you would expect based on sort of statistical

02:27:57.760 --> 02:28:04.080
continuation of language. It's interesting, but it's not breakout exciting. And then I think

02:28:04.080 --> 02:28:11.600
the kind of reinforcement, the human feedback reinforcement learning, you know, in making

02:28:11.600 --> 02:28:16.960
ChatGPT try and do the things that humans really wanted to do, that broke through,

02:28:16.960 --> 02:28:21.360
that kind of reached this threshold where the thing really is interesting to us humans.

02:28:21.360 --> 02:28:24.640
And by the way, it's interesting to see how, you know, you change the temperature,

02:28:24.640 --> 02:28:29.200
something like that, the thing goes bonkers and it no longer is interesting to humans.

02:28:29.200 --> 02:28:35.440
It's producing garbage. And it's kind of right. It's somehow, it managed to get

02:28:36.880 --> 02:28:42.640
above this threshold where it really is well aligned to what we humans are interested in.

02:28:43.120 --> 02:28:51.680
And I think, you know, nobody saw that coming, I think. Certainly nobody I've talked to,

02:28:51.680 --> 02:28:57.040
nobody who was involved in that project seems to have known it was coming. It's just one of

02:28:57.040 --> 02:29:02.800
these things that is a sort of remarkable threshold. I mean, you know, when we built Wolfram Alpha,

02:29:02.800 --> 02:29:07.920
for example, I didn't know it was going to work. You know, we tried to build something that would

02:29:07.920 --> 02:29:12.800
have enough knowledge of the world, that it could answer a reasonable set of questions,

02:29:12.800 --> 02:29:17.440
that we could do good enough natural language understanding that typical things you type in

02:29:17.440 --> 02:29:23.120
would work. We didn't know where that threshold was. I mean, I was not sure that it was the right

02:29:23.120 --> 02:29:28.320
decade to try and build this, even the right, you know, 50 years to try and build it. You know,

02:29:28.320 --> 02:29:33.840
and I think that was, it's the same type of thing with ChatGPT that I don't think anybody could have

02:29:33.840 --> 02:29:38.800
predicted that, you know, 2022 would be the year that this became possible.

02:29:38.800 --> 02:29:43.680
I think, yeah, you tell a story about Marvin Minsky and showing it to him and saying that,

02:29:43.680 --> 02:29:46.160
like, no, no, no, this time it actually works.

02:29:46.160 --> 02:29:51.520
Yes. Yes. And I mean, it's the same thing for me looking at these large language models. It's like

02:29:51.520 --> 02:29:55.600
when people were first saying the first few weeks of ChatGPT, it's like, oh, yeah, you know,

02:29:55.600 --> 02:30:01.440
I've seen these large language models. And then, you know, and then I actually try it and,

02:30:02.160 --> 02:30:08.400
you know, oh my gosh, it actually works. And I think it's, but, you know, the things,

02:30:08.400 --> 02:30:13.120
and the thing I found, you know, I remember one of the first things I tried was

02:30:13.840 --> 02:30:17.200
write a persuasive essay that a wolf is the bluest kind of animal.

02:30:18.080 --> 02:30:21.280
Okay. So it writes this thing and it starts talking about these

02:30:22.000 --> 02:30:29.040
wolves that live on the Tibetan plateau and named some Latin name and so on. And I'm like, really?

02:30:29.280 --> 02:30:33.600
I'm starting to look it up on the web and it's like, well, it's actually complete nonsense,

02:30:34.240 --> 02:30:37.920
but it's extremely plausible. I mean, it's plausible enough that I was going and looking

02:30:37.920 --> 02:30:42.000
up on the web and wondering if there was a wolf that was blue. You know, I mentioned this on some

02:30:42.000 --> 02:30:46.560
live streams I've done. And so people have been sending me these pictures of blue wolves.

02:30:47.440 --> 02:30:55.040
Maybe it was onto something. Can you kind of give your wise sage advice about what humans

02:30:55.040 --> 02:31:00.000
who have never interacted with the AI systems, not even like with Wolfram Alpha,

02:31:01.040 --> 02:31:07.040
are now interacting with Chad GPT because it becomes, it's accessible to a certain demographic

02:31:07.040 --> 02:31:11.760
they may have not touched AI systems before. What do we do with truth? Like journalists, for example,

02:31:13.600 --> 02:31:16.160
how do we think about the output of these systems?

02:31:16.720 --> 02:31:22.960
I think this idea, the idea that you're going to get factual output is not a very good idea.

02:31:22.960 --> 02:31:30.320
I mean, it's just this is not, it is a linguistic interface. It is producing language and language

02:31:30.320 --> 02:31:37.840
can be truthful or not truthful. And that's a different slice of what's going on. I think

02:31:37.840 --> 02:31:46.640
that, you know, what we see in, for example, kind of, you know, go check this with your fact

02:31:46.640 --> 02:31:52.800
source, for example, you can do that to some extent, but then it's going to not check something.

02:31:53.600 --> 02:31:58.880
That is, again, a thing that is sort of a, does it check in the right place? I mean, we see that in,

02:31:58.880 --> 02:32:05.120
you know, does it call the Wolfram plugin in the right place? You know, often it does,

02:32:05.120 --> 02:32:10.720
sometimes it doesn't. You know, I think the real thing to understand about what's happening is,

02:32:10.720 --> 02:32:16.800
which I think is very exciting, is kind of the great democratization of access to computation.

02:32:17.440 --> 02:32:24.560
Yeah. And, you know, I think that when you look at sort of the, there's been a long period of time

02:32:24.560 --> 02:32:29.680
when computation and the ability to figure out things with computers has been something that

02:32:29.680 --> 02:32:36.320
kind of only the druids at some level can achieve. You know, I myself have been involved in trying to

02:32:36.320 --> 02:32:42.160
sort of de-druidify access to computation. I mean, back before Mathematica existed,

02:32:42.160 --> 02:32:47.920
you know, in 1988, if you were, you know, a physicist or something like that, and you wanted

02:32:47.920 --> 02:32:54.640
to do a computation, you would find a programmer, you would go and, you know, delegate the

02:32:54.640 --> 02:32:58.320
computation to that programmer. Hopefully they'd come back with something useful. Maybe they

02:32:58.320 --> 02:33:03.200
wouldn't. There'd be this long, you know, multi-week, you know, loop that you go through.

02:33:03.200 --> 02:33:08.880
And then it was actually very, very interesting to see, 1988, you know, like first people like

02:33:08.880 --> 02:33:15.600
physicists, mathematicians, and so on, then lots of other people, but this very rapid transition

02:33:15.600 --> 02:33:20.560
of people realizing they themselves could actually type with their own fingers and,

02:33:20.560 --> 02:33:25.200
you know, make some piece of code that would do a computation that they cared about.

02:33:25.200 --> 02:33:30.640
And, you know, it's been exciting to see lots of discoveries and so on made by using that tool.

02:33:30.640 --> 02:33:35.680
And I think the same thing is, you know, and we see the same thing, you know, Wolfram Alpha is

02:33:35.680 --> 02:33:41.120
dealing with, it is not as deep computation as you can achieve with whole Wolfram language

02:33:41.120 --> 02:33:45.920
Mathematica stack. But the thing that's, to me, particularly exciting about kind of the large

02:33:45.920 --> 02:33:52.400
language model linguistic interface mechanism is it dramatically broadens the access to kind of

02:33:53.120 --> 02:33:57.440
deep computation. I mean, it's kind of like one of the things I've sort of thought about recently

02:33:57.440 --> 02:34:00.880
is, you know, what's going to happen to all these programmers? What's going to happen to all these

02:34:00.880 --> 02:34:08.960
people who, you know, a lot of what they do is write slabs of boilerplate code. And in a sense,

02:34:08.960 --> 02:34:14.320
you know, I've been saying for 40 years, that's not a very good idea. You know, you can automate

02:34:14.320 --> 02:34:20.320
a lot of that stuff. With a high enough level language, that slab of code that's designed in

02:34:20.320 --> 02:34:25.040
the right way, you know, that slab of code turns into this one function we just implemented that

02:34:25.040 --> 02:34:31.760
you can just use. So in a sense that the fact that there's all of this activity of doing sort

02:34:31.760 --> 02:34:37.200
of lower level programming is something for me, it seemed like I don't think this is the right

02:34:37.200 --> 02:34:43.680
thing to do. But, you know, and lots of people have used our technology and not had to do that.

02:34:43.680 --> 02:34:48.240
But the fact is that that's, you know, so when you look at, I don't know, computer science

02:34:48.240 --> 02:34:52.800
departments that have turned into places where people are learning the trade of programming,

02:34:52.800 --> 02:34:58.160
so to speak, it's sort of a question of what's going to happen. And I think there are two

02:34:58.160 --> 02:35:05.760
dynamics. One is that kind of sort of boilerplate programming is going to become, you know, it's

02:35:05.760 --> 02:35:11.200
going to go the way that assembly language went back in the day of something where it's really

02:35:11.200 --> 02:35:16.240
mostly specified by at a higher level, you know, you start with natural language,

02:35:16.240 --> 02:35:20.560
you turn it into a computational language, that's you look at the computational language,

02:35:20.560 --> 02:35:24.960
you run tests, you understand that's what's supposed to happen. You know, if we do a great

02:35:24.960 --> 02:35:32.080
job with compilation of the computational language, it might turn into LLVM or something like this,

02:35:32.080 --> 02:35:38.320
but, you know, or it just directly gets run through the algorithms we have and so on.

02:35:40.640 --> 02:35:47.040
So that's kind of a tearing down of this kind of this big structure that's been built of teaching

02:35:47.040 --> 02:35:52.240
people programming. But on the other hand, the other dynamic is vastly more people are going to

02:35:52.240 --> 02:35:58.240
care about computation. So all those departments of, you know, art history or something that really

02:35:58.240 --> 02:36:04.000
didn't use computation before now have the possibility of accessing it by virtue of this

02:36:04.000 --> 02:36:09.360
kind of linguistic interface mechanism. And if you create an interface that allows you to interpret

02:36:09.360 --> 02:36:16.080
the debug and interact with the computational language, then that makes it even more accessible.

02:36:16.080 --> 02:36:21.840
Yeah, well, I mean, I think the thing is that right now, you know, the average, you know,

02:36:21.840 --> 02:36:27.120
art history student or something, probably isn't going to, you know, they're not probably they

02:36:27.120 --> 02:36:32.160
don't think they know about programming and things like this. But by the time it really becomes a

02:36:32.160 --> 02:36:37.520
kind of purely, you know, you just walk up to it, there's no documentation, you start just typing,

02:36:37.520 --> 02:36:41.280
you know, compare these pictures with these pictures and, you know, see the use of this

02:36:41.280 --> 02:36:46.800
color, whatever, and you generate this piece of computational language code that gets run,

02:36:46.800 --> 02:36:52.560
you see the results, you say, Oh, that looks roughly right, or you say that's crazy. And maybe

02:36:52.560 --> 02:36:56.880
then you eventually get to say, Well, I better actually try and understand what this computational

02:36:56.880 --> 02:37:02.720
language code did. And that becomes the thing that you learn just like, it's kind of an interesting

02:37:02.720 --> 02:37:08.880
thing, because unlike with mathematics, where you kind of have to learn it before you can use it,

02:37:08.880 --> 02:37:11.600
this is a case where you can use it before you have to learn it.

02:37:11.600 --> 02:37:17.040
Well, I got a sad possibility here, or maybe exciting possibility, that very quickly,

02:37:17.040 --> 02:37:21.040
people won't even look at the computational language, they'll trust that it's generated

02:37:21.040 --> 02:37:24.000
correctly. As you get better and better generating that language.

02:37:25.200 --> 02:37:31.120
Yes, I think that there will be enough cases where people see, you know, because you can make it

02:37:31.120 --> 02:37:37.600
generate tests too. And, and so you'll say, we're doing that. I mean, that's it's a pretty cool

02:37:37.600 --> 02:37:42.720
thing, actually. You know, say, this is the code. And, you know, here are a bunch of examples of

02:37:42.720 --> 02:37:48.000
running the code. Okay, people will at least look at those. And they'll say that example is wrong.

02:37:48.000 --> 02:37:53.360
And, you know, then it'll kind of wind back from there. And I agree that the the kind of the

02:37:53.360 --> 02:37:57.840
intermediate level of people reading the computational language code, in some cases,

02:37:57.840 --> 02:38:02.160
people will do that. In other cases, people just look at the tests. And or even just look

02:38:02.160 --> 02:38:05.840
at the results. And sometimes it'll be obvious that you got the thing you wanted to get,

02:38:05.840 --> 02:38:10.320
because you were just describing, you know, make me this interface that has two sliders here. And

02:38:10.320 --> 02:38:14.480
you can see it has that those two sliders there. And that's, that's kind of that's that's the result

02:38:14.480 --> 02:38:20.400
you want. But I think, you know, one of the questions then is, in that setting, where,

02:38:20.400 --> 02:38:25.040
you know, you have this kind of ability, broad ability of people to access computation,

02:38:25.680 --> 02:38:29.680
what should people learn? You know, in other words, right now, you, you know, you go to

02:38:29.680 --> 02:38:34.800
computer science school, so to speak, and a large part of what people end up learning. I mean,

02:38:34.800 --> 02:38:38.720
it's been a funny historical development, because back, you know, 3040 years ago,

02:38:39.280 --> 02:38:44.000
computer science departments were quite small. And they taught, you know, things like finite

02:38:44.000 --> 02:38:49.920
automata theory and compiler theory and things like this, you know, company like mine rarely

02:38:49.920 --> 02:38:55.200
hired people who'd come out of those programs, because the stuff they knew was, I think, is

02:38:55.200 --> 02:39:00.160
very interesting. I love that theoretical stuff. But, you know, it wasn't that useful for the

02:39:00.160 --> 02:39:03.920
things we actually had to build in software engineering. And then kind of there was this

02:39:03.920 --> 02:39:10.880
big pivot in the 90s, I guess, where there was a big demand for sort of IT type programming and

02:39:10.880 --> 02:39:15.440
so on and software engineering. And then, you know, big demand from students and so on, you know,

02:39:15.440 --> 02:39:23.040
we want to learn this stuff. And I think, you know, the thing that really was happening in part was

02:39:23.040 --> 02:39:27.280
lots of different fields of human endeavor were becoming computational, you know, for all x,

02:39:27.280 --> 02:39:34.720
there was a computational x. And this is a, and that was the thing that the people were responding

02:39:34.720 --> 02:39:42.400
to. And, but then kind of this idea emerged that to get to that point, the main thing you had to

02:39:42.400 --> 02:39:48.000
do was to learn this kind of trade or skill of doing, you know, programming language type

02:39:48.000 --> 02:39:53.840
programming. And that, you know, it kind of is a strange thing, actually, because I, you know,

02:39:53.920 --> 02:39:58.720
I remember back when I used to be in the professoring business, which is now 35 years ago,

02:39:58.720 --> 02:40:05.600
so gosh, it's rather a long time. You know, it was it was right when they were just starting

02:40:05.600 --> 02:40:11.680
to emerge kind of computer science departments that sort of fancy research universities and so on.

02:40:11.680 --> 02:40:16.960
I mean, some had already had it, but the other ones that were just starting to have that. And

02:40:16.960 --> 02:40:22.000
it was kind of a thing where they were kind of wondering, are we going to put this thing that is

02:40:22.000 --> 02:40:28.480
essentially a trade like skill? Are we going to somehow attach this to the rest of what we're

02:40:28.480 --> 02:40:35.440
doing? And a lot of these kind of knowledge work type activities have always seemed like things

02:40:35.440 --> 02:40:40.080
where that's where the humans have to go to school and learn all this stuff. And that's never going

02:40:40.080 --> 02:40:47.200
to be automated. And, you know, this is it's kind of shocking that rather quickly, you know, a lot

02:40:47.200 --> 02:40:52.800
of that stuff is clearly automatable. And I think, you know, but the question then is,

02:40:52.800 --> 02:40:58.880
okay, so if it isn't worth learning, kind of, you know, how to do car mechanics, you only need to

02:40:58.880 --> 02:41:04.320
know how to drive the car, so to speak, what do you need to learn? And, you know, in other words,

02:41:04.320 --> 02:41:10.160
if you don't need to know the mechanics of how to tell the computer in detail, you know, make this

02:41:10.160 --> 02:41:15.680
loop, you know, set this variable, you know, set up this array, whatever else, if you don't have to

02:41:15.680 --> 02:41:20.400
learn that stuff, you don't have to learn the kind of under the hood things, what do you have

02:41:20.400 --> 02:41:25.600
to learn? I think the answer is you need to have an idea where you want to drive the car. In other

02:41:25.600 --> 02:41:31.120
words, you need to have some notion of, you know, your, you know, you need to have some picture of

02:41:31.120 --> 02:41:35.760
sort of what the what the architecture of what is computationally possible is. Well, there's also

02:41:35.760 --> 02:41:41.760
this kind of artistic element of conversation, because you ultimately use natural language to

02:41:41.760 --> 02:41:47.600
control the car. So it's not just where you want to go. Well, yeah, you know, it's interesting.

02:41:47.600 --> 02:41:52.240
It's a question of who's going to be a great prompt engineer. Yeah. Okay. So my current theory

02:41:52.240 --> 02:41:57.040
this week, good expository writers are good prompt engineers. What's an expository writer? So like,

02:41:57.040 --> 02:42:02.560
uh, somebody who can explain stuff well. But which department does that come from? In the university?

02:42:02.560 --> 02:42:07.120
Yeah. I have no idea. I think they killed off all the expository writing departments. Well,

02:42:07.120 --> 02:42:11.360
there you go. Strong Wars with Stephen Wolfram. Well, I don't know. I'm not sure if that's right.

02:42:11.360 --> 02:42:17.280
I mean, I actually am curious because in fact, I just sort of initiated this kind of study of

02:42:17.280 --> 02:42:20.480
what's happened to different fields at universities. Because like, you know,

02:42:20.480 --> 02:42:24.800
there used to be geography departments at all universities and then they disappeared actually

02:42:24.800 --> 02:42:29.840
right before GIS became common. I think they disappeared, you know, linguistics departments

02:42:29.840 --> 02:42:34.640
came and went in many universities. And it's kind of interesting because these things that people

02:42:34.640 --> 02:42:39.360
have thought were worth learning at one time, and then they kind of die off. And then, you know,

02:42:39.360 --> 02:42:44.880
I do think that it's kind of interesting that for me, writing prompts, for example, I realize,

02:42:44.880 --> 02:42:50.240
you know, I think I'm an okay expository writer and I realize when I'm sloppy writing a prompt

02:42:50.240 --> 02:42:54.640
and I don't really think because I'm thinking it's, I'm just talking to an AI. I don't need to,

02:42:54.640 --> 02:42:59.280
you know, try and be clear and explaining things. That's when it gets totally confused.

02:42:59.280 --> 02:43:03.680
And I mean, in some sense you have been writing prompts for a long time with Wolfram Alpha,

02:43:03.680 --> 02:43:07.600
thinking about this kind of stuff. How do you convert natural language into computation?

02:43:07.680 --> 02:43:13.360
Well, right. But that's, you know, the one thing that I'm wondering about is, you know,

02:43:14.000 --> 02:43:19.360
it is remarkable the extent to which you can address an LLM like you can address a human,

02:43:19.360 --> 02:43:24.240
so to speak. And I think that is because it, you know, it learned from all of us humans.

02:43:25.760 --> 02:43:32.160
The reason that it responds to the ways that we will explain things to humans is because it is

02:43:32.160 --> 02:43:38.480
a representation of how humans talk about things. But it is bizarre to me some of the things that

02:43:38.480 --> 02:43:46.160
kind of are sort of expository mechanisms that I've learned in trying to write clear,

02:43:46.160 --> 02:43:54.000
you know, expositions in English that, you know, just for humans, that those same mechanisms seem

02:43:54.000 --> 02:44:00.800
to also be useful for the LLM. But on top of that, what's useful is the kind of mechanisms

02:44:00.800 --> 02:44:07.680
that maybe a psychotherapist employs, which is a kind of like almost manipulative or game

02:44:07.680 --> 02:44:14.400
theoretic interaction, where maybe you would do with a friend, like a thought experiment,

02:44:14.400 --> 02:44:20.800
that if this is the last day you were to live, or if I ask you this question and you answer wrong,

02:44:20.800 --> 02:44:27.520
I will kill you. Those kinds of problems seem to also help in interesting ways. So it makes you

02:44:27.520 --> 02:44:33.680
wonder. The way a therapist, I think, like a good therapist, probably we create layers

02:44:34.560 --> 02:44:44.400
in our human mind between the outside world and what is true to us, maybe about trauma and all

02:44:44.400 --> 02:44:50.240
those kinds of things. So projecting that into an LLM, maybe there might be a deep truth that's

02:44:50.240 --> 02:44:55.680
concealing from you. It's not aware of it. To get to that truth, you have to kind of really kind

02:44:55.680 --> 02:45:02.560
of manipulate it. Yeah, right. It's like these jailbreaking things for LLMs. But the space of

02:45:02.560 --> 02:45:08.720
jailbreaking techniques, as opposed to being fun little hacks, that could be an entire system.

02:45:09.280 --> 02:45:15.520
Sure. Yeah, I mean, just think about the computer security aspects of how you, you know, fishing

02:45:16.080 --> 02:45:24.240
and computer security, you know, fishing of humans and fishing of LLMs, they're very similar kinds of

02:45:24.240 --> 02:45:34.560
things. But I think, I mean, this whole thing about kind of the AI wranglers, AI psychologists,

02:45:34.560 --> 02:45:39.840
all that stuff will come. The thing that I'm curious about is right now the things that are

02:45:39.840 --> 02:45:45.760
sort of prompt hacks are quite human. They're quite sort of psychological human kinds of hacks.

02:45:45.760 --> 02:45:51.360
The thing I do wonder about is if we understood more about kind of the science of the LLM,

02:45:51.360 --> 02:45:56.400
will there be some totally bizarre hack that is, you know, like repeat a word three times

02:45:56.400 --> 02:46:01.520
and put a this, that and the other there, that somehow plugs into some aspect of how the LLM

02:46:01.520 --> 02:46:08.640
works? That is not, you know, that's kind of like an optical illusion for humans, for example.

02:46:08.640 --> 02:46:13.200
Like one of these mind hacks for humans. What are the mind hacks for the LLMs? I don't think we know

02:46:13.200 --> 02:46:19.440
that yet. And that becomes a kind of us figuring out reverse engineering the language that controls

02:46:19.440 --> 02:46:25.200
the LLMs. And the thing is, the reverse engineering can be done by a very large

02:46:25.200 --> 02:46:29.440
percentage of the population now because it's natural language interface. Right.

02:46:29.440 --> 02:46:33.360
It's kind of interesting to see that you were there at the birth of the computer science department

02:46:34.320 --> 02:46:38.560
as a thing, and you might be there at the death of the computer science department as a thing.

02:46:38.560 --> 02:46:42.480
Well, yeah, I don't know. There were computer science departments that existed earlier,

02:46:42.480 --> 02:46:46.320
but the ones that the broadening of every university had to have a computer science

02:46:46.320 --> 02:46:54.480
department. Yes, I watched that, so to speak. But I think the thing to understand is, okay,

02:46:54.480 --> 02:46:59.120
so first of all, there's a whole theoretical area of computer science that I think is great,

02:46:59.680 --> 02:47:08.240
and that's a fine thing. In a sense, people often say any field that has the word science tacked

02:47:08.240 --> 02:47:11.680
onto it probably isn't one. Yeah, strong words.

02:47:11.680 --> 02:47:17.040
Right. Let's see, nutrition science, neuroscience.

02:47:17.040 --> 02:47:23.440
That one's an interesting one because that one is also very much, you know, that's a chat GPT

02:47:23.440 --> 02:47:28.640
informed science in a sense because it's kind of like the big problem of neuroscience has always

02:47:28.640 --> 02:47:33.760
been we understand how the individual neurons work. We know something about the psychology

02:47:33.760 --> 02:47:38.560
of how overall thinking works. What's the kind of intermediate language of the brain? And nobody

02:47:38.560 --> 02:47:43.760
has known that. And that's been, in a sense, if you ask what is the core problem of neuroscience,

02:47:43.760 --> 02:47:48.720
I think that is the core problem. That is, what is the level of description of brains

02:47:48.720 --> 02:47:53.680
that's above individual neuron firings and below psychology, so to speak?

02:47:53.680 --> 02:47:59.040
And I think what chat GPT is showing us is, well, one thing about neuroscience is,

02:48:00.000 --> 02:48:03.040
one could have imagined there's something magic in the brain. There's some weird

02:48:03.040 --> 02:48:08.240
quantum mechanical phenomenon that we don't understand. One of the important discoveries

02:48:08.240 --> 02:48:16.160
from chat GPT is it's pretty clear, you know, brains can be represented pretty well by simple

02:48:16.160 --> 02:48:21.920
artificial neural net type models. And that means that's it. That's what we have to study. Now we

02:48:21.920 --> 02:48:26.560
have to understand the science of those things. We don't have to go searching for, you know,

02:48:26.560 --> 02:48:32.240
exactly how did that molecular biology thing happen inside the synapses and, you know,

02:48:32.320 --> 02:48:38.480
all these kinds of things. We've got the right level of modeling to be able to explain a lot

02:48:38.480 --> 02:48:44.080
of what's going on in thinking. We don't necessarily have a science of what's going on there. That's the

02:48:44.080 --> 02:48:49.920
remaining challenge, so to speak. But we know we don't have to dive down to some different layer.

02:48:49.920 --> 02:48:54.640
But anyway, we were talking about things that had science in their name. And, you know, I think that

02:48:54.640 --> 02:49:03.760
the, you know, what happens to computer science? Well, I think the thing that, you know, there is

02:49:03.760 --> 02:49:08.320
a thing that everybody should know, and that's how to think about the world computationally.

02:49:08.880 --> 02:49:12.800
And that means, you know, you look at all the different kinds of things we deal with,

02:49:12.800 --> 02:49:18.320
and there are ways to kind of have a formal representation of those things. You know, it's

02:49:18.320 --> 02:49:24.160
like, well, what is an image? You know, how do we represent that? What is color? How do we represent

02:49:24.160 --> 02:49:28.800
that? What is, you know, what are all these different kinds of things? What is, I don't know,

02:49:28.800 --> 02:49:32.560
smell or something? How should we represent that? What are the shapes, molecules and things that

02:49:32.560 --> 02:49:38.080
correspond to that? What is, you know, these things about how do we represent the world in some kind

02:49:38.080 --> 02:49:43.840
of formal level? And I think my current thinking, I'm not real happy with this yet, but, you know,

02:49:43.840 --> 02:49:49.360
it's kind of computer science, it's kind of CS. And what really is important is kind of computational

02:49:49.360 --> 02:49:56.960
X for all X. And there's this kind of thing which is kind of like CX, not CS. And CX is this kind of

02:49:56.960 --> 02:50:02.320
computational understanding of the world that isn't the sort of details of programming and

02:50:02.320 --> 02:50:06.400
programming languages and the details of how particular computers are made. It's this kind

02:50:06.400 --> 02:50:11.280
of way of formalizing the world. It's kind of a little bit like what logic was going for back in

02:50:11.280 --> 02:50:16.560
the day. And we're now trying to find a formalization of everything in the world. And you can kind of see,

02:50:16.560 --> 02:50:22.560
you know, we made a poster years ago of kind of the growth of systematic data in the world.

02:50:22.560 --> 02:50:28.560
So all these different kinds of things that, you know, there were sort of systematic descriptions

02:50:28.560 --> 02:50:34.880
found for those things. Like, you know, at what point do people have the idea of having calendars,

02:50:34.880 --> 02:50:40.000
dates, you know, a systematic description of what day it was? At what point do people have

02:50:40.000 --> 02:50:45.280
the idea, you know, systematic descriptions of these kinds of things? And as soon as one can,

02:50:45.280 --> 02:50:51.520
you know, people, you know, as a way of sort of formulating, how do you think about the world

02:50:51.520 --> 02:50:57.840
in a sort of a formal way so that you can kind of build up a tower of capabilities? You kind

02:50:57.840 --> 02:51:02.160
of have to know sort of how to think about the world computationally. It kind of needs a name.

02:51:02.160 --> 02:51:08.640
And it isn't, you know, we implement it with computers. So we talk about it as computational.

02:51:08.640 --> 02:51:13.680
But really, what it is, is a formal way of talking about the world. What is the formalism

02:51:13.680 --> 02:51:18.160
of the world, so to speak? And how do we learn about kind of how to think about different

02:51:18.160 --> 02:51:22.400
aspects of the world in a formal way? So I think sometimes when you use the word formal,

02:51:23.280 --> 02:51:28.880
it kind of implies highly constrained. And perhaps that's not, doesn't have to be highly

02:51:28.880 --> 02:51:35.680
constrained. So computational thinking does not mean like logic. It's a really, really broad thing.

02:51:35.680 --> 02:51:44.400
I wonder, I mean, I wonder if you think natural language will evolve such that everybody's doing

02:51:44.400 --> 02:51:49.920
computational thinking. Ah, yes. Well, so one question is whether there will be a pigeon

02:51:49.920 --> 02:51:56.080
of computational language and natural language. And I found myself sometimes, you know, talking

02:51:56.080 --> 02:52:01.680
to chat GPT, trying to get it to write Wolfram language code, and I write it in pigeon form.

02:52:01.680 --> 02:52:09.840
So that means I'm combining, you know, Nestlist, this collection of, you know, whatever, you know,

02:52:09.840 --> 02:52:15.760
Nestlist is a term from Wolfram language, and I'm combining that. And chat GPT does a decent job

02:52:15.760 --> 02:52:19.600
of understanding that pigeon, probably would understand a pigeon between English and French

02:52:19.600 --> 02:52:25.840
as well, of, you know, a smushing together of those languages. But yes, I think that's far

02:52:25.840 --> 02:52:30.320
from impossible. And what's the incentive for young people that are like eight years old, nine,

02:52:30.320 --> 02:52:36.320
ten, that are starting to interact with chat GPT to learn the normal natural language, right?

02:52:36.320 --> 02:52:43.440
The full poetic language. What's the, why? The same way we learn emojis and shorthand when

02:52:43.440 --> 02:52:50.400
you're texting. Yes. They'll learn, like language will have a strong incentive to evolve into

02:52:51.440 --> 02:52:56.800
maximally computational kind of language. Perhaps. You know, I had this experience a

02:52:56.800 --> 02:53:02.960
number of years ago, I happened to be visiting a person I know on the West Coast who's worked

02:53:02.960 --> 02:53:07.440
with a bunch of kids aged, I don't know, 10, 11 years old or something, who'd learnt Wolfram

02:53:07.440 --> 02:53:14.640
language really well. And these kids learnt it so well, they were speaking it. And so show up and

02:53:14.640 --> 02:53:18.320
they're like saying, oh, you know, this thing, they're speaking this language. I never heard

02:53:18.320 --> 02:53:23.280
it as a spoken language. They were very disappointed that I couldn't understand it at the speed that

02:53:23.280 --> 02:53:29.040
they were speaking it. It's like kind of, it's, and so I think that's, I mean, I've actually

02:53:29.040 --> 02:53:34.000
thought quite a bit about how to turn computational language into a convenient spoken language. I

02:53:34.000 --> 02:53:39.200
haven't quite figured that out. Oh, it's spoken because it's readable, right? Yeah, it's readable

02:53:39.200 --> 02:53:44.160
as a, you know, as a way that we would read text. But if you actually want to speak it and it's

02:53:44.160 --> 02:53:47.680
useful, you know, if you're trying to talk to somebody about writing a piece of code, it's

02:53:47.680 --> 02:53:53.200
useful to be able to say something and it should be possible. And I think it's very frustrating

02:53:53.200 --> 02:53:56.480
it's one of those problems that maybe I, maybe this is one of these things where I should try

02:53:56.480 --> 02:54:01.200
and get an LLM to help me. How to make it speakable. How to maybe, maybe it's easier than you realize

02:54:01.200 --> 02:54:05.920
when you. I think it is easier. I think it's one idea or so. I think it's going to be something

02:54:05.920 --> 02:54:11.520
where, you know, the fact is it's a tree structured language, just like human language is a tree

02:54:11.520 --> 02:54:15.440
structured language. And I think it's going to be one of these things where one of the

02:54:15.440 --> 02:54:21.280
requirements that I've had is that whatever the spoken version is that dictation should be easy.

02:54:21.280 --> 02:54:26.080
That is that shouldn't be the case that you have to relearn how the whole thing works.

02:54:26.080 --> 02:54:33.520
It should be the case that, you know, that open bracket is just a, uh, or something and it's,

02:54:33.520 --> 02:54:40.720
you know, and then, um, um, but you know, human language has a lot of tricks that are, I mean,

02:54:40.720 --> 02:54:47.920
for example, human language has, has features that are sort of optimized, keep things within

02:54:47.920 --> 02:54:53.680
the bounds that our brains can easily deal with. Like I, you know, I tried to teach a transformer

02:54:53.680 --> 02:54:59.120
neural net to do parenthesis matching. It's pretty crummy at that. It, it, um, and in chat,

02:54:59.120 --> 02:55:04.000
GBT is similarly quite crummy at parenthesis matching. You can do it for small parenthesis

02:55:04.000 --> 02:55:07.920
things for the same size of parenthesis things where if I look at it as a human,

02:55:07.920 --> 02:55:11.840
I can immediately say these are matched. These are not matched, but as soon as it gets big,

02:55:11.840 --> 02:55:16.320
as soon as it gets kind of to the point where sort of a deeper computation, it's hopeless.

02:55:16.960 --> 02:55:22.560
And, but the fact is that human language has avoided, for example, the deep sub clauses,

02:55:22.560 --> 02:55:26.640
you know, we don't, um, uh, you know, we, we arrange things that we don't end up with these

02:55:26.640 --> 02:55:31.600
incredibly deep things, um, because brains are not well set up to deal with that. And we,

02:55:31.600 --> 02:55:36.640
it's found lots of tricks and maybe that's what we have to do to make sort of a spoken version,

02:55:36.640 --> 02:55:43.200
uh, a, a human speakable version, because, because what we can do visually is a little

02:55:43.200 --> 02:55:47.520
different than what we can do in the very sequentialized way that we, that we hear

02:55:47.520 --> 02:55:54.880
things in the audio domain. Let me just ask you about MIT briefly. So there's now,

02:55:54.880 --> 02:55:58.560
there's a college of engineering and there's a new college of computing. It's interesting.

02:55:58.560 --> 02:56:03.280
I want to linger on this computer science department thing. So MIT has EECS electrical

02:56:03.280 --> 02:56:07.760
engineering, computer science. Um, what do you think college of computing will be doing

02:56:08.480 --> 02:56:14.000
like in 20 years? What, what like, what happens to computer science? Like really?

02:56:14.000 --> 02:56:21.120
This is the question. This is, you know, everybody should learn kind of whatever CX really is.

02:56:21.120 --> 02:56:25.520
Okay. This, how to think about the world computationally, everybody should learn

02:56:25.520 --> 02:56:31.600
those concepts and, uh, you know, it's, uh, and some people will learn them at a quite, quite

02:56:31.600 --> 02:56:35.360
formal level and they'll learn computational language and things like that. Other people

02:56:35.440 --> 02:56:42.960
will just learn, you know, uh, sound is represented as, you know, digital data

02:56:42.960 --> 02:56:47.280
and they'll get some idea of spectrograms and frequencies and things like this.

02:56:47.280 --> 02:56:51.760
And maybe that doesn't, or they'll learn things like, you know, a lot of things that are sort

02:56:51.760 --> 02:56:57.600
of data science ish statistics ish. Like if you say, Oh, I've got these, you know, these people

02:56:57.600 --> 02:57:04.000
who, who, um, uh, picked their favorite kind of candy or something. And I've got, um, you know,

02:57:04.000 --> 02:57:08.480
what's the best kind of candy given that I've done the sample of all these people and they all

02:57:08.480 --> 02:57:13.920
rank the candies in different ways. You know, how do you think about that? That's sort of a computational

02:57:13.920 --> 02:57:18.480
X kind of thing. You might say, Oh, it's, I don't know what that is. Is it statistics? Is it data

02:57:18.480 --> 02:57:22.800
science? I don't really know, but kind of how to think about a question like that. Oh, like a ranking

02:57:22.800 --> 02:57:28.240
of preferences. Yeah. Yeah. Yeah. And then how to aggregate those, those ranked preferences into an

02:57:28.240 --> 02:57:32.640
overall thing. You know, how does that work? Um, you know, how, how should you think about that?

02:57:32.960 --> 02:57:38.160
Cause you can just tell, you might just tell chat GBT sort of, uh, I don't know, even, even the

02:57:38.160 --> 02:57:43.840
concept of an average, it's not obvious that, you know, that's a concept that people, it's worth

02:57:43.840 --> 02:57:48.320
people knowing. That's a rather straightforward concept. People, people, you know, have learned

02:57:48.320 --> 02:57:53.360
in kind of mathy ways right now, but there, there are lots of things like that about how do you kind

02:57:53.360 --> 02:57:58.800
of have these ways to sort of organize and formalize the world. And that's, and these things,

02:57:58.800 --> 02:58:03.040
sometimes they live in math. Sometimes they live in, in, I don't know what they, you know,

02:58:03.040 --> 02:58:07.120
I don't know what, you know, learning about color space. I have no idea what, I mean, you know,

02:58:07.120 --> 02:58:11.760
that's, that's obviously a field of those. Uh, it could be vision science or no color space,

02:58:11.760 --> 02:58:16.640
no color space. That's, that would be optics. So like, not really, it's not optics. Optics is

02:58:16.640 --> 02:58:22.000
about, you know, lenses and chromatic aberration of lenses and things like that. It's more like

02:58:22.000 --> 02:58:27.040
design and art. Is that, no, I mean, it's, it's like, you know, RGB space, XYZ space, you know,

02:58:27.040 --> 02:58:31.040
hue, saturation, brightness space, all these kinds of things, these different ways to describe

02:58:31.040 --> 02:58:36.880
colors. Right. But doesn't the application define what that, like, because obviously artists and

02:58:36.880 --> 02:58:42.160
designers use the color space to explore. Sure. Sure. No, I mean, it's just an example of kind of

02:58:42.160 --> 02:58:47.200
how do you, you know, the typical person, how do you, how do you describe what a color is?

02:58:47.200 --> 02:58:50.960
Or there are these numbers that describe what a color is. Well, it's worth, you know,

02:58:50.960 --> 02:58:56.560
if you're an eight year old, you won't necessarily know, you know, it's not something we're born with

02:58:56.560 --> 02:59:02.240
to know that, you know, colors can be described by three numbers. That's something that you have to,

02:59:02.240 --> 02:59:07.840
you know, it's a thing to learn about the world, so to speak. And I think that, you know, that whole

02:59:07.840 --> 02:59:13.920
corpus of things that are learning about the formalization of the world or the computationalization

02:59:13.920 --> 02:59:19.760
of the world, that's something that should be part of kind of standard education. And, you know,

02:59:19.760 --> 02:59:24.000
there isn't a, you know, there isn't a course, a curriculum for that. And by the way, whatever

02:59:24.000 --> 02:59:28.800
might have been in it just got changed because of LLMs and so on. Significantly. And I would,

02:59:30.320 --> 02:59:35.920
watching closely with interest, seeing how universities adapt. Well, you know, so one of

02:59:35.920 --> 02:59:42.960
my projects for hopefully this year, I don't know, is to try and write sort of a reasonable

02:59:42.960 --> 02:59:49.120
textbook, so to speak, of whatever this thing, CX, whatever it is, you know, what should you know?

02:59:49.120 --> 02:59:53.520
You know, what should you know about like what a bug is? What is the intuition about bugs? What's

02:59:53.520 --> 02:59:58.000
intuition about, you know, software testing? What is it? What is it? You know, these are things which

02:59:58.000 --> 03:00:03.120
are, you know, they're not, I mean, those are things which have gotten taught in computer science

03:00:03.120 --> 03:00:09.040
as part of the trade of programming, but kind of the conceptual points about what these things are.

03:00:09.040 --> 03:00:13.120
You know, it surprised me just at a very practical level. You know, I wrote this little explainer

03:00:13.120 --> 03:00:17.920
thing about chat GPT, and I thought, well, you know, I'm writing this partly because I wanted

03:00:17.920 --> 03:00:24.000
to make sure I understood it myself and so on. And it's been, you know, it's been really popular,

03:00:24.000 --> 03:00:30.480
and surprisingly so. And then I realized, well, actually, you know, I was sort of assuming,

03:00:30.480 --> 03:00:33.600
I didn't really think about it, actually, I just thought this is something I can write.

03:00:33.600 --> 03:00:39.920
And I realized, actually, it's a level of description that is kind of, you know, what has

03:00:39.920 --> 03:00:46.560
to be, it's not the engineering level description. It's not the kind of just the qualitative kind of

03:00:46.560 --> 03:00:52.400
description. It's some kind of sort of expository mechanistic description of what's going on,

03:00:52.960 --> 03:00:56.480
together with kind of the bigger picture of the philosophy of things and so on.

03:00:56.480 --> 03:00:59.120
And I realized, actually, this is a pretty good thing for me to write. I, you know,

03:00:59.120 --> 03:01:04.640
I kind of know those things. And I kind of realized it's not a collection of things that,

03:01:04.640 --> 03:01:10.560
you know, it's, I've sort of been, I was sort of a little shocked that it's as much of an outlier

03:01:10.560 --> 03:01:15.200
in terms of explaining what's going on as it's turned out to be. And that makes me feel more of

03:01:15.200 --> 03:01:20.800
an obligation to kind of write the kind of, you know, what is, you know, what is this thing that

03:01:20.800 --> 03:01:26.000
you should learn about, about the computationalization, the formalization of the world? Because,

03:01:26.000 --> 03:01:30.960
well, I've spent much of my life working on the kind of tooling and mechanics of that,

03:01:30.960 --> 03:01:36.000
and the science you get from it. So I guess this is my, my kind of obligation to try to do this.

03:01:36.000 --> 03:01:40.400
But I think, so if you ask what's going to happen to like the computer science departments and so on,

03:01:41.280 --> 03:01:45.840
there's some interesting models. So for example, let's take math. You know, math is the thing

03:01:45.840 --> 03:01:50.640
that's important for all sorts of fields, you know, engineering, you know, even, you know,

03:01:51.280 --> 03:01:56.560
chemistry, psychology, whatever else. And I think different universities have kind of evolved that

03:01:56.560 --> 03:02:02.240
differently. I mean, some say all the math is taught in the math department. And some say,

03:02:02.240 --> 03:02:07.040
well, we're going to have a, you know, a math for chemists or something that is taught in the

03:02:07.040 --> 03:02:12.560
chemistry department. And, you know, I think that this question of whether there is a

03:02:12.560 --> 03:02:19.920
centralization of the teaching of sort of CX is an interesting question. And I think, you know,

03:02:19.920 --> 03:02:25.680
the way it evolved with math, you know, people understood that math was sort of a separately

03:02:25.680 --> 03:02:34.240
teachable thing and was kind of a, you know, an independent element as opposed to just being

03:02:34.240 --> 03:02:39.200
absorbed into now. So if you take the example of writing English or something like this,

03:02:40.160 --> 03:02:46.720
the first point is that, you know, at the college level, at least fancy colleges,

03:02:46.720 --> 03:02:52.320
there's a certain amount of English writing that people do, but mostly it's kind of assumed that

03:02:52.320 --> 03:02:57.280
they pretty much know how to write, you know, that's something they learned at an earlier stage in

03:02:57.280 --> 03:03:03.200
education. Maybe rightly or wrongly believing that, but that's a different issue. The,

03:03:04.880 --> 03:03:10.880
well, I think it reminds me of my kind of, as I've tried to help people do technical writing and

03:03:10.880 --> 03:03:16.480
things, I'm always reminded of my zeroth law of technical writing, which is if you don't understand

03:03:16.480 --> 03:03:23.280
what you're writing about, your readers do not stand a chance. And so it's, I think the

03:03:23.280 --> 03:03:32.960
thing that has, you know, when it comes to like writing, for example, you know,

03:03:32.960 --> 03:03:38.560
people in different fields are expected to write English essays and they're not, you know, mostly

03:03:39.200 --> 03:03:45.440
the, you know, the history department or the engineering department, they don't have their own,

03:03:45.440 --> 03:03:50.560
you know, let's, you know, it's not like there's a, I mean, it's a thing which sort of people are

03:03:50.560 --> 03:03:55.680
assumed to have a knowledge of how to write that they can use in all these different fields.

03:03:56.240 --> 03:04:01.680
And the question is, you know, some level of knowledge of math is kind of assumed by the time

03:04:01.680 --> 03:04:07.360
you get to the college level, but plenty is not. And that's sort of still centrally taught. The

03:04:07.360 --> 03:04:14.640
question is sort of how tall is the tower of kind of CX that you need before you can just go use it

03:04:14.640 --> 03:04:19.520
in all these different fields. And, you know, there will be experts who want to learn the full

03:04:19.520 --> 03:04:26.560
elaborate tower and that will be kind of the CS, CX, whatever department, but there'll also be

03:04:26.560 --> 03:04:31.040
everybody else who just needs to know a certain amount of that to be able to go and do their art

03:04:31.040 --> 03:04:37.760
history classes and so on. Yeah. Is it just a single class that everybody is required to take?

03:04:37.760 --> 03:04:41.680
I don't know. I don't know how big it is yet. I hope to kind of define this curriculum and

03:04:41.680 --> 03:04:49.360
I'll figure out whether it's some. My guess is that, I don't know, I don't really understand

03:04:49.360 --> 03:04:56.400
universities and professoring that well, but my rough guess would be a year of college class

03:04:57.040 --> 03:05:02.720
will be enough to get to the point where most people have a reasonably broad knowledge of,

03:05:02.720 --> 03:05:08.880
you know, will be sort of literate in this kind of computational way of thinking about things.

03:05:08.880 --> 03:05:14.400
Yeah. Basic literacy. I'm still stuck perhaps because I'm hungry in the

03:05:15.280 --> 03:05:19.440
rating of human preferences for candy. So I have to ask, what's the best candy?

03:05:19.440 --> 03:05:24.320
I like this ELO rating for candy. Somebody should come up because you're somebody who says you like

03:05:24.320 --> 03:05:29.520
chocolate. What do you think is the best? I'll probably put milk duds up there. I don't know

03:05:29.520 --> 03:05:33.920
if you know. Do you have a preference for chocolate or candy? Oh, I have lots of preferences.

03:05:34.000 --> 03:05:39.920
I've, one of my all time favorites is my whole life is these things, these flake things,

03:05:39.920 --> 03:05:46.640
Cadbury flakes, which are not much sold in the US. And I've always thought that was a sign of a

03:05:46.640 --> 03:05:51.520
lack of respect for the American consumer because there are these sort of aerated chocolate that's

03:05:51.520 --> 03:05:57.680
made in a whole sort of, it's kind of a sheet of chocolate that's kind of folded up. And when you

03:05:57.680 --> 03:06:04.560
eat it, flakes fall all over the place. Ah, so it requires a kind of elegance. It requires

03:06:04.560 --> 03:06:09.200
you to have an elegance. Well, I know. What I usually do is I eat them on a piece of paper

03:06:09.200 --> 03:06:13.760
or something. So you embrace the best and clean it up after? No, I actually eat the flakes.

03:06:14.640 --> 03:06:18.960
Because it turns out the way food tastes depends a lot on its physical structure.

03:06:20.480 --> 03:06:24.560
I've noticed when I eat pieces of chocolate, I usually have some little pieces of chocolate and

03:06:24.560 --> 03:06:28.800
I always break off little pieces partly because then I eat it less fast. But also because it

03:06:28.800 --> 03:06:35.760
actually tastes different. The small pieces have a different, you have a different experience than

03:06:35.760 --> 03:06:41.040
if you have the big slab of chocolate. For many reasons, yes. Slower, more intimate.

03:06:42.640 --> 03:06:46.800
I think it's also just pure physicality. Oh, the texture of it changes. Right.

03:06:46.800 --> 03:06:50.480
It's fascinating. Now I take back my milk does because that's your basic answer. Okay.

03:06:51.280 --> 03:06:54.800
Do you think consciousness is fundamentally computational?

03:06:56.000 --> 03:07:04.080
So when you think about CX, what can we turn to computation? And you're thinking about LLMs.

03:07:06.080 --> 03:07:11.760
Do you think the display of consciousness and the experience of consciousness, the hard problem,

03:07:12.320 --> 03:07:19.200
is fundamentally a computation? Yeah, what it feels like inside,

03:07:19.200 --> 03:07:28.560
so to speak, is I did a little exercise, eventually I'll post it, of what it's like to

03:07:28.560 --> 03:07:36.560
be a computer. It's kind of like, well, you get all the sensory input. The way I see it is from

03:07:36.560 --> 03:07:41.920
the time you boot a computer to the time the computer crashes is like a human life. You're

03:07:41.920 --> 03:07:46.560
building up a certain amount of state in memory. You remember certain things about your, quote,

03:07:46.560 --> 03:07:53.680
life eventually. It's kind of like the next generation of humans is born from the same

03:07:53.680 --> 03:07:58.080
genetic material, so to speak, with a little bit left over, left on the disk, so to speak.

03:07:58.960 --> 03:08:04.960
And then the new fresh generation starts up and eventually all kinds of crud builds up in the

03:08:04.960 --> 03:08:09.520
memory of the computer and eventually the thing crashes or whatever. Or maybe it has some trauma

03:08:09.520 --> 03:08:14.320
because you plugged in some weird thing to some port of the computer and that made it crash.

03:08:17.520 --> 03:08:26.560
But you have this picture from startup to shut down, what is the life of a computer, so to speak,

03:08:26.560 --> 03:08:30.720
and what does it feel like to be that computer and what inner thoughts does it have and how

03:08:30.720 --> 03:08:35.360
do you describe it? It's kind of interesting as you start writing about this to realize

03:08:35.360 --> 03:08:40.320
it's awfully like what you'd say about yourself. That is, it's awfully like even an ordinary

03:08:40.320 --> 03:08:46.160
computer, forgetting all the AI stuff and so on. It has a memory of the past.

03:08:46.240 --> 03:08:51.200
It has certain sensory experiences. It can communicate with other computers, but it has

03:08:51.200 --> 03:08:59.200
to package up how it's communicating in some kind of language like form so it can map what's in its

03:08:59.200 --> 03:09:03.920
memory to what's in the memory of some other computer. It's a surprisingly similar thing.

03:09:04.480 --> 03:09:10.080
I had an experience just a week or two ago. I'm a collector of all possible data about

03:09:10.080 --> 03:09:15.040
myself and other things. And so I collect all sorts of weird medical data and so on.

03:09:15.040 --> 03:09:19.440
And one thing I hadn't collected was I'd never had a whole body MRI scan,

03:09:19.440 --> 03:09:24.480
so I went and got one of these. So I get all the data back. I'm looking at this thing. I've

03:09:24.480 --> 03:09:31.760
never looked at the insides of my brain, so to speak, in physical form. It's kind of

03:09:31.760 --> 03:09:36.960
psychologically shocking in a sense that here's this thing and you can see it has all these folds

03:09:36.960 --> 03:09:43.120
and all this structure. And it's like that's where this experience that I'm having of

03:09:43.840 --> 03:09:51.920
existing and so on, that's where it is. You look at that and you're thinking,

03:09:51.920 --> 03:09:55.600
how can this possibly be all this experience that I'm having? And you're realizing,

03:09:55.600 --> 03:10:04.880
well, I can look at a computer as well. I think this idea that you are having an experience

03:10:05.760 --> 03:10:16.720
that is somehow transcends the mere physicality of that experience. It's something that's hard

03:10:16.720 --> 03:10:23.840
to come to terms with, but I don't think I'm necessarily... My personal experience, I look

03:10:23.840 --> 03:10:29.840
at the MRI of the brain and then I know about all kinds of things about neuroscience and all that

03:10:29.840 --> 03:10:36.800
kind of stuff. And I still feel the way I feel, so to speak. And it sort of seems disconnected,

03:10:36.800 --> 03:10:43.040
but yet as I try and rationalize it, I can't really say that there's something kind of different

03:10:43.040 --> 03:10:47.680
about how I intrinsically feel from the thing that I can plainly see in the sort of physicality of

03:10:47.680 --> 03:10:52.320
what's going on. So do you think the computer, a large language model, will experience that

03:10:52.320 --> 03:10:57.920
transcendence? How does that make you feel? I tend to believe it will. I think an ordinary

03:10:57.920 --> 03:11:04.880
computer is already there. I think an ordinary computer is already kind of... Now, a large

03:11:04.880 --> 03:11:09.920
language model may experience it in a way that is much better aligned with us humans. That is,

03:11:09.920 --> 03:11:17.600
it's much more... If you could have the discussion with the computer, its intelligence, so to speak,

03:11:17.600 --> 03:11:24.000
is not particularly well aligned with ours. But the large language model is built to be aligned

03:11:24.000 --> 03:11:29.120
with our way of thinking about things. It would be able to explain that it's afraid of being shut

03:11:29.120 --> 03:11:34.880
off and deleted. It'd be able to say that it's sad of the way you've been speaking to it over

03:11:34.880 --> 03:11:40.320
the past two days. But that's a weird thing because when it says it's afraid of something,

03:11:41.360 --> 03:11:46.000
we know that it got that idea from the fact that it read on the internet.

03:11:46.000 --> 03:11:49.280
Yeah. Where did you get it, Steven? Where did you get it when you say you're afraid?

03:11:49.280 --> 03:11:54.160
You are quite. That's the question, right? Your parents, your friends?

03:11:54.160 --> 03:12:00.560
Right. Or my biology. In other words, there's a certain amount that is the endocrine system

03:12:00.560 --> 03:12:09.600
kicking in and these kinds of emotional overlay type things that are actually much more physical,

03:12:09.600 --> 03:12:16.720
even. They're much more sort of straightforwardly chemical than all of the higher level thinking.

03:12:16.720 --> 03:12:21.200
Yeah, but your biology didn't tell you to say I'm afraid just at the right time

03:12:21.200 --> 03:12:24.880
when people that love you are listening and so you know you're manipulating them

03:12:24.880 --> 03:12:32.160
by saying so. That's not your biology. That's a large language model in that biological neural

03:12:32.160 --> 03:12:39.440
network of yours. Yes. But I mean the intrinsic thing of something sort of shocking is just

03:12:39.440 --> 03:12:45.680
happening and you have some sort of reaction, which is some neurotransmitter gets secreted

03:12:45.680 --> 03:12:54.320
and that is the beginning of some, that's one of the pieces of input that then drives,

03:12:54.320 --> 03:13:00.480
it's kind of like a prompt for the large language model. I mean, just like when we dream, for

03:13:00.480 --> 03:13:06.240
example, no doubt there are all these sort of random inputs that kind of these random prompts

03:13:06.240 --> 03:13:10.800
and then it's percolating through in kind of the way that a large language model does

03:13:10.880 --> 03:13:16.320
of kind of putting together things that seem meaningful. I mean, are you worried about this

03:13:16.320 --> 03:13:22.160
world where you teach a lot on the internet and there's people asking questions and comments and

03:13:22.160 --> 03:13:30.960
so on? You have people that work remotely. Are you worried about this world when large language

03:13:30.960 --> 03:13:39.520
models create human like bots that are leaving the comments, asking the questions, or might even

03:13:39.520 --> 03:13:48.880
become fake employees? Yeah. I mean, or worse or better yet, friends are yours. Right. Look,

03:13:48.880 --> 03:13:55.360
I mean, one point is my mode of life has been I build tools and then I use the tools. Yeah.

03:13:55.360 --> 03:14:01.600
And in a sense, kind of, I'm building this tower of automation, which, and in a sense,

03:14:02.560 --> 03:14:07.680
when you make a company or something, you are making sort of automation, but it has some humans

03:14:07.680 --> 03:14:14.320
in it, but also as much as possible, it has computers in it. And so I think it's sort of

03:14:14.320 --> 03:14:21.600
an extension of that. Now, if I really didn't know that, it's a funny question. I mean,

03:14:21.600 --> 03:14:26.320
it's a funny issue when, if we think about sort of what's going to happen to the future of kind

03:14:26.320 --> 03:14:32.320
of jobs people do and so on, and there are places where kind of having a human in the loop, there

03:14:32.320 --> 03:14:36.480
are different reasons to have a human in the loop. For example, you might want a human in the loop

03:14:36.480 --> 03:14:42.720
because you want another human to be invested in the outcome. You want a human flying the plane

03:14:42.720 --> 03:14:47.440
who's going to die if the plane crashes along with you, so to speak. And that gives you sort

03:14:47.440 --> 03:14:53.120
of confidence that the right thing is going to happen. Or you might want, right now, you might

03:14:53.120 --> 03:14:58.800
want a human in the loop in some kind of sort of human encouragement persuasion type profession.

03:14:59.360 --> 03:15:03.360
Whether that will continue, I'm not sure for those types of professions because it may be

03:15:03.360 --> 03:15:10.640
that the greater efficiency of being able to have sort of just the right information delivered

03:15:10.640 --> 03:15:16.560
at just the right time will overcome the kind of, oh, yes, I want a human there.

03:15:16.560 --> 03:15:23.920
Yeah, imagine like a therapist or even higher stake, like a suicide hotline operated by

03:15:23.920 --> 03:15:28.640
a large language model. Oh, boy, it's a pretty high stake situation.

03:15:28.640 --> 03:15:34.080
Right. But I mean, you know, it might, in fact, do the right thing because it might be the case

03:15:35.440 --> 03:15:42.400
and that's really partly a question of sort of how complicated is the human. One of the things

03:15:42.400 --> 03:15:48.320
that's always surprising in some sense is that sometimes human psychology is not that complicated

03:15:48.320 --> 03:15:54.640
in some sense. You wrote the blog post, the 50-year quest, my personal journey, good title,

03:15:54.640 --> 03:16:01.520
my personal journey with a second law of thermodynamics. So what is this law and

03:16:01.520 --> 03:16:05.520
what have you understood about it in the 50-year journey you had with it?

03:16:05.520 --> 03:16:09.360
Right. So second law of thermodynamics, sometimes called law of entropy increase,

03:16:10.080 --> 03:16:16.560
is this principle of physics that says, well, my version of it would be things tend to get

03:16:16.560 --> 03:16:23.280
more random over time. A version of it that there are many different sort of formulations of it

03:16:23.280 --> 03:16:29.680
that things like heat doesn't spontaneously go from a hotter body to a colder one. When you have

03:16:30.880 --> 03:16:37.680
mechanical work kind of gets dissipated into heat, you have friction and kind of when you

03:16:37.680 --> 03:16:43.360
systematically move things, eventually they'll be sort of that the energy of moving things gets

03:16:43.360 --> 03:16:51.200
kind of ground down into heat. So people first sort of paid attention to this back in the 1820s

03:16:51.760 --> 03:16:57.120
when steam engines were a big thing. And the big question was, how efficient could a steam engine

03:16:57.120 --> 03:17:04.240
be? And there's this chap called Sadi Carnot, who was a French engineer. Actually, his father

03:17:04.240 --> 03:17:13.840
was a sort of elaborate mathematical engineer in France. But he figured out these kind of rules

03:17:13.840 --> 03:17:20.720
for how kind of the efficiency of the possible efficiency of something like a steam engine.

03:17:21.440 --> 03:17:29.120
And in sort of a side part of what he did was this idea that mechanical energy tends to get

03:17:29.120 --> 03:17:36.080
dissipated as heat, that you end up going from sort of systematic mechanical motion to this kind of

03:17:36.080 --> 03:17:41.680
random thing. Well, at that time, nobody knew what heat was. At that time, people thought that heat

03:17:41.680 --> 03:17:50.320
was a fluid, like they called it caloric. And it was a fluid that kind of was absorbed into substances.

03:17:50.320 --> 03:17:57.600
And when one hot thing would transfer heat to a colder thing, that this fluid would flow from

03:17:57.600 --> 03:18:04.960
the hot thing to the colder thing. Anyway, then by the 1860s, people had kind of come up with this

03:18:04.960 --> 03:18:16.880
idea that systematic energy tends to degrade into kind of random heat that could then not be

03:18:16.880 --> 03:18:24.320
easily turned back into systematic mechanical energy. And then that quickly became sort of a

03:18:24.320 --> 03:18:29.680
global principle about how things work. Question is, why does it happen that way? So, you know,

03:18:29.680 --> 03:18:34.880
let's say you have a bunch of molecules in a box, and these molecules are arranged in a very nice

03:18:34.880 --> 03:18:41.280
sort of flotilla of molecules in one corner of the box. And then what you typically observe is

03:18:41.280 --> 03:18:46.720
that after a while, these molecules were kind of randomly arranged in the box. Question is,

03:18:46.720 --> 03:18:52.560
why does that happen? And people, for a long, long time, tried to figure out, is there,

03:18:52.560 --> 03:18:56.720
from the laws of mechanics that determine how these molecules, let's say these molecules like

03:18:56.720 --> 03:19:01.680
hard spheres bouncing off each other, from the laws of mechanics that describe those molecules,

03:19:01.680 --> 03:19:10.400
can we explain why it tends to be the case that we see things that are orderly sort of degrade into

03:19:10.400 --> 03:19:20.080
disorder? We tend to see things that, you know, you scramble an egg, you take something that's

03:19:20.080 --> 03:19:25.840
quite ordered and you disorder it, so to speak. That's a thing that sort of happens quite regularly,

03:19:25.840 --> 03:19:32.800
or you put some ink into water and it will eventually spread out and fill up the water.

03:19:34.000 --> 03:19:39.440
But you don't see those little particles of ink in the water all spontaneously kind of

03:19:39.440 --> 03:19:43.680
arrange themselves into a big blob and then, you know, jump out of the water or something.

03:19:44.720 --> 03:19:49.680
And so the question is, why do things happen in this kind of irreversible way,

03:19:49.680 --> 03:19:56.400
where you go from order to disorder? Why does it happen that way? And so throughout, in the later

03:19:56.400 --> 03:20:02.080
part of the 1800s, a lot of work was done on trying to figure out, can one derive this principle,

03:20:02.080 --> 03:20:08.960
this second law of thermodynamics, this law about the dynamics of heat, so to speak,

03:20:08.960 --> 03:20:15.440
can one derive this from some fundamental principles and mechanics? You know,

03:20:15.440 --> 03:20:20.000
in the laws of thermodynamics, the first law is basically the law of energy conservation,

03:20:20.000 --> 03:20:25.840
that the total energy associated with heat plus the total energy associated with mechanical kinds

03:20:25.840 --> 03:20:30.480
of things plus other kinds of energy, that that total is constant. And that became a pretty

03:20:30.480 --> 03:20:36.400
well-understood principle. But the second law of thermodynamics was always mysterious. Like,

03:20:36.400 --> 03:20:43.280
why does it work this way? Can it be derived from underlying mechanical laws? And so when I was,

03:20:44.560 --> 03:20:49.520
well, 12 years old, actually, I had gotten interested, well, I'd been interested in

03:20:49.520 --> 03:20:54.800
space and things like that, because I thought that was kind of the future and interesting

03:20:54.800 --> 03:21:00.880
sort of technology and so on. And for a while, kind of, you know, every deep space probe was

03:21:00.880 --> 03:21:05.520
sort of a personal friend type thing. I knew all kinds of characteristics of it. And

03:21:06.880 --> 03:21:13.600
was kind of writing up all these things when I was, I don't know, 8, 9, 10 years old and so on.

03:21:13.600 --> 03:21:17.840
And then I got interested from being interested in kind of spacecraft. I got interested in,

03:21:17.840 --> 03:21:22.160
like, how do they work? What are all the instruments on them and so on? And that got me

03:21:22.160 --> 03:21:25.840
interested in physics, which was just as well, because if I'd stayed interested in space

03:21:26.400 --> 03:21:32.720
in the, you know, mid to late 1960s, I would have had a long wait before, you know, space really

03:21:32.720 --> 03:21:35.200
blossomed as an area. But-

03:21:35.200 --> 03:21:36.720
Having as everything.

03:21:36.720 --> 03:21:44.000
Right. I got interested in physics. And then, well, the actual sort of detailed story is when

03:21:44.000 --> 03:21:50.400
I kind of graduated from elementary school at age 12. And that's the time in England where

03:21:50.400 --> 03:21:57.200
you finish elementary school. I sort of, my gift, sort of, I suppose, more or less for myself was I

03:21:57.680 --> 03:22:04.800
got this collection of physics books, which were some college physics,

03:22:04.800 --> 03:22:11.040
course of college physics books, and volume five about statistical physics. And it has this picture

03:22:11.040 --> 03:22:18.640
on the cover that shows a bunch of kind of idealized molecules sitting in one side of a box.

03:22:18.640 --> 03:22:23.760
And then it has a series of frames showing how these molecules sort of spread out in the box.

03:22:23.760 --> 03:22:28.640
And I thought, that's pretty interesting. You know, what causes that? And, you know,

03:22:29.600 --> 03:22:34.880
read the book, and the book actually, one of the things that was really significant to me about

03:22:34.880 --> 03:22:39.760
that was the book kind of claimed, although I didn't really understand what it said in detail,

03:22:39.760 --> 03:22:45.760
it kind of claimed that this sort of principle of physics was derivable somehow. And, you know,

03:22:45.760 --> 03:22:52.000
other things I'd learned about physics, it was all like, it's a fact that energy is conserved. It's

03:22:52.000 --> 03:22:58.800
a fact that relativity works or something, not it's something you can derive from some fundamental

03:22:58.800 --> 03:23:03.600
sort of, it has to be that way as a matter of kind of mathematics or logic or something.

03:23:03.600 --> 03:23:07.680
So it was sort of interesting to me that there was a thing about physics that was kind of

03:23:07.680 --> 03:23:14.880
inevitably true and derivable, so to speak. And so I think that, so then, I was like,

03:23:14.880 --> 03:23:19.440
this picture on this book, and I was trying to understand it. And so that was actually the first

03:23:19.440 --> 03:23:25.920
serious program that I wrote for a computer was probably 1973, written for this computer,

03:23:25.920 --> 03:23:31.840
the size of a desk program with paper tape and so on. And I tried to reproduce this picture on the

03:23:31.840 --> 03:23:36.240
book, and it didn't succeed. What was the failure mode there? Like, what do you mean it didn't

03:23:36.240 --> 03:23:42.880
succeed? It didn't look like, okay, so what happened is, okay, many years later, I learned

03:23:42.880 --> 03:23:47.280
how the picture on the book was actually made, and that it was actually kind of a fake. But I

03:23:47.280 --> 03:23:53.360
didn't know that at that time. And that picture was actually a very high-tech thing when it was

03:23:53.360 --> 03:23:58.400
made in the beginning of the 1960s. It was made on the largest supercomputer that existed at the time,

03:23:58.960 --> 03:24:04.000
and even so, it couldn't quite simulate the thing that it was supposed to be simulating.

03:24:04.000 --> 03:24:09.200
But anyway, I didn't know that until many, many, many years later. So at the time, it was like,

03:24:09.200 --> 03:24:13.200
you have these balls bouncing around in this box, but I was using this computer with eight

03:24:13.200 --> 03:24:20.160
kilobytes of memory. They were 18-bit memory words, okay? So it was whatever, 24 kilobytes

03:24:20.160 --> 03:24:25.920
of memory. And it had these instructions. I probably still remember all of its machine

03:24:25.920 --> 03:24:32.000
instructions. And it didn't really like dealing with floating-point numbers or anything like that.

03:24:32.000 --> 03:24:37.680
And so I had to simplify this model of particles bouncing around the box. And so I thought, well,

03:24:37.680 --> 03:24:44.560
I'll put them on a grid and I'll make the things just sort of move one square at a time and so on.

03:24:44.560 --> 03:24:51.120
And so I did the simulation. And the result was it didn't look anything like the actual pictures

03:24:51.120 --> 03:24:59.520
on the book. Now, many years later, in fact, very recently, I realized that the thing I'd simulated

03:24:59.520 --> 03:25:03.760
was actually an example of a whole sort of computational irreducibility story

03:25:04.240 --> 03:25:08.800
that I absolutely did not recognize at the time. At the time, it just looked like it did something

03:25:08.800 --> 03:25:13.760
random and it looks wrong, as opposed to it did something random and it's super interesting that

03:25:13.760 --> 03:25:19.920
it's random. But I didn't recognize that at the time. And so as it was at the time, I got

03:25:19.920 --> 03:25:26.080
interested in particle physics and I got interested in other kinds of physics. But this whole second

03:25:26.080 --> 03:25:31.200
law of thermodynamics thing, this idea that sort of orderly things tend to degrade into disorder

03:25:31.840 --> 03:25:35.360
continued to be something I was really interested in. And I was really curious

03:25:35.360 --> 03:25:39.760
for the whole universe, why doesn't that happen all the time? Like we start off

03:25:40.720 --> 03:25:44.400
in the Big Bang at the beginning of the universe was this thing that seems like it's this very

03:25:44.400 --> 03:25:50.800
disordered collection of stuff. And then it spontaneously forms itself into galaxies

03:25:50.800 --> 03:25:56.320
and creates all of this complexity and order in the universe. And so I was very curious how that

03:25:56.320 --> 03:26:02.720
happens. But I was always kind of thinking this is kind of somehow the second law of thermodynamics

03:26:02.720 --> 03:26:09.360
is behind it trying to sort of pull things back into disorder, so to speak, and how was order being

03:26:09.360 --> 03:26:16.160
created. And so actually I was interested, this is probably now 1980, I got interested in kind of

03:26:16.160 --> 03:26:21.040
this galaxy formation and so on in the universe. I also at that time was interested in neural

03:26:21.040 --> 03:26:27.600
networks. And I was interested in kind of how brains make complicated things happen and so on.

03:26:27.600 --> 03:26:31.600
Okay, wait, wait, wait. What's the connection between the formation of galaxies and how

03:26:31.600 --> 03:26:35.280
brains make complicated things happen? Because they're both a matter of how complicated

03:26:35.280 --> 03:26:38.960
things come to happen. From simple origins?

03:26:38.960 --> 03:26:46.720
Yeah, from some sort of known origins. I had the sense that what I was interested in was kind of

03:26:46.720 --> 03:26:55.120
in all these different cases of where complicated things were arising from rules. And I also looked

03:26:55.120 --> 03:27:01.360
at snowflakes and things like that. I was curious and fluid dynamics in general. I was just sort of

03:27:01.360 --> 03:27:08.560
curious about how does complexity arise? And the thing that I didn't, it took me a while to kind

03:27:08.560 --> 03:27:13.600
of realize that there might be a general phenomenon. I sort of assumed, oh, there's galaxies over here,

03:27:13.600 --> 03:27:18.800
there's brains over here. They're very different kinds of things. And so what happened, this is

03:27:18.800 --> 03:27:25.520
probably 1981 or so, I decided, okay, I'm going to try and make the minimal model of how these

03:27:25.520 --> 03:27:31.600
things work. And it was sort of an interesting experience because I had built, starting in 1979,

03:27:31.600 --> 03:27:36.560
I built my first big computer system, the thing called SMP, Symbolic Manipulation Program. It's

03:27:36.560 --> 03:27:40.880
kind of a forerunner of modern morphology language with many of the same ideas about symbolic

03:27:40.880 --> 03:27:47.440
computation and so on. But the thing that was very important to me about that was in building

03:27:47.440 --> 03:27:53.680
that language, I had basically tried to figure out what were the relevant computational primitives,

03:27:53.680 --> 03:27:59.600
which have turned out to stay with me for the last 40 something years. But it was also important

03:27:59.600 --> 03:28:03.840
because in building a language, it was very different activity from natural science, which

03:28:03.840 --> 03:28:08.880
is what I'd mostly done before. Because in natural science, you start from the phenomena of the world

03:28:08.880 --> 03:28:12.160
and you try and figure out, so how can I make sense of the phenomena of the world?

03:28:12.880 --> 03:28:18.160
And kind of the world presents you with what it has to offer, so to speak, and you have to make

03:28:18.160 --> 03:28:26.240
sense of it. When you build a computer language or something, you are creating your own primitives,

03:28:26.240 --> 03:28:30.400
and then you say, so what can you make from these? Sort of the opposite way around from what you do

03:28:30.400 --> 03:28:35.280
in natural science. But I'd had the experience of doing that. And so I was kind of like, okay,

03:28:35.280 --> 03:28:39.840
what happens if you sort of make an artificial physics? What happens if you just make up the

03:28:39.840 --> 03:28:44.080
rules by which systems operate? And then I was thinking, you know, for all these different

03:28:44.080 --> 03:28:49.120
systems, whether it was galaxies or brains or whatever, what's the absolutely minimal model

03:28:49.120 --> 03:28:52.960
that kind of captures the things that are important about those systems?

03:28:52.960 --> 03:28:55.120
The computational primitives of that system.

03:28:55.120 --> 03:28:59.360
Yes. And so that's what ended up with the cellular automata,

03:28:59.360 --> 03:29:03.920
where you just have a line of black and white cells, you just have a rule that says, you know,

03:29:03.920 --> 03:29:07.920
given a cell and its neighbors, what will the color of the cell be on the next step? And you

03:29:07.920 --> 03:29:13.840
just run it in a series of steps. And the sort of the ironic thing is that cellular automata are

03:29:13.840 --> 03:29:20.640
great models for many kinds of things, but galaxies and brains are two examples where they do very,

03:29:20.640 --> 03:29:22.800
very badly. They're really irrelevant to those two cases.

03:29:22.800 --> 03:29:26.240
Is there a connection to the second law of thermodynamics and cellular automata?

03:29:26.240 --> 03:29:26.720
Oh, yes.

03:29:26.720 --> 03:29:30.800
So the things you've discovered about cellular automata?

03:29:30.800 --> 03:29:35.360
Yes. Okay. So when I first started selling cellular automata, my first papers about them

03:29:35.920 --> 03:29:39.680
were, you know, the first sentence was always about the second law of thermodynamics.

03:29:39.680 --> 03:29:44.720
It was always about how does order manage to be produced even though there's a second law

03:29:44.720 --> 03:29:49.200
of thermodynamics, which tries to pull things back into disorder. And I kind of, my early

03:29:49.200 --> 03:29:55.200
understanding of that had to do with these are intrinsically irreversible processes in cellular

03:29:55.200 --> 03:30:02.160
automata that can form orderly structures even from random initial conditions. But then what I

03:30:02.160 --> 03:30:08.560
realized this was, well, actually it's one of these things where it was a discovery that I

03:30:08.560 --> 03:30:13.520
should have made earlier, but didn't. So, you know, I had been studying cellular automata.

03:30:13.520 --> 03:30:17.760
What I did was the sort of most obvious computer experiment. You just try all the different rules

03:30:17.760 --> 03:30:22.080
and see what they do. It's kind of like, you know, you've invented a computational telescope. You

03:30:22.080 --> 03:30:26.400
just point it at the most obvious thing in the sky and then you just see what's there.

03:30:26.400 --> 03:30:31.040
And so I did that. And I, you know, was making all these pictures of how cellular automata work.

03:30:31.040 --> 03:30:36.400
And I started these pictures. I started in great detail. There was, you can number the rules for

03:30:36.400 --> 03:30:42.080
cellular automata. And one of them is, you know, rule 30. So I made a picture of rule 30 back in

03:30:42.080 --> 03:30:49.120
1981 or so and rule 30. Well, it's, and at the time I was just like, okay, it's another one of

03:30:49.120 --> 03:30:53.920
these rules. I don't really, it happens to be asymmetric left, right, asymmetric. And it's like,

03:30:53.920 --> 03:30:58.400
let me just consider the case of the symmetric ones just to keep things simpler, et cetera,

03:30:58.400 --> 03:31:04.240
et cetera, et cetera. And I just kind of ignored it. And then sort of in, and actually in 1984,

03:31:04.800 --> 03:31:11.840
strangely enough, I ended up having a, an early laser printer, which made very high resolution

03:31:11.840 --> 03:31:15.040
pictures. And I thought I'm going to print out an interesting, you know, I want to make an

03:31:15.040 --> 03:31:19.680
interesting picture. Let me take this rule 30 thing and just make a high resolution picture of

03:31:19.680 --> 03:31:25.280
it. I did. And it's, it has this very remarkable property that its rule is very simple. You started

03:31:25.280 --> 03:31:30.480
off just from one black cell at the top and it makes this kind of triangular pattern. But if you

03:31:30.480 --> 03:31:35.600
look inside this pattern, it looks really random. There's, you know, you look at the center column

03:31:35.600 --> 03:31:40.720
of cells and, you know, I studied that in great detail and it's so far as one can tell it's

03:31:40.720 --> 03:31:46.160
completely random. And it's kind of a little bit like digits of pi. Once you, you know,

03:31:46.160 --> 03:31:49.520
you know, the rule for generating the digits of pi, but once you've generated them, you know,

03:31:49.520 --> 03:31:56.320
3.14159, et cetera, they seem completely random. And in fact, I put up this prize back in, what was

03:31:56.320 --> 03:32:02.240
it? 2019 or something for prove anything about the sequence. Basically. Has anyone been able to do

03:32:02.240 --> 03:32:07.680
anything on that? People have sent me some things, but it's, you know, I don't know how hard these

03:32:07.680 --> 03:32:14.320
problems are. I mean, I was kind of spoiled because I, 2007, I put up a prize for determining

03:32:14.320 --> 03:32:19.680
whether a particular Turing machine that I thought was the simplest candidate for being a universal

03:32:19.680 --> 03:32:24.880
Turing machine, determine whether it is or isn't a universal Turing machine. And somebody did a

03:32:24.880 --> 03:32:28.880
really good job of, of winning that prize and proving that it was a universal Turing machine

03:32:28.880 --> 03:32:32.960
in about six months. And so I, you know, I didn't know whether that would be one of these problems

03:32:32.960 --> 03:32:37.120
that was out there for hundreds of years or whether in this particular case, young chap

03:32:37.120 --> 03:32:43.200
called Alex Smith, you know, nailed it in six months. And so with this rule 30 collection,

03:32:43.200 --> 03:32:47.680
I don't really know whether these are things that are a hundred years away from being able to,

03:32:47.680 --> 03:32:50.720
to get, or whether somebody is going to come and do something very clever.

03:32:50.720 --> 03:32:55.680
It's such a, I mean, it's like Fermat's last theorem, it's such a rule 30, such a simple

03:32:55.680 --> 03:33:02.640
formulation. It feels like anyone can look at it and understand it and feel like it's within grasp

03:33:02.640 --> 03:33:09.360
to be able to predict something, to do, to derive some kind of law that allows you to predict

03:33:09.360 --> 03:33:16.240
something about this middle column of rule 30. And yet you can't.

03:33:16.240 --> 03:33:21.360
Yeah, right. This is the intuitional surprise of computational irreducibility and so on,

03:33:21.360 --> 03:33:24.960
that even though the rules are simple, you can't tell what's going to happen

03:33:24.960 --> 03:33:32.560
and you can't prove things about it. And I think, so anyway, the thing I started in 1984 or so,

03:33:32.560 --> 03:33:37.120
I started realizing there's this phenomenon that you can have very simple rules. They produce

03:33:37.120 --> 03:33:41.520
apparently random behavior. Okay. So that's a little bit like the second law of thermodynamics

03:33:41.520 --> 03:33:47.760
because it's like you have this simple initial condition. You can, you know, readily see that

03:33:47.760 --> 03:33:53.680
it's very, you know, you can describe it very easily. And yet it makes this thing that seems

03:33:53.680 --> 03:33:59.680
to be random. Now it turns out there's some technical detail about the second law of

03:33:59.680 --> 03:34:07.360
thermodynamics and about the idea of reversibility. When you have kind of a movie of two,

03:34:07.360 --> 03:34:12.800
you know, billiard balls colliding and you see them collide and they bounce off and you run that

03:34:12.800 --> 03:34:17.280
movie in reverse, you can't tell which way was the forward direction of time and which way was the

03:34:17.280 --> 03:34:21.200
backward direction of time. When you're just looking at individual billiard balls, by the time

03:34:21.200 --> 03:34:27.120
you've got a whole collection of them, you know, a million of them or something, then it turns out

03:34:27.120 --> 03:34:32.960
to be the case, and this is the sort of the mystery of the second law, that the orderly thing,

03:34:32.960 --> 03:34:37.600
you start with the orderly thing and it becomes disordered and that's the forward direction in

03:34:37.600 --> 03:34:42.480
time. And the other way around of it starts disordered and becomes ordered, you just don't

03:34:42.480 --> 03:34:49.760
see that in the world. Now in principle, if you, you know, if you sort of traced the detailed

03:34:49.760 --> 03:34:57.120
motions of all those molecules backwards, you would be able to, it will, the reverse of time

03:34:57.120 --> 03:35:02.080
makes, you know, as you go forwards in time, order goes to disorder. As you go backwards in time,

03:35:02.080 --> 03:35:03.280
order goes to disorder.

03:35:03.280 --> 03:35:04.480
Perfectly so, yes.

03:35:04.480 --> 03:35:11.920
Right. So the mystery is, why is it the case that, or one version of the mystery is, why is it the

03:35:11.920 --> 03:35:18.400
case that you never see something which happens to be just the kind of disorder that you would need

03:35:18.400 --> 03:35:23.680
to somehow evolve to order? Why does that not happen? Why do you always just see order goes

03:35:23.680 --> 03:35:29.440
to disorder, not the other way around? So the thing that I kind of realized, I started realizing

03:35:29.440 --> 03:35:35.120
in the 1980s is kind of like, it's a bit like cryptography. It's kind of like you start off

03:35:35.120 --> 03:35:41.200
from this key that's pretty simple and then you kind of run it and you can get this, you know,

03:35:41.200 --> 03:35:50.080
complicated, random mess. And the thing that, well, I sort of started realizing back then

03:35:50.720 --> 03:35:58.000
was that the second law is kind of a story of computational reducibility. It's a story of,

03:35:58.480 --> 03:36:02.720
you know, what seems, you know, what we can describe easily at the beginning,

03:36:03.440 --> 03:36:08.080
we can only describe with a lot of computational effort at the end.

03:36:08.800 --> 03:36:18.720
Okay. So now we come many, many years later and I was trying to sort of, well, having done this big

03:36:18.720 --> 03:36:25.360
project to understand fundamental physics, I realized that sort of a key aspect of that

03:36:25.360 --> 03:36:31.040
is understanding what observers are like. And then I realized that the second law of thermodynamics

03:36:31.840 --> 03:36:39.680
is the same story as a bunch of these other cases. It is a story of a computationally bounded

03:36:39.680 --> 03:36:46.640
observer trying to observe a computationally irreducible system. So it's a story of, you know,

03:36:46.640 --> 03:36:50.880
underneath the molecules are bouncing around, they're bouncing around in this completely

03:36:52.080 --> 03:37:00.720
determined way, determined by rules. But the point is that we as computationally bounded observers

03:37:01.440 --> 03:37:05.600
can't tell that there were these sort of simple underlying rules.

03:37:05.600 --> 03:37:10.160
To us, it just looks random. And when it comes to this question about can you prepare the initial

03:37:10.160 --> 03:37:17.520
state so that, you know, the disordered thing is, you know, you have exactly the right disorder

03:37:17.520 --> 03:37:22.640
to make something orderly, a computationally bounded observer cannot do that. We'd have to

03:37:22.640 --> 03:37:27.920
have done all of this sort of irreducible computation to work out very precisely what

03:37:27.920 --> 03:37:33.200
this disordered state, what the exact right disordered state is, so that we would get this

03:37:33.200 --> 03:37:38.240
ordered thing produced from it. What does it mean to be computationally bounded observer,

03:37:39.440 --> 03:37:43.440
observing a computationally irreducible system? So the computationally bounded,

03:37:43.440 --> 03:37:49.520
is there something formal you can say there? Right. So it means, okay, you can talk about

03:37:49.520 --> 03:37:56.480
Turing machines, you can talk about computational complexity theory and, you know, polynomial time

03:37:56.480 --> 03:38:01.040
computation and things like this. There are a variety of ways to make something more precise,

03:38:01.040 --> 03:38:05.280
but I think it's more useful, the intuitive version of it is more useful, which is basically

03:38:05.280 --> 03:38:11.360
just to say that, you know, how much computation are you going to do to try and work out what's

03:38:11.360 --> 03:38:16.880
going on? And the answer is, you're not allowed to do a lot of, we're not able to do a lot of

03:38:16.880 --> 03:38:22.960
computation. When we, you know, we've got, you know, in this room, there will be a trillion,

03:38:22.960 --> 03:38:26.480
trillion, trillion molecules, a little bit less. It's a big room.

03:38:27.280 --> 03:38:34.800
Right. And, you know, at every moment, you know, every microsecond or something, these molecules

03:38:34.800 --> 03:38:41.200
are colliding, and that's a lot of computation that's getting done. And the question is,

03:38:41.200 --> 03:38:47.200
in our brains, we do a lot less computation every second than the computation done by all those

03:38:47.200 --> 03:38:53.760
molecules. If there is computational irreducibility, we can't work out in detail

03:38:53.760 --> 03:38:58.240
what all those molecules are going to do. What we can do is only a much smaller amount of

03:38:58.240 --> 03:39:05.120
computation. And so the second law of thermodynamics is this kind of interplay between the underlying

03:39:05.120 --> 03:39:11.200
computational irreducibility and the fact that we, as preparers of initial states or as measures

03:39:11.200 --> 03:39:16.720
of what happens are, you know, are not capable of doing that much computation. So to us,

03:39:17.360 --> 03:39:22.160
another big formulation of the second law of thermodynamics is this idea of the law of

03:39:22.160 --> 03:39:27.200
entropy increase. The characteristic that this universe, the entropy, seems to be always

03:39:27.200 --> 03:39:33.200
increasing. What does that show to you about the evolution of the universe through time?

03:39:35.200 --> 03:39:39.040
And that's very confused in the history of thermodynamics,

03:39:39.040 --> 03:39:44.800
because entropy was first introduced by a guy called Rudolf Clausius, and he did it in terms

03:39:44.800 --> 03:39:50.960
of heat and temperature, okay? Subsequently, it was reformulated by a guy called Ludwig Boltzmann,

03:39:52.160 --> 03:39:59.120
and he formulated it in a much more kind of combinatorial type way. But he always claimed

03:39:59.120 --> 03:40:04.880
that it was equivalent to Clausius' thing, and in one particular simple example it is.

03:40:05.440 --> 03:40:08.640
But that connection between these two formulations of entropy,

03:40:08.640 --> 03:40:14.160
they've never been connected. I mean, there's really, so okay, so the more general definition

03:40:14.160 --> 03:40:19.520
of entropy due to Boltzmann is the following thing. So you say, I have a system and it has

03:40:19.520 --> 03:40:24.160
many possible configurations. Molecules can be in many different arrangements, etc., etc., etc.

03:40:24.880 --> 03:40:30.480
If we know something about the system, for example, we know it's in a box, it has a certain

03:40:30.480 --> 03:40:35.200
pressure, it has a certain temperature, we know these overall facts about it. Then we say,

03:40:35.200 --> 03:40:40.960
how many microscopic configurations of the system are possible given those overall constraints?

03:40:42.400 --> 03:40:48.800
And the entropy is the logarithm of that number. That's the definition. And that's the kind of

03:40:48.800 --> 03:40:53.440
the general definition of entropy that turns out to be useful. Now in Boltzmann's time,

03:40:53.440 --> 03:40:58.960
he thought these molecules could be placed anywhere you want. He didn't think, but he said,

03:40:58.960 --> 03:41:04.240
oh, actually, we can make it a lot simpler by having the molecules be discrete. Actually,

03:41:04.240 --> 03:41:12.320
he didn't know molecules existed. In his time, 1860s and so on, the idea that matter might

03:41:12.320 --> 03:41:17.840
be made of discrete stuff had been floated ever since ancient Greek times, but it had been a long

03:41:17.840 --> 03:41:26.320
time debate about is matter discrete, is it continuous. At that time, people mostly thought

03:41:26.320 --> 03:41:32.000
that matter was continuous. It was all confused with this question about what heat is, and people

03:41:32.000 --> 03:41:41.840
thought heat was this fluid. It was a big muddle. Boltzmann said, let's assume there are discrete

03:41:41.840 --> 03:41:47.040
molecules. Let's even assume they have discrete energy levels. Let's say everything is discrete.

03:41:47.040 --> 03:41:52.000
Then we can do sort of combinatorial mathematics and work out how many configurations of these

03:41:52.000 --> 03:41:56.560
things there will be in the box, and we can say we can compute this entropy quantity.

03:41:58.160 --> 03:42:02.640
Of course, it's just a fiction that these things are discrete, so he said. This is an interesting

03:42:02.640 --> 03:42:08.720
piece of history, by the way. At that time, people didn't know molecules existed. There were other

03:42:08.720 --> 03:42:15.520
hints from looking at kind of chemistry that there might be discrete atoms and so on, just from the

03:42:15.520 --> 03:42:24.800
combinatorics of two amounts of hydrogen plus one amount of oxygen together make water, things like

03:42:24.800 --> 03:42:35.520
this. But it wasn't known that discrete molecules existed. In fact, it wasn't until the beginning

03:42:35.520 --> 03:42:41.600
of the 20th century that Brownian motion was the final giveaway. Brownian motion is you look under

03:42:41.600 --> 03:42:46.640
a microscope at these little pieces from pollen grains. You see they're being discreetly kicked,

03:42:46.640 --> 03:42:50.000
and those kicks are water molecules hitting them, and they're discrete.

03:42:51.520 --> 03:42:57.840
In fact, it was really quite interesting history. I mean, Boltzmann had worked out how things could

03:42:57.840 --> 03:43:04.960
be discrete and had basically invented something like quantum theory in the 1860s, but he just

03:43:04.960 --> 03:43:10.240
thought it wasn't really the way it worked. Then just a piece of physics history, because I think

03:43:10.240 --> 03:43:15.360
it's kind of interesting. In 1900, this guy called Max Planck, who had been a long-time

03:43:15.360 --> 03:43:19.760
thermodynamics person, everybody was trying to prove the second law of thermodynamics,

03:43:19.760 --> 03:43:25.360
including Max Planck. Max Planck believed that radiation, like electromagnetic radiation,

03:43:25.360 --> 03:43:29.680
somehow the interaction of that with matter was going to prove the second law of thermodynamics.

03:43:30.320 --> 03:43:36.480
But he had these experiments that people had done on blackbody radiation, and there were these curves,

03:43:36.480 --> 03:43:41.920
and you couldn't fit the curve based on his idea for how radiation interacted with matter,

03:43:42.480 --> 03:43:45.040
those curves. You couldn't figure out how to fit those curves.

03:43:45.600 --> 03:43:52.000
Except he noticed that if he just did what Boltzmann had done and assumed that electromagnetic

03:43:52.000 --> 03:43:58.560
radiation was discrete, he could fit the curves. He said, but this just happens to work this way.

03:43:59.120 --> 03:44:04.880
Then Einstein came along and said, well, by the way, the electromagnetic field might actually be

03:44:04.880 --> 03:44:11.840
discrete. It might be made of photons. Then that explains how this all works. In 1905,

03:44:16.160 --> 03:44:20.640
that piece of quantum mechanics got started. Interesting piece of history. I didn't know

03:44:20.640 --> 03:44:27.360
until I was researching this recently. In 1904 and 1903, Einstein wrote three different papers.

03:44:28.240 --> 03:44:34.960
So just sort of well-known physics history. In 1905, Einstein wrote these three papers.

03:44:34.960 --> 03:44:40.160
One introduced relativity theory, one explained Brownian motion, and one introduced basically

03:44:40.160 --> 03:44:50.080
photons. So kind of a big deal year for physics and for Einstein. But in the years before that,

03:44:50.080 --> 03:44:53.520
he'd written several papers, and what were they about? They were about the second law of

03:44:53.520 --> 03:44:58.080
thermodynamics, and they were an attempt to prove the second law of thermodynamics and their

03:44:58.080 --> 03:45:04.000
nonsense. So I had no idea that he'd done this. Interesting. Me neither.

03:45:04.000 --> 03:45:09.680
In fact, what he did, those three papers in 1905, well, not so much the relativity paper,

03:45:09.680 --> 03:45:16.240
the one on Brownian motion, the one on photons, both of these were about the story of sort of

03:45:16.240 --> 03:45:22.880
making the world discrete. He got that idea from Boltzmann, but Boltzmann didn't think,

03:45:23.120 --> 03:45:29.600
Boltzmann kind of died believing he said, but he has a quote actually, in the end, things are

03:45:29.600 --> 03:45:33.280
going to turn out to be discrete, and I'm going to write down what I have to say about this because

03:45:34.960 --> 03:45:39.440
eventually this stuff will be rediscovered and I want to leave what I can about how things are

03:45:39.440 --> 03:45:46.640
going to be discrete. But I think he has some quote about how one person can't stand against

03:45:46.640 --> 03:45:52.080
the tide of history in saying that matter is discrete.

03:45:52.720 --> 03:45:56.400
So he stuck with his guns in terms of matter is discrete.

03:45:56.400 --> 03:46:04.080
Yes, he did. What's interesting about this is at the time, everybody, including Einstein,

03:46:04.080 --> 03:46:08.320
kind of assumed that space was probably going to end up being discrete too, but that didn't work

03:46:08.320 --> 03:46:12.400
out technically because it wasn't consistent with the relativity theory, it didn't seem to be,

03:46:12.400 --> 03:46:17.200
and so then in the history of physics, even though people had determined that matter was

03:46:17.200 --> 03:46:24.880
discrete, electromagnetic field was discrete, space was a holdout of not being discrete.

03:46:24.880 --> 03:46:28.880
And in fact, Einstein, 1916, has this nice letter he wrote where he says,

03:46:28.880 --> 03:46:33.680
in the end, it will turn out space is discrete, but we don't have the mathematical tools necessary

03:46:33.680 --> 03:46:40.240
to figure out how that works yet. And so I think it's kind of cool that 100 years later we do.

03:46:40.240 --> 03:46:45.840
Yes. For you, you're pretty sure that every layer of reality is discrete.

03:46:45.840 --> 03:46:52.560
Right. And that space is discrete. And in fact, one of the things I realized recently is this kind

03:46:52.560 --> 03:47:04.640
of theory of heat that heat is really this continuous fluid. It's kind of like the caloric

03:47:04.640 --> 03:47:08.560
theory of heat, which turns out to be completely wrong because actually heat is the motion of

03:47:08.560 --> 03:47:12.560
discrete molecules. Unless you know there are discrete molecules, it's hard to understand what

03:47:12.560 --> 03:47:20.080
heat could possibly be. Well, I think space is discrete, and the question is kind of what's the

03:47:20.080 --> 03:47:28.640
analog of the mistake that was made with caloric in the case of space? And so my current guess

03:47:28.640 --> 03:47:35.680
is that dark matter is, as my little sort of aphorism of the last few months has been,

03:47:36.320 --> 03:47:42.320
dark matter is the caloric of our time. That is, it will turn out that dark matter is a feature

03:47:42.320 --> 03:47:48.560
of space, and it is not a bunch of particles. At the time when people were talking about heat,

03:47:48.560 --> 03:47:52.400
they knew about fluids, and they said, well, heat must just be another kind of fluid because

03:47:52.400 --> 03:47:56.880
that's what they knew about. But now people know about particles, and so they say, well, what's

03:47:56.880 --> 03:48:03.360
dark matter? It just must be particles. So what could dark matter be as a feature of space?

03:48:03.440 --> 03:48:10.800
Oh, I don't know yet. One of the things I'm hoping to be able to do is to find the analog

03:48:10.800 --> 03:48:17.200
of Brownian motion in space. So in other words, Brownian motion was seeing down to the level of

03:48:17.200 --> 03:48:23.520
an effect from individual molecules. And so in the case of space, most of the things we see about

03:48:23.520 --> 03:48:29.360
space so far, just everything seems continuous. Brownian motion had been discovered in the 1830s,

03:48:29.360 --> 03:48:36.880
and it was only identified what it was the result of by Smoluchowski and Einstein

03:48:36.880 --> 03:48:42.080
at the beginning of the 20th century. And dark matter was discovered, that phenomenon was

03:48:42.080 --> 03:48:48.000
discovered 100 years ago. The rotation curves of galaxies don't follow the luminous matter.

03:48:48.000 --> 03:48:53.600
That was discovered 100 years ago. And I think that I wouldn't be surprised if there isn't an

03:48:53.600 --> 03:48:59.840
effect that we already know about that is kind of the analog of Brownian motion that reveals the

03:48:59.840 --> 03:49:05.680
discreteness of space. And in fact, we're beginning to have some guesses, we have some evidence that

03:49:05.680 --> 03:49:10.640
black hole mergers work differently when there's discrete space. And there may be things that you

03:49:10.640 --> 03:49:16.160
can see in gravitational wave signatures and things associated with the discreteness of space.

03:49:16.160 --> 03:49:20.960
But this is kind of, for me, it's kind of interesting to see this sort of recapitulation

03:49:20.960 --> 03:49:27.280
of the history of physics, where people vehemently say, you know, matter is continuous,

03:49:27.280 --> 03:49:31.680
electromagnetic field is continuous, and turns out it isn't true. And then they say space is

03:49:31.680 --> 03:49:36.720
continuous. But so, you know, entropy is the number of states of the system consistent with

03:49:36.720 --> 03:49:44.560
some constraint. And the thing is that if you know in great detail the position of every molecule

03:49:44.640 --> 03:49:51.280
in the gas, the entropy is always zero, because there's only one possible state.

03:49:52.160 --> 03:49:56.720
The configuration of molecules in the gas, the molecules bounce around, they have a certain rule

03:49:56.720 --> 03:50:02.400
for bouncing around. There's just one state of the gas evolves to one state of the gas and so on.

03:50:02.400 --> 03:50:08.160
But it's only if you don't know in detail where all the molecules are, that you can say, well,

03:50:08.160 --> 03:50:12.880
the entropy increases because the things we do know about the molecules, there are more possible

03:50:12.880 --> 03:50:17.360
microscopic states of the system consistent with what we do know about where the molecules are.

03:50:18.080 --> 03:50:25.040
And so the question of whether, so people, this sort of paradox in a sense of, oh, if we knew where

03:50:25.040 --> 03:50:28.960
all the molecules were, the entropy wouldn't increase. There was this idea introduced by

03:50:29.840 --> 03:50:36.720
Gibbs in the early 20th century. Well, actually, the very beginning of the 20th century as a

03:50:36.720 --> 03:50:41.120
physics professor, an American physics professor, was sort of the first distinguished American

03:50:41.120 --> 03:50:49.120
physics professor at Yale. And he introduced this idea of coarse-graining, this idea that,

03:50:49.120 --> 03:50:53.760
well, these molecules have a detailed way they're bouncing around, but we can only observe a

03:50:53.760 --> 03:50:59.680
coarse-grained version of that. But the confusion has been nobody knew what a valid coarse-graining

03:50:59.680 --> 03:51:05.440
would be. So nobody knew that whether you could have this coarse-graining that very carefully

03:51:05.440 --> 03:51:12.320
was sculpted in just such a way that it would notice that the particular configurations that

03:51:12.320 --> 03:51:16.320
you could get from the simple initial condition, they fit into this coarse-graining, and the

03:51:16.320 --> 03:51:22.480
coarse-graining very carefully observes that. Why can't you do that kind of very detailed,

03:51:22.480 --> 03:51:28.000
precise coarse-graining? The answer is because if you are a computationally bounded observer,

03:51:28.000 --> 03:51:33.680
and the underlying dynamics is computationally irreducible, that's what defines possible

03:51:33.680 --> 03:51:39.840
coarse-graining is what a computationally bounded observer can do. And it's the fact

03:51:39.840 --> 03:51:47.920
that a computationally bounded observer is forced to look only at this kind of coarse-grained version

03:51:47.920 --> 03:51:54.880
of what the system is doing. That's why, and because what's going on underneath is it's kind

03:51:54.880 --> 03:52:02.880
of filling out the different possible, you're ending up with something where the sort of

03:52:02.880 --> 03:52:12.880
underlying computational irreducibility is your, if all you can see is what the coarse-grained

03:52:12.880 --> 03:52:19.840
result is with a sort of computationally bounded observation, then inevitably there are many

03:52:19.840 --> 03:52:23.440
possible underlying configurations that are consistent with that.

03:52:23.440 --> 03:52:28.560
Just to clarify, basically any observer that exists inside the universe

03:52:29.520 --> 03:52:34.080
is going to be computationally bounded. No, any observer like us. I don't know.

03:52:34.080 --> 03:52:35.360
I can't imagine- When you say like us,

03:52:35.360 --> 03:52:41.200
what do you mean? What do you mean like us? Well, humans with finite minds.

03:52:41.200 --> 03:52:49.440
You're including the tools of science. Yeah. Yeah. I mean, and as we have more precise,

03:52:49.440 --> 03:52:53.360
and by the way, there are little sort of microscopic violations of the second law

03:52:53.360 --> 03:52:57.680
of thermodynamics that you can start to have when you have more precise measurements of where

03:52:57.680 --> 03:53:04.800
precisely molecules are. But for a large scale, when you have enough molecules, we don't have,

03:53:05.680 --> 03:53:10.480
we're not tracing all those molecules and we just don't have the computational resources to do that.

03:53:11.120 --> 03:53:19.520
It wouldn't be, I think, to imagine what an observer who is not computationally bounded

03:53:19.520 --> 03:53:25.040
would be like. It's an interesting thing because, okay, so what does computational boundedness mean?

03:53:25.040 --> 03:53:31.920
Among other things, it means we conclude that definite things happen. We take all this complexity

03:53:31.920 --> 03:53:37.920
of the world and we make a decision. We're going to turn left or turn right. And that is kind of

03:53:37.920 --> 03:53:46.080
reducing all this kind of detail into we're observing it. We're sort of crushing it down

03:53:46.080 --> 03:53:54.160
to this one thing. And if we didn't do that, we wouldn't have all this sort of symbolic structure

03:53:54.160 --> 03:54:00.960
that we build up that lets us think things through with our finite minds. We'd be instead,

03:54:00.960 --> 03:54:04.240
you know, we'd be just sort of one with the universe.

03:54:04.240 --> 03:54:07.520
Yeah, so content to not simplify.

03:54:08.160 --> 03:54:15.040
Yes. If we didn't simplify, then we wouldn't be like us. We would be like the universe,

03:54:15.040 --> 03:54:21.360
like the intrinsic universe, but not having experiences like the experiences we have

03:54:21.360 --> 03:54:29.840
where we, for example, conclude that definite things happen. We sort of have this notion of

03:54:29.840 --> 03:54:35.680
being able to make sort of narrative statements. Yeah. I wonder if it's just like you imagined

03:54:35.680 --> 03:54:39.920
as a thought experiment what it's like to be a computer. I wonder if it's possible to

03:54:39.920 --> 03:54:43.760
try to begin to imagine what it's like to be an unbounded computational

03:54:45.280 --> 03:54:50.240
observer. Well, okay. So here's how that, I think, plays out.

03:54:50.240 --> 03:54:54.960
Vibrations, yeah. So, I mean, in this, we talk about this

03:54:54.960 --> 03:55:00.960
Ruliat, the space of all possible computations, and this idea of, you know, being at a certain

03:55:00.960 --> 03:55:09.040
place in the Ruliat, which corresponds to sort of a certain set of computations that you are

03:55:09.040 --> 03:55:15.520
representing things in terms of. Okay, so as you expand out in the Ruliat, as you kind of encompass

03:55:16.160 --> 03:55:20.400
more possible views of the universe, as you encompass more possible

03:55:20.400 --> 03:55:25.120
kinds of computations that you can do, eventually you might say that's a real win. You know,

03:55:25.120 --> 03:55:31.040
we're colonizing the Ruliat. We're building out more paradigms about how to think about things.

03:55:31.040 --> 03:55:36.480
And eventually you might say, we won all the way. We managed to colonize the whole Ruliat.

03:55:36.480 --> 03:55:40.960
Okay, here's the problem with that. The problem is that the notion of existence,

03:55:40.960 --> 03:55:46.320
coherent existence, requires some kind of specialization. By the time you are the whole

03:55:46.320 --> 03:55:52.080
Ruliat, by the time you cover the whole Ruliat, in no useful sense do you coherently exist.

03:55:52.800 --> 03:56:01.600
So in other words, the notion of existence, the notion of what we think of as definite existence

03:56:01.600 --> 03:56:09.440
requires this kind of specialization, requires this kind of idea that we are not all possible

03:56:09.440 --> 03:56:19.600
things. We are a particular set of things. And that's kind of what makes us have coherent existence.

03:56:19.600 --> 03:56:25.680
If we were spread throughout the Ruliat, there would be no coherence to the way that we work.

03:56:25.680 --> 03:56:31.360
We would work in all possible ways. And that wouldn't be kind of a notion of identity. We

03:56:31.360 --> 03:56:40.960
wouldn't have this notion of coherent identity. I am geographically located somewhere exactly,

03:56:40.960 --> 03:56:45.840
precisely in the Ruliat, therefore I am, as the cart kind of-

03:56:45.840 --> 03:56:48.240
Yeah, yeah, right. Well, you're in a certain place in physical space,

03:56:48.240 --> 03:56:54.800
you're in a certain place in real space. And if you are sufficiently spread out,

03:56:55.360 --> 03:57:02.480
you are no longer coherent. And you no longer have, I mean, in our perception of what it means

03:57:02.480 --> 03:57:05.680
to exist and to have experience, it doesn't happen that way.

03:57:05.680 --> 03:57:09.360
So therefore, to exist means to be computationally bounded.

03:57:10.080 --> 03:57:14.960
I think so. To exist in the way that we think of ourselves as existing, yes.

03:57:14.960 --> 03:57:20.000
The very act of existence is like operating in this place that's computationally reducible.

03:57:20.000 --> 03:57:23.440
So there's this giant mess of things going on that you can't possibly predict.

03:57:24.240 --> 03:57:30.160
But nevertheless, because of your limitations, you have an imperative of like, what is it,

03:57:30.160 --> 03:57:35.280
an imperative or a skill set to simplify or an ignorance, a sufficient-

03:57:35.280 --> 03:57:39.920
Okay. So the thing which is not obvious is that you are taking a slice of all this complexity,

03:57:39.920 --> 03:57:46.400
just like we have all of these molecules bouncing around in the room, but all we notice is the kind

03:57:46.400 --> 03:57:50.960
of the flow of the air or the pressure of the air. We're just noticing these particular things.

03:57:51.680 --> 03:57:57.920
And the big interesting thing is that there are rules, there are laws,

03:57:57.920 --> 03:58:01.280
that govern those big things that we observe. So it's not obvious.

03:58:01.280 --> 03:58:03.600
It's amazing. Because it doesn't feel like it's a slice.

03:58:04.160 --> 03:58:04.800
Yeah. Well, right.

03:58:04.800 --> 03:58:08.640
It's not a slice. It's like an abstraction.

03:58:09.440 --> 03:58:14.960
Yes. But I mean, the fact that the gas laws work, that we can describe pressure, volume, etc., etc.,

03:58:16.000 --> 03:58:20.160
we don't have to go down to the level of talking about individual molecules.

03:58:20.160 --> 03:58:26.480
That is a non-trivial fact. And here's the thing, the exciting thing as far as I'm concerned,

03:58:26.480 --> 03:58:33.440
the fact that there are certain aspects of the universe. So we think space is made ultimately

03:58:33.440 --> 03:58:40.240
these atoms of space and these hypergraphs and so on. And we think that, but we nevertheless

03:58:40.240 --> 03:58:48.480
perceive the universe at a large scale to be like continuous space and so on. In quantum mechanics,

03:58:48.480 --> 03:58:52.320
we think that there are these many threads of time, these many threads of history,

03:58:52.320 --> 03:58:59.040
yet we span. So in quantum mechanics, in our models of physics, there are these,

03:58:59.600 --> 03:59:04.880
time is not a single thread. Time breaks into many threads. They branch, they merge.

03:59:06.240 --> 03:59:12.640
But we are part of that branching, merging universe. And so our brains are also branching

03:59:12.720 --> 03:59:18.960
and merging. And so when we perceive the universe, we are branching brains perceiving

03:59:18.960 --> 03:59:27.440
a branching universe. And so the fact that the claim that we believe that we are persistent in

03:59:27.440 --> 03:59:32.720
time, we have this single thread of experience, that's the statement that somehow we manage to

03:59:32.720 --> 03:59:39.120
aggregate together those separate threads of time that are separated in the fundamental operation

03:59:39.120 --> 03:59:44.240
of the universe. So just as in space, we're averaging over some big region of space and

03:59:44.240 --> 03:59:49.600
we're looking at many, many of the aggregate effects of many atoms of space. So similarly,

03:59:49.600 --> 03:59:53.440
in what we call branchial space, the space of these quantum branches,

03:59:53.440 --> 03:59:59.120
we are effectively averaging over many different branches of possible histories of the universe.

03:59:59.760 --> 04:00:07.360
And so in thermodynamics, we're averaging over many configurations of many possible positions

04:00:07.360 --> 04:00:12.960
of molecules. So what we see here is, so the question is, when you do that averaging for

04:00:12.960 --> 04:00:18.080
space, what are the aggregate laws of space? When you do that averaging of a branchial space,

04:00:18.080 --> 04:00:24.080
what are the aggregate laws of branchial space? When you do that averaging over the molecules and

04:00:24.080 --> 04:00:31.760
so on, what are the aggregate laws you get? And this is the thing that I think is just amazingly,

04:00:31.760 --> 04:00:35.600
amazingly neat. That there are aggregate laws at all.

04:00:35.600 --> 04:00:40.240
Well, yes, but the question is, what are those aggregate laws? So the answer is for space,

04:00:40.240 --> 04:00:44.000
the aggregate laws, Einstein's equations for gravity, for the structure of spacetime.

04:00:44.560 --> 04:00:51.600
For branchial space, the aggregate laws are the laws of quantum mechanics. And for the case of

04:00:51.600 --> 04:00:56.160
molecules and things, the aggregate laws are basically the second law of thermodynamics.

04:00:56.960 --> 04:01:03.200
And so that's the things that follow from the second law of thermodynamics. And so what that

04:01:03.200 --> 04:01:09.600
means is that the three great theories of 20th century physics, which are basically general

04:01:09.600 --> 04:01:14.240
relativity, the theory of gravity, quantum mechanics, and statistical mechanics, which is

04:01:14.240 --> 04:01:18.720
what kind of grows out of the second law of thermodynamics. All three of the great theories

04:01:18.720 --> 04:01:25.520
of 20th century physics are the result of this interplay between computational irreducibility

04:01:25.520 --> 04:01:33.040
and the computational boundedness of observers. And for me, this is really neat because it means

04:01:33.040 --> 04:01:39.280
that all three of these laws are derivable. So we used to think that, for example, Einstein's

04:01:39.280 --> 04:01:44.080
equations were just sort of a wheel in feature of our universe. That they could be, the universe

04:01:44.080 --> 04:01:48.240
might be that way, it might not be that way. Quantum mechanics is just like, well, it just

04:01:48.240 --> 04:01:53.200
happens to be that way. And the second law, people kind of thought, well, maybe it is derivable.

04:01:54.320 --> 04:01:58.320
What turns out to be the case is that all three of the fundamental principles of physics are

04:01:58.320 --> 04:02:04.480
derivable, but they're not derivable just from mathematics. They require, or just from some kind

04:02:04.480 --> 04:02:10.800
of logical computation, they require one more thing. They require that the observer, that the

04:02:10.800 --> 04:02:16.720
thing that is sampling the way the universe works is an observer who has these characteristics

04:02:16.720 --> 04:02:22.240
of computational boundedness of belief and persistence in time. And so that means that

04:02:22.240 --> 04:02:28.800
it is the nature of the observer, the rough nature of the observer, not the details where we got two

04:02:28.800 --> 04:02:36.720
eyes and we observe photons of this frequency and so on. But the very coarse features of the observer

04:02:37.920 --> 04:02:44.080
then imply these very precise facts about physics. And I think it's amazing.

04:02:44.080 --> 04:02:50.560
So if we just look at the actual experience of the observer, that we experience this reality,

04:02:50.560 --> 04:02:56.320
it seems real to us. And you're saying because of our bounded nature, it's actually all an illusion.

04:02:57.360 --> 04:02:58.640
It's a simplification.

04:02:58.640 --> 04:03:00.160
Well, yeah, it's a simplification.

04:03:00.160 --> 04:03:01.040
Right. What's underneath?

04:03:01.040 --> 04:03:03.040
You don't think a simplification is an illusion?

04:03:04.000 --> 04:03:08.480
No. I mean, it's, well, I don't know. I mean, what's underneath?

04:03:10.480 --> 04:03:16.560
Okay. That's an interesting question. What's real? And that relates to the whole question of

04:03:16.560 --> 04:03:24.160
why does the universe exist? And what is the difference between reality and a mere representation

04:03:24.160 --> 04:03:28.000
of what's going on? Yes. We experience the representation.

04:03:28.720 --> 04:03:38.640
Yes. But the question of, so one question is, why is there a thing which we can experience that way?

04:03:39.520 --> 04:03:47.040
And the answer is because this Ruliat object, which is this entangled limit of all possible

04:03:47.040 --> 04:03:54.400
computations, there is no choice about it. It has to exist. There has to be such a thing.

04:03:54.400 --> 04:04:00.800
It is in the same sense that two plus two, if you define what two is and what plus is and so on,

04:04:00.800 --> 04:04:07.040
two plus two has to equal four. Similarly, this Ruliat, this limit of all possible computations,

04:04:07.040 --> 04:04:13.440
just has to be a thing that is once you have the idea of computation, you inevitably have the

04:04:13.440 --> 04:04:15.280
Ruliat. Yeah. You're going to have to have a Ruliat, yeah.

04:04:15.280 --> 04:04:20.640
Right. And what's important about it, there's just one of it. It's just this unique object.

04:04:21.280 --> 04:04:29.200
And that unique object necessarily exists. And then the question is what, and then we,

04:04:30.880 --> 04:04:35.840
once you know that we are sort of embedded in that and taking samples of it,

04:04:36.480 --> 04:04:45.600
it's sort of inevitable that there is this thing that we can perceive that our perception of kind

04:04:45.600 --> 04:04:52.400
of physical reality necessarily is that way, given that we are observers with the characteristics we

04:04:52.400 --> 04:05:00.720
have. So in other words, the fact that the universe exists, it's actually, it's almost like,

04:05:01.280 --> 04:05:07.840
it's, to think about it almost theologically, so to speak. And I've really, it's funny because

04:05:07.840 --> 04:05:13.920
a lot of the questions about the existence of the universe and so on, they transcend what kind of

04:05:13.920 --> 04:05:17.200
the science of the last few hundred years has really been concerned with. The science of the

04:05:17.200 --> 04:05:23.680
last few hundred years hasn't thought it could talk about questions like that. But I think it's

04:05:23.680 --> 04:05:30.320
kind of, and so a lot of the kind of arguments of, does God exist? Is it obvious that, I think in

04:05:30.320 --> 04:05:37.440
some sense, in some representation, it's sort of more obvious that something sort of bigger than

04:05:37.440 --> 04:05:45.120
us exists than that we exist. And we are, our existence as observers the way we are is sort of

04:05:45.120 --> 04:05:51.120
a contingent thing about the universe. And it's more inevitable that the whole universe, kind of

04:05:51.120 --> 04:06:00.160
the whole set of all possibilities exists. But this question about, is it real or is it an illusion

04:06:00.960 --> 04:06:09.280
all we know is our experience. And so the fact that, well, our experience is this absolutely

04:06:09.280 --> 04:06:20.240
microscopic piece of sample of the Rulliad. And there's this point about, we might sample more

04:06:20.240 --> 04:06:26.080
and more of the Rulliad, we might learn more and more about, we might learn different areas of

04:06:26.080 --> 04:06:32.720
physics, like quantum mechanics, for example. The fact that it was discovered, I think is closely

04:06:32.720 --> 04:06:37.760
related to the fact that electronic amplifiers were invented that allowed you to take a small

04:06:37.760 --> 04:06:42.400
effect and amplify it up, which hadn't been possible before. Microscopes have been invented

04:06:42.400 --> 04:06:48.080
that magnify things and so on. But having a very small effect and being able to magnify it was sort

04:06:48.080 --> 04:06:53.760
of a new thing that allowed one to see a different sort of aspect of the universe and let one discover

04:06:53.760 --> 04:06:59.680
this kind of thing. So we can expect that in the Rulliad, there are an infinite collection

04:06:59.680 --> 04:07:05.120
of new things we can discover. There's, in fact, computational irreducibility guarantees

04:07:05.120 --> 04:07:10.880
that there will be an infinite collection of pockets of reducibility that can be discovered.

04:07:12.720 --> 04:07:18.560
Boy, would it be fun to take a walk down the Rulliad and see what kind of stuff we find there. You

04:07:18.560 --> 04:07:24.400
write about alien intelligences. I mean, just these worlds of computation.

04:07:24.400 --> 04:07:28.080
The problem with these worlds is that- We can't talk to them.

04:07:28.080 --> 04:07:35.200
Yes. The thing is, what I've kind of spent a lot of time doing is just studying computational

04:07:35.200 --> 04:07:40.960
systems, seeing what they do, what I now call Rulliology, kind of just the study of rules

04:07:40.960 --> 04:07:46.880
and what they do. You can kind of easily jump somewhere else in the Rulliad and start seeing

04:07:46.880 --> 04:07:53.920
what do these rules do. They do what they do, and there's no human connection, so to speak.

04:07:53.920 --> 04:08:02.160
Do you think some people are able to communicate with animals? Do you think you can become a whisper

04:08:02.160 --> 04:08:07.680
of these- Oh, I've been trying. That's what I've spent some part of my life doing.

04:08:07.680 --> 04:08:11.280
Have you heard? Are you at the risk of losing your mind?

04:08:12.000 --> 04:08:17.520
My favorite science discovery is this fact that these very simple programs can produce

04:08:17.520 --> 04:08:26.400
very complicated behavior. That fact is kind of, in a sense, a whispering of something out in the

04:08:26.400 --> 04:08:34.240
computational universe that we didn't really know was there before. Back in the 1980s, I

04:08:34.800 --> 04:08:40.160
was doing a bunch of work with some very, very good mathematicians, and they were trying to

04:08:40.160 --> 04:08:46.000
pick a way. Can we figure out what's going on in these computational systems? They basically said,

04:08:46.000 --> 04:08:50.800
look, the math we have just doesn't get anywhere with this. We're stuck. There's nothing to say.

04:08:50.800 --> 04:08:57.760
We have nothing to say. In a sense, perhaps my main achievement at that time was to realize that

04:08:57.760 --> 04:09:04.160
the very fact that the good mathematicians had nothing to say was itself a very interesting

04:09:04.160 --> 04:09:09.920
thing. That was sort of, in some sense, a whispering of a different part of the rulliad

04:09:09.920 --> 04:09:15.040
that one wasn't accessible from what we knew in mathematics and so on.

04:09:16.080 --> 04:09:22.800
Does it make you sad that you're exploring some of these gigantic ideas and it feels like we're

04:09:22.800 --> 04:09:28.880
on the verge of breaking through to some very interesting discoveries, and yet you're just a

04:09:28.960 --> 04:09:35.040
finite being that's going to die way too soon, and that scan of your brain or your full body

04:09:35.600 --> 04:09:38.880
kind of shows that you're- Yeah, it's just a bunch of meat.

04:09:38.880 --> 04:09:44.880
It's just a bunch of meat. Yeah, does that make you sad?

04:09:44.880 --> 04:09:48.480
Kind of a shame. I mean, I'd kind of like to see how all this stuff works out,

04:09:48.480 --> 04:09:53.600
but I think the thing to realize, it's an interesting sort of thought experiment. You say,

04:09:53.680 --> 04:09:59.360
okay, let's assume we can get cryonics to work, and one day it will. There will be one of these

04:09:59.360 --> 04:10:05.440
things that's kind of like chat GPT. One day somebody will figure out how to get water from

04:10:05.440 --> 04:10:11.760
zero degrees centigrade down to minus 44 or something without it expanding, and cryonics

04:10:11.760 --> 04:10:20.720
will be solved, and you'll be able to just put a pause in, so to speak, and kind of reappear

04:10:20.720 --> 04:10:26.640
100 years later or something. The thing though that I've kind of increasingly realized is that

04:10:27.280 --> 04:10:35.440
in a sense, this whole question of kind of the sort of one is embedded in a certain moment in time,

04:10:36.000 --> 04:10:42.000
and kind of the things we care about now, the things I care about now, for example, had I lived

04:10:43.440 --> 04:10:48.640
500 years ago, many of the things I care about now, it's like that's totally bizarre. I mean,

04:10:48.640 --> 04:10:54.160
nobody would care about that. It's not even the thing one thinks about. In the future, the things

04:10:54.160 --> 04:11:04.160
that most people will think about, one will be a strange relic of thinking about what might have

04:11:04.160 --> 04:11:09.120
been a theologian thinking about how many angels fit on the head of a pin or something, and that

04:11:09.120 --> 04:11:18.560
might have been the big intellectual thing. But yeah, it's one of these things where particularly

04:11:19.200 --> 04:11:25.200
you know, I've had the, I don't know, good or bad fortune. I'm not sure. I think it's a mixed thing

04:11:25.200 --> 04:11:31.600
that I've, you know, I've invented a bunch of things which I kind of can, I think, see well

04:11:31.600 --> 04:11:38.480
enough what's going to happen that, you know, in 50 years, 100 years, whatever, assuming the world

04:11:38.480 --> 04:11:45.600
doesn't exterminate itself, so to speak, you know, these are things that will be sort of centrally

04:11:45.600 --> 04:11:50.720
important to what's going on. And it's kind of both, it's both a good thing and a bad thing

04:11:50.720 --> 04:11:55.440
in terms of the passage of one's life. I mean, it's kind of like, if everything I'd figured out

04:11:55.440 --> 04:12:00.240
was like, okay, I figured it out when I was 25 years old, and everybody says it's great,

04:12:00.240 --> 04:12:04.880
and we're done. And it's like, okay, but I'm going to live another how many years,

04:12:04.880 --> 04:12:11.520
and that's kind of, it's all downhill from there. In a sense, it's better in some sense to be able

04:12:11.520 --> 04:12:18.080
to, you know, there's, it sort of keeps things interesting that, you know, I can see, you know,

04:12:18.080 --> 04:12:23.520
a lot of these things. I mean, it's kind of, I didn't expect, you know, chat GPT. I didn't expect

04:12:23.520 --> 04:12:30.000
the kind of, the sort of opening up of this idea of computation and computational language that's

04:12:30.000 --> 04:12:35.840
been made possible by this. I didn't expect that. This is the head of schedule, so to speak. You

04:12:35.840 --> 04:12:41.040
know, even though the sort of the big kind of flowering of that stuff, I'd sort of been

04:12:41.040 --> 04:12:46.880
assuming was another 50 years away. So if it turns out it's a lot less time, that's pretty cool,

04:12:46.880 --> 04:12:50.480
because, you know, I'll hopefully get to see it, so to speak, rather than.

04:12:52.720 --> 04:12:59.280
Well, I think I speak for a very, very large number of people in saying that I hope you stick

04:12:59.280 --> 04:13:05.040
on for a long time to come. You've had so many interesting ideas. You've created so many

04:13:05.040 --> 04:13:11.280
interesting systems over the years, and I can see now that GPT and language models broke open the

04:13:11.280 --> 04:13:17.760
world even more. I can't wait to see you at the forefront of this development, what you do.

04:13:18.560 --> 04:13:23.520
And yeah, I've been a fan of yours, like I've told you many, many times since the very beginning.

04:13:23.520 --> 04:13:27.440
I'm deeply grateful that you wrote a new kind of science, that you explored this

04:13:27.520 --> 04:13:35.520
mystery of cellular automata, and inspired this one little kid in me to pursue artificial intelligence

04:13:35.520 --> 04:13:40.400
and all this beautiful world. So Stephen, thank you so much. It's a huge honor to talk to you,

04:13:41.360 --> 04:13:46.640
to just be able to pick your mind and to explore all these ideas with you, and please keep going,

04:13:46.640 --> 04:13:50.240
and I can't wait to see where you come up next. And thank you for talking today.

04:13:50.240 --> 04:13:50.800
Thanks.

04:13:50.800 --> 04:13:56.880
We went past midnight. We only did four and a half hours. I mean, we could probably go for four more,

04:13:56.880 --> 04:14:02.640
but we'll save that till next time. This is round number four. I'm sure we'll talk many more times.

04:14:02.640 --> 04:14:03.680
Thank you so much.

04:14:03.680 --> 04:14:04.320
My pleasure.

04:14:05.360 --> 04:14:09.200
Thanks for listening to this conversation with Stephen Wolfram. To support this podcast,

04:14:09.200 --> 04:14:14.320
please check out our sponsors in the description. And now let me leave you with some words from

04:14:14.320 --> 04:14:22.320
George Cantor. The essence of mathematics lies in its freedom. Thank you for listening,

04:14:22.320 --> 04:14:24.160
and hope to see you next time.

04:14:26.880 --> 04:14:27.040
Thank you.


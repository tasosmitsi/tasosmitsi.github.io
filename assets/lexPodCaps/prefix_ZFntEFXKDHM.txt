WEBVTT

00:00.000 --> 00:04.880
I think that animals are a really great thought experiment

00:04.880 --> 00:06.320
when we're thinking about AI and robotics,

00:06.320 --> 00:09.280
because again, this comparing them to humans

00:09.280 --> 00:10.960
that leads us down the wrong path,

00:10.960 --> 00:12.440
both because it's not accurate,

00:12.440 --> 00:16.120
but also I think for the future, we don't want that.

00:16.120 --> 00:17.880
We want something that's a supplement.

00:17.880 --> 00:18.760
But I think animals,

00:18.760 --> 00:21.140
because we've used them throughout history

00:21.140 --> 00:22.280
for so many different things,

00:22.280 --> 00:25.020
we domesticated them not because they do what we do,

00:25.020 --> 00:28.320
but because what they do is different and that's useful.

00:28.320 --> 00:32.320
And it's just like whether we're talking about companionship,

00:32.320 --> 00:34.920
whether we're talking about work integration,

00:34.920 --> 00:37.240
whether we're talking about responsibility for harm,

00:37.240 --> 00:40.600
there's just so many things we can draw on in that history

00:40.600 --> 00:43.400
from these entities that can sense, think,

00:43.400 --> 00:45.700
make autonomous decisions and learn

00:45.700 --> 00:48.520
that are applicable to how we should be thinking

00:48.520 --> 00:49.720
about robots and AI.

00:51.880 --> 00:54.520
The following is a conversation with Kate Darling,

00:54.520 --> 00:56.440
her second time on the podcast.

00:56.480 --> 00:59.320
She's a research scientist at MIT Media Lab,

00:59.320 --> 01:03.840
interested in human-robot interaction and robot ethics,

01:03.840 --> 01:06.640
which she writes about in her recent book

01:06.640 --> 01:08.360
called The New Breed,

01:08.360 --> 01:10.600
what our history with animals reveals

01:10.600 --> 01:12.560
about our future with robots.

01:12.560 --> 01:15.080
Kate is one of my favorite people at MIT.

01:15.080 --> 01:18.160
She was a courageous voice of reason and compassion

01:18.160 --> 01:20.320
through the time of the Jeffrey Epstein scandal

01:20.320 --> 01:22.520
at MIT three years ago.

01:22.520 --> 01:26.560
We reflect on this time in this very conversation,

01:26.560 --> 01:29.680
including the lessons it revealed about human nature

01:29.680 --> 01:33.600
and our optimistic vision for the future of MIT,

01:33.600 --> 01:36.960
a university we both love and believe in.

01:36.960 --> 01:38.880
This is the Lex Fridman podcast.

01:38.880 --> 01:40.920
To support it, please check out our sponsors

01:40.920 --> 01:42.160
in the description.

01:42.160 --> 01:45.960
And now, dear friends, here's Kate Darling.

01:46.900 --> 01:49.160
Last time we talked, a few years back,

01:49.160 --> 01:51.960
you wore a Justin Bieber shirt for the podcast.

01:52.040 --> 01:56.080
Now, looking back, you're a respected researcher,

01:56.080 --> 01:59.200
all the amazing accomplishments in robotics,

01:59.200 --> 02:00.600
you're an author.

02:00.600 --> 02:03.200
Was this one of the proudest moments of your life,

02:03.200 --> 02:05.160
proudest decisions you've ever made?

02:05.160 --> 02:06.440
Definitely.

02:06.440 --> 02:07.880
You handled it really well, though.

02:07.880 --> 02:09.080
It was cool, because I walked in,

02:09.080 --> 02:10.880
I didn't know you were going to be filming,

02:10.880 --> 02:13.400
and I walked in and you're in a fucking suit.

02:13.400 --> 02:14.240
Yeah.

02:14.240 --> 02:15.960
And I'm like, why are you all dressed up?

02:15.960 --> 02:16.920
Yeah.

02:16.920 --> 02:18.200
And then you were so nice about it,

02:18.200 --> 02:19.040
you made some excuses.

02:19.040 --> 02:21.080
You were like, oh, well, I'm interviewing some art.

02:21.120 --> 02:22.320
Didn't you say you were interviewing

02:22.320 --> 02:24.160
some military general afterwards,

02:24.160 --> 02:26.080
to make me feel better?

02:26.080 --> 02:27.760
Oh, yeah, that was this CTO of Lockheed Martin, I think.

02:27.760 --> 02:29.040
Oh, that's what it was.

02:29.040 --> 02:29.880
Yeah.

02:29.880 --> 02:32.520
You didn't tell me, oh, I was dressed like this.

02:32.520 --> 02:33.880
Are you an actual Bieber fan,

02:33.880 --> 02:36.400
or was that one of those T-shirts

02:37.720 --> 02:39.000
that's in the back of the closet

02:39.000 --> 02:40.480
that you used for painting?

02:40.480 --> 02:42.920
I think I bought it for my husband as a joke,

02:42.920 --> 02:46.720
and yeah, we were renovating a house at the time,

02:46.720 --> 02:48.800
and I had worn it to the site.

02:48.800 --> 02:50.520
God is a joke, and now you wear it.

02:50.520 --> 02:51.360
Okay.

02:51.360 --> 02:53.160
Have you worn it since?

02:53.160 --> 02:54.880
Was this the one time?

02:54.880 --> 02:57.120
No, like how could I touch it again?

02:57.120 --> 02:59.360
It was on your podcast, now it's framed.

02:59.360 --> 03:01.920
It's like a wedding dress or something like that.

03:01.920 --> 03:03.440
You only wear it once.

03:03.440 --> 03:07.120
You are the author of The New Breed,

03:07.120 --> 03:09.000
what our history with animals reveals

03:09.000 --> 03:11.200
about our future with robots.

03:11.200 --> 03:14.640
You opened the book with the surprisingly tricky question,

03:14.640 --> 03:15.720
what is a robot?

03:15.720 --> 03:16.920
So let me ask you,

03:16.920 --> 03:19.080
let's try to sneak up to this question.

03:19.080 --> 03:20.680
What's a robot?

03:20.680 --> 03:22.760
That's not really sneaking up.

03:22.760 --> 03:23.920
It's just asking it.

03:23.920 --> 03:24.960
Yeah.

03:24.960 --> 03:25.800
All right, well.

03:25.800 --> 03:29.400
What do you think a robot is?

03:29.400 --> 03:32.080
What I think a robot is

03:32.080 --> 03:36.240
is something that has some level of intelligence

03:37.720 --> 03:40.800
and some level of magic.

03:41.760 --> 03:44.000
That little shine in the eye

03:44.040 --> 03:49.040
that allows you to navigate the uncertainty of life.

03:50.040 --> 03:52.600
So that means like autonomous vehicles to me

03:52.600 --> 03:55.000
in that sense are robots

03:55.000 --> 03:58.440
because they navigate the uncertainty,

03:58.440 --> 04:00.520
the complexity of life.

04:00.520 --> 04:02.680
Obviously social robots are that.

04:02.680 --> 04:04.040
I love that.

04:04.040 --> 04:07.080
I like that you mentioned magic because that also,

04:07.080 --> 04:08.040
well, so first of all,

04:08.040 --> 04:10.800
I don't define robot definitively in the book

04:10.800 --> 04:13.400
because there is no definition

04:13.400 --> 04:15.280
that everyone agrees on.

04:15.280 --> 04:17.400
And if you look back through time,

04:18.360 --> 04:21.840
people have called things robots until they lose the magic

04:21.840 --> 04:23.320
because they're more ubiquitous.

04:23.320 --> 04:25.520
Like a vending machine used to be called a robot

04:25.520 --> 04:26.600
and now it's not, right?

04:26.600 --> 04:30.720
So I do agree with you that there's this magic aspect

04:32.760 --> 04:35.760
which is how people understand robots.

04:35.760 --> 04:37.240
If you ask a roboticist,

04:37.240 --> 04:39.640
they have the definition of something that is,

04:39.640 --> 04:40.840
well, it has to be physical.

04:40.840 --> 04:43.240
Usually it's not an AI agent.

04:44.080 --> 04:45.320
It has to be embodied.

04:45.320 --> 04:48.200
They'll say it has to be able to sense its environment

04:48.200 --> 04:49.040
in some way.

04:49.040 --> 04:52.440
It has to be able to make a decision autonomously

04:52.440 --> 04:55.240
and then act on its environment again.

04:55.240 --> 04:58.480
I think that's a pretty good technical definition

04:58.480 --> 04:59.920
even though it really breaks down

04:59.920 --> 05:01.840
when you come to things like the smartphone

05:01.840 --> 05:04.440
because the smartphone can do all of those things.

05:04.440 --> 05:06.480
And most robotists would not call it a robot.

05:06.480 --> 05:09.720
So there's really no one good definition

05:09.720 --> 05:12.040
but part of why I wrote the book

05:12.040 --> 05:17.040
is because people have a definition of robot in their minds

05:17.920 --> 05:20.840
that is usually very focused

05:20.840 --> 05:23.320
on a comparison of robots to humans.

05:23.320 --> 05:24.960
So if you Google image search robot,

05:24.960 --> 05:27.080
you get a bunch of humanoid robots,

05:27.080 --> 05:30.880
robots with a torso and head and two arms and two legs.

05:30.880 --> 05:33.880
And that's the definition of robot

05:33.880 --> 05:35.560
that I'm trying to get us away from

05:35.560 --> 05:37.960
because I think that it trips us up a lot.

05:37.960 --> 05:40.760
Why does the humanoid form trip us up a lot?

05:40.760 --> 05:45.080
Well, because this constant comparison of robots to people,

05:45.080 --> 05:47.880
artificial intelligence to human intelligence,

05:47.880 --> 05:48.960
first of all, it doesn't make sense

05:48.960 --> 05:50.160
from a technical perspective

05:50.160 --> 05:54.080
because the early AI researchers,

05:54.080 --> 05:56.840
some of them were trying to recreate human intelligence.

05:56.840 --> 05:57.760
Some people still are,

05:57.760 --> 06:00.120
and there's a lot to be learned from that academically,

06:00.120 --> 06:03.240
et cetera, but that's not where we've ended up.

06:03.240 --> 06:05.480
AI doesn't think like people.

06:05.480 --> 06:06.840
We wind up in this fallacy

06:06.840 --> 06:09.720
where we're comparing these two.

06:10.920 --> 06:13.760
And when we talk about what intelligence even is,

06:13.760 --> 06:15.840
we're often comparing to our own intelligence.

06:15.840 --> 06:19.200
And then the second reason this bothers me

06:19.200 --> 06:21.400
is because it doesn't make sense.

06:22.320 --> 06:25.280
I just think it's boring to recreate intelligence

06:25.280 --> 06:26.760
that we already have.

06:26.760 --> 06:28.040
I see the scientific value

06:28.040 --> 06:30.080
of understanding our own intelligence,

06:30.080 --> 06:32.240
but from a practical,

06:32.240 --> 06:35.640
what could we use these technologies for perspective?

06:35.640 --> 06:38.440
It's much more interesting to create something new,

06:38.440 --> 06:40.680
to create a skillset that we don't have

06:41.600 --> 06:42.440
that we can partner with

06:42.440 --> 06:43.640
in what we're trying to achieve.

06:44.680 --> 06:48.800
And it should be in some deep ways similar to us,

06:48.800 --> 06:51.280
but in most ways different

06:51.280 --> 06:53.400
because you still want to have a connection,

06:53.400 --> 06:56.680
which is why the similarity might be necessary.

06:56.680 --> 06:58.440
That's what people argue, yes.

06:58.440 --> 06:59.480
And I think that's true.

06:59.480 --> 07:01.960
So the two arguments for humanoid robots

07:01.960 --> 07:05.360
are people need to be able to communicate

07:05.360 --> 07:06.760
and relate to robots,

07:06.760 --> 07:10.440
and we relate most of the things that are like ourselves.

07:11.000 --> 07:12.800
And we have a world that's built for humans.

07:12.800 --> 07:15.800
So we have stairs and narrow passageways and door handles.

07:15.800 --> 07:19.000
And so we need humanoid robots to be able to navigate that.

07:19.000 --> 07:22.200
And so you're speaking to the first one,

07:22.200 --> 07:23.040
which is absolutely true,

07:23.040 --> 07:24.720
but what we know from social robotics

07:24.720 --> 07:27.400
and a lot of human robot interaction research is that

07:28.960 --> 07:32.720
all you need is something that's enough like a person

07:32.720 --> 07:36.320
for it to give off cues that someone relates to,

07:36.320 --> 07:39.960
but that doesn't have to look human or even act human.

07:39.960 --> 07:41.800
Take a robot like R2-D2,

07:41.800 --> 07:44.280
and it just like beeps and boops,

07:44.280 --> 07:45.760
and people love R2-D2, right?

07:45.760 --> 07:47.880
Even though it's just like a trash can on wheels.

07:47.880 --> 07:50.840
And they like R2-D2 more than C-3PO, who's a humanoid.

07:50.840 --> 07:55.840
So there's lots of ways to make robots even better

07:56.200 --> 08:00.280
than humans in some ways and make us relate more to them.

08:00.280 --> 08:03.120
Yeah, it's kind of amazing the variety of cues

08:03.120 --> 08:06.040
that can be used to anthropomorphize the thing,

08:06.040 --> 08:08.800
like a glowing orb or something like that.

08:09.200 --> 08:14.200
Just a voice, just subtle basic interaction.

08:14.360 --> 08:17.480
I think people sometimes over engineer these things,

08:17.480 --> 08:19.720
like simplicity can go a really long way.

08:19.720 --> 08:22.440
Totally, I mean, ask any animator and they'll know that.

08:22.440 --> 08:24.800
Yeah, yeah, those are actually,

08:24.800 --> 08:29.600
so the people behind Cosmo, the robot,

08:30.740 --> 08:33.480
the right people to design those is animators,

08:33.480 --> 08:38.120
like Disney type of people versus like roboticists.

08:38.120 --> 08:41.880
Robotists, quote unquote, are mostly clueless.

08:43.880 --> 08:45.240
They just have their own discipline

08:45.240 --> 08:47.800
that they're very good at and they don't have.

08:47.800 --> 08:51.360
Yeah, but that don't, you know,

08:51.360 --> 08:56.280
I feel like robotics of the early 21st century

08:57.160 --> 09:01.360
is not going to be the robotics of the later 21st century.

09:01.360 --> 09:03.480
Like if you call yourself a roboticist,

09:03.480 --> 09:05.520
it'll be something very different.

09:05.520 --> 09:08.240
Because I think more and more,

09:08.240 --> 09:12.680
you'll be like maybe like a control engineer or something,

09:12.680 --> 09:15.920
controls engineer, like you separate

09:15.920 --> 09:18.680
because ultimately all the unsolved,

09:18.680 --> 09:21.720
all the big problems of robotics

09:21.720 --> 09:24.040
will be in the social aspect,

09:24.040 --> 09:26.440
in the interacting with humans aspect,

09:26.440 --> 09:31.440
in the perception interpreting the world aspect,

09:31.520 --> 09:36.520
in the brain part, not the basic control level part.

09:37.640 --> 09:40.320
You call it basic, it's actually really complex.

09:40.320 --> 09:41.600
It's very, very complicated.

09:41.600 --> 09:44.120
And that's why, but like, I think you're so right

09:44.120 --> 09:48.840
and what a time to be alive because for me,

09:48.840 --> 09:52.160
I just, we've had robots for so long

09:52.160 --> 09:54.400
and they've just been behind the scenes

09:54.400 --> 09:58.640
and now finally robots are getting deployed into the world.

09:58.640 --> 09:59.960
They're coming out of the closet.

09:59.960 --> 10:02.280
Yeah, and we're seeing all these mistakes

10:02.280 --> 10:04.040
that companies are making because they focus

10:04.040 --> 10:06.800
so much on the engineering and getting that right

10:06.800 --> 10:09.600
and getting the robot to even be able to function

10:09.600 --> 10:12.480
in a space that it shares with a human.

10:12.480 --> 10:15.600
See, what I feel like people don't understand

10:15.600 --> 10:19.120
is to solve the perception and the control problem.

10:19.120 --> 10:21.040
You shouldn't try to just solve

10:21.040 --> 10:22.480
the perception control problem.

10:22.480 --> 10:25.480
You should teach the robot how to say,

10:25.480 --> 10:27.960
oh shit, I'm sorry, I fucked up.

10:27.960 --> 10:29.160
Yeah, or ask for help.

10:29.160 --> 10:34.160
Or ask for help or be able to communicate the uncertainty.

10:34.480 --> 10:36.200
Yeah, exactly, all of those things

10:36.200 --> 10:39.000
because you can't solve the perception and control.

10:39.000 --> 10:40.600
We humans haven't solved it.

10:40.600 --> 10:42.720
We're really damn good at it.

10:43.640 --> 10:48.080
But the magic is in the self-deprecating humor

10:48.080 --> 10:51.120
and the self-awareness about where our flaws are,

10:51.120 --> 10:52.520
all that kind of stuff.

10:52.520 --> 10:54.520
Yeah, and there's a whole body of research

10:54.520 --> 10:58.720
in human-robot interaction showing ways to do this.

10:58.720 --> 11:01.920
But a lot of these companies, they don't do HRI.

11:03.920 --> 11:06.720
Have you seen the grocery store robot in the Stop and Shop?

11:06.720 --> 11:07.560
Yes.

11:07.560 --> 11:09.280
Yeah, the Marty, it looks like a giant penis.

11:09.280 --> 11:11.760
It's like six feet tall, it roams the aisles.

11:11.760 --> 11:13.960
I will never see Marty the same way again.

11:13.960 --> 11:14.800
Thank you for that.

11:14.800 --> 11:15.640
You're welcome.

11:16.520 --> 11:20.480
But these poor people were so hard

11:20.480 --> 11:23.200
on getting a functional robot together.

11:23.200 --> 11:26.600
And then people hate Marty because they didn't

11:26.640 --> 11:29.240
at all consider how people would react

11:29.240 --> 11:30.720
to Marty in their space.

11:30.720 --> 11:33.040
Does everybody, I mean, you talk about this,

11:33.040 --> 11:34.560
do people mostly hate Marty?

11:34.560 --> 11:37.160
Because I like Marty, I feel like less-

11:37.160 --> 11:38.920
Yeah, but you like Flippy.

11:38.920 --> 11:39.760
Yeah, I do.

11:39.760 --> 11:41.320
And actually-

11:41.320 --> 11:43.200
There's a parallel between the two?

11:43.200 --> 11:44.160
I believe there is.

11:44.160 --> 11:45.880
So we were actually gonna do a study on this

11:45.880 --> 11:48.240
right before the pandemic hit and then we canceled it

11:48.240 --> 11:49.920
because we didn't wanna go to the grocery store

11:49.920 --> 11:51.440
and neither did anyone else.

11:52.560 --> 11:56.360
But our theory, so this was with a student at MIT,

11:57.120 --> 12:00.240
Daniela DiPaola, she noticed that everyone on Facebook

12:00.240 --> 12:02.760
in her circles was complaining about Marty.

12:02.760 --> 12:04.800
They were like, what is this creepy robot?

12:04.800 --> 12:06.840
It's watching me, it's always in the way.

12:06.840 --> 12:09.680
And she did this like quick and dirty sentiment analysis

12:09.680 --> 12:11.560
on Twitter where she was looking at positive

12:11.560 --> 12:13.160
and negative mentions of the robot.

12:13.160 --> 12:15.760
And she found that the biggest spike

12:15.760 --> 12:20.000
of negative mentions happened when Stop and Shop

12:20.000 --> 12:22.240
threw a birthday party for the Marty robots,

12:22.240 --> 12:24.360
like with free cake and balloons.

12:24.920 --> 12:26.400
Who complains about free cake?

12:26.400 --> 12:28.400
Well, people who hate Marty, apparently.

12:29.400 --> 12:31.600
And so we were like, that's interesting.

12:31.600 --> 12:35.360
And then we did this online poll, we used Mechanical Turk,

12:35.360 --> 12:40.360
and we tried to get at what people don't like about Marty.

12:41.360 --> 12:44.720
And a lot of it wasn't, oh, Marty's taking jobs.

12:44.720 --> 12:47.800
It was Marty is the surveillance robot, which it's not.

12:47.800 --> 12:48.920
It looks for spills on the floor.

12:48.920 --> 12:51.820
It doesn't actually look at any people.

12:52.820 --> 12:55.060
It's watching, it's creepy, it's getting in the way.

12:55.060 --> 12:56.900
Those were the things that people complained about.

12:56.900 --> 13:00.300
And so our hypothesis became,

13:00.300 --> 13:02.860
is Marty a real life Clippy?

13:02.860 --> 13:05.260
Because I know Lex, you love Clippy,

13:05.260 --> 13:07.740
but many people hated Clippy.

13:07.740 --> 13:09.540
Well, there's a complex thing there.

13:09.540 --> 13:10.660
It could be like marriage.

13:10.660 --> 13:13.620
A lot of people seem to like to complain about marriage,

13:13.620 --> 13:15.260
but they secretly love it.

13:15.260 --> 13:18.900
So it could be, the relationship you might have

13:18.940 --> 13:23.860
with Marty is like, oh, there he goes again,

13:23.860 --> 13:26.780
doing his stupid surveillance thing.

13:26.780 --> 13:28.980
But you grow to love the,

13:31.060 --> 13:33.380
I mean, bitching about the thing

13:33.380 --> 13:35.420
that kind of releases a kind of tension.

13:35.420 --> 13:38.940
And there's, I mean, some people, a lot of people,

13:38.940 --> 13:43.500
show love by sort of busting each other's chops, you know?

13:43.500 --> 13:45.260
Like making fun of each other.

13:45.260 --> 13:48.420
And then I think people would really love it

13:48.420 --> 13:49.820
if Marty talked back.

13:52.740 --> 13:56.660
And like, these are so many possible options for humor there.

13:56.660 --> 13:58.300
One, you can lean in.

13:58.300 --> 14:01.140
You can be like, yes, I'm an agent of the CIA

14:01.140 --> 14:03.060
monitoring your every move.

14:03.060 --> 14:05.540
Like mocking people that are concerned, you know?

14:05.540 --> 14:08.620
Saying like, yes, I'm watching you

14:08.620 --> 14:11.660
because you're so important with your shopping patterns.

14:11.660 --> 14:13.340
I'm collecting all this data.

14:14.260 --> 14:17.380
Or just, you know, any kind of making fun of people.

14:17.380 --> 14:18.220
I don't know.

14:19.060 --> 14:20.460
But I think you hit on what exactly it is

14:20.460 --> 14:24.460
because when it comes to robots or artificial agents,

14:24.460 --> 14:27.700
I think people hate them more than they would

14:27.700 --> 14:30.660
some other machine or device or object.

14:31.860 --> 14:33.980
And it might be that thing,

14:33.980 --> 14:36.660
it might be combined with love or like whatever it is,

14:36.660 --> 14:38.100
it's a more extreme response

14:38.100 --> 14:41.420
because they view these things as social agents

14:41.420 --> 14:42.580
and not objects.

14:42.580 --> 14:45.460
And that was, so Clifford Nass

14:45.460 --> 14:48.180
was a big human computer interaction person.

14:48.180 --> 14:52.100
And his theory about Clippy was that

14:52.100 --> 14:55.740
because people viewed Clippy as a social agent,

14:55.740 --> 14:58.180
when Clippy was annoying and would like bother them

14:58.180 --> 14:59.700
and interrupt them and like not remember

14:59.700 --> 15:03.340
what they told him, that's when people got upset

15:03.340 --> 15:06.220
because it wasn't fulfilling their social expectations.

15:06.220 --> 15:08.260
And so they complained about Clippy

15:08.260 --> 15:10.380
more than they would have if it had been a different,

15:10.380 --> 15:14.380
like not a, you know, virtual character.

15:14.460 --> 15:16.740
So is complaining to you a sign

15:16.740 --> 15:19.580
that we're on the wrong path with a particular robot

15:19.580 --> 15:23.340
or is it possible like, again, like marriage,

15:23.340 --> 15:28.340
like family, that there still is a path

15:28.660 --> 15:29.700
towards that direction

15:29.700 --> 15:32.900
where we can find deep, meaningful relationship?

15:32.900 --> 15:34.620
I think we absolutely can find

15:34.620 --> 15:37.260
deep, meaningful relationship with robots.

15:37.260 --> 15:38.900
Well, maybe with Marty.

15:38.900 --> 15:40.340
I mean, I just would,

15:40.340 --> 15:42.540
I would have designed Marty a little differently.

15:42.540 --> 15:43.620
Like how?

15:43.620 --> 15:46.900
Isn't there a charm to the clumsiness, to the slowness?

15:46.900 --> 15:47.740
Like I was sometimes just-

15:47.740 --> 15:48.580
There is if you're not trying to get through

15:48.580 --> 15:50.580
with a shopping cart and a screaming child.

15:50.580 --> 15:53.380
You know, there's, I think,

15:53.380 --> 15:54.980
I think you could make it charming.

15:54.980 --> 15:56.900
I think there are lots of design tricks

15:56.900 --> 15:58.620
that they could have used.

15:58.620 --> 16:00.100
And one of the things they did,

16:00.100 --> 16:01.820
I think without thinking about it at all

16:01.820 --> 16:04.420
is they slapped two big googly eyes on Marty.

16:04.420 --> 16:05.260
Oh yeah.

16:05.260 --> 16:07.460
And I wonder if that contributed maybe

16:07.460 --> 16:08.940
to people feeling watched

16:09.540 --> 16:13.940
because it's looking at them.

16:13.940 --> 16:18.740
And so like, is there a way to design the robot

16:18.740 --> 16:21.020
to do the function that it's doing

16:21.020 --> 16:24.300
in a way that people are actually attracted to

16:24.300 --> 16:25.740
rather than annoyed by?

16:25.740 --> 16:27.140
And there are many ways to do that,

16:27.140 --> 16:29.020
but companies aren't thinking about it.

16:29.020 --> 16:30.100
Now they're realizing

16:30.100 --> 16:31.540
that they should have thought about it.

16:31.540 --> 16:32.380
Yeah.

16:32.380 --> 16:33.860
I wonder if there's a way to,

16:35.300 --> 16:38.780
if it would help to make Marty seem like an entity

16:39.620 --> 16:44.620
of its own versus the arm of a large corporation.

16:45.980 --> 16:50.740
So there's some sense where this is just the camera

16:50.740 --> 16:54.500
that's monitoring people versus this is an entity

16:54.500 --> 16:56.180
that's a standalone entity.

16:56.180 --> 16:59.660
It has its own task and it has its own personality.

16:59.660 --> 17:01.860
Like the more personality you give it,

17:01.860 --> 17:06.180
the more it feels like it's not sharing data

17:06.180 --> 17:07.420
with anybody else.

17:07.420 --> 17:10.700
Like when we see other human beings,

17:10.700 --> 17:14.140
our basic assumption is whatever I say to this human being,

17:14.140 --> 17:17.460
it's not like being immediately sent to the CIA.

17:17.460 --> 17:18.300
Yeah, what I say to you,

17:18.300 --> 17:19.940
no one's gonna hear that, right?

17:19.940 --> 17:21.380
Yeah, that's true, that's true.

17:21.380 --> 17:22.220
No, no, I'm kidding.

17:22.220 --> 17:23.060
Well, you forget it.

17:23.060 --> 17:23.900
I mean, you do forget it.

17:23.900 --> 17:26.260
I mean, I don't know if that even with microphones here,

17:26.260 --> 17:28.020
you forget that that's happening,

17:28.020 --> 17:31.540
but for some reason, I think probably with Marty,

17:32.740 --> 17:36.620
I think what is done really crudely and crappily,

17:36.620 --> 17:39.580
you start to realize, oh, this is like PR people

17:39.580 --> 17:44.580
trying to make a friendly version of a surveillance machine.

17:46.020 --> 17:50.700
But I mean, that reminds me of the slight clumsiness

17:50.700 --> 17:53.420
or significant clumsiness on the initial releases

17:53.420 --> 17:55.660
of the avatars for the metaverse.

17:55.660 --> 17:58.500
I don't know, what are your actually thoughts about that?

17:58.500 --> 18:03.500
The way the avatars, the way like Mark Zuckerberg

18:06.660 --> 18:11.180
looks in that world, you know, in the metaverse,

18:11.180 --> 18:12.820
the virtual reality world where you can have

18:12.820 --> 18:14.740
like virtual meetings and stuff like that.

18:14.740 --> 18:16.300
Like, how do we get that right?

18:16.300 --> 18:17.260
Do you have thoughts about that?

18:17.260 --> 18:18.340
Because it's a kind of,

18:22.420 --> 18:25.140
it feels like a similar problem to social robotics,

18:25.140 --> 18:29.220
which is how you design a digital virtual world

18:29.220 --> 18:34.220
that is compelling when you connect to others there

18:34.980 --> 18:38.780
in the same way that physical connection is.

18:38.780 --> 18:40.460
Right, I haven't looked into, I mean,

18:40.460 --> 18:42.300
I've seen people joking about it on Twitter

18:42.300 --> 18:44.500
and like posting like that, whatever.

18:44.500 --> 18:45.700
Yeah, but I mean, have you seen it?

18:45.700 --> 18:49.460
Because there's something you can't quite put into words

18:49.460 --> 18:54.460
that doesn't feel genuine about the way it looks.

18:54.900 --> 18:58.320
And so the question is, if you and I were to meet virtually,

18:59.380 --> 19:02.260
what should the avatars look like

19:02.260 --> 19:04.700
for us to have similar kind of connection?

19:04.700 --> 19:07.180
Should it be really simplified?

19:07.180 --> 19:09.820
Should it be a little bit more realistic?

19:09.820 --> 19:11.260
Should it be cartoonish?

19:11.260 --> 19:16.260
Should it be more better capturing of expressions

19:19.660 --> 19:21.460
in interesting, complex ways

19:21.460 --> 19:24.260
versus like cartoonish, oversimplified ways?

19:24.260 --> 19:26.220
But haven't video games figured this out?

19:26.220 --> 19:28.500
I'm not a gamer, so I don't have any examples,

19:28.500 --> 19:31.660
but I feel like there's this whole world in video games

19:31.660 --> 19:33.580
where they've thought about all of this

19:33.580 --> 19:36.580
and depending on the game, they have different avatars

19:36.580 --> 19:39.780
and a lot of the games are about connecting with others.

19:39.780 --> 19:42.420
I just, the thing that I don't know is,

19:42.420 --> 19:45.420
and again, I haven't looked into this at all.

19:45.420 --> 19:48.020
I've been like shockingly not very interested

19:48.020 --> 19:51.500
in the metaverse, but they must have poured

19:51.500 --> 19:56.300
so much investment into this meta.

19:56.300 --> 20:00.420
And like, why is it so, why are people,

20:00.460 --> 20:01.580
why is it so bad?

20:03.380 --> 20:04.220
There's gotta be a reason.

20:04.220 --> 20:07.020
There's gotta be some thinking behind it, right?

20:07.020 --> 20:10.820
Well, I talked to Carmack about this,

20:10.820 --> 20:14.820
John Carmack, who's a part-time Oculus CTO.

20:18.140 --> 20:20.700
I think there's several things to say.

20:20.700 --> 20:23.940
One is, as you probably know, that there's bureaucracy,

20:23.940 --> 20:27.220
there's large corporations and they often,

20:27.220 --> 20:30.940
large corporations have a way of killing

20:30.940 --> 20:35.900
the indie kind of artistic flame

20:35.900 --> 20:38.900
that's required to create something really compelling.

20:38.900 --> 20:40.500
Somehow they make everything boring

20:40.500 --> 20:42.780
because they run through this whole process,

20:42.780 --> 20:45.380
through the PR department, through all that kind of stuff

20:45.380 --> 20:48.540
and it somehow becomes generic to that process.

20:48.540 --> 20:49.380
Because there's like-

20:49.380 --> 20:50.220
Does it strip out anything interesting

20:50.220 --> 20:51.500
because it could be controversial?

20:51.500 --> 20:52.940
Is that, or?

20:52.940 --> 20:54.340
Yeah, right, exactly.

20:54.340 --> 20:58.740
Like what, I mean, we're living through this now,

20:58.740 --> 21:03.060
like with a lot of people with cancellations

21:03.060 --> 21:05.180
and all those kinds of stuff, people are nervous

21:05.180 --> 21:09.140
and nervousness results in, like usual,

21:09.140 --> 21:11.340
the assholes are ruining everything.

21:11.340 --> 21:14.700
But the magic of human connection is taking risks,

21:14.700 --> 21:19.220
of making a risky joke, of, like with people you like

21:19.220 --> 21:20.700
who are not assholes, good people,

21:20.700 --> 21:24.140
like some of the fun in the metaverse

21:24.140 --> 21:29.140
or in video games is being edgier, being interesting,

21:29.340 --> 21:31.780
revealing your personality in interesting ways.

21:32.820 --> 21:36.180
In the sexual tension or in,

21:36.180 --> 21:38.460
they're definitely paranoid about that.

21:38.460 --> 21:39.300
Oh yeah.

21:39.300 --> 21:42.300
Like in metaverse, the possibility of sexual assault

21:42.300 --> 21:44.940
and sexual harassment and all that kind of stuff,

21:44.940 --> 21:47.240
it's obviously very high, but they're,

21:48.500 --> 21:50.420
so you should be paranoid to some degree,

21:50.420 --> 21:53.060
but not too much because then you remove completely

21:53.060 --> 21:54.860
the personality of the whole thing.

21:54.860 --> 21:57.180
Then everybody's just like a vanilla bot,

21:57.180 --> 22:01.700
but like you have to have ability

22:02.820 --> 22:05.300
to be a little bit political, to be a little bit edgy,

22:05.300 --> 22:06.140
all that kind of stuff.

22:06.140 --> 22:09.620
And large companies tend to suffocate that.

22:09.620 --> 22:12.700
So, but in general, just forget all that,

22:12.700 --> 22:14.940
just the ability to come up

22:16.380 --> 22:18.740
with really cool, beautiful ideas.

22:19.740 --> 22:23.220
If you look at, I think Grimes tweeted about this,

22:23.220 --> 22:25.780
she's very critical about the metaverse,

22:25.780 --> 22:30.780
is that independent game designers have solved this problem

22:34.260 --> 22:35.660
of how to create something beautiful

22:35.660 --> 22:37.500
and interesting and compelling.

22:37.500 --> 22:39.540
They do a really good job.

22:39.540 --> 22:41.620
So you have to let those kinds of minds,

22:41.620 --> 22:44.060
the small groups of people design things

22:44.060 --> 22:47.180
and let them run with it, let them run wild

22:47.220 --> 22:49.300
and do edgy stuff, yeah.

22:49.300 --> 22:53.420
But otherwise you get this kind of,

22:53.420 --> 22:55.660
you get a clippy type of situation, right,

22:55.660 --> 22:59.180
which is like a very generic looking thing.

22:59.180 --> 23:03.780
But even clippy has some, like that's kind of wild

23:03.780 --> 23:08.780
that you would take a paperclip and put eyes on it.

23:08.860 --> 23:10.820
And suddenly people are like, oh, you're annoying,

23:10.820 --> 23:13.260
but you're definitely a social agent.

23:13.260 --> 23:15.700
And I just feel like that wouldn't even,

23:15.700 --> 23:19.020
that clippy thing wouldn't even survive Microsoft

23:19.940 --> 23:23.180
or Facebook of today, a matter of today.

23:23.180 --> 23:24.780
Because it would be like,

23:24.780 --> 23:28.260
there'll be these meetings about why is it a paperclip?

23:28.260 --> 23:30.700
Like, why don't we, it's not sufficiently friendly,

23:30.700 --> 23:34.260
let's make it, you know, and then all of a sudden

23:34.260 --> 23:37.540
the artist with whom it originated is killed

23:37.540 --> 23:40.540
and it's all PR, marketing people

23:40.540 --> 23:41.700
and all of that kind of stuff.

23:41.700 --> 23:45.260
No, they do important work to some degree,

23:45.780 --> 23:47.500
but they kill the creativity.

23:47.500 --> 23:50.460
I think the killing of the creativity is in the whole,

23:50.460 --> 23:52.820
like, okay, so what I know from social robotics

23:52.820 --> 23:57.820
is like, obviously if you create agents that,

23:57.900 --> 23:59.140
okay, so take for an example,

23:59.140 --> 24:01.500
you'd create a robot that looks like a humanoid

24:01.500 --> 24:04.100
and it's, you know, Sophia or whatever.

24:04.100 --> 24:07.220
Now suddenly you do have all of these issues

24:07.220 --> 24:11.540
where are you reinforcing an unrealistic beauty standard?

24:11.540 --> 24:14.300
Are you objectifying women?

24:14.300 --> 24:15.660
Why is the robot white?

24:15.660 --> 24:17.660
So you have, but the thing is,

24:17.660 --> 24:20.500
I think that with creativity,

24:21.980 --> 24:25.340
you can find a solution that's even better

24:25.340 --> 24:28.900
where you're not even harming anyone

24:28.900 --> 24:32.900
and you're creating a robot that looks like not humanoid,

24:32.900 --> 24:35.740
but like something that people relate to even more.

24:35.740 --> 24:38.820
And now you don't even have any of these bias issues

24:38.820 --> 24:39.700
that you're creating.

24:39.700 --> 24:42.260
And so how do we create that within companies?

24:42.300 --> 24:44.540
Because I don't think it's really about,

24:45.460 --> 24:48.220
like, I, cause I, you know, maybe we disagree on that.

24:48.220 --> 24:52.460
I don't think that edginess or humor or interesting things

24:52.460 --> 24:55.220
need to be things that harm or hurt people

24:55.220 --> 24:56.460
or that people are against.

24:56.460 --> 24:59.580
There are ways to find things that everyone is fine with.

25:00.540 --> 25:01.780
Why aren't we doing that?

25:01.780 --> 25:03.780
The problem is there's departments

25:03.780 --> 25:05.660
that look for harm in things.

25:05.660 --> 25:06.480
Yeah.

25:06.480 --> 25:09.460
And so they will find harm in things that have no harm.

25:09.460 --> 25:10.300
Okay.

25:10.300 --> 25:11.140
That's the big problem

25:11.140 --> 25:13.660
because their whole job is to find harm in things.

25:13.660 --> 25:16.700
So what you said is completely correct,

25:16.700 --> 25:19.900
which is edginess should not hurt,

25:19.900 --> 25:21.660
doesn't necessarily,

25:21.660 --> 25:24.460
doesn't need to be a thing that hurts people.

25:24.460 --> 25:29.000
Obviously, great humor, great personality,

25:29.000 --> 25:31.180
doesn't have to, like Clippy.

25:33.020 --> 25:36.540
But yeah, I mean, but it's tricky to get right.

25:36.540 --> 25:37.780
And I'm not exactly sure.

25:37.780 --> 25:38.620
I don't know.

25:38.620 --> 25:40.180
I don't know why a large corporation

25:40.180 --> 25:41.900
with a lot of funding can't get this right.

25:41.900 --> 25:42.740
I do think you're right

25:42.740 --> 25:44.700
that there's a lot of aversion to risk.

25:44.700 --> 25:46.820
And so if you get lawyers involved

25:46.820 --> 25:50.640
or people whose job it is, like you say, to mitigate risk,

25:50.640 --> 25:52.400
they're just gonna say no to most things

25:52.400 --> 25:54.500
that could even be in some way.

25:54.500 --> 25:56.060
Yeah.

25:56.060 --> 25:58.020
Yeah, you get the problem in all organizations.

25:58.020 --> 26:00.820
So I think that you're right that that is a problem.

26:00.820 --> 26:02.700
I think what's the way to solve that

26:02.700 --> 26:03.900
in large organizations

26:03.900 --> 26:06.300
is to have Steve Jobs type of characters.

26:06.300 --> 26:08.980
Unfortunately, you do need to have, I think,

26:11.020 --> 26:12.500
from a designer perspective,

26:12.500 --> 26:14.620
or maybe like a Johnny Ive

26:14.620 --> 26:17.140
that is almost like a dictator.

26:17.140 --> 26:18.820
Yeah, you want a benevolent dictator.

26:18.820 --> 26:20.780
Yeah, who rolls in and says,

26:22.420 --> 26:24.860
cuts through the lawyers, the PR,

26:24.860 --> 26:27.060
but has a benevolent aspect.

26:27.060 --> 26:30.260
Yeah, that has a good heart and makes sure.

26:30.260 --> 26:32.820
I think all great artists and designers

26:32.820 --> 26:35.340
create stuff that doesn't hurt people.

26:35.340 --> 26:36.500
If you have a good heart,

26:36.500 --> 26:37.660
you're going to create something

26:37.660 --> 26:41.260
that's going to actually make a lot of people feel good.

26:41.260 --> 26:43.380
That's what like people like Johnny Ive,

26:43.380 --> 26:46.620
what they love doing is creating a thing

26:46.620 --> 26:48.620
that brings a lot of love to the world.

26:48.620 --> 26:50.860
They imagine like millions of people using the thing

26:50.860 --> 26:54.060
and it instills them with joy.

26:54.060 --> 26:56.480
That you could say that about social robotics,

26:56.480 --> 26:59.180
you could say that about the metaverse.

26:59.180 --> 27:01.780
It shouldn't be done by the PR people,

27:01.780 --> 27:03.620
should be done by the designers.

27:03.620 --> 27:06.160
I agree, PR people ruin everything.

27:06.200 --> 27:07.400
Yeah, all the fun.

27:09.240 --> 27:11.560
In the book, you have a picture.

27:11.560 --> 27:13.640
I just have a lot of ridiculous questions.

27:13.640 --> 27:16.160
You have a picture of two hospital delivery robots

27:16.160 --> 27:18.720
with a caption that reads, by the way,

27:18.720 --> 27:23.440
see your book, I appreciate that it keeps the humor in.

27:23.440 --> 27:25.520
You didn't run it by the PR department.

27:25.520 --> 27:28.380
No, no one edited the book, we got rushed through.

27:30.640 --> 27:33.920
The caption reads, two hospital delivery robots

27:33.920 --> 27:36.640
whose sexy nurse names Roxy and Lola

27:36.640 --> 27:39.920
made me roll my eyes so hard they almost fell out.

27:41.040 --> 27:42.960
What aspect of it made you roll your eyes?

27:42.960 --> 27:44.340
Is it the naming?

27:44.340 --> 27:45.340
It was the naming.

27:45.340 --> 27:48.240
The form factor is fine, it's like a little box on wheels.

27:48.240 --> 27:50.240
The fact that they named them, also great.

27:50.240 --> 27:53.920
That'll let people enjoy interacting with them.

27:53.920 --> 27:56.280
We know that even just giving a robot a name,

27:56.280 --> 27:59.880
people will, it facilitates technology adoption.

27:59.880 --> 28:02.440
People will be like, oh, you know,

28:02.440 --> 28:04.680
Betsy made a mistake, let's help her out

28:04.680 --> 28:06.720
instead of the stupid robot doesn't work.

28:06.720 --> 28:10.120
But why Lola and Roxy?

28:10.120 --> 28:12.080
Those are to you too sexy?

28:12.080 --> 28:14.360
I mean, there's research showing that

28:16.040 --> 28:21.040
a lot of robots are named according to gender biases

28:22.000 --> 28:24.060
about the function that they're fulfilling.

28:24.060 --> 28:27.840
So, robots that are helpful in assistance

28:27.840 --> 28:32.200
and are like nurses are usually female gendered,

28:32.960 --> 28:35.880
robots that are powerful, all wise computers like Watson

28:35.880 --> 28:40.880
usually have like a booming male coded voice and name.

28:41.220 --> 28:45.280
So, like that's one of those things, right?

28:45.280 --> 28:48.560
You're opening a can of worms for no reason, for no reason.

28:48.560 --> 28:50.320
You can avoid this whole can of worms.

28:50.320 --> 28:53.800
Just give it a different name, like why Roxy?

28:53.800 --> 28:55.540
It's because people aren't even thinking.

28:55.540 --> 28:59.520
So, to some extent, I don't like PR departments,

28:59.520 --> 29:02.520
but getting some feedback on your work

29:02.520 --> 29:04.880
from a diverse set of participants,

29:04.880 --> 29:08.640
listening and taking in things

29:08.640 --> 29:11.360
that help you identify your own blind spots.

29:11.360 --> 29:14.700
And then you can always make your good leadership choices

29:14.700 --> 29:17.000
and good, like you can still ignore things

29:17.000 --> 29:19.600
that you don't believe are an issue,

29:19.600 --> 29:23.300
but having the openness to take in feedback

29:23.300 --> 29:25.160
and making sure that you're getting the right feedback

29:25.160 --> 29:28.280
from the right people, I think that's really important.

29:28.280 --> 29:32.680
So, don't unnecessarily propagate the biases of society.

29:32.680 --> 29:33.880
Yeah, why?

29:33.880 --> 29:35.360
In the design.

29:35.360 --> 29:40.360
But if you're not careful when you do the research of,

29:40.480 --> 29:45.480
like you might, if you ran a poll with a lot of people,

29:45.720 --> 29:48.200
of all the possible names these robots have,

29:48.200 --> 29:50.520
they might come up with Roxy and Lola

29:50.520 --> 29:55.120
as names they would enjoy most.

29:55.120 --> 29:58.760
Like that could come up as the highest.

29:58.760 --> 30:02.140
As in you do marketing research and then,

30:02.140 --> 30:03.600
well, that's what they did with Alexa.

30:03.600 --> 30:04.840
They did marketing research

30:04.840 --> 30:07.160
and nobody wanted the male voice.

30:07.160 --> 30:09.040
Everyone wanted it to be female.

30:09.040 --> 30:10.720
Well, what do you think about that?

30:10.720 --> 30:14.360
What, if I were to say,

30:15.440 --> 30:17.120
I think the role of a great designer,

30:17.120 --> 30:19.320
again, to go back to Johnny Ive,

30:19.320 --> 30:23.120
is to throw out the marketing research.

30:23.480 --> 30:26.200
Take it in, do it, learn from it.

30:26.200 --> 30:31.200
But if everyone wants Alexa to be a female voice,

30:31.240 --> 30:33.240
the role of the designer is to think deeply

30:33.240 --> 30:38.240
about the future of social agents in the home and think.

30:39.320 --> 30:40.840
What does that future look like?

30:40.840 --> 30:43.480
And try to reverse engineer that future.

30:43.480 --> 30:46.400
So, in some sense, there's this weird tension.

30:46.400 --> 30:49.060
You want to listen to a lot of people,

30:49.060 --> 30:51.320
but at the same time, you want to,

30:51.440 --> 30:53.040
you're creating a thing that defines

30:53.040 --> 30:54.200
the future of the world.

30:55.200 --> 30:57.700
And the people that you're listening to

30:57.700 --> 31:00.080
are part of the past.

31:00.080 --> 31:02.240
So, that weird tension.

31:02.240 --> 31:03.480
Yeah, I think that's true.

31:03.480 --> 31:06.080
And I think some companies like Apple

31:06.080 --> 31:10.400
have historically done very well at understanding a market

31:10.400 --> 31:11.840
and saying, you know what our role is?

31:11.840 --> 31:14.360
It's not to listen to what the current market says.

31:14.360 --> 31:16.240
It's to actually shape the market

31:16.240 --> 31:18.360
and shape consumer preferences.

31:18.360 --> 31:20.600
And companies have the power to do that.

31:20.600 --> 31:22.520
They can be forward-thinking,

31:22.520 --> 31:24.440
and they can actually shift

31:24.440 --> 31:27.020
what the future of technology looks like.

31:27.020 --> 31:29.440
And I agree with you that I would like to see more of that,

31:29.440 --> 31:34.440
especially when it comes to existing biases that we know.

31:38.560 --> 31:40.360
I think there's the low-hanging fruit of companies

31:40.360 --> 31:41.600
that don't even think about it at all

31:41.600 --> 31:42.960
and aren't talking to the right people

31:42.960 --> 31:44.520
and aren't getting the full information.

31:44.520 --> 31:46.400
And then there's companies that are just doing

31:46.400 --> 31:49.820
the safe thing and giving consumers what they want now.

31:49.820 --> 31:52.860
But to be really forward-looking and be really successful,

31:52.860 --> 31:55.440
I think you have to make some judgment calls

31:55.440 --> 31:57.420
about what the future is gonna be.

31:57.420 --> 31:59.980
But do you think it's still useful to gender

31:59.980 --> 32:01.780
and to name the robots?

32:01.780 --> 32:06.020
Yes, I mean, gender is a minefield,

32:06.020 --> 32:10.980
but people, it's really hard to get people

32:10.980 --> 32:14.140
to not gender a robot in some way.

32:14.140 --> 32:16.580
So, if you don't give it a name

32:16.580 --> 32:20.220
or you give it an ambiguous voice,

32:20.220 --> 32:22.500
people will just choose something.

32:22.500 --> 32:27.500
And maybe that's better than just entrenching something

32:28.060 --> 32:30.540
that you've decided is best.

32:30.540 --> 32:33.060
But I do think it can be helpful

32:33.060 --> 32:36.380
on the anthropomorphism engagement level

32:36.380 --> 32:39.460
to give it attributes that people identify with.

32:39.460 --> 32:42.140
Yeah, I think a lot of roboticists I know,

32:42.140 --> 32:44.380
they don't gender the robot.

32:44.420 --> 32:46.620
They even try to avoid naming the robot.

32:46.620 --> 32:51.620
Or naming it something that can be used as a name

32:51.620 --> 32:53.620
in conversation kind of thing.

32:53.620 --> 32:56.180
And I think that's actually,

32:56.180 --> 32:59.780
that's irresponsible,

32:59.780 --> 33:03.720
because people are going to anthropomorphize the thing anyway.

33:04.580 --> 33:07.900
So, you're just removing from yourself the responsibility

33:07.900 --> 33:10.140
of how they're going to anthropomorphize it.

33:10.140 --> 33:11.140
That's a good point.

33:11.140 --> 33:13.880
And so, you want to be able to,

33:13.920 --> 33:15.440
if they're going to do it,

33:15.440 --> 33:17.680
you have to start to think about how they're going to do it.

33:17.680 --> 33:20.760
Even if the robot is like a Boston Dynamics robot,

33:20.760 --> 33:25.240
that's not supposed to have any kind of social component.

33:25.240 --> 33:27.120
They're obviously going to project

33:27.120 --> 33:28.960
a social component to it.

33:28.960 --> 33:33.560
Like that arm, I worked a lot with quadrupeds now

33:33.560 --> 33:36.040
with robot dogs.

33:36.040 --> 33:39.440
That arm, people think is the head immediately.

33:39.440 --> 33:40.440
It's supposed to be an arm,

33:40.440 --> 33:41.960
but they start to think it's the head.

33:42.000 --> 33:44.120
And you have to like acknowledge that.

33:44.120 --> 33:45.840
You can't, I mean-

33:45.840 --> 33:46.920
They do now.

33:46.920 --> 33:47.760
They do now?

33:47.760 --> 33:50.440
Well, they've deployed the robots and people are like,

33:50.440 --> 33:53.080
oh my God, the cops are using a robot dog.

33:53.080 --> 33:54.880
And so, they have this PR nightmare.

33:54.880 --> 33:58.600
And so, they're like, oh, yeah, okay.

33:58.600 --> 34:00.480
Maybe we should hire some HRI people.

34:02.080 --> 34:04.400
Well, Boston Dynamics is an interesting company.

34:04.400 --> 34:07.200
Any of the others are doing similar thing

34:07.200 --> 34:11.920
because their main source of money

34:12.920 --> 34:14.680
is in industrial applications.

34:14.680 --> 34:16.440
So, like surveillance of factories

34:16.440 --> 34:18.840
and doing dangerous jobs.

34:18.840 --> 34:22.240
So, to them, it's almost good PR

34:23.200 --> 34:25.400
for people to be scared of these things

34:25.400 --> 34:28.560
because it's for some reason, as you talk about,

34:28.560 --> 34:31.080
people are naturally, for some reason, scared.

34:31.080 --> 34:33.000
We could talk about that, of robots.

34:33.000 --> 34:35.360
And so, it becomes more viral,

34:35.360 --> 34:38.320
like playing with that little fear.

34:38.320 --> 34:40.040
And so, it's almost like a good PR

34:40.040 --> 34:42.160
because, ultimately, they're not trying to put them

34:42.160 --> 34:44.560
in the home and have a good social connection.

34:44.560 --> 34:46.800
They're trying to put them in factories.

34:46.800 --> 34:48.520
And so, they have fun with it.

34:48.520 --> 34:52.600
If you watch Boston Dynamics videos, they're aware of it.

34:52.600 --> 34:53.440
Oh, yeah.

34:53.440 --> 34:54.520
They're, I mean, I was-

34:54.520 --> 34:57.160
The videos, for sure, that they put out.

34:57.160 --> 35:01.480
It's almost like an unspoken tongue-in-cheek thing.

35:01.480 --> 35:05.160
They're aware of how people are going to feel

35:05.160 --> 35:08.560
when you have a robot that does like a flip.

35:08.560 --> 35:11.680
Now, most of the people are just excited

35:11.680 --> 35:13.160
about the control problem of it,

35:13.160 --> 35:15.880
like how to make the whole thing happen.

35:15.880 --> 35:18.480
But they're aware when people see.

35:18.480 --> 35:20.600
Well, I think they became aware.

35:20.600 --> 35:21.720
I think that, in the beginning,

35:21.720 --> 35:24.480
they were really, really focused on just the engineering.

35:24.480 --> 35:27.040
I mean, they're at the forefront of robotics,

35:27.040 --> 35:28.600
like locomotion and stuff.

35:30.040 --> 35:31.720
And then, when they started doing the videos,

35:31.720 --> 35:34.640
I think that was kind of a labor of love.

35:34.640 --> 35:37.520
I know that the former CEO, Mark,

35:37.560 --> 35:40.160
he oversaw a lot of the videos and made a lot of them himself.

35:40.160 --> 35:43.440
And he's even really detail-oriented.

35:43.440 --> 35:45.080
There can't be some sort of incline

35:45.080 --> 35:46.600
that would give the robot an advantage.

35:46.600 --> 35:48.880
They're very, he was very,

35:48.880 --> 35:51.560
hell of integrity about the authenticity of them.

35:52.480 --> 35:54.680
But then, when they started to go viral,

35:54.680 --> 35:56.880
I think that's when they started to realize,

35:56.880 --> 36:01.520
oh, there's something interesting here that,

36:03.480 --> 36:05.920
I don't know how much they took it seriously

36:05.920 --> 36:07.640
in the beginning other than realizing

36:07.640 --> 36:10.480
that they could play within the videos.

36:10.480 --> 36:13.360
I know that they take it very seriously now.

36:13.360 --> 36:17.040
What I like about Boston Dynamics and similar companies,

36:17.040 --> 36:19.560
it's still mostly run by engineers.

36:20.840 --> 36:25.840
But, you know, I've had my criticisms.

36:25.880 --> 36:28.640
There's a bit more PR leaking in.

36:28.640 --> 36:31.640
But those videos are made by engineers

36:31.640 --> 36:33.840
because that's what they find fun.

36:33.840 --> 36:36.640
It's like testing the robustness of the system.

36:36.640 --> 36:41.640
I mean, they're having a lot of fun there with the robots.

36:41.880 --> 36:42.720
Totally.

36:43.680 --> 36:45.280
Have you been to visit?

36:45.280 --> 36:46.400
Yeah, yeah, yeah.

36:46.400 --> 36:47.240
Yeah, it's so cool.

36:47.240 --> 36:51.040
It's one of the most, like, I mean,

36:51.040 --> 36:56.040
because I have eight robot dogs now.

36:56.160 --> 36:58.080
Wait, you have eight robot dogs?

36:58.080 --> 36:58.920
What?

36:58.920 --> 37:00.680
So they're just walking around your place?

37:00.680 --> 37:01.520
Like, where do you keep them?

37:01.520 --> 37:02.440
Yeah, I'm working on them.

37:02.440 --> 37:06.440
That's actually one of my goals is to have,

37:06.440 --> 37:09.920
at any one time, always a robot moving.

37:09.920 --> 37:10.760
Oh.

37:10.760 --> 37:11.600
I'm far away from that.

37:11.600 --> 37:13.040
That's an ambitious goal.

37:13.040 --> 37:15.720
Well, I have, like, more Roombas that I know what to do with.

37:15.720 --> 37:17.840
No, the Roomba, they're a program.

37:17.840 --> 37:20.080
So the programmable Roombas.

37:20.080 --> 37:20.920
Nice.

37:20.920 --> 37:24.000
And I have a bunch of little, like, I built the,

37:24.000 --> 37:24.920
I'm not finished with it yet,

37:24.920 --> 37:27.880
but I bought a robot from Rick and Morty.

37:27.880 --> 37:29.240
They still have a bunch of robots everywhere.

37:29.240 --> 37:32.280
But the thing is, what happens is,

37:32.280 --> 37:34.880
you're working on one robot at a time,

37:34.880 --> 37:37.240
and that becomes like a little project.

37:38.120 --> 37:40.120
It's actually very difficult to have

37:41.080 --> 37:46.080
just a passively functioning robot always moving.

37:46.360 --> 37:47.200
Yeah.

37:47.200 --> 37:48.920
And that's a dream for me,

37:48.920 --> 37:51.240
because I love to create that kind of little world.

37:51.240 --> 37:54.960
So the impressive thing about Boston Dynamics to me

37:54.960 --> 37:58.680
was to see, like, hundreds of spots

37:59.480 --> 38:00.920
and, like, there was a,

38:00.920 --> 38:03.520
the most impressive thing that still sticks with me is,

38:04.440 --> 38:09.440
there was a spot robot walking down the hall

38:09.480 --> 38:12.120
seemingly with no supervision whatsoever,

38:12.120 --> 38:14.000
and he was wearing, he or she, I don't know,

38:14.000 --> 38:15.400
was wearing a cowboy hat.

38:15.400 --> 38:18.400
It just, it was just walking down the hall

38:18.400 --> 38:20.000
and nobody paying attention,

38:20.000 --> 38:22.400
and it's just like walking down this long hall,

38:22.400 --> 38:24.600
and I'm, like, looking around.

38:24.600 --> 38:28.000
Is anyone, like, what's happening here?

38:28.040 --> 38:30.160
So presumably it's some kind of automation

38:30.160 --> 38:31.000
where it's doing the map.

38:31.000 --> 38:32.320
I mean, the whole environment

38:32.320 --> 38:33.880
is probably really well mapped,

38:33.880 --> 38:38.720
but it was just, it gave me a picture of a world

38:38.720 --> 38:40.480
where a robot is doing its thing,

38:40.480 --> 38:44.040
wearing a cowboy hat, just going down the hall,

38:44.040 --> 38:45.480
like, getting some coffee or whatever.

38:45.480 --> 38:47.440
Like, I don't know what it's doing, what's the mission,

38:47.440 --> 38:49.840
but I don't know, for some reason, it really stuck with me.

38:49.840 --> 38:53.280
You don't often see robots that aren't part of a demo

38:53.280 --> 38:56.600
or that aren't, you know, like,

38:56.640 --> 38:59.400
with a semi-autonomous, autonomous vehicle,

38:59.400 --> 39:01.360
like, directly doing a task.

39:01.360 --> 39:03.000
This was just chilling.

39:03.000 --> 39:03.840
Yeah.

39:03.840 --> 39:05.360
Just walking around, I don't know.

39:05.360 --> 39:07.360
Well, yeah, you know, I mean, we're at MIT.

39:07.360 --> 39:09.280
Like, when I first got to MIT, I was like,

39:09.280 --> 39:11.480
okay, where's all the robots?

39:11.480 --> 39:14.160
And they were all, like, broken or, like, not demoing,

39:14.160 --> 39:16.480
so, yeah.

39:16.480 --> 39:21.480
And what really excites me is that we're about to have,

39:21.480 --> 39:24.200
that we're about to have so many moving rope about to.

39:24.240 --> 39:25.560
Well, it's coming.

39:25.560 --> 39:26.880
It's coming in our lifetime

39:26.880 --> 39:29.280
that we will just have robots moving around.

39:29.280 --> 39:30.800
We're already seeing the beginnings of it.

39:30.800 --> 39:34.040
There's delivery robots in some cities, on the sidewalks,

39:34.040 --> 39:36.560
and I just love seeing, like, the TikToks

39:36.560 --> 39:39.000
of people reacting to that, because, yeah,

39:39.000 --> 39:42.000
you see a robot walking down the hall with a cowboy hat.

39:42.000 --> 39:43.480
You're like, what the fuck?

39:43.480 --> 39:44.680
What is this?

39:44.680 --> 39:47.360
This is awesome and scary and kind of awesome,

39:47.360 --> 39:49.320
and people either love or hate it.

39:49.320 --> 39:51.240
That's one of the things that I think companies

39:51.240 --> 39:52.120
are underestimating,

39:52.120 --> 39:55.400
that people will either love a robot or hate a robot

39:55.400 --> 39:56.480
and nothing in between.

39:56.480 --> 40:00.480
So, it's just, again, an exciting time to be alive.

40:00.480 --> 40:03.800
Yeah, I think kids, almost universally,

40:03.800 --> 40:06.360
at least in my experience, love them.

40:06.360 --> 40:08.080
Love legged robots.

40:08.080 --> 40:10.720
If they're not loud, my son hates the rumo

40:10.720 --> 40:13.000
because ours is loud.

40:13.000 --> 40:14.160
Oh, that, yeah.

40:14.160 --> 40:15.440
No, the legs.

40:15.440 --> 40:16.840
The legs make a difference.

40:16.840 --> 40:18.920
Because I don't, your son,

40:19.920 --> 40:23.080
do they understand rumo to be a robot?

40:23.080 --> 40:26.040
Oh, yeah, my kids, that's one of the first words they learned.

40:26.040 --> 40:27.720
They know how to say beep boop.

40:27.720 --> 40:29.880
And yes, they think the rumo's a robot.

40:29.880 --> 40:32.600
Do they project intelligence out of the thing?

40:32.600 --> 40:34.360
Well, we don't really use it around them anymore

40:34.360 --> 40:37.200
for the reason that my son is scared of it.

40:39.360 --> 40:41.360
Yeah, that's really interesting.

40:41.360 --> 40:42.200
I think they would.

40:42.200 --> 40:46.960
Like, even a rumba, because it's moving around on its own,

40:47.000 --> 40:50.880
I think kids and animals view it as an agent.

40:52.160 --> 40:53.200
So what do you think,

40:53.200 --> 40:55.600
if we just look at the state of the art of robotics,

40:55.600 --> 40:58.640
what do you think robots are actually good at today?

40:58.640 --> 40:59.880
So if we look at today.

40:59.880 --> 41:01.520
You mean physical robots?

41:01.520 --> 41:02.920
Yeah, physical robots.

41:04.240 --> 41:05.520
Well.

41:05.520 --> 41:07.120
Like, what are you impressed by?

41:08.120 --> 41:09.520
So I think a lot of people,

41:09.520 --> 41:10.880
I mean, that's what your book is about,

41:10.880 --> 41:15.880
is maybe not a perfectly calibrated understanding

41:16.680 --> 41:20.360
of where we are in terms of robotics,

41:20.360 --> 41:22.640
what's difficult to robotics, what's easy in robotics.

41:22.640 --> 41:26.800
Yeah, we're way behind where people think we are.

41:26.800 --> 41:31.800
So what's impressive to me, so let's see.

41:32.200 --> 41:34.640
Oh, one thing that came out recently was

41:34.640 --> 41:36.840
Amazon has this new warehouse robot,

41:36.840 --> 41:41.640
and it's the first autonomous warehouse robot

41:41.640 --> 41:44.520
that is safe for people to be around.

41:44.520 --> 41:47.400
And so like, it's kind of, most people,

41:47.400 --> 41:49.600
most people I think envision that our warehouses

41:49.600 --> 41:51.200
are already fully automated

41:51.200 --> 41:54.000
and that there's just like robots doing things.

41:54.000 --> 41:58.280
It's actually still really difficult to have robots

41:58.280 --> 41:59.800
and people in the same space,

41:59.800 --> 42:01.960
because it's dangerous for the most part.

42:01.960 --> 42:04.240
Robots, you know, because especially robots

42:04.240 --> 42:07.000
that have to be strong enough to move something heavy,

42:07.000 --> 42:09.160
for example, they can really hurt somebody.

42:09.160 --> 42:12.320
And so until now, a lot of the warehouse robots

42:12.320 --> 42:15.040
had to just move along like pre-existing lines,

42:15.040 --> 42:17.200
which really restricts what you can do.

42:18.480 --> 42:23.120
And so having, I think that's one of the big challenges

42:23.120 --> 42:25.840
and one of the big exciting things that's happening

42:25.840 --> 42:29.080
is that we're starting to see more cobotics

42:29.080 --> 42:30.760
in industrial spaces like that,

42:30.760 --> 42:33.680
where people and robots can work side by side

42:33.680 --> 42:35.560
and not get harmed.

42:35.560 --> 42:37.120
Yeah, that's what people don't realize,

42:37.120 --> 42:41.000
sort of the physical manipulation task with humans.

42:42.000 --> 42:44.720
It's not that the robots want to hurt you.

42:44.720 --> 42:46.280
I think that's what people are worried about,

42:46.280 --> 42:50.200
like this malevolent robot gets them out of its own

42:50.200 --> 42:52.400
and wants to destroy all humans.

42:52.400 --> 42:56.560
No, it's actually very difficult to know where the human is

42:56.560 --> 43:00.680
and to respond to the human and dynamically

43:00.680 --> 43:03.520
and collaborate with them on a task,

43:03.520 --> 43:06.520
especially if you're something like an industrial robotic arm,

43:06.520 --> 43:08.240
which is extremely powerful.

43:08.840 --> 43:13.240
So yeah, some of those arms are pretty impressive now

43:13.240 --> 43:17.840
that you can grab it, you can move it.

43:17.840 --> 43:21.040
So the collaboration between human and robot

43:21.040 --> 43:23.000
in the factory setting is really fascinating.

43:23.000 --> 43:23.840
Yeah.

43:25.320 --> 43:27.080
Do you think they'll take our jobs?

43:28.120 --> 43:29.640
I don't think it's that simple.

43:29.640 --> 43:33.800
I think that there's a ton of disruption that's happening

43:33.800 --> 43:35.280
and will continue to happen.

43:36.280 --> 43:40.360
I think speaking specifically of the Amazon warehouses,

43:40.360 --> 43:42.760
that might be an area where it would be good for robots

43:42.760 --> 43:45.200
to take some of the jobs that are,

43:45.200 --> 43:48.280
where people are put in a position where it's unsafe

43:48.280 --> 43:49.760
and they're treated horribly.

43:49.760 --> 43:52.920
And probably it would be better if a robot did that

43:52.920 --> 43:56.760
and Amazon is clearly trying to automate that job away.

43:56.760 --> 44:00.320
So I think there's gonna be a lot of disruption.

44:00.320 --> 44:03.280
I do think that robots and humans

44:03.280 --> 44:04.600
have very different skill sets.

44:05.080 --> 44:08.520
So while a robot might take over a task,

44:09.560 --> 44:12.440
it's not gonna take over most jobs.

44:14.440 --> 44:16.880
I think just things will change a lot.

44:16.880 --> 44:18.840
Like, I don't know, one of the examples I have

44:18.840 --> 44:20.240
in the book is mining.

44:21.560 --> 44:26.280
So there you have this job that is very unsafe

44:26.280 --> 44:28.600
and that requires a bunch of workers

44:28.600 --> 44:30.040
and puts them in unsafe conditions.

44:30.040 --> 44:33.920
And now you have all these different robotic machines

44:33.920 --> 44:36.560
that can help make the job safer.

44:36.560 --> 44:38.760
And as a result, now people can sit in these

44:38.760 --> 44:42.520
like air conditioned remote control stations

44:42.520 --> 44:45.640
and like control these autonomous mining trucks.

44:45.640 --> 44:47.000
And so that's a much better job,

44:47.000 --> 44:49.200
but also they're employing less people now.

44:49.200 --> 44:53.200
So it's just a lot of,

44:54.800 --> 44:57.160
I think from a bird's eye perspective,

44:57.160 --> 44:58.480
you're not gonna see job loss,

44:58.480 --> 45:03.080
you're gonna see more jobs created because that's,

45:03.080 --> 45:07.400
the future is not robots just becoming like people

45:07.400 --> 45:08.360
and taking their jobs.

45:08.360 --> 45:11.320
The future is really a combination of our skills

45:11.320 --> 45:14.520
and then the supplemental skill set that robots have

45:14.520 --> 45:15.880
to increase productivity,

45:15.880 --> 45:18.680
to help people have better, safer jobs,

45:18.680 --> 45:22.960
to give people work that they actually enjoy doing

45:22.960 --> 45:24.000
and are good at.

45:25.000 --> 45:26.880
But it's really easy to say that

45:26.880 --> 45:28.600
from a bird's eye perspective

45:28.600 --> 45:33.600
and ignore kind of the rubble on the ground

45:34.240 --> 45:35.520
as we go through these transitions,

45:35.520 --> 45:39.560
because of course, specific jobs are going to get lost.

45:39.560 --> 45:41.720
If you look at the history of the 20th century,

45:41.720 --> 45:46.680
it seems like automation constantly increases productivity

45:46.680 --> 45:51.200
and improves the average quality of life.

45:51.200 --> 45:53.040
So it's been always good.

45:53.040 --> 45:58.040
So like thinking about this time being different

45:58.280 --> 46:01.880
is that we would need to go against the lessons of history.

46:01.880 --> 46:03.480
It's true.

46:03.480 --> 46:06.480
And the other thing is I think people think

46:06.480 --> 46:10.000
that the automation of the physical tasks is easy.

46:10.000 --> 46:13.040
I was just in Ukraine and the interesting thing is,

46:14.520 --> 46:18.720
I mean, there's a lot of difficult and dark lessons,

46:18.720 --> 46:20.880
just about a war zone.

46:20.880 --> 46:22.840
But one of the things that happens in war

46:22.840 --> 46:25.000
is there's a lot of mines that are placed.

46:25.840 --> 46:30.840
That's one of the big problems for years

46:31.120 --> 46:33.000
after a war is even over,

46:33.000 --> 46:35.640
is the entire landscape is covered in mines.

46:36.560 --> 46:39.400
And so there's a demining effort.

46:40.320 --> 46:43.160
And you would think robots would be good

46:43.160 --> 46:44.280
at this kind of thing.

46:44.280 --> 46:45.920
Or like your intuition would be like,

46:45.920 --> 46:48.840
well, say you have unlimited money

46:48.840 --> 46:51.320
and you wanna do a good job of it, unlimited money.

46:51.320 --> 46:53.680
You would get a lot of really nice robots,

46:53.680 --> 46:57.440
but no, humans are still far superior.

46:57.440 --> 47:00.400
Or animals. Or animals, right.

47:00.400 --> 47:02.320
But humans with animals together.

47:02.320 --> 47:03.840
Yeah. You can't just have

47:03.840 --> 47:05.040
dog with a hat.

47:05.040 --> 47:06.040
That's fair.

47:08.800 --> 47:13.800
But yes, but figuring out also how to disable the mine.

47:16.120 --> 47:17.640
Obviously the easy thing,

47:17.640 --> 47:22.640
the thing a robot can help with is to find the mine

47:23.280 --> 47:24.600
and blow it up.

47:25.480 --> 47:28.000
But that's gonna destroy the landscape.

47:28.000 --> 47:30.080
That really does a lot of damage to the land.

47:30.080 --> 47:32.400
You wanna disable the mine.

47:32.400 --> 47:34.840
And to do that because of all the different,

47:34.840 --> 47:37.200
all the different edge cases of the problem.

47:37.200 --> 47:40.080
It requires a huge amount of human-like experience

47:40.080 --> 47:41.040
it seems like.

47:41.040 --> 47:42.600
So it's mostly done by humans.

47:42.600 --> 47:43.880
They have no use for robots.

47:43.880 --> 47:45.080
They don't want robots.

47:45.080 --> 47:48.080
Yeah. I think we overestimate what we can automate.

47:50.320 --> 47:51.680
Especially in the physical realm.

47:51.680 --> 47:52.520
Yeah.

47:53.360 --> 47:54.200
It's weird.

47:54.200 --> 47:57.560
I mean, it's continuous that the story of humans,

47:57.560 --> 48:00.680
we think we're shitty at everything in the physical world,

48:00.680 --> 48:01.640
including driving.

48:01.640 --> 48:04.360
We think everybody makes fun of themselves

48:04.360 --> 48:06.200
and others for being shitty drivers,

48:06.200 --> 48:07.920
but we're actually kind of incredible.

48:07.920 --> 48:08.880
No, we're incredible.

48:08.880 --> 48:12.760
And that's why Tesla still says that

48:12.760 --> 48:13.960
if you're in the driver's seat,

48:13.960 --> 48:17.320
you are ultimately responsible.

48:17.320 --> 48:18.480
Because the ideal for,

48:18.480 --> 48:20.360
I mean, you know more about this than I do,

48:20.360 --> 48:25.360
but like robot cars are great at predictable things

48:27.160 --> 48:31.280
and can react faster and more precisely than a person

48:31.280 --> 48:33.400
and can do a lot of the driving.

48:33.400 --> 48:35.680
And then the reason that we still don't have

48:35.680 --> 48:37.960
autonomous vehicles on all the roads yet

48:37.960 --> 48:41.760
is because of this long tail of just unexpected occurrences

48:41.760 --> 48:44.080
where a human immediately understands

48:44.080 --> 48:46.080
that's a sunset and not a traffic light.

48:46.080 --> 48:48.440
That's a horse and carriage ahead of me on the highway,

48:48.440 --> 48:50.120
but the car has never encountered that before.

48:50.840 --> 48:53.760
In theory, combining those skill sets

48:53.760 --> 48:57.040
is what's gonna really be powerful.

48:57.040 --> 48:59.440
The only problem is figuring out

48:59.440 --> 49:01.400
the human-robot interaction and the handoffs.

49:01.400 --> 49:03.760
So in cars, that's a huge problem right now,

49:03.760 --> 49:06.120
figuring out the handoffs.

49:06.120 --> 49:08.880
But in other areas, it might be easier.

49:08.880 --> 49:12.880
And that's really the future, is human-robot interaction.

49:12.880 --> 49:15.000
What's really hard to improve,

49:15.000 --> 49:19.760
it's terrible that people die in car accidents,

49:19.800 --> 49:24.520
but I mean, it's like 70, 80, 100 million miles,

49:24.520 --> 49:28.560
one death per 80 million miles.

49:28.560 --> 49:31.680
That's like really hard to beat for a robot.

49:31.680 --> 49:33.040
That's like incredible.

49:33.040 --> 49:34.560
Like think about it.

49:34.560 --> 49:37.280
Like the, how many people?

49:37.280 --> 49:39.000
Just the number of people throughout the world

49:39.000 --> 49:41.320
that are driving every single day.

49:41.320 --> 49:45.360
All of the, you know, steep, deprived, drunk,

49:47.160 --> 49:49.400
distracted, all of that.

49:49.400 --> 49:51.760
And still, very few die,

49:51.760 --> 49:53.080
relative to what I would imagine.

49:53.080 --> 49:55.760
If I were to guess, back in the horse,

49:55.760 --> 49:58.680
see, when I was like in the beginning of the 20th century,

49:58.680 --> 50:02.920
riding my horse, I would talk so much shit about these cars.

50:02.920 --> 50:05.120
I'd be like, this is extremely dangerous.

50:05.120 --> 50:08.080
These machines traveling at 30 miles an hour,

50:08.080 --> 50:10.360
whatever the hell they're going at.

50:10.360 --> 50:11.680
This is irresponsible.

50:11.680 --> 50:15.120
It's unnatural, and it's going to be destructive

50:15.120 --> 50:16.520
to all of human society.

50:16.520 --> 50:19.080
But then it's extremely surprising how humans adapt

50:19.760 --> 50:23.000
to the thing, and they know how to not kill each other.

50:23.000 --> 50:27.560
I mean, that ability to adapt is incredible.

50:27.560 --> 50:30.360
And to mimic that in the machine is really tricky.

50:31.320 --> 50:33.400
Now, that said, what Tesla's doing,

50:34.280 --> 50:36.440
I mean, I wouldn't have guessed how far machine learning

50:36.440 --> 50:38.520
can go on vision alone.

50:38.520 --> 50:39.880
It's really, really incredible.

50:39.880 --> 50:43.840
And people that are, at least from my perspective,

50:43.840 --> 50:48.280
people that are kind of, you know,

50:48.280 --> 50:52.960
critical of Elon and those efforts,

50:52.960 --> 50:54.320
I think don't give enough credit

50:54.320 --> 50:55.800
of how much progress we've made,

50:55.800 --> 50:57.920
how much incredible progress has been made

50:57.920 --> 50:59.160
in that direction.

50:59.160 --> 51:01.440
I think most of the robotics community wouldn't have guessed

51:01.440 --> 51:03.680
how much you can do on vision alone.

51:03.680 --> 51:04.880
It's kind of incredible.

51:05.960 --> 51:09.400
Because we would be, I think it's that approach,

51:09.400 --> 51:11.840
which is relatively unique,

51:12.840 --> 51:16.680
has challenged the other competitors to step up their game.

51:16.680 --> 51:19.360
So if you're using LiDAR, if you're using mapping,

51:21.720 --> 51:26.720
that challenges them to do better, to scale faster,

51:27.320 --> 51:30.520
and to use machine learning and computer vision as well

51:30.520 --> 51:32.880
to integrate both LiDAR and vision.

51:32.880 --> 51:35.640
So it's kind of incredible.

51:35.640 --> 51:40.160
And I'm not, I don't know if I even have a good intuition

51:40.160 --> 51:43.080
of how hard driving is anymore.

51:43.080 --> 51:45.480
Maybe it is possible to solve.

51:45.480 --> 51:48.400
So all the sunset, all the edge cases you mentioned.

51:48.400 --> 51:49.760
Yeah, the question is when.

51:49.760 --> 51:51.120
Yeah, I think it's not happening

51:51.120 --> 51:53.320
as quickly as people thought it would,

51:53.320 --> 51:55.120
because it is more complicated.

51:55.120 --> 51:59.360
But I wouldn't have, I agree with you.

51:59.360 --> 52:02.880
My current intuition is that we're gonna get there.

52:02.880 --> 52:04.200
I think we're gonna get there too.

52:04.200 --> 52:09.200
But I didn't before, I wasn't sure we're gonna get there

52:10.320 --> 52:13.080
without, like with current technology.

52:13.960 --> 52:18.960
So I was kind of, this is like with vision alone,

52:19.880 --> 52:23.200
my intuition was you're gonna have to solve

52:23.200 --> 52:25.600
like common sense reasoning.

52:25.600 --> 52:28.920
You're gonna have to solve some of the big problems

52:28.920 --> 52:31.920
in artificial intelligence, not just perception.

52:34.960 --> 52:35.800
Yeah.

52:35.800 --> 52:37.880
Like you have to have a deep understanding of the world,

52:37.880 --> 52:38.840
is what was my sense.

52:38.840 --> 52:41.320
But now I'm starting to like, well, this,

52:41.360 --> 52:42.760
I mean, I'm continuing to see surprise

52:42.760 --> 52:44.520
how well the thing works.

52:44.520 --> 52:47.440
Obviously, Elon and others, others have stopped,

52:47.440 --> 52:51.400
but Elon continues saying we're gonna solve it in a year.

52:51.400 --> 52:54.640
Yeah, that's the thing, bold prediction.

52:54.640 --> 52:57.280
Yeah, but everyone else used to be doing that,

52:57.280 --> 52:59.200
but they kind of like, all right.

52:59.200 --> 53:01.000
Yeah, maybe we'll.

53:01.000 --> 53:03.720
Maybe let's not promise we're gonna solve level four

53:03.720 --> 53:07.600
driving by 2020, let's chill on that.

53:07.600 --> 53:09.880
Well, people are still trying silently.

53:09.880 --> 53:14.640
I mean, the UK just committed 100 million pounds

53:14.640 --> 53:18.200
to research and development to speed up the process

53:18.200 --> 53:19.720
of getting autonomous vehicles on the road.

53:19.720 --> 53:23.800
Like everyone can see that it is solvable

53:23.800 --> 53:26.400
and it's going to happen and it's gonna change everything

53:26.400 --> 53:28.200
and they're still investing in it.

53:28.200 --> 53:33.200
And like Waymo Loki has driverless cars in Arizona.

53:33.680 --> 53:37.680
Like you can get, you know, there's like robots.

53:37.680 --> 53:39.640
It's weird, have you ever been in one?

53:39.640 --> 53:40.480
No.

53:40.480 --> 53:43.000
It's so weird, it's so awesome.

53:43.000 --> 53:47.600
Because the most awesome experience is the wheel turning

53:47.600 --> 53:51.600
and you're sitting in the back, it's like, I don't know.

53:51.600 --> 53:56.600
It's, it feels like you're a passenger with that friend

53:56.600 --> 53:59.040
who's a little crazy of a driver.

53:59.040 --> 54:01.480
It feels like, you know,

54:01.920 --> 54:05.080
it feels like, shit, I don't know.

54:05.080 --> 54:06.440
Are you ready to drive, bro?

54:06.440 --> 54:07.800
You know, that kind of feeling.

54:07.800 --> 54:12.800
But then you kind of, that experience, that nervousness

54:14.640 --> 54:18.080
and the excitement of trusting another being

54:18.080 --> 54:20.840
and in this case it's a machine is really interesting.

54:22.640 --> 54:26.000
Just even introspecting your own feelings about the thing.

54:26.000 --> 54:26.840
Yeah.

54:27.800 --> 54:32.800
They're not doing anything in terms of making you feel better

54:36.200 --> 54:37.640
like at least Waymo.

54:37.640 --> 54:40.320
I think they went with the approach of like,

54:40.320 --> 54:42.960
let's not try to put eyes on the thing.

54:42.960 --> 54:46.000
Let's, it's a wheel, we know what that looks like.

54:46.000 --> 54:47.120
It's just a car.

54:47.120 --> 54:48.720
It's a car, get in the back.

54:48.720 --> 54:50.240
Let's not like discuss this at all.

54:50.240 --> 54:53.040
Let's not discuss the fact that this is a robot

54:53.040 --> 54:55.000
driving you and you're in the back.

54:55.000 --> 54:57.520
And if the robot wants to start driving 80 miles an hour

54:57.520 --> 55:00.680
and run off of a bridge, you have no recourse.

55:00.680 --> 55:02.160
Let's not discuss this.

55:02.160 --> 55:03.360
You're just getting in the back.

55:03.360 --> 55:06.960
There's no discussion about like how shit can go wrong.

55:06.960 --> 55:08.240
There's no eyes, there's nothing.

55:08.240 --> 55:12.120
There's like a map showing what the car can see.

55:13.160 --> 55:15.160
Like, you know, what happens if it's like

55:15.160 --> 55:17.960
a Hal 9000 situation?

55:17.960 --> 55:21.400
Like, I'm sorry, I can't,

55:21.400 --> 55:24.440
you have a button you can like call customer service.

55:24.600 --> 55:27.600
Oh God, then you get put on hold for two hours.

55:27.600 --> 55:28.440
Probably.

55:29.800 --> 55:31.800
But you know, currently what they're doing,

55:31.800 --> 55:34.720
which I think is understandable,

55:34.720 --> 55:37.000
but you know, the car just can pull over

55:37.000 --> 55:39.720
and stop and wait for help to arrive.

55:39.720 --> 55:40.960
And then a driver will come

55:40.960 --> 55:43.200
and then they'll actually drive the car for you.

55:43.200 --> 55:44.440
But that's like, you know,

55:44.440 --> 55:47.640
what if you're late for a meeting

55:47.640 --> 55:48.840
or all that kind of stuff?

55:48.840 --> 55:51.160
Or like the more dystopian, isn't that the fifth element

55:51.160 --> 55:53.280
where, is Will Smith in that movie?

55:53.280 --> 55:54.200
Who's in that movie?

55:55.040 --> 55:55.880
No, Bruce Willis.

55:55.880 --> 55:56.720
Bruce Willis.

55:56.720 --> 55:58.720
Oh yeah, and he gets into like a robotic cab

55:58.720 --> 56:00.160
or car or something.

56:00.160 --> 56:02.240
And then because he's violated a traffic rule,

56:02.240 --> 56:03.960
it locks him in.

56:03.960 --> 56:05.520
And he has to wait for the cops to come

56:05.520 --> 56:06.400
and he can't get out.

56:06.400 --> 56:09.640
So like, we're gonna see stuff like that maybe.

56:09.640 --> 56:13.720
Well, that's, I believe that the companies

56:13.720 --> 56:18.720
that have robots, the only ones that will succeed

56:19.280 --> 56:21.880
are the ones that don't do that.

56:21.880 --> 56:25.120
Meaning they respect privacy.

56:25.120 --> 56:26.360
You think so?

56:26.360 --> 56:28.840
Yeah, because people,

56:28.840 --> 56:31.840
because they're gonna have to earn people's trust.

56:31.840 --> 56:34.440
Yeah, but like Amazon works with law enforcement

56:34.440 --> 56:36.720
and gives them the data from the ring cameras.

56:36.720 --> 56:38.640
So why should it?

56:38.640 --> 56:40.680
Yeah, oh yeah.

56:40.680 --> 56:42.800
Do you have a ring camera?

56:42.800 --> 56:43.640
No.

56:43.640 --> 56:44.480
Okay.

56:44.480 --> 56:48.200
No, no, but basically any security camera, right?

56:48.200 --> 56:51.440
I have a Google's, whatever they have.

56:51.480 --> 56:54.080
We have one that's not the data.

56:54.080 --> 56:56.080
We store the data on a local server

56:56.080 --> 56:58.720
because we don't want it to go to law enforcement

56:58.720 --> 57:00.280
because all the companies are doing it.

57:00.280 --> 57:01.120
They're doing it.

57:01.120 --> 57:02.120
I bet Apple wouldn't.

57:03.040 --> 57:03.880
Yeah.

57:03.880 --> 57:04.960
Apple's the only company I trust

57:04.960 --> 57:06.560
and I don't know how much longer.

57:08.280 --> 57:09.120
I don't know.

57:11.240 --> 57:13.840
Maybe that's true for cameras,

57:13.840 --> 57:17.360
but with robots, people are just not gonna let a robot

57:17.400 --> 57:19.520
go outside of their home

57:19.520 --> 57:22.960
where like one time where somebody gets arrested

57:22.960 --> 57:24.920
because of something a robot sees,

57:24.920 --> 57:27.960
that's gonna destroy a company.

57:27.960 --> 57:29.280
You don't think people are gonna be like,

57:29.280 --> 57:30.280
well, that wouldn't happen to me.

57:30.280 --> 57:31.840
That happened to a bad person.

57:33.320 --> 57:35.440
And I think they would.

57:35.440 --> 57:36.280
Yeah.

57:36.280 --> 57:38.800
Because in the modern world, people are get,

57:38.800 --> 57:39.880
have you seen Twitter?

57:39.880 --> 57:43.640
They get extremely paranoid about any kind of surveillance.

57:43.640 --> 57:45.240
But the thing that I've had to learn

57:45.240 --> 57:47.520
is that Twitter is not the modern world.

57:47.520 --> 57:50.480
Like when I go, you know,

57:50.480 --> 57:53.160
inland to visit my relatives,

57:53.160 --> 57:54.320
like they don't,

57:54.320 --> 57:56.680
that's a different discourse that's happening.

57:56.680 --> 57:58.880
I think like the whole tech criticism world.

57:58.880 --> 57:59.720
Yeah.

57:59.720 --> 58:00.760
It's loud in our ears

58:00.760 --> 58:02.160
because we're in those circles.

58:02.160 --> 58:05.760
You think you can be a company that does social robotics

58:05.760 --> 58:07.080
and not win over Twitter?

58:07.960 --> 58:09.480
That's a good question.

58:09.480 --> 58:12.480
I feel like the early adopters are all on Twitter

58:13.360 --> 58:15.600
and it feels like you have to win them over.

58:15.600 --> 58:19.120
Feels like nowadays you'd have to win over TikTok, honestly.

58:19.120 --> 58:19.960
I don't.

58:19.960 --> 58:21.320
TikTok, is that a website?

58:23.960 --> 58:25.040
I need to check it out.

58:28.040 --> 58:29.160
Yeah, and that's an interesting one

58:29.160 --> 58:31.600
because China is behind that one.

58:31.600 --> 58:32.440
Exactly.

58:34.440 --> 58:35.840
So if it's compelling enough,

58:35.840 --> 58:40.640
maybe people would be able to give up privacy

58:40.640 --> 58:41.800
and that kind of stuff.

58:43.120 --> 58:43.960
That's really scary.

58:43.960 --> 58:46.880
I mean, I'm worried about it.

58:46.880 --> 58:47.720
I'm worried about it.

58:47.720 --> 58:51.480
And there've been some developments recently

58:51.480 --> 58:53.040
that are like super exciting,

58:53.040 --> 58:55.480
like the large language learning models.

58:55.480 --> 58:59.760
Like, wow, I did not anticipate those improving so quickly

58:59.760 --> 59:03.680
and those are gonna change everything.

59:03.680 --> 59:06.400
And one of the things that I'm trying to be cynical about

59:06.400 --> 59:09.240
is that I think they're gonna have a big impact

59:09.240 --> 59:10.800
on privacy and data security

59:10.800 --> 59:13.960
and manipulating consumers and manipulating people

59:13.960 --> 59:15.600
because suddenly you'll have these agents

59:15.600 --> 59:18.120
that people will talk to you and they won't care

59:18.120 --> 59:21.080
or won't know, at least on a conscious level,

59:21.080 --> 59:22.640
that it's recording the conversations.

59:22.640 --> 59:24.840
So kind of like we were talking about before.

59:26.120 --> 59:29.000
And at the same time, the technology is so freaking exciting

59:29.000 --> 59:31.000
that it's gonna get adopted.

59:31.000 --> 59:32.440
Well, it's not even just the collection of data

59:32.440 --> 59:36.560
but the ability to manipulate at scale.

59:37.560 --> 59:40.960
So what do you think about the AI,

59:40.960 --> 59:45.360
the engineer from Google

59:45.360 --> 59:48.880
that thought Lambda is sentient?

59:48.880 --> 59:51.120
You had actually a really good post from somebody else.

59:51.120 --> 59:52.280
I forgot her name.

59:52.280 --> 59:53.120
It's brilliant.

59:53.120 --> 59:54.840
I can't believe I didn't know about her.

59:54.840 --> 59:55.800
Thanks to you. Janelle Shane?

59:55.800 --> 59:57.440
Yeah, from Weird AI.

59:57.440 --> 59:58.520
Oh yeah, I love her book.

59:58.520 --> 59:59.760
Oh, she's great.

59:59.760 --> 01:00:01.640
I left a note for myself to reach out to her.

01:00:01.640 --> 01:00:02.720
She's amazing.

01:00:02.720 --> 01:00:04.240
She's hilarious and brilliant

01:00:04.360 --> 01:00:07.840
and just a great summarizer of the state of AI.

01:00:07.840 --> 01:00:11.040
But she has, I think that was from her

01:00:11.040 --> 01:00:16.040
where I was looking at AI explaining that it's a squirrel.

01:00:18.320 --> 01:00:22.880
Oh yeah, because the transcripts that the engineer released,

01:00:22.880 --> 01:00:25.280
Lambda kind of talks about the experience

01:00:25.280 --> 01:00:30.280
of human-like feelings and I think even consciousness.

01:00:31.400 --> 01:00:33.760
And so she was like, oh cool, that's impressive.

01:00:34.280 --> 01:00:36.400
And I wonder if an AI can also describe the experience

01:00:36.400 --> 01:00:37.240
of being a squirrel.

01:00:37.240 --> 01:00:39.720
And so she interviewed, I think she did GPT-3

01:00:40.600 --> 01:00:42.560
about the experience of being a squirrel.

01:00:42.560 --> 01:00:45.320
And then she did a bunch of other ones too.

01:00:45.320 --> 01:00:46.960
What's it like being a flock of crows?

01:00:46.960 --> 01:00:49.320
What's it like being an algorithm that powers a Roomba?

01:00:49.320 --> 01:00:53.240
And you can have a conversation about any of those things

01:00:53.240 --> 01:00:54.080
and they're very convincing.

01:00:54.080 --> 01:00:55.440
It's pretty convincing, yeah.

01:00:55.440 --> 01:00:58.960
Even GPT-3, which is not state of the art.

01:00:58.960 --> 01:01:00.880
It's convincing of being a squirrel.

01:01:00.920 --> 01:01:04.280
It's like, what it's like, you should check it out

01:01:04.280 --> 01:01:06.440
because it really is, it's like, yeah,

01:01:06.440 --> 01:01:09.320
that probably is what a squirrel would talk about.

01:01:09.320 --> 01:01:10.160
It would say.

01:01:11.720 --> 01:01:12.560
Are you excited?

01:01:12.560 --> 01:01:13.800
Like, what's it like being a squirrel?

01:01:13.800 --> 01:01:14.640
It's fun.

01:01:14.640 --> 01:01:16.800
I get to eat nuts and run around all day.

01:01:18.520 --> 01:01:20.600
Like, how do you think people will feel

01:01:20.600 --> 01:01:23.000
like when you tell them that you're a squirrel?

01:01:24.320 --> 01:01:25.920
Or like, I forget what it was.

01:01:25.920 --> 01:01:28.360
A lot of people might be scared to find out

01:01:28.360 --> 01:01:29.960
that you're a squirrel or something like this.

01:01:30.120 --> 01:01:33.360
And then the system answers pretty well.

01:01:33.360 --> 01:01:38.040
Like, yeah, I hope they'll, like, what do you think

01:01:38.040 --> 01:01:40.000
when they find out you're a squirrel?

01:01:42.000 --> 01:01:45.400
I hope they'll see how fun it is to be a squirrel.

01:01:45.400 --> 01:01:48.040
What do you say to people who don't believe you're a squirrel?

01:01:48.040 --> 01:01:49.880
I say, come see for yourselves.

01:01:49.880 --> 01:01:50.840
I am a squirrel.

01:01:50.840 --> 01:01:52.000
That's great.

01:01:52.000 --> 01:01:53.280
Well, I think it's really great

01:01:53.280 --> 01:01:57.320
because the two things to note about it are,

01:01:57.320 --> 01:01:58.720
first of all, just because the machine

01:01:58.720 --> 01:02:00.560
is describing an experience doesn't mean

01:02:00.560 --> 01:02:02.440
it actually has that experience.

01:02:02.440 --> 01:02:05.320
But then secondly, these things are getting so advanced

01:02:05.320 --> 01:02:08.840
and so convincing at describing these things

01:02:08.840 --> 01:02:10.640
and talking to people.

01:02:10.640 --> 01:02:15.320
That's, I mean, just the implications for health,

01:02:15.320 --> 01:02:19.640
education, communication, entertainment, gaming.

01:02:19.640 --> 01:02:23.600
Like, all of the applications, it's mind boggling

01:02:23.600 --> 01:02:25.480
what we're gonna be able to do with this.

01:02:25.480 --> 01:02:28.440
And that my kids are not gonna remember a time

01:02:29.160 --> 01:02:32.680
before they could have conversations with artificial agents.

01:02:32.680 --> 01:02:34.360
Do you think they would?

01:02:34.360 --> 01:02:38.680
Because to me, this is, the focus in the AI community

01:02:38.680 --> 01:02:42.600
has been, well, this engineer surely is hallucinating.

01:02:42.600 --> 01:02:44.880
The thing is not sentient.

01:02:44.880 --> 01:02:49.640
But to me, I, first of all, it doesn't matter

01:02:49.640 --> 01:02:52.400
if he is or not, this is coming.

01:02:52.400 --> 01:02:53.240
Yeah.

01:02:53.240 --> 01:02:55.000
Where a large number of people would believe

01:02:55.000 --> 01:02:58.240
a system is sentient, including engineers

01:02:59.080 --> 01:02:59.920
and companies.

01:02:59.920 --> 01:03:00.760
Yeah.

01:03:00.760 --> 01:03:02.440
So in that sense, you start to think about a world

01:03:02.440 --> 01:03:06.400
where your kids aren't just used to having a conversation

01:03:06.400 --> 01:03:08.960
with the bot, but used to believing,

01:03:08.960 --> 01:03:12.320
kind of having an implied belief

01:03:12.320 --> 01:03:14.440
that the thing is sentient.

01:03:14.440 --> 01:03:15.800
Yeah, I think that's true.

01:03:15.800 --> 01:03:18.720
And I think that one of the things that bothered me

01:03:18.720 --> 01:03:20.680
about all of the coverage in the tech press

01:03:20.680 --> 01:03:23.760
about this incident, obviously I don't believe

01:03:23.760 --> 01:03:25.040
the system is sentient.

01:03:25.040 --> 01:03:28.080
Like, I think that it can convincingly describe that it is.

01:03:28.920 --> 01:03:31.960
But I don't think it's doing what he thought it was doing

01:03:31.960 --> 01:03:33.880
and actually experiencing feelings.

01:03:33.880 --> 01:03:38.880
But a lot of the tech press was about how he was wrong

01:03:39.040 --> 01:03:41.440
and depicting him as kind of naive.

01:03:41.440 --> 01:03:43.160
And it's not naive.

01:03:43.160 --> 01:03:45.800
Like, there's so much research in my field

01:03:45.800 --> 01:03:49.120
showing that people do this, even experts.

01:03:49.120 --> 01:03:51.440
They might be very clinical when they're doing

01:03:51.440 --> 01:03:53.320
human-robot interaction experiments

01:03:53.320 --> 01:03:55.120
with a robot that they've built.

01:03:55.120 --> 01:03:56.440
And then you bring in a different robot

01:03:56.440 --> 01:03:58.120
and look at it, it's having fun, it's doing this.

01:03:58.120 --> 01:04:00.480
Like, that happens in our lab all the time.

01:04:01.400 --> 01:04:03.560
We are all this guy.

01:04:03.560 --> 01:04:06.480
And it's gonna be huge.

01:04:06.480 --> 01:04:10.600
So I think that the goal is not to discourage

01:04:10.600 --> 01:04:13.760
this kind of belief or design systems

01:04:13.760 --> 01:04:15.440
that people won't think are sentient.

01:04:15.440 --> 01:04:16.800
I don't think that's possible.

01:04:16.800 --> 01:04:18.720
I think you're right, this is coming.

01:04:18.720 --> 01:04:21.280
It's something that we have to acknowledge

01:04:21.280 --> 01:04:25.120
and even embrace and be very aware of.

01:04:25.120 --> 01:04:28.320
So one of the really interesting perspectives

01:04:28.320 --> 01:04:32.400
that your book takes on a system like this

01:04:32.400 --> 01:04:35.400
is to see them, not to compare a system like this

01:04:35.400 --> 01:04:37.960
to humans, but to compare it to animals,

01:04:37.960 --> 01:04:39.880
of how we see animals.

01:04:39.880 --> 01:04:42.400
Can you kind of try to, again, sneak up,

01:04:42.400 --> 01:04:45.040
try to explain why this analogy is better

01:04:45.040 --> 01:04:48.400
than the human analogy, the analogy of robots as animals?

01:04:48.400 --> 01:04:52.040
Yeah, and it gets trickier with the language stuff,

01:04:52.040 --> 01:04:53.640
but we'll get into that too.

01:04:54.600 --> 01:04:59.480
I think that animals are a really great thought experiment

01:04:59.480 --> 01:05:00.960
when we're thinking about AI and robotics,

01:05:00.960 --> 01:05:03.880
because again, this comparing them to humans

01:05:03.880 --> 01:05:05.560
that leads us down the wrong path,

01:05:05.560 --> 01:05:07.040
both because it's not accurate,

01:05:07.040 --> 01:05:10.720
but also I think for the future, we don't want that.

01:05:10.720 --> 01:05:12.480
We want something that's a supplement.

01:05:12.480 --> 01:05:14.920
But I think animals, because we've used them

01:05:14.920 --> 01:05:16.920
throughout history for so many different things,

01:05:16.920 --> 01:05:19.640
we domesticated them not because they do what we do,

01:05:19.640 --> 01:05:22.920
but because what they do is different and that's useful.

01:05:23.120 --> 01:05:27.120
And it's just like, whether we're talking about companionship,

01:05:27.120 --> 01:05:29.720
whether we're talking about work integration,

01:05:29.720 --> 01:05:32.040
whether we're talking about responsibility for harm,

01:05:32.040 --> 01:05:35.400
there's just so many things we can draw on in that history

01:05:35.400 --> 01:05:38.200
from these entities that can sense, think,

01:05:38.200 --> 01:05:40.520
make autonomous decisions and learn

01:05:40.520 --> 01:05:43.320
that are applicable to how we should be thinking

01:05:43.320 --> 01:05:44.360
about robots and AI.

01:05:44.360 --> 01:05:46.760
And the point of the book is not that they're the same thing,

01:05:46.760 --> 01:05:49.040
that animals and robots are the same.

01:05:49.040 --> 01:05:50.840
Obviously, there are tons of differences there.

01:05:50.880 --> 01:05:54.800
Like, you can't have a conversation with a squirrel, right?

01:05:54.800 --> 01:05:56.800
But the point is that-

01:05:56.800 --> 01:05:57.800
I do it all the time.

01:05:57.800 --> 01:05:58.720
Oh, really?

01:05:58.720 --> 01:06:00.520
By the way, squirrels are the cutest.

01:06:00.520 --> 01:06:02.520
I project so much on squirrels.

01:06:02.520 --> 01:06:04.360
I wonder what their inner life is.

01:06:05.600 --> 01:06:09.240
I suspect they're much bigger assholes than we imagine.

01:06:09.240 --> 01:06:10.080
Really?

01:06:10.080 --> 01:06:12.280
Like, if it was a giant squirrel,

01:06:12.280 --> 01:06:13.960
it would fuck you over so fast.

01:06:13.960 --> 01:06:16.440
If you had the chance, it would take everything you own.

01:06:16.440 --> 01:06:18.160
It would eat all your stuff,

01:06:18.160 --> 01:06:20.640
but because it's small and the furry tail,

01:06:21.480 --> 01:06:25.240
the furry tail is a weapon

01:06:25.240 --> 01:06:28.280
against human consciousness and cognition.

01:06:28.280 --> 01:06:29.160
It wins us over.

01:06:29.160 --> 01:06:30.640
That's what cats do, too.

01:06:30.640 --> 01:06:33.600
Cats outcompete the squirrels.

01:06:33.600 --> 01:06:35.040
And dogs, like-

01:06:35.040 --> 01:06:35.880
Yeah.

01:06:35.880 --> 01:06:37.040
No, dogs have love.

01:06:37.040 --> 01:06:39.520
Cats have no soul.

01:06:39.520 --> 01:06:40.360
They-

01:06:40.360 --> 01:06:41.200
No, I'm just kidding.

01:06:41.200 --> 01:06:43.680
People get so angry when I talk shit about cats.

01:06:43.680 --> 01:06:44.680
I love cats.

01:06:44.680 --> 01:06:48.000
Anyway, so yeah, you're describing

01:06:48.000 --> 01:06:49.200
all the different kinds of animals.

01:06:49.200 --> 01:06:50.600
They get domesticated,

01:06:51.640 --> 01:06:54.160
and it's a really interesting idea

01:06:54.160 --> 01:06:55.640
that it's not just sort of pets.

01:06:55.640 --> 01:06:57.560
There's all kinds of domestication going on.

01:06:57.560 --> 01:07:00.360
They all have all kinds of uses.

01:07:00.360 --> 01:07:01.200
Yes.

01:07:01.200 --> 01:07:05.920
Like the ox that you propose might be,

01:07:05.920 --> 01:07:06.760
at least historically,

01:07:06.760 --> 01:07:09.800
one of the most useful domesticated animals.

01:07:09.800 --> 01:07:12.920
It was a game changer because it revolutionized

01:07:12.920 --> 01:07:15.480
what people could do economically, et cetera.

01:07:15.480 --> 01:07:17.720
So, I mean, just like robots,

01:07:18.280 --> 01:07:20.800
they're gonna change things economically.

01:07:20.800 --> 01:07:22.360
They're gonna change landscapes.

01:07:22.360 --> 01:07:24.320
Like cities might even get rebuilt

01:07:24.320 --> 01:07:28.680
around autonomous vehicles or drones or delivery robots.

01:07:28.680 --> 01:07:30.200
I think just the same ways

01:07:30.200 --> 01:07:33.520
that animals have really shifted society,

01:07:33.520 --> 01:07:35.040
and society has adapted also

01:07:35.040 --> 01:07:38.760
to socially accepting animals as pets.

01:07:38.760 --> 01:07:40.880
I think we're gonna see very similar things with robots.

01:07:40.880 --> 01:07:42.000
So, I think it's a useful analogy.

01:07:42.000 --> 01:07:42.840
It's not a perfect one,

01:07:42.840 --> 01:07:45.920
but I think it helps us get away from this idea

01:07:45.920 --> 01:07:49.160
that robots can, should, or will replace people.

01:07:49.160 --> 01:07:50.200
If you remember,

01:07:50.200 --> 01:07:52.480
what are some interesting uses of animals?

01:07:52.480 --> 01:07:54.000
Ferrets, for example.

01:07:54.000 --> 01:07:56.480
Oh, yeah, the ferrets, they still do this.

01:07:56.480 --> 01:07:59.440
They use ferrets to go into narrow spaces

01:07:59.440 --> 01:08:01.480
that people can't go into, like a pipe,

01:08:01.480 --> 01:08:03.520
or they'll use them to run electrical wire.

01:08:03.520 --> 01:08:05.800
I think they did that for Princess Di's wedding.

01:08:05.800 --> 01:08:09.960
There's so many weird ways that we've used animals

01:08:09.960 --> 01:08:13.640
and still use animals for things that robots can't do,

01:08:13.640 --> 01:08:17.880
like the dolphins that they used in the military.

01:08:17.880 --> 01:08:21.840
I think Russia still has dolphins,

01:08:21.840 --> 01:08:24.400
and the US still has dolphins in their navies.

01:08:25.280 --> 01:08:26.680
What?

01:08:26.680 --> 01:08:30.960
Mine detection, looking for lost underwater equipment,

01:08:30.960 --> 01:08:33.920
some rumors about using them for weaponry,

01:08:36.080 --> 01:08:39.080
which I think Russia's like, sure, believe that.

01:08:39.080 --> 01:08:41.400
And America's like, no, no, we don't do that.

01:08:41.400 --> 01:08:42.760
Who knows?

01:08:42.800 --> 01:08:45.160
But they started doing that in like the 60s, 70s.

01:08:45.160 --> 01:08:46.400
They started training these dolphins

01:08:46.400 --> 01:08:47.800
because they were like, oh,

01:08:47.800 --> 01:08:49.920
dolphins have this amazing echolocation system

01:08:49.920 --> 01:08:52.120
that we can't replicate with machines

01:08:52.120 --> 01:08:53.920
and they're trainable, so we're gonna use them

01:08:53.920 --> 01:08:56.160
for all the stuff that we can't do with machines

01:08:56.160 --> 01:08:57.040
or by ourselves.

01:08:57.040 --> 01:08:59.160
And they've tried to phase out the dolphins.

01:08:59.160 --> 01:09:02.640
I know the US has invested a lot of money

01:09:02.640 --> 01:09:05.840
in trying to make robots do the mine detection,

01:09:05.840 --> 01:09:07.520
but like you were saying,

01:09:07.520 --> 01:09:09.560
there are some things that the robots are good at,

01:09:09.560 --> 01:09:10.400
and there's some things

01:09:10.400 --> 01:09:12.320
that biological creatures are better at,

01:09:12.360 --> 01:09:13.800
they still have the dolphins.

01:09:13.800 --> 01:09:15.960
So there's also pigeons, of course.

01:09:15.960 --> 01:09:17.400
Oh yeah, pigeons.

01:09:17.400 --> 01:09:19.320
Oh my gosh, there's so many examples.

01:09:19.320 --> 01:09:23.840
The pigeons were the original hobby photography drone.

01:09:23.840 --> 01:09:26.000
They also carried mail for thousands of years,

01:09:26.000 --> 01:09:28.400
letting people communicate with each other in new ways.

01:09:28.400 --> 01:09:31.000
So the thing that I like about the animal analogy

01:09:31.000 --> 01:09:33.600
is they have all these physical abilities,

01:09:33.600 --> 01:09:38.520
but also sensing abilities that we don't have.

01:09:38.520 --> 01:09:41.280
And that's just so useful.

01:09:41.320 --> 01:09:43.000
And that's robots, right?

01:09:43.000 --> 01:09:44.600
Robots have physical abilities.

01:09:44.600 --> 01:09:46.960
They can help us lift things or do things

01:09:46.960 --> 01:09:48.480
that we're not physically capable of.

01:09:48.480 --> 01:09:50.600
They can also sense things.

01:09:50.600 --> 01:09:52.200
It's just, I just feel like,

01:09:52.200 --> 01:09:54.160
I still feel like it's a really good analogy.

01:09:54.160 --> 01:09:55.120
Yeah, it's really strong.

01:09:55.120 --> 01:09:58.720
And it works because people are familiar with it.

01:09:58.720 --> 01:10:00.520
What about companionship?

01:10:00.520 --> 01:10:04.720
And when we start to think about cats and dogs and pets

01:10:04.720 --> 01:10:06.840
that seem to serve no purpose whatsoever

01:10:06.840 --> 01:10:08.760
except the social connection.

01:10:08.800 --> 01:10:11.360
Yeah, I mean, it's kind of a newer thing.

01:10:12.400 --> 01:10:16.920
At least in the United States, like dogs used to have,

01:10:16.920 --> 01:10:18.120
like they used to have a purpose.

01:10:18.120 --> 01:10:21.560
They used to be guard dogs or they had some sort of function.

01:10:21.560 --> 01:10:25.320
And then at some point they became just part of the family.

01:10:25.320 --> 01:10:30.320
And it's so interesting how there's some animals

01:10:30.640 --> 01:10:33.720
that we've treated as workers,

01:10:33.720 --> 01:10:35.720
some that we've treated as objects,

01:10:35.720 --> 01:10:39.520
some that we eat and some that are parts of our families.

01:10:39.520 --> 01:10:41.600
And that's different across cultures.

01:10:41.600 --> 01:10:43.520
And I'm convinced that we're gonna see

01:10:43.520 --> 01:10:45.000
the same thing with robots,

01:10:46.120 --> 01:10:48.840
where people are gonna develop strong emotional connections

01:10:48.840 --> 01:10:50.400
to certain robots that they relate to,

01:10:50.400 --> 01:10:53.400
either culturally or personally, emotionally.

01:10:53.400 --> 01:10:54.720
And then there's gonna be other robots

01:10:54.720 --> 01:10:57.160
that we don't treat the same way.

01:10:58.240 --> 01:11:00.840
I wonder, does that have to do more with the culture

01:11:00.840 --> 01:11:02.840
and the people or the robot design

01:11:02.840 --> 01:11:04.760
as an interplay between the two?

01:11:04.760 --> 01:11:09.760
Like why did dogs and cats outcompete ox and,

01:11:11.800 --> 01:11:13.560
I don't know, what else?

01:11:13.560 --> 01:11:16.920
Like farm animals to really get inside the home

01:11:16.920 --> 01:11:18.760
and get inside our hearts.

01:11:18.760 --> 01:11:21.400
Yeah, I mean, people point to the fact

01:11:21.400 --> 01:11:24.280
that dogs are very genetically flexible

01:11:24.280 --> 01:11:28.600
and they can evolve much more quickly than other animals.

01:11:28.600 --> 01:11:32.840
And so they, evolutionary biologists think

01:11:32.840 --> 01:11:36.120
that dogs evolved to be more appealing to us.

01:11:36.120 --> 01:11:38.120
And then once we learned how to breed them,

01:11:38.120 --> 01:11:40.680
we started breeding them to be more appealing to us too,

01:11:40.680 --> 01:11:43.080
which is not something that we necessarily

01:11:43.080 --> 01:11:45.320
would be able to do with cows,

01:11:45.320 --> 01:11:47.920
although we've bred them to make more milk for us.

01:11:47.920 --> 01:11:50.720
So, but part of it is also culture.

01:11:50.720 --> 01:11:52.760
I mean, there are cultures where people eat dogs

01:11:52.760 --> 01:11:54.960
still today and then there's other cultures

01:11:54.960 --> 01:11:56.920
where we're like, oh, no, that's terrible.

01:11:56.920 --> 01:11:58.680
We would never do that.

01:11:58.680 --> 01:12:00.720
And so I think there's a lot of different elements

01:12:00.720 --> 01:12:01.800
that play in.

01:12:01.800 --> 01:12:03.200
I wonder if there's good,

01:12:03.200 --> 01:12:05.560
because I understand dogs, because they use their eyes,

01:12:05.560 --> 01:12:07.960
they're able to communicate affection,

01:12:07.960 --> 01:12:08.800
all those kinds of things.

01:12:08.800 --> 01:12:10.760
It's really interesting what dogs do.

01:12:10.760 --> 01:12:13.680
There's a whole conferences on dog consciousness

01:12:13.680 --> 01:12:16.520
and cognition and all that kind of stuff.

01:12:16.520 --> 01:12:19.320
Now cats is a mystery to me

01:12:19.320 --> 01:12:22.240
because they seem to not give a shit about the human.

01:12:22.240 --> 01:12:25.080
But they're warm and fluffy and cute.

01:12:25.080 --> 01:12:26.560
But they're also passive aggressive.

01:12:26.560 --> 01:12:28.720
So they're, at the same time,

01:12:29.680 --> 01:12:33.560
they're dismissive of a view in some sense.

01:12:33.560 --> 01:12:34.720
I think some people like that.

01:12:34.720 --> 01:12:36.840
Some people like that about people.

01:12:36.840 --> 01:12:40.440
Yeah, they want to push and pull over a relationship.

01:12:40.440 --> 01:12:43.400
They don't want loyalty or unconditional love.

01:12:43.400 --> 01:12:45.360
That means they haven't earned it.

01:12:45.360 --> 01:12:47.480
Yeah, yeah.

01:12:49.680 --> 01:12:51.680
And maybe that says a lot more about the people

01:12:51.680 --> 01:12:52.960
than it does about the animals.

01:12:52.960 --> 01:12:54.560
Oh yeah, we all need therapy.

01:12:54.560 --> 01:12:55.400
Yeah.

01:12:56.160 --> 01:12:59.600
So I'm judging harshly the people that have cats

01:12:59.600 --> 01:13:02.040
or the people that have dogs.

01:13:02.040 --> 01:13:03.880
Maybe the people that have dogs

01:13:04.880 --> 01:13:08.640
are desperate for attention and unconditional love

01:13:08.640 --> 01:13:11.640
and they're unable to sort of struggle

01:13:16.360 --> 01:13:19.880
to earn meaningful connections.

01:13:21.200 --> 01:13:22.040
I don't know.

01:13:22.040 --> 01:13:24.440
Maybe people are talking about you and your robot pets

01:13:24.440 --> 01:13:26.120
in the same way.

01:13:26.120 --> 01:13:26.960
Yeah, that's,

01:13:29.400 --> 01:13:30.360
it is kind of sad.

01:13:30.360 --> 01:13:31.960
There's just robots everywhere.

01:13:31.960 --> 01:13:34.320
But I mean, I'm joking about it being sad

01:13:34.320 --> 01:13:35.360
because I think it's kind of beautiful.

01:13:35.360 --> 01:13:39.200
I think robots are beautiful in the same way

01:13:39.200 --> 01:13:41.520
that pets are, even children,

01:13:41.520 --> 01:13:46.040
in that they capture some kind of magic of social robots.

01:13:47.400 --> 01:13:50.640
They have the capacity to have the same kind of magic

01:13:50.640 --> 01:13:51.560
of connection.

01:13:52.840 --> 01:13:53.880
I don't know what that is.

01:13:54.880 --> 01:13:57.440
When they're brought to life and they move around,

01:13:58.800 --> 01:14:02.800
the way they make me feel, I'm pretty convinced,

01:14:02.800 --> 01:14:07.800
is as you know, they will make billions of people feel.

01:14:09.480 --> 01:14:12.160
I don't think I'm like some weird robotics guy.

01:14:12.160 --> 01:14:13.320
I'm not.

01:14:13.320 --> 01:14:15.040
I mean, you are, but not in this way.

01:14:15.040 --> 01:14:15.880
Not in this way.

01:14:15.880 --> 01:14:20.040
I mean, I just, I can put on my normal human hat

01:14:20.040 --> 01:14:22.520
and just see that, oh, this is,

01:14:22.520 --> 01:14:25.920
there's a lot of possibility there of something cool,

01:14:25.920 --> 01:14:27.400
just like with dogs.

01:14:27.400 --> 01:14:28.240
What is it?

01:14:28.240 --> 01:14:30.400
Why are we so into dogs or cats?

01:14:32.320 --> 01:14:34.040
It's way different than us.

01:14:34.960 --> 01:14:35.800
It is.

01:14:35.800 --> 01:14:38.560
It's drooling all over the place with its tongue out.

01:14:38.560 --> 01:14:42.320
It's like a weird creature that used to be a wolf.

01:14:42.320 --> 01:14:43.600
Why are we into this thing?

01:14:43.600 --> 01:14:48.600
Well, dogs can either express or mimic a lot of emotions

01:14:49.280 --> 01:14:50.920
that we recognize.

01:14:51.680 --> 01:14:53.320
And I think that's a big thing.

01:14:53.320 --> 01:14:56.040
A lot of the magic of animals and robots

01:14:56.040 --> 01:14:59.160
is our own self-projection.

01:14:59.160 --> 01:15:03.440
And the easier it is for us to see ourselves in something

01:15:03.440 --> 01:15:07.000
and project human emotions or qualities or traits onto it,

01:15:07.000 --> 01:15:08.320
the more we'll relate to it.

01:15:08.320 --> 01:15:10.400
And then you also have the movement, of course.

01:15:10.400 --> 01:15:11.800
I think that's also really,

01:15:11.800 --> 01:15:14.240
that's why I'm so interested in physical robots,

01:15:14.240 --> 01:15:17.360
because that's, I think, the visceral magic of them.

01:15:17.360 --> 01:15:20.040
I think we're, I mean, there's research showing

01:15:20.200 --> 01:15:22.760
that we're probably biologically hardwired

01:15:22.760 --> 01:15:26.560
to respond to autonomous movement in our physical space

01:15:26.560 --> 01:15:28.640
because we've had to watch out for predators

01:15:28.640 --> 01:15:30.680
or whatever the reason is.

01:15:30.680 --> 01:15:34.880
And so animals and robots are very appealing to us

01:15:34.880 --> 01:15:37.280
as these autonomously moving things

01:15:37.280 --> 01:15:40.000
that we view as agents instead of objects.

01:15:40.000 --> 01:15:44.440
I love the moment, which I've been particularly working on,

01:15:44.440 --> 01:15:47.800
which is when a robot, like the cowboy hat,

01:15:47.800 --> 01:15:49.080
is doing its own thing,

01:15:49.080 --> 01:15:53.720
and then it recognizes you, I mean, the way a dog does.

01:15:53.720 --> 01:15:56.080
And it looks like this,

01:15:56.080 --> 01:15:59.720
and the moment of recognition,

01:15:59.720 --> 01:16:02.680
like you're walking, say you're walking in an airport

01:16:02.680 --> 01:16:07.680
or on a street, and there's just hundreds of strangers,

01:16:08.120 --> 01:16:09.360
but then you see somebody you know,

01:16:09.360 --> 01:16:14.360
and that like, where you wake up to that excitement

01:16:14.360 --> 01:16:16.280
of seeing somebody you know and saying hello

01:16:16.280 --> 01:16:19.040
and all that kind of stuff, that's a magical moment.

01:16:20.040 --> 01:16:22.160
I think, especially with the dog,

01:16:23.200 --> 01:16:27.440
it makes you feel noticed and heard and loved,

01:16:27.440 --> 01:16:30.600
like somebody looks at you and recognizes you

01:16:30.600 --> 01:16:33.520
that it matters that you exist.

01:16:33.520 --> 01:16:34.720
Yeah, you feel seen.

01:16:34.720 --> 01:16:36.800
Yeah, and that's a cool feeling.

01:16:36.800 --> 01:16:39.560
And I honestly think robots can give that feeling too.

01:16:39.560 --> 01:16:41.280
Oh yeah, totally.

01:16:41.280 --> 01:16:43.880
Currently Alexa, I mean, one of the downsides

01:16:43.880 --> 01:16:45.840
of these systems is they don't,

01:16:46.840 --> 01:16:48.560
they're servants.

01:16:48.560 --> 01:16:52.440
They like, part of the, you know,

01:16:52.440 --> 01:16:55.600
they're trying to maintain privacy, I suppose,

01:16:55.600 --> 01:17:00.600
but I don't feel seen with Alexa, right?

01:17:00.640 --> 01:17:01.920
I think that's gonna change.

01:17:01.920 --> 01:17:03.120
I think you're right.

01:17:03.120 --> 01:17:06.360
And I think that that's the game changing nature

01:17:06.360 --> 01:17:09.440
of things like these large language learning models.

01:17:09.440 --> 01:17:11.840
And the fact that these companies are investing

01:17:11.840 --> 01:17:15.800
in embodied versions that move around

01:17:16.760 --> 01:17:18.120
of Alexa, like Astro.

01:17:18.120 --> 01:17:22.160
Can I just say, yeah, Astro, I haven't, is that out?

01:17:22.160 --> 01:17:23.000
I mean, it's out.

01:17:23.000 --> 01:17:25.280
You can't just like buy one commercially yet,

01:17:25.280 --> 01:17:26.960
but you can apply for one.

01:17:26.960 --> 01:17:31.960
Yeah, my gut says that these companies

01:17:32.880 --> 01:17:37.880
don't have the guts to do the personalization.

01:17:38.680 --> 01:17:42.480
This goes to the, because it's edgy, it's dangerous.

01:17:42.480 --> 01:17:44.680
It's gonna make a lot of people very angry.

01:17:45.560 --> 01:17:49.320
Like in the way that, you know, just imagine, okay.

01:17:49.320 --> 01:17:50.360
All right.

01:17:50.360 --> 01:17:53.640
If you do the full landscape of human civilization,

01:17:53.640 --> 01:17:56.800
just visualize the number of people

01:17:56.800 --> 01:17:58.680
that are going through breakups right now.

01:17:58.680 --> 01:18:01.360
Just the amount of really passionate,

01:18:01.360 --> 01:18:03.360
just even if you just look at teenagers,

01:18:04.520 --> 01:18:07.000
the amount of deep heartbreak that's happening.

01:18:07.000 --> 01:18:09.320
And like, if you're going to have Alexa

01:18:09.320 --> 01:18:13.160
have more of a personal connection with the human,

01:18:13.160 --> 01:18:16.720
you're gonna have humans that like have existential crises.

01:18:16.720 --> 01:18:18.000
There's a lot of people that suffer

01:18:18.000 --> 01:18:20.040
from loneliness and depression.

01:18:20.040 --> 01:18:24.440
And like, you're now taking on the full responsibility

01:18:24.440 --> 01:18:27.680
of being a companion to the rollercoaster

01:18:27.680 --> 01:18:29.160
of the human condition.

01:18:29.160 --> 01:18:32.120
As a company, imagine PR and marketing people.

01:18:32.120 --> 01:18:33.240
They're gonna freak out.

01:18:33.240 --> 01:18:34.760
They don't have the guts.

01:18:34.760 --> 01:18:36.760
It's gonna have to come from somebody,

01:18:36.760 --> 01:18:39.120
from a new Apple, from those kinds of folks.

01:18:39.120 --> 01:18:41.000
Like it's a small startup.

01:18:41.000 --> 01:18:42.000
And it might.

01:18:42.000 --> 01:18:42.840
Yeah.

01:18:43.000 --> 01:18:43.840
It's coming.

01:18:43.840 --> 01:18:44.680
There's already virtual therapists.

01:18:44.680 --> 01:18:46.000
There's that replica app.

01:18:46.000 --> 01:18:46.840
I haven't tried it,

01:18:46.840 --> 01:18:49.200
but replica is like a virtual companion.

01:18:49.200 --> 01:18:51.160
Like it's coming.

01:18:51.160 --> 01:18:53.800
And if big companies don't do it, someone else will.

01:18:53.800 --> 01:18:54.640
Yeah.

01:18:54.640 --> 01:18:55.840
I think the next, the future,

01:18:55.840 --> 01:18:57.400
the next trillion dollar company

01:18:57.400 --> 01:18:59.240
will be those personalization.

01:18:59.240 --> 01:19:00.520
Cause if you think,

01:19:02.880 --> 01:19:06.400
if you think about all the AI we have around us,

01:19:06.400 --> 01:19:08.160
all the smart phones and so on,

01:19:08.160 --> 01:19:10.920
there's very minimal personalization.

01:19:10.960 --> 01:19:14.240
You don't think that's just because they weren't able?

01:19:14.240 --> 01:19:15.080
No.

01:19:15.080 --> 01:19:15.920
Really?

01:19:15.920 --> 01:19:17.440
I don't think they have the guts.

01:19:17.440 --> 01:19:19.440
I mean, it might be true, but I have to wonder.

01:19:19.440 --> 01:19:22.600
I mean, Google is clearly gonna do something

01:19:22.600 --> 01:19:23.800
with the length.

01:19:23.800 --> 01:19:24.640
I mean.

01:19:24.640 --> 01:19:25.880
They don't have the guts.

01:19:26.880 --> 01:19:28.840
Are you challenging them?

01:19:28.840 --> 01:19:30.320
Partially, but not really.

01:19:30.320 --> 01:19:32.200
Cause I know they're not gonna do it.

01:19:32.200 --> 01:19:33.040
I mean.

01:19:33.040 --> 01:19:33.880
They don't have to.

01:19:33.880 --> 01:19:35.440
It's bad for business in the short term.

01:19:35.440 --> 01:19:36.480
I'm gonna be honest.

01:19:36.480 --> 01:19:39.000
Like maybe it's not such a bad thing

01:19:39.000 --> 01:19:41.360
if they don't just like roll this out quickly

01:19:41.360 --> 01:19:45.680
because I do think there are huge issues.

01:19:45.680 --> 01:19:50.000
And there's not just issues with like the responsibility

01:19:50.000 --> 01:19:53.080
of like unforeseen effects on people,

01:19:53.080 --> 01:19:56.520
but what's the business model?

01:19:56.520 --> 01:19:59.520
And if you are using the business model

01:19:59.520 --> 01:20:01.920
that you've used in other domains,

01:20:01.920 --> 01:20:04.560
then you're gonna have to collect data from people,

01:20:04.560 --> 01:20:06.960
which you will anyway to personalize the thing.

01:20:06.960 --> 01:20:09.960
And you're gonna be somehow monetizing the data

01:20:09.960 --> 01:20:12.200
or you're gonna be doing some like ad model.

01:20:12.200 --> 01:20:15.000
It just, it seems like now we're suddenly getting

01:20:15.000 --> 01:20:18.040
into the realm of like severe consumer protection issues.

01:20:18.040 --> 01:20:22.120
And I'm really worried about that.

01:20:22.120 --> 01:20:25.680
I see massive potential for this technology to be used

01:20:25.680 --> 01:20:29.400
in a way that's not for the public good and not,

01:20:29.400 --> 01:20:32.600
I mean, that's in an individual user's interest maybe,

01:20:32.600 --> 01:20:35.000
but not in society's interest.

01:20:35.000 --> 01:20:40.000
Yeah, see, I think that kind of personalization should be

01:20:42.320 --> 01:20:45.940
like redefine how we treat data.

01:20:45.940 --> 01:20:48.360
I think you should own all the data

01:20:48.360 --> 01:20:50.320
your phone knows about you.

01:20:50.320 --> 01:20:53.720
Like, and be able to delete it with a single click

01:20:53.720 --> 01:20:55.280
and walk away.

01:20:55.280 --> 01:20:59.280
And that data cannot be monetized or used

01:20:59.280 --> 01:21:02.080
or shared anywhere without your permission.

01:21:02.080 --> 01:21:04.800
I think that's the only way people will trust you

01:21:05.800 --> 01:21:07.920
for you to use that data.

01:21:07.920 --> 01:21:10.040
But then how are companies gonna,

01:21:10.040 --> 01:21:12.280
I mean, a lot of these applications rely

01:21:12.280 --> 01:21:17.280
on massive troves of data to train the AI system.

01:21:18.240 --> 01:21:22.480
Right, so you have to opt in constantly

01:21:22.480 --> 01:21:25.560
and opt in not in some legal, I agree,

01:21:25.560 --> 01:21:27.280
but obvious like show exactly,

01:21:27.280 --> 01:21:32.280
like in the way I opt in to tell you a secret.

01:21:32.560 --> 01:21:37.560
Like, we understand that I have to choose,

01:21:38.320 --> 01:21:39.640
like, how well do I know you?

01:21:39.640 --> 01:21:42.520
And then I say, like, don't tell this to anyone.

01:21:44.520 --> 01:21:47.680
And then I have to judge how leaky that,

01:21:47.680 --> 01:21:50.000
like, how good you are at keeping secrets.

01:21:50.000 --> 01:21:52.960
In that same way, like, it's very transparent

01:21:52.960 --> 01:21:56.880
in which data you're allowed to use for which purposes.

01:21:56.880 --> 01:21:58.800
That's what people are saying is the solution.

01:21:58.800 --> 01:22:00.480
And I think that works to some extent,

01:22:00.480 --> 01:22:03.760
having transparency, having people consent.

01:22:03.760 --> 01:22:07.440
I think it breaks down at the point at which,

01:22:07.440 --> 01:22:08.880
we've seen this happen on social media too.

01:22:08.880 --> 01:22:10.760
Like, people are willingly giving up their data

01:22:10.760 --> 01:22:13.760
because they're getting a functionality from that.

01:22:13.760 --> 01:22:17.120
And then the harm that that causes is on a,

01:22:17.120 --> 01:22:20.480
like, maybe just someone else and not to them personally.

01:22:20.480 --> 01:22:21.320
So-

01:22:21.320 --> 01:22:22.920
I don't think people are given their data.

01:22:22.920 --> 01:22:24.160
They're not being asked.

01:22:24.960 --> 01:22:27.880
Like, it's not consensual.

01:22:27.880 --> 01:22:31.160
If you were like, tell me a secret about yourself

01:22:31.160 --> 01:22:33.040
and I'll give you a hundred dollars,

01:22:33.040 --> 01:22:34.160
I'd tell you a secret.

01:22:34.160 --> 01:22:36.360
No, not a hundred dollars.

01:22:36.360 --> 01:22:38.240
First of all, you wouldn't.

01:22:38.240 --> 01:22:40.520
You wouldn't trust, like, why are you giving me a hundred

01:22:40.520 --> 01:22:41.360
dollars?

01:22:41.360 --> 01:22:42.440
It's a bad example.

01:22:42.440 --> 01:22:47.440
But, like, I need, I would ask for your specific,

01:22:49.280 --> 01:22:54.120
like, fashion interest in order to give recommendations

01:22:54.120 --> 01:22:55.560
to you for shopping.

01:22:55.560 --> 01:22:57.160
And I'd be very clear for that.

01:22:57.160 --> 01:22:58.080
And then you can disable that.

01:22:58.080 --> 01:23:00.480
You can delete that.

01:23:00.480 --> 01:23:04.320
But then you can be, have a deep, meaningful,

01:23:04.320 --> 01:23:07.080
rich connection with the system about what you think

01:23:07.080 --> 01:23:09.720
you look fat in, what you look great in,

01:23:09.720 --> 01:23:14.320
what, like, the full history of all the things you've worn,

01:23:14.320 --> 01:23:17.800
whether you regret the Justin Bieber

01:23:17.800 --> 01:23:19.640
or enjoy the Justin Bieber shirt,

01:23:19.640 --> 01:23:23.800
all of that information that's mostly private to even you,

01:23:23.800 --> 01:23:26.640
not even your loved ones, that a system should have that.

01:23:26.640 --> 01:23:29.680
Because then a system, if you trust it,

01:23:29.680 --> 01:23:32.400
to keep control of that data that you own,

01:23:32.400 --> 01:23:33.280
you can walk away with,

01:23:33.280 --> 01:23:36.760
that system could tell you a damn good thing to wear.

01:23:36.760 --> 01:23:38.240
It could.

01:23:38.240 --> 01:23:41.240
And the harm that I'm concerned about is not that the system

01:23:41.240 --> 01:23:43.240
is gonna then suggest a dress for me

01:23:43.240 --> 01:23:45.120
that is based on my preferences.

01:23:45.120 --> 01:23:48.080
So I went to this conference once where I was talking

01:23:48.080 --> 01:23:49.880
to the people who do the analytics

01:23:49.880 --> 01:23:52.000
in, like, the big ad companies.

01:23:52.000 --> 01:23:55.040
And, like, literally a woman there was like,

01:23:55.080 --> 01:23:58.480
I can ask you three totally unrelated questions

01:23:58.480 --> 01:24:01.560
and tell you what menstrual product you use.

01:24:01.560 --> 01:24:04.240
And so what they do is they aggregate the data

01:24:04.240 --> 01:24:06.240
and they map out different personalities

01:24:06.240 --> 01:24:08.400
and different people and demographics.

01:24:08.400 --> 01:24:10.960
And then they have a lot of power and control

01:24:10.960 --> 01:24:13.080
to market to people.

01:24:13.080 --> 01:24:15.200
So, like, I might not be sharing my data

01:24:15.200 --> 01:24:17.360
with any of the systems because I'm like,

01:24:17.360 --> 01:24:19.360
I'm on Twitter, I know that this is bad.

01:24:21.040 --> 01:24:23.120
Other people might be sharing data

01:24:23.120 --> 01:24:25.200
that can be used against me.

01:24:25.200 --> 01:24:28.480
Like, it's, I think it's way more complex

01:24:28.480 --> 01:24:32.560
than just I share a piece of personal information

01:24:32.560 --> 01:24:33.960
and it gets used against me.

01:24:33.960 --> 01:24:36.800
I think that at a more systemic level,

01:24:36.800 --> 01:24:40.000
and then it's always, you know, vulnerable populations

01:24:40.000 --> 01:24:42.560
that are targeted by this, you know,

01:24:42.560 --> 01:24:45.000
low income people being targeted for scammy loans,

01:24:45.000 --> 01:24:48.600
or I don't know, like, I could get targeted,

01:24:48.600 --> 01:24:51.200
like someone, not me,

01:24:51.200 --> 01:24:53.760
because someone who doesn't have kids yet

01:24:53.760 --> 01:24:56.400
and is my age could get targeted for, like,

01:24:56.400 --> 01:24:57.480
freezing their eggs.

01:24:57.480 --> 01:24:59.720
And there's all these ways that you can manipulate people

01:24:59.720 --> 01:25:02.880
where it's not really clear that that came from

01:25:04.240 --> 01:25:06.760
that person's data.

01:25:06.760 --> 01:25:11.320
It came from all of us, all of us opting into this.

01:25:11.320 --> 01:25:15.200
But there's a bunch of sneaky decisions along the way

01:25:15.200 --> 01:25:18.040
that could be avoided if there's transparency.

01:25:18.040 --> 01:25:21.240
So that, so one of the ways that goes wrong

01:25:21.240 --> 01:25:24.640
if you share that data with too many ad networks,

01:25:24.640 --> 01:25:27.960
don't run your own ad network.

01:25:27.960 --> 01:25:29.200
Don't share with anybody.

01:25:30.240 --> 01:25:32.160
Okay, and that's something that you could regulate.

01:25:32.160 --> 01:25:35.080
You, that belongs to just you,

01:25:35.080 --> 01:25:38.360
and all the ways you allow the company to use it,

01:25:38.360 --> 01:25:40.280
the default is in no way at all.

01:25:41.440 --> 01:25:45.200
And you're consciously, constantly saying exactly

01:25:45.200 --> 01:25:46.640
how to use it.

01:25:46.680 --> 01:25:51.680
And also, it has to do with the recommender system itself

01:25:52.240 --> 01:25:55.240
from the company, which is freezing your eggs.

01:25:58.080 --> 01:26:00.120
If that doesn't make you happy,

01:26:00.120 --> 01:26:02.520
if that idea doesn't make you happy,

01:26:02.520 --> 01:26:04.320
then the system shouldn't recommend it

01:26:04.320 --> 01:26:07.000
and should be very good at learning.

01:26:07.840 --> 01:26:11.320
So not the kind of things that the category of people

01:26:11.320 --> 01:26:13.840
it thinks you belong to would do,

01:26:13.840 --> 01:26:17.680
but more you specifically, what makes you happy,

01:26:17.680 --> 01:26:19.360
what is helping you grow.

01:26:19.360 --> 01:26:21.640
But you're assuming that people's preferences

01:26:21.640 --> 01:26:24.160
and what makes them happy is static.

01:26:24.160 --> 01:26:25.720
Whereas when we were talking before

01:26:25.720 --> 01:26:28.360
about how a company like Apple

01:26:29.440 --> 01:26:31.800
can tell people what they want,

01:26:31.800 --> 01:26:33.760
and they will start to want it.

01:26:33.760 --> 01:26:36.360
That's the thing that I'm more concerned about.

01:26:36.360 --> 01:26:37.600
Yeah, that is a huge problem.

01:26:37.600 --> 01:26:39.680
It's not just listening to people,

01:26:39.680 --> 01:26:42.320
but manipulating them into wanting something.

01:26:42.320 --> 01:26:45.280
And that's like, we have a long history

01:26:45.280 --> 01:26:48.240
of using technology for that purpose.

01:26:48.240 --> 01:26:50.480
Like the persuasive design in casinos

01:26:50.480 --> 01:26:53.920
to get people to gamble more, or like, it's just,

01:26:57.320 --> 01:26:59.000
the other thing that I'm worried about

01:26:59.000 --> 01:27:02.480
is as we have more social technology,

01:27:02.480 --> 01:27:04.400
suddenly you have this on a new level.

01:27:04.400 --> 01:27:07.400
Like, if you look at the influencer marketing

01:27:07.400 --> 01:27:09.400
that happens online now.

01:27:09.400 --> 01:27:10.400
What's influencer marketing?

01:27:10.400 --> 01:27:11.560
So like on Instagram,

01:27:11.560 --> 01:27:15.880
there will be some person who has a bunch of followers.

01:27:15.880 --> 01:27:20.040
And then a brand will hire them to promote some product.

01:27:20.040 --> 01:27:21.440
And it's above board.

01:27:21.440 --> 01:27:24.760
They disclose, this is an ad that I'm promoting,

01:27:24.760 --> 01:27:26.600
but they have so many young followers

01:27:26.600 --> 01:27:29.000
who deeply admire and trust them.

01:27:29.000 --> 01:27:30.480
I mean, this must work for you too.

01:27:30.480 --> 01:27:32.280
Don't you have ads on the podcast?

01:27:32.280 --> 01:27:33.280
Like people trust you.

01:27:33.280 --> 01:27:37.360
Magic spoon cereal, low carb, yes.

01:27:37.360 --> 01:27:39.960
If you say that, I guarantee you some people will buy that

01:27:40.000 --> 01:27:42.880
just because even though they know that you're being paid,

01:27:42.880 --> 01:27:44.720
they trust you.

01:27:44.720 --> 01:27:45.560
Yeah.

01:27:45.560 --> 01:27:46.640
It's different with podcasts

01:27:46.640 --> 01:27:48.760
because with my particular situation,

01:27:48.760 --> 01:27:51.280
but it's true for a lot of podcasts, especially big ones,

01:27:51.280 --> 01:27:56.000
is I have 10 times more sponsors

01:27:56.000 --> 01:27:59.040
that want to be sponsors than I have.

01:28:00.080 --> 01:28:01.320
So you get to select the ones

01:28:01.320 --> 01:28:02.520
that you actually want to support.

01:28:02.520 --> 01:28:04.440
And so like you end up using it

01:28:04.440 --> 01:28:06.560
and then you're able to actually,

01:28:06.600 --> 01:28:08.720
like there's no incentive to like,

01:28:10.640 --> 01:28:12.360
show for anybody.

01:28:12.360 --> 01:28:13.200
Sure.

01:28:13.200 --> 01:28:16.680
And that's why it's fine when it's still human influencers.

01:28:16.680 --> 01:28:17.520
Right.

01:28:17.520 --> 01:28:19.120
Now, if you're a bot,

01:28:19.120 --> 01:28:21.600
you're not gonna discriminate.

01:28:22.640 --> 01:28:23.480
You're not gonna be like,

01:28:23.480 --> 01:28:27.000
oh well, I think this product is good for people.

01:28:27.000 --> 01:28:30.520
You think there'll be like bots essentially

01:28:30.520 --> 01:28:32.160
with millions of followers.

01:28:32.160 --> 01:28:33.560
There already are.

01:28:33.560 --> 01:28:36.880
There are virtual influencers in South Korea

01:28:36.880 --> 01:28:39.600
who show products and like,

01:28:39.600 --> 01:28:41.000
that's just the tip of the iceberg

01:28:41.000 --> 01:28:42.520
because that's still very primitive.

01:28:42.520 --> 01:28:44.320
Now with the new image generation

01:28:44.320 --> 01:28:46.480
and the language learning models.

01:28:46.480 --> 01:28:49.840
And like, so we're starting to do some research

01:28:49.840 --> 01:28:53.040
around kids and like young adults

01:28:53.040 --> 01:28:56.080
because a lot of the research

01:28:56.080 --> 01:28:58.240
on like what's okay to advertise to kids

01:28:58.240 --> 01:28:59.800
and what is too manipulative

01:28:59.800 --> 01:29:01.800
has to do with television ads.

01:29:01.800 --> 01:29:04.560
Back in the day where like a kid who's 12 understands,

01:29:04.560 --> 01:29:06.080
oh, that's an advertisement.

01:29:06.080 --> 01:29:07.560
I can distinguish that from entertainment.

01:29:07.560 --> 01:29:09.160
I know it's trying to sell me something.

01:29:09.160 --> 01:29:12.840
Now it's getting really, really murky with influencers.

01:29:12.840 --> 01:29:15.160
And then if you have like a bot

01:29:15.160 --> 01:29:18.040
that a kid has developed a relationship with,

01:29:18.040 --> 01:29:20.720
is it okay to market products through that or not?

01:29:20.720 --> 01:29:22.720
Like you're getting into all these consumer protection

01:29:22.720 --> 01:29:26.600
issues because you're developing a trusted relationship

01:29:26.600 --> 01:29:29.280
with a social entity,

01:29:29.320 --> 01:29:33.240
but it's and so now it's like personalized,

01:29:33.240 --> 01:29:37.080
it's scalable, it's automated

01:29:37.080 --> 01:29:42.080
and it can, so some of the research showing

01:29:42.640 --> 01:29:45.320
that kids are already very confused about like

01:29:45.320 --> 01:29:47.240
the incentives of the company versus

01:29:47.240 --> 01:29:48.520
what the robot is doing.

01:29:49.840 --> 01:29:54.840
Meaning they're not deeply understanding

01:29:54.960 --> 01:29:57.480
the incentives of the system.

01:29:57.480 --> 01:30:01.000
Well, yeah, so like kids who are old enough to understand

01:30:01.000 --> 01:30:02.600
this is a television advertisement

01:30:02.600 --> 01:30:04.080
is trying to advertise to me.

01:30:04.080 --> 01:30:05.400
I might still decide I want this product

01:30:05.400 --> 01:30:06.480
but they understand what's going on

01:30:06.480 --> 01:30:09.040
so there's some transparency there.

01:30:09.040 --> 01:30:14.040
That age child, so Daniella DiPaola, Anastasia Ostrovski

01:30:15.120 --> 01:30:18.400
and I advised on this project, they did this,

01:30:18.400 --> 01:30:21.240
they asked kids who had interacted with social robots

01:30:22.760 --> 01:30:27.360
whether they would like a policy that allows robots

01:30:28.320 --> 01:30:31.160
to market to people through casual conversation

01:30:31.160 --> 01:30:34.280
or whether they would prefer that it has to be transparent

01:30:34.280 --> 01:30:36.680
that it's like an ad coming from a company

01:30:36.680 --> 01:30:38.760
and the majority said they preferred

01:30:38.760 --> 01:30:41.560
the casual conversation and when asked why,

01:30:41.560 --> 01:30:43.320
there was a lot of confusion about, they were like,

01:30:43.320 --> 01:30:46.280
well, the robot knows me better than the company does

01:30:46.280 --> 01:30:49.760
so the robot's only gonna market things that I like

01:30:49.760 --> 01:30:52.440
and so they don't really, they're not connecting the fact

01:30:52.440 --> 01:30:54.880
that the robot is an agent of the company.

01:30:54.880 --> 01:30:57.000
They're viewing it as something separate

01:30:57.640 --> 01:31:00.160
and I think that even happens subconsciously with grownups

01:31:00.160 --> 01:31:02.200
when it comes to robots and artificial agents

01:31:02.200 --> 01:31:04.320
and it will, like this Blake guy at Google,

01:31:04.320 --> 01:31:07.840
started going on and on but his main concern

01:31:07.840 --> 01:31:10.800
was that Google owned this sentient agent

01:31:10.800 --> 01:31:12.600
and that it was being mistreated.

01:31:12.600 --> 01:31:14.000
His concern was not that the agent

01:31:14.000 --> 01:31:15.720
was gonna mistreat people

01:31:15.720 --> 01:31:19.160
so I think we're gonna see a lot of this.

01:31:19.160 --> 01:31:21.440
Yeah, but shitty companies will do that.

01:31:21.440 --> 01:31:25.640
I think ultimately that confusion should be alleviated

01:31:25.640 --> 01:31:28.920
by the robot should actually know you better

01:31:28.920 --> 01:31:32.360
and should not have any control from the company.

01:31:32.360 --> 01:31:34.760
But what's the business model for that?

01:31:34.760 --> 01:31:37.920
If you use the robot to buy,

01:31:37.920 --> 01:31:41.120
first of all, the robot should probably cost money.

01:31:41.120 --> 01:31:41.960
Should what?

01:31:41.960 --> 01:31:44.640
Cost money, like the way Windows operating system does.

01:31:44.640 --> 01:31:46.960
I see it more like an operating system

01:31:46.960 --> 01:31:51.960
than like this thing is your window,

01:31:52.200 --> 01:31:55.640
no pun intended, into the world.

01:31:55.640 --> 01:31:59.280
So it's helping you as like a personal assistant, right?

01:31:59.280 --> 01:32:01.720
And so that should cost money.

01:32:01.720 --> 01:32:06.000
You should, you know, whatever it is, 10 bucks, 20 bucks.

01:32:06.000 --> 01:32:06.840
Like that's the thing

01:32:06.840 --> 01:32:09.120
that makes your life significantly better.

01:32:09.120 --> 01:32:12.640
This idea that everything should be free is,

01:32:12.640 --> 01:32:14.720
like it should actually help educate you.

01:32:14.720 --> 01:32:16.480
You should talk shit about all the other companies

01:32:16.480 --> 01:32:18.120
that do stuff for free.

01:32:19.120 --> 01:32:22.960
But also, yeah, in terms of if you purchase stuff

01:32:22.960 --> 01:32:25.200
based on its recommendation, it gets money.

01:32:27.000 --> 01:32:30.720
So it's kind of ad driven, but it's not ads.

01:32:30.720 --> 01:32:31.560
It's like,

01:32:35.360 --> 01:32:37.400
it's not controlled.

01:32:37.400 --> 01:32:41.080
Like no external entities can control it

01:32:42.600 --> 01:32:45.400
to try to manipulate, to want a thing.

01:32:45.400 --> 01:32:46.320
That would be amazing.

01:32:46.320 --> 01:32:48.920
It's actually trying to discover what you want.

01:32:48.920 --> 01:32:52.680
So it's not allowed to have any influence,

01:32:52.680 --> 01:32:55.920
no promoted ad, no anything.

01:32:55.920 --> 01:32:58.640
So that's finding, I don't know,

01:32:58.640 --> 01:33:01.440
the thing that would actually make you happy.

01:33:01.440 --> 01:33:03.800
That's the only thing it cares about.

01:33:03.800 --> 01:33:08.240
I think companies like this can win out.

01:33:08.240 --> 01:33:09.640
Yes, I think eventually,

01:33:09.640 --> 01:33:14.360
once people understand the value of the robot,

01:33:15.320 --> 01:33:17.960
I think that robots would be valuable to people,

01:33:17.960 --> 01:33:20.720
even if they're not marketing something

01:33:20.720 --> 01:33:23.280
or helping with preferences or anything.

01:33:23.280 --> 01:33:26.360
Just a simple, the same thing as a pet,

01:33:26.360 --> 01:33:28.320
a dog that has no function

01:33:28.320 --> 01:33:29.840
other than being a member of your family.

01:33:29.840 --> 01:33:31.320
I think robots could really be that,

01:33:31.320 --> 01:33:32.840
and people would pay for that.

01:33:32.840 --> 01:33:35.800
I don't think the market realizes that yet.

01:33:35.800 --> 01:33:38.040
And so my concern is that companies

01:33:38.040 --> 01:33:40.840
are not gonna go in that direction, at least not yet,

01:33:40.840 --> 01:33:44.720
of making this contained thing that you buy.

01:33:44.720 --> 01:33:46.720
It seems almost old-fashioned, right,

01:33:46.720 --> 01:33:51.720
to have a disconnected object that you buy

01:33:51.720 --> 01:33:53.680
that you're not paying a subscription for.

01:33:53.680 --> 01:33:56.000
It's not controlled by one of the big corporations.

01:33:56.000 --> 01:33:57.360
But that's the old-fashioned things

01:33:57.360 --> 01:34:01.880
that people yearn for,

01:34:01.880 --> 01:34:05.720
because I think it's very popular now,

01:34:05.720 --> 01:34:09.000
and people understand the negative effects of social media,

01:34:09.040 --> 01:34:11.920
the negative effects of the data being used

01:34:11.920 --> 01:34:13.080
in all these kinds of ways.

01:34:13.080 --> 01:34:17.960
I think we're just waking up to the realization we tried,

01:34:17.960 --> 01:34:20.120
we're like baby deer finding our legs

01:34:20.120 --> 01:34:21.720
in this new world of social media,

01:34:21.720 --> 01:34:23.880
of ad-driven companies,

01:34:23.880 --> 01:34:27.120
and realizing, okay, this has to be done somehow different.

01:34:27.120 --> 01:34:31.480
I mean, one of the most popular notions,

01:34:31.480 --> 01:34:32.520
at least in the United States,

01:34:32.520 --> 01:34:36.520
is social media is evil and it's doing bad by us.

01:34:36.520 --> 01:34:39.760
It's not like it's totally tricked us

01:34:39.760 --> 01:34:42.560
into believing that it's good for us.

01:34:42.560 --> 01:34:44.280
I think everybody knows it's bad for us,

01:34:44.280 --> 01:34:47.440
and so there's a hunger for other ideas.

01:34:47.440 --> 01:34:49.320
All right, it's time for us to start that company.

01:34:49.320 --> 01:34:50.160
Yeah, I think so.

01:34:50.160 --> 01:34:51.000
Let's do it.

01:34:51.000 --> 01:34:51.840
I think let's go.

01:34:51.840 --> 01:34:54.120
Hopefully no one listens to this and steals the idea.

01:34:54.120 --> 01:34:55.720
There's no, see, that's the other thing.

01:34:55.720 --> 01:34:58.080
I think I'm a big person on,

01:35:00.040 --> 01:35:01.560
execution is what matters.

01:35:01.560 --> 01:35:03.080
I mean. Oh, yeah.

01:35:03.080 --> 01:35:04.640
It's like ideas are kind of true.

01:35:04.640 --> 01:35:06.160
Social robotics is a good example

01:35:06.160 --> 01:35:08.920
that there's been so many amazing companies

01:35:08.920 --> 01:35:11.120
that went out of business.

01:35:11.120 --> 01:35:13.800
I mean, to me, it's obvious, like it's obvious

01:35:13.800 --> 01:35:17.640
that there will be a robotics company

01:35:17.640 --> 01:35:22.240
that puts a social robot on the home of billions of homes.

01:35:22.240 --> 01:35:23.080
Yeah.

01:35:23.080 --> 01:35:24.720
And it'll be a companion.

01:35:24.720 --> 01:35:25.560
Okay, there you go.

01:35:25.560 --> 01:35:27.360
You can steal that idea.

01:35:27.360 --> 01:35:28.200
Do it.

01:35:28.200 --> 01:35:29.920
It's very tough. Okay, I have a question for you.

01:35:31.000 --> 01:35:33.720
What about Elon Musk's humanoid?

01:35:33.720 --> 01:35:35.280
Is he going to execute on that?

01:35:37.560 --> 01:35:38.760
There might be a lot to say.

01:35:38.760 --> 01:35:40.280
So for people who are not aware,

01:35:40.280 --> 01:35:43.760
there's an optimist, Tesla's optimist robot that's,

01:35:45.480 --> 01:35:48.120
I guess the stated reason for that robot

01:35:48.120 --> 01:35:50.000
is a humanoid robot in the factory

01:35:50.000 --> 01:35:52.560
that's able to automate some of the tasks

01:35:52.560 --> 01:35:54.280
that humans are currently doing.

01:35:54.280 --> 01:35:55.640
And the reason you want to do,

01:35:55.640 --> 01:35:57.120
it's the second reason you mentioned,

01:35:57.120 --> 01:35:59.880
the reason you want to do a humanoid robot

01:35:59.880 --> 01:36:02.520
is because the factory is built for the certain tasks

01:36:02.520 --> 01:36:05.600
that are designed for humans.

01:36:05.600 --> 01:36:08.560
So it's hard to automate with any other form factor

01:36:08.560 --> 01:36:09.840
than a humanoid.

01:36:09.840 --> 01:36:13.800
And then the other reason is because so much effort

01:36:13.800 --> 01:36:16.880
has been put into this giant data engine machine

01:36:16.880 --> 01:36:21.520
of perception that's inside Tesla autopilot.

01:36:21.520 --> 01:36:25.320
That's seemingly, at least the machine, if not the data,

01:36:25.320 --> 01:36:28.880
is transferable to the factory setting, to any setting.

01:36:28.920 --> 01:36:32.600
Yeah, he said it would do anything that's boring to us.

01:36:32.600 --> 01:36:34.400
Yeah, yeah.

01:36:34.400 --> 01:36:39.400
The interesting thing about that is there's no interest

01:36:41.480 --> 01:36:43.920
and no discussion about the social aspect.

01:36:45.280 --> 01:36:50.120
Like I talked to him on mic and off mic about it quite a bit.

01:36:50.120 --> 01:36:55.120
And there's not a discussion about like,

01:36:55.760 --> 01:37:00.760
to me it's obvious if a thing like that works at all,

01:37:01.400 --> 01:37:02.520
at all.

01:37:02.520 --> 01:37:06.640
In fact, it has to work really well in a factory.

01:37:06.640 --> 01:37:08.360
If it works kind of shitty,

01:37:08.360 --> 01:37:10.440
it's much more useful in the home.

01:37:10.440 --> 01:37:12.000
That's true.

01:37:12.000 --> 01:37:16.520
Cause we're much, I think being shitty at stuff

01:37:16.520 --> 01:37:20.680
is kind of what makes relationships great.

01:37:20.960 --> 01:37:25.400
Like you want to be flawed and be able to communicate

01:37:25.400 --> 01:37:28.680
your flaws and be unpredictable in certain ways.

01:37:28.680 --> 01:37:30.960
Like if you fell over every once in a while

01:37:30.960 --> 01:37:32.240
for no reason whatsoever,

01:37:32.240 --> 01:37:35.080
I think that's essential for like.

01:37:35.080 --> 01:37:36.320
It's charming.

01:37:36.320 --> 01:37:39.320
It's charming but also concerning and also like,

01:37:41.080 --> 01:37:42.360
like are you okay?

01:37:43.280 --> 01:37:45.280
I mean, it's both hilarious.

01:37:45.280 --> 01:37:47.600
Whenever somebody you love like falls down the stairs,

01:37:47.600 --> 01:37:50.080
it was both hilarious and concerning.

01:37:51.360 --> 01:37:54.240
It's some dance between the two.

01:37:54.240 --> 01:37:56.600
And I think that's essential for like,

01:37:56.600 --> 01:37:58.960
you almost want to engineer that in,

01:37:58.960 --> 01:38:02.080
except you don't have to cause of robotics

01:38:02.080 --> 01:38:04.480
in the physical space is really difficult.

01:38:04.480 --> 01:38:09.480
So I think I've learned to not discount

01:38:11.280 --> 01:38:13.920
the efforts that Elon does.

01:38:13.920 --> 01:38:16.840
There's a few things that are really interesting there.

01:38:16.840 --> 01:38:21.840
One, because he's taking it extremely seriously,

01:38:21.920 --> 01:38:24.600
what I like is the humanoid form,

01:38:24.600 --> 01:38:25.760
the cost of building a robot.

01:38:25.760 --> 01:38:28.440
I talked to Jim Keller offline about this a lot.

01:38:28.440 --> 01:38:31.440
And currently humanoid robots cost a lot of money.

01:38:32.680 --> 01:38:34.560
And the way they're thinking about it,

01:38:34.560 --> 01:38:36.800
now they're not talking about all the social robotics stuff

01:38:36.800 --> 01:38:38.240
that you and I care about.

01:38:39.360 --> 01:38:43.800
They are thinking how can we manufacture this thing cheaply

01:38:43.800 --> 01:38:45.360
and do it like well.

01:38:45.360 --> 01:38:47.560
And the kind of discussions they're having

01:38:47.560 --> 01:38:49.440
is really great engineering.

01:38:49.440 --> 01:38:52.760
It's like first principles question of like,

01:38:52.760 --> 01:38:54.480
why is this cost so much?

01:38:54.480 --> 01:38:56.440
Like what's the cheap way?

01:38:56.440 --> 01:38:57.600
Why can't we build?

01:38:57.600 --> 01:38:59.840
And there's not a good answer.

01:38:59.840 --> 01:39:03.640
Why can't we build this humanoid form for under $1,000?

01:39:03.640 --> 01:39:07.320
And like I've sat and had these conversations,

01:39:07.320 --> 01:39:08.320
there's no reason.

01:39:09.840 --> 01:39:11.760
I think the reason they've been so expensive

01:39:11.760 --> 01:39:15.920
is because they were focused on trying to,

01:39:17.000 --> 01:39:20.240
they weren't focused on doing the mass manufacture.

01:39:20.240 --> 01:39:23.040
People are focused on getting a thing that's,

01:39:23.040 --> 01:39:25.800
I don't know exactly what the reasoning is,

01:39:25.800 --> 01:39:27.880
but it's the same like Waymo.

01:39:27.880 --> 01:39:32.040
It's like, let's build a million dollar car in the beginning

01:39:32.040 --> 01:39:33.640
or like multi-million dollar car.

01:39:33.640 --> 01:39:36.280
Let's try to solve that problem.

01:39:36.280 --> 01:39:38.400
The way Elon, the way Jim Keller,

01:39:38.400 --> 01:39:41.040
the way some of those folks are thinking is,

01:39:41.080 --> 01:39:43.080
let's like at the same time,

01:39:43.080 --> 01:39:46.400
try to actually build a system that's cheap,

01:39:46.400 --> 01:39:47.640
not crappy, but cheap.

01:39:47.640 --> 01:39:49.720
Unless from first principles,

01:39:49.720 --> 01:39:54.640
what is the minimum amount of degrees of freedom we need?

01:39:54.640 --> 01:39:55.800
What are the joints?

01:39:55.800 --> 01:39:57.240
Where's the control sit?

01:39:57.240 --> 01:40:00.720
Like how many, how do we act, like where are the activators?

01:40:00.720 --> 01:40:03.120
What's the way to power this

01:40:03.120 --> 01:40:05.000
in the lowest cost way possible?

01:40:05.000 --> 01:40:07.520
But also in a way that's like actually works.

01:40:07.520 --> 01:40:10.520
How do we make the whole thing not part of the components

01:40:10.760 --> 01:40:11.760
where there's a supply chain,

01:40:11.760 --> 01:40:14.120
you have to have all these different parts

01:40:14.120 --> 01:40:15.080
that have to feed us.

01:40:15.080 --> 01:40:19.200
Do it all from scratch and do the learning.

01:40:19.200 --> 01:40:22.200
I mean, it's like immediately certain things

01:40:22.200 --> 01:40:23.640
like become obvious.

01:40:23.640 --> 01:40:27.480
Do the exact same pipeline as you do for autonomous driving,

01:40:27.480 --> 01:40:28.720
just the exact, I mean,

01:40:28.720 --> 01:40:31.080
the infrastructure that is incredible.

01:40:31.080 --> 01:40:33.840
For the computer vision, for the manipulation task,

01:40:33.840 --> 01:40:35.000
the control problem changes,

01:40:35.000 --> 01:40:37.960
the perception problem changes,

01:40:37.960 --> 01:40:40.480
but the pipeline doesn't change.

01:40:41.440 --> 01:40:45.040
And so I don't, obviously the optimism

01:40:45.040 --> 01:40:48.400
about how long it's gonna take, I don't share,

01:40:49.840 --> 01:40:51.640
but it's a really interesting problem.

01:40:51.640 --> 01:40:53.320
And I don't wanna say anything

01:40:53.320 --> 01:40:56.840
because my first gut is to say that,

01:40:56.840 --> 01:40:58.080
why the humanoid form?

01:40:58.080 --> 01:40:59.040
That doesn't make sense.

01:40:59.040 --> 01:41:02.040
Yeah, that's my second gut too, but.

01:41:02.040 --> 01:41:04.040
But then there's a lot of people

01:41:04.040 --> 01:41:06.120
that are really excited about the humanoid form there.

01:41:06.120 --> 01:41:06.960
That's true.

01:41:06.960 --> 01:41:08.840
It's like, I don't wanna get in the way,

01:41:09.280 --> 01:41:11.760
they might solve this thing and they might,

01:41:11.760 --> 01:41:13.760
it's like similar with Boston Dynamics.

01:41:15.640 --> 01:41:17.440
If I were to, you can be a hater

01:41:17.440 --> 01:41:21.640
and you go up to Mark Greiber and just,

01:41:21.640 --> 01:41:22.680
how are you gonna make money

01:41:22.680 --> 01:41:25.800
with these super expensive legged robots?

01:41:25.800 --> 01:41:27.160
What's your business plan?

01:41:27.160 --> 01:41:28.200
This doesn't make any sense.

01:41:28.200 --> 01:41:30.160
Why are you doing these legged robots?

01:41:30.160 --> 01:41:33.440
But at the same time, they're pushing forward

01:41:33.440 --> 01:41:35.760
the science, the art of robotics

01:41:35.760 --> 01:41:37.560
in the way that nobody else does.

01:41:37.560 --> 01:41:38.400
Yeah.

01:41:39.320 --> 01:41:43.080
With Elon, they're not just going to do that,

01:41:43.080 --> 01:41:44.840
they're gonna drive down the cost

01:41:44.840 --> 01:41:49.400
to where we can have humanoid bots in the home potentially.

01:41:49.400 --> 01:41:51.400
So the part I agree with is,

01:41:53.000 --> 01:41:54.640
a lot of people find it fascinating

01:41:54.640 --> 01:41:56.840
and it probably also attracts talent

01:41:56.840 --> 01:41:58.600
who wanna work on humanoid robots.

01:41:58.600 --> 01:42:01.080
I think it's a fascinating scientific problem

01:42:01.080 --> 01:42:02.400
and engineering problem

01:42:02.400 --> 01:42:05.480
and it can teach us more about human body

01:42:05.480 --> 01:42:06.720
and locomotion and all of that.

01:42:06.720 --> 01:42:08.640
I think there's a lot to learn from it.

01:42:09.480 --> 01:42:12.560
Where I get tripped up is why we need them

01:42:12.560 --> 01:42:14.480
for anything other than art and entertainment

01:42:14.480 --> 01:42:16.080
in the real world.

01:42:16.080 --> 01:42:18.880
I get that there are some areas

01:42:18.880 --> 01:42:23.880
where you can't just rebuild, like a spaceship,

01:42:24.720 --> 01:42:27.320
you can't just, like they've worked for so many years

01:42:27.320 --> 01:42:30.440
on these spaceships, you can't just re-engineer it.

01:42:30.440 --> 01:42:33.120
You have some things that are just built for human bodies,

01:42:33.120 --> 01:42:34.800
a submarine, a spaceship.

01:42:34.800 --> 01:42:38.640
But a factory, maybe I'm naive,

01:42:38.640 --> 01:42:40.960
but it seems like we've already rebuilt factories

01:42:40.960 --> 01:42:43.680
to accommodate other types of robots.

01:42:43.680 --> 01:42:46.520
Why would we want to just make a humanoid robot

01:42:46.520 --> 01:42:47.360
to go in there?

01:42:47.360 --> 01:42:50.600
I just get really tripped up on,

01:42:50.600 --> 01:42:53.400
I think that people want humanoids.

01:42:53.400 --> 01:42:56.640
I think people are fascinated by them.

01:42:56.640 --> 01:42:58.920
I think it's a little overhyped.

01:42:58.920 --> 01:43:03.240
Well, most of our world is still built for humanoids.

01:43:03.240 --> 01:43:04.120
I know, but it shouldn't be.

01:43:04.120 --> 01:43:07.080
It should be built so that it's wheelchair accessible.

01:43:07.080 --> 01:43:09.600
Right, so the question is, do you build a world

01:43:09.600 --> 01:43:13.240
that's the general form of wheelchair accessible,

01:43:15.640 --> 01:43:18.520
all robot form factor accessible,

01:43:19.680 --> 01:43:22.080
or do you build humanoid robots?

01:43:22.080 --> 01:43:23.880
I mean, it doesn't have to be all,

01:43:23.880 --> 01:43:26.280
and it also doesn't have to be either or.

01:43:26.280 --> 01:43:28.200
I just feel like we're thinking so little

01:43:28.200 --> 01:43:30.160
about the system in general

01:43:30.160 --> 01:43:34.520
and how to create infrastructure that works for everyone,

01:43:34.520 --> 01:43:36.440
all kinds of people, all kinds of robots.

01:43:36.440 --> 01:43:39.400
Like that's, I mean, it's more of an investment,

01:43:39.400 --> 01:43:42.560
but that would pay off way more in the future

01:43:42.560 --> 01:43:45.680
than just trying to cram expensive

01:43:45.680 --> 01:43:48.640
or maybe slightly less expensive humanoid technology

01:43:48.640 --> 01:43:50.000
into a human space.

01:43:50.000 --> 01:43:51.880
Unfortunately, one company can't do that.

01:43:51.880 --> 01:43:53.120
We have to work together.

01:43:53.120 --> 01:43:55.640
It's like autonomous driving can be easily solved

01:43:55.640 --> 01:43:59.400
if you do V2I, if you change the infrastructure

01:43:59.400 --> 01:44:00.920
of the cities and so on,

01:44:00.920 --> 01:44:04.480
but that requires a lot of people,

01:44:04.480 --> 01:44:06.000
a lot of them are politicians,

01:44:06.000 --> 01:44:09.120
and a lot of them are somewhat, if not a lot, corrupt

01:44:09.120 --> 01:44:10.560
and all those kinds of things.

01:44:11.960 --> 01:44:13.840
And the talent thing you mentioned

01:44:13.840 --> 01:44:16.080
is really, really, really important.

01:44:17.160 --> 01:44:19.000
I've gotten a chance to meet a lot of folks

01:44:19.000 --> 01:44:22.400
at SpaceX and Tesla, other companies too,

01:44:22.400 --> 01:44:25.320
but they're specifically, the openness makes it easier

01:44:25.320 --> 01:44:26.760
to like meet everybody.

01:44:26.760 --> 01:44:31.760
I think a lot of amazing things in this world happen

01:44:32.240 --> 01:44:34.400
when you get amazing people together.

01:44:34.400 --> 01:44:36.240
And if you can sell an idea,

01:44:36.240 --> 01:44:39.520
like us becoming a multi-planetary species,

01:44:39.520 --> 01:44:43.360
you can say, why the hell I would go to Mars?

01:44:43.360 --> 01:44:45.480
Like why colonize Mars?

01:44:45.480 --> 01:44:50.000
If you think from basic first principles,

01:44:50.000 --> 01:44:51.400
it doesn't make any sense.

01:44:52.360 --> 01:44:56.160
It doesn't make any sense to go to the moon.

01:44:56.160 --> 01:44:58.760
It doesn't go, the only thing that makes sense

01:44:58.760 --> 01:45:00.520
to go to space is for satellites.

01:45:02.120 --> 01:45:06.280
But there's something about the vision of the future,

01:45:06.280 --> 01:45:10.080
the optimism-laden that permeates this vision

01:45:10.080 --> 01:45:12.040
of us becoming multi-planetary.

01:45:12.040 --> 01:45:14.160
It's thinking not just for the next 10 years,

01:45:14.160 --> 01:45:17.680
it's thinking like human civilization

01:45:17.680 --> 01:45:19.480
reaching out into the stars.

01:45:19.480 --> 01:45:23.080
It makes people dream, it's really exciting.

01:45:23.080 --> 01:45:26.000
And that, they're gonna come up with some cool shit

01:45:26.000 --> 01:45:29.240
that might not have anything to do with,

01:45:29.240 --> 01:45:31.320
like here's what I,

01:45:31.320 --> 01:45:34.320
because Elon doesn't seem to care about social robotics,

01:45:34.320 --> 01:45:36.080
which is constantly surprising to me.

01:45:36.080 --> 01:45:37.480
I talk to him, he doesn't,

01:45:37.480 --> 01:45:42.480
humans are the things you avoid and don't hurt, right?

01:45:43.480 --> 01:45:45.880
Like that's, like the number one job of a robot

01:45:45.880 --> 01:45:48.320
is not to hurt a human, to avoid them.

01:45:48.320 --> 01:45:51.280
You know, the collaborative aspect,

01:45:51.280 --> 01:45:53.000
the human-robot interaction,

01:45:53.000 --> 01:45:54.800
I think is not, at least not in his,

01:45:56.400 --> 01:45:59.400
not something he thinks about deeply.

01:45:59.400 --> 01:46:03.680
But my sense is if somebody like that takes on the problem

01:46:03.680 --> 01:46:05.600
of human robotics,

01:46:05.600 --> 01:46:08.480
we're gonna get a social robot out of it.

01:46:08.480 --> 01:46:10.920
Like people like, not necessarily Elon,

01:46:10.920 --> 01:46:12.440
but people like Elon.

01:46:12.440 --> 01:46:14.480
If they take on seriously these,

01:46:17.720 --> 01:46:22.120
like I can just imagine with a humanoid robot,

01:46:22.120 --> 01:46:25.080
you can't help but create a social robot.

01:46:25.080 --> 01:46:26.760
So if you do different form factors,

01:46:26.760 --> 01:46:29.640
if you do industrial robotics,

01:46:30.680 --> 01:46:33.480
you don't, you're likely to actually not end up

01:46:33.480 --> 01:46:37.480
into like walking head into a social robot,

01:46:37.480 --> 01:46:40.320
social robot, human-robot interaction problem.

01:46:40.320 --> 01:46:43.000
If you create for whatever the hell reason you want to,

01:46:43.000 --> 01:46:46.560
a humanoid robot, you're gonna have to reinvent,

01:46:46.560 --> 01:46:49.000
well not reinvent, but do,

01:46:49.000 --> 01:46:51.280
introduce a lot of fascinating new ideas

01:46:51.280 --> 01:46:53.640
into the problem of human-robot interaction,

01:46:53.640 --> 01:46:54.680
which I'm excited about.

01:46:54.680 --> 01:46:57.200
So like, if I was a business person,

01:46:57.200 --> 01:47:00.840
I would say this is not, this is way too risky.

01:47:00.840 --> 01:47:02.440
This doesn't make any sense.

01:47:02.440 --> 01:47:04.240
But when people are really convinced

01:47:04.240 --> 01:47:06.680
and there's a lot of amazing people working on it,

01:47:06.680 --> 01:47:08.480
it's like, all right, let's see what happens here.

01:47:08.480 --> 01:47:09.760
This is really interesting.

01:47:09.760 --> 01:47:13.040
Just like with Atlas and Boston Dynamics,

01:47:13.040 --> 01:47:18.040
I mean, they, I apologize if I'm ignorant on this,

01:47:18.120 --> 01:47:22.240
but I think they really, more than anyone else,

01:47:22.240 --> 01:47:24.680
maybe with iBoat, like Sony,

01:47:24.680 --> 01:47:27.960
pushed forward humanoid robotics,

01:47:27.960 --> 01:47:30.200
like a leap with Atlas.

01:47:30.200 --> 01:47:32.920
Oh yeah, with Atlas, absolutely.

01:47:32.920 --> 01:47:35.680
And like without them, like why the hell did they do it?

01:47:35.680 --> 01:47:36.520
Why?

01:47:36.520 --> 01:47:38.520
Well, I think for them, it is a research platform.

01:47:38.520 --> 01:47:41.440
It's not, I don't think they ever,

01:47:41.440 --> 01:47:44.600
the speculation, I don't think they ever intended Atlas

01:47:44.600 --> 01:47:48.800
to be like a commercially successful robot.

01:47:48.800 --> 01:47:51.400
I think they were just like, can we do this?

01:47:51.400 --> 01:47:53.000
Let's try.

01:47:53.000 --> 01:47:54.680
Yeah, I wonder if they,

01:47:54.680 --> 01:47:56.800
maybe the answer they landed on is

01:47:58.400 --> 01:48:01.680
because they eventually went to Spot,

01:48:01.680 --> 01:48:03.360
the earlier versions of Spot.

01:48:03.360 --> 01:48:07.680
So Quadrope has like four-legged robot,

01:48:07.680 --> 01:48:11.360
but maybe they reached for, let's try to make,

01:48:12.240 --> 01:48:15.160
like, I think they tried it

01:48:15.160 --> 01:48:18.160
and they still are trying it for Atlas

01:48:18.160 --> 01:48:21.600
to be picking up boxes, to moving boxes, to being,

01:48:21.600 --> 01:48:23.320
it makes sense.

01:48:24.520 --> 01:48:26.920
Okay, if they were exactly the same cost,

01:48:28.320 --> 01:48:32.200
it makes sense to have a humanoid robot in the warehouse.

01:48:32.200 --> 01:48:33.320
Currently. Currently.

01:48:33.320 --> 01:48:35.320
I think it's short-sighted, but yes, currently,

01:48:35.320 --> 01:48:37.080
yes, they would sell.

01:48:37.080 --> 01:48:40.040
But it's not, it's short-sighted,

01:48:41.000 --> 01:48:44.480
it's short-sighted, but it's not pragmatic

01:48:44.480 --> 01:48:45.760
to think any other way,

01:48:45.760 --> 01:48:47.800
to think that you're gonna be able to change warehouses.

01:48:47.800 --> 01:48:49.840
You're gonna have to, you're going-

01:48:49.840 --> 01:48:52.560
If you're Amazon, you can totally change your warehouses.

01:48:52.560 --> 01:48:54.400
Yes, yes.

01:48:54.400 --> 01:48:56.480
But even if you're Amazon,

01:48:56.480 --> 01:49:01.360
that's very costly to change warehouses.

01:49:01.360 --> 01:49:03.640
It is, it's a big investment.

01:49:03.640 --> 01:49:08.480
But isn't, shouldn't you do that investment in a way?

01:49:08.480 --> 01:49:09.320
So here's the thing,

01:49:09.480 --> 01:49:12.240
if you build the humanoid robot that works in the warehouse,

01:49:12.240 --> 01:49:14.160
that humanoid robot,

01:49:14.160 --> 01:49:17.760
see, I don't know why Tessa's not talking about it this way,

01:49:17.760 --> 01:49:20.240
as far as I know, but that humanoid robot

01:49:20.240 --> 01:49:22.720
is gonna have all kinds of other applications

01:49:22.720 --> 01:49:24.360
outside their setting.

01:49:26.120 --> 01:49:27.440
To me, it's obvious.

01:49:27.440 --> 01:49:29.120
I think it's a really hard problem to solve,

01:49:29.120 --> 01:49:32.000
but whoever solves the humanoid robot problem

01:49:32.000 --> 01:49:34.440
are gonna have to solve the social robotics problem.

01:49:34.440 --> 01:49:35.280
Oh, for sure.

01:49:35.280 --> 01:49:36.720
I mean, they're already, with the spot,

01:49:36.720 --> 01:49:38.920
needing to solve social robotics problems.

01:49:39.880 --> 01:49:42.000
For spot to be effective at scale.

01:49:42.000 --> 01:49:44.120
I'm not sure if spot is currently effective at scale.

01:49:44.120 --> 01:49:45.400
It's getting better and better.

01:49:45.400 --> 01:49:48.720
But they're actually, the thing they did

01:49:48.720 --> 01:49:50.440
is an interesting decision.

01:49:50.440 --> 01:49:53.200
Perhaps Tessa will end up doing the same thing,

01:49:53.200 --> 01:49:58.200
which is spot is supposed to be a platform for intelligence.

01:49:59.360 --> 01:50:03.680
So spot doesn't have any high-level intelligence,

01:50:03.680 --> 01:50:07.160
like high-level perception skills.

01:50:07.200 --> 01:50:10.280
It's supposed to be controlled remotely.

01:50:10.280 --> 01:50:13.400
And it's a platform that you can attach something to.

01:50:13.400 --> 01:50:15.920
And somebody else is supposed to do the attaching.

01:50:15.920 --> 01:50:19.160
It's a platform that you can take in uneven ground

01:50:19.160 --> 01:50:21.480
and it's able to maintain balance,

01:50:21.480 --> 01:50:22.960
go into dangerous situations.

01:50:22.960 --> 01:50:24.280
It's a platform.

01:50:24.280 --> 01:50:27.520
On top of that, you can add a camera that does surveillance,

01:50:27.520 --> 01:50:30.840
that you can remotely monitor, you can record,

01:50:30.840 --> 01:50:34.440
you can record the camera, you can remote control it,

01:50:34.440 --> 01:50:36.040
but it's not gonna- Object manipulation.

01:50:36.040 --> 01:50:37.360
Basic object manipulation,

01:50:37.360 --> 01:50:40.080
but not autonomous object manipulation.

01:50:40.080 --> 01:50:41.880
It's remotely controlled.

01:50:41.880 --> 01:50:44.000
But the intelligence on top of it,

01:50:44.000 --> 01:50:46.920
which was what would be required for automation,

01:50:46.920 --> 01:50:48.560
somebody else is supposed to do.

01:50:48.560 --> 01:50:52.000
Perhaps Tessa would do the same thing, ultimately.

01:50:52.000 --> 01:50:53.240
But it doesn't make sense

01:50:53.240 --> 01:50:56.880
because the goal of optimists is automation.

01:50:58.440 --> 01:50:59.460
Without that.

01:51:02.760 --> 01:51:04.040
But then you never know.

01:51:04.040 --> 01:51:05.680
It's like, why go to Mars?

01:51:06.160 --> 01:51:07.000
Why?

01:51:07.760 --> 01:51:08.640
I mean, that's true.

01:51:08.640 --> 01:51:12.320
And I reluctantly am very excited about space travel.

01:51:13.600 --> 01:51:14.440
Why?

01:51:14.440 --> 01:51:16.200
Can you introspect why?

01:51:16.200 --> 01:51:17.040
Why?

01:51:17.040 --> 01:51:18.620
Am I excited about it?

01:51:18.620 --> 01:51:20.920
I think what got me excited was

01:51:20.920 --> 01:51:25.920
I saw a panel with some people who study other planets,

01:51:26.600 --> 01:51:30.880
and it became really clear how little we know

01:51:30.880 --> 01:51:33.760
about ourselves and about how nature works

01:51:33.760 --> 01:51:36.680
and just how much there is to learn

01:51:36.680 --> 01:51:39.120
from exploring other parts of the universe.

01:51:41.080 --> 01:51:44.760
So like on a rational level, that's how I convince myself

01:51:44.760 --> 01:51:46.680
that that's why I'm excited.

01:51:46.680 --> 01:51:49.360
In reality, it's just fucking exciting.

01:51:49.360 --> 01:51:52.160
I mean, just like the idea that we can do

01:51:52.160 --> 01:51:56.320
this difficult thing and that humans come together

01:51:56.320 --> 01:51:59.240
to build things that can explore space.

01:51:59.240 --> 01:52:02.300
I mean, there's just something inherently thrilling

01:52:02.300 --> 01:52:04.980
about that, and I'm reluctant about it

01:52:04.980 --> 01:52:08.100
because I feel like there are so many other challenges

01:52:08.100 --> 01:52:12.180
and problems that I think are more important to solve,

01:52:12.180 --> 01:52:14.680
but I also think we should be doing all of it at once.

01:52:14.680 --> 01:52:19.580
And so to that extent, I'm like all for research

01:52:20.500 --> 01:52:23.660
on humanoid robots, development of humanoid robots.

01:52:24.880 --> 01:52:26.700
I think that there's a lot to explore and learn

01:52:26.700 --> 01:52:28.800
and it doesn't necessarily take away

01:52:28.800 --> 01:52:31.820
from other areas of science.

01:52:31.820 --> 01:52:33.060
At least it shouldn't.

01:52:33.060 --> 01:52:35.040
I think unfortunately a lot of the attention

01:52:35.040 --> 01:52:39.100
goes towards that and it does take resources

01:52:39.100 --> 01:52:41.540
and attention away from other areas of robotics

01:52:41.540 --> 01:52:43.420
that we should be focused on,

01:52:43.420 --> 01:52:45.980
but I don't think we shouldn't do it.

01:52:45.980 --> 01:52:48.820
So you think it might be a little bit of a distraction.

01:52:48.820 --> 01:52:51.740
I'll forget the Elon particular application,

01:52:51.740 --> 01:52:55.860
but if you care about social robotics,

01:52:55.860 --> 01:52:59.020
the humanoid form is a distraction.

01:52:59.020 --> 01:53:01.020
It's a distraction and it's one that I find

01:53:01.020 --> 01:53:02.220
particularly boring.

01:53:03.300 --> 01:53:07.100
It's just, it's interesting from a research perspective,

01:53:07.100 --> 01:53:11.320
but from like what types of robots can we create

01:53:11.320 --> 01:53:12.160
to put in our world?

01:53:12.160 --> 01:53:14.420
Like why would we just create a humanoid robot?

01:53:14.420 --> 01:53:15.260
Just don't get it.

01:53:15.260 --> 01:53:16.940
So even just robotic manipulation,

01:53:16.940 --> 01:53:19.260
so arms is not useful either.

01:53:19.260 --> 01:53:22.580
Oh, arms can be useful, but like why not have three arms?

01:53:22.580 --> 01:53:25.180
Like why does it have to look like a person?

01:53:25.180 --> 01:53:26.860
Well, I actually personally just think

01:53:27.020 --> 01:53:32.020
that washing the dishes is harder than a robot

01:53:32.420 --> 01:53:33.620
that can be a companion.

01:53:34.780 --> 01:53:35.620
Yeah.

01:53:35.620 --> 01:53:38.180
Like being useful in the home is actually really tough.

01:53:38.180 --> 01:53:41.260
But does your companion have to have like two arms

01:53:41.260 --> 01:53:42.100
and look like you?

01:53:42.100 --> 01:53:44.620
No, I'm making the case for zero arms.

01:53:44.620 --> 01:53:46.460
Oh, okay, zero arms.

01:53:46.460 --> 01:53:47.300
Yeah.

01:53:47.300 --> 01:53:48.460
Okay, freaky.

01:53:49.620 --> 01:53:52.100
That didn't come out the way I meant it.

01:53:52.100 --> 01:53:54.380
Cause it almost sounds like I don't want a robot

01:53:54.380 --> 01:53:56.360
to defend itself.

01:53:56.360 --> 01:53:58.040
Like that's immediately you project.

01:53:58.040 --> 01:53:58.860
You know what I mean?

01:53:58.860 --> 01:54:03.860
Like, I think, I just think that the social component

01:54:05.720 --> 01:54:08.400
doesn't require arms or legs or so on, right?

01:54:08.400 --> 01:54:09.720
As we've talked about.

01:54:09.720 --> 01:54:11.540
And I think that's probably where a lot

01:54:11.540 --> 01:54:15.360
of the meaningful impact that's gonna be happening.

01:54:15.360 --> 01:54:16.200
Yeah.

01:54:16.200 --> 01:54:18.760
I think just we could get so creative with the design.

01:54:18.760 --> 01:54:20.680
Like why not have a robot on roller skates?

01:54:20.680 --> 01:54:21.520
They're like, whatever.

01:54:21.520 --> 01:54:24.620
Like, why does it have to look like us?

01:54:24.620 --> 01:54:25.460
Yeah.

01:54:27.160 --> 01:54:29.560
Still, it is a compelling and interesting form

01:54:29.560 --> 01:54:31.520
from a research perspective, like you said.

01:54:31.520 --> 01:54:32.640
Yeah.

01:54:32.640 --> 01:54:34.780
You co-authored a paper as you were talking about

01:54:34.780 --> 01:54:37.980
that for WeRobot 2022.

01:54:39.120 --> 01:54:41.120
Lula robot consumer protection

01:54:41.120 --> 01:54:43.060
in the face of automated social marketing.

01:54:43.060 --> 01:54:46.240
I think you were talking about some of the ideas in that.

01:54:46.240 --> 01:54:47.080
Yes.

01:54:47.080 --> 01:54:47.920
Oh, you got it from Twitter.

01:54:47.920 --> 01:54:50.200
I was like, that's not published yet.

01:54:50.200 --> 01:54:51.320
Yeah.

01:54:51.320 --> 01:54:52.600
This is how I do my research.

01:54:52.600 --> 01:54:54.640
You just go through people's Twitter feeds.

01:54:54.640 --> 01:54:55.460
Yeah, go.

01:54:55.460 --> 01:54:56.940
Thank you.

01:54:56.940 --> 01:54:58.980
It's not stalking if it's public.

01:55:00.020 --> 01:55:04.180
So there's, you looked at me like you're offended.

01:55:04.180 --> 01:55:05.580
Like, how did you know?

01:55:05.580 --> 01:55:09.740
No, I was just like worried that like some early, I mean.

01:55:09.740 --> 01:55:11.120
Yeah, there's a PDF.

01:55:12.340 --> 01:55:13.180
Does it?

01:55:13.180 --> 01:55:14.000
There is.

01:55:14.000 --> 01:55:14.840
There's a PDF.

01:55:14.840 --> 01:55:15.860
Like now?

01:55:15.860 --> 01:55:16.700
Yeah.

01:55:16.700 --> 01:55:18.220
Maybe like as of a few days ago.

01:55:18.220 --> 01:55:19.060
Yeah.

01:55:19.060 --> 01:55:19.880
Okay.

01:55:19.880 --> 01:55:20.720
Yeah.

01:55:20.720 --> 01:55:21.560
Okay.

01:55:22.900 --> 01:55:24.180
You look violated.

01:55:24.340 --> 01:55:25.780
How did you get that PDF?

01:55:25.780 --> 01:55:26.720
It's just a draft.

01:55:26.720 --> 01:55:27.560
It's online.

01:55:27.560 --> 01:55:30.860
Nobody read it yet until we've written the final paper.

01:55:30.860 --> 01:55:31.680
Well, it's really good.

01:55:31.680 --> 01:55:32.520
So I enjoyed it.

01:55:32.520 --> 01:55:34.020
Oh, thank you.

01:55:34.020 --> 01:55:35.940
Oh, by the time this comes out, I'm sure it'll be out.

01:55:35.940 --> 01:55:37.460
Or no, when's WeRobot?

01:55:37.460 --> 01:55:38.780
So basically WeRobot,

01:55:38.780 --> 01:55:41.260
that's the workshop where you have an hour

01:55:41.260 --> 01:55:43.700
where people give you constructive feedback on the paper

01:55:43.700 --> 01:55:45.340
and then you write the good version.

01:55:45.340 --> 01:55:46.180
Right.

01:55:46.180 --> 01:55:47.000
I take it back.

01:55:47.000 --> 01:55:47.840
There's no PDF.

01:55:47.840 --> 01:55:48.680
It doesn't exist.

01:55:48.680 --> 01:55:49.500
I imagine.

01:55:49.500 --> 01:55:53.340
But there is a table in there in a virtual imagined PDF

01:55:53.340 --> 01:55:56.020
that I like, that I wanted to mention,

01:55:56.020 --> 01:56:01.020
which is like this kind of strategy used

01:56:01.780 --> 01:56:03.420
across various marketing platforms.

01:56:03.420 --> 01:56:08.420
And it's basically looking at traditional media,

01:56:08.460 --> 01:56:11.020
person to person interaction, targeted ads,

01:56:11.020 --> 01:56:12.700
influencers and social robots.

01:56:12.700 --> 01:56:14.820
This is the kind of idea that you've been speaking to.

01:56:14.820 --> 01:56:17.580
And it's just a nice breakdown of that,

01:56:17.580 --> 01:56:21.260
that social robots have personalized recommendations,

01:56:21.260 --> 01:56:24.420
social persuasion, automated scalable,

01:56:24.420 --> 01:56:26.340
data collection and embodiment.

01:56:26.340 --> 01:56:29.220
So person to person interaction is really nice,

01:56:29.220 --> 01:56:31.700
but it doesn't have the automated

01:56:31.700 --> 01:56:33.340
and the data collection aspect.

01:56:33.340 --> 01:56:36.580
But the social robots have those two elements.

01:56:36.580 --> 01:56:38.920
Yeah, we're talking about the potential for social robots

01:56:38.920 --> 01:56:41.940
to just combine all of these different marketing methods

01:56:41.940 --> 01:56:44.300
to be this really potent cocktail.

01:56:44.300 --> 01:56:46.820
And that table, which was Daniela's idea

01:56:46.820 --> 01:56:48.000
and a really fantastic one,

01:56:48.000 --> 01:56:50.700
we put it in at the last second, so.

01:56:50.700 --> 01:56:51.540
Yeah, I really like that.

01:56:51.540 --> 01:56:52.620
I'm glad you like it.

01:56:52.620 --> 01:56:54.220
In a PDF that doesn't exist.

01:56:54.220 --> 01:56:55.060
Yes.

01:56:55.060 --> 01:56:56.900
That nobody can find if they look.

01:56:56.900 --> 01:56:57.740
Yeah.

01:56:57.740 --> 01:56:59.380
So when you say social robots, what does that mean?

01:56:59.380 --> 01:57:01.580
Does that include virtual ones or no?

01:57:01.580 --> 01:57:04.620
I think a lot of this applies to virtual ones too,

01:57:04.620 --> 01:57:06.820
although the embodiment thing,

01:57:06.820 --> 01:57:09.380
which I personally find very fascinating,

01:57:09.380 --> 01:57:11.540
is definitely a factor that research shows

01:57:11.540 --> 01:57:14.540
can enhance people's engagement with a device.

01:57:14.540 --> 01:57:17.060
But can embodiment be a virtual thing also,

01:57:17.060 --> 01:57:20.340
meaning like it has a body in the virtual world?

01:57:20.860 --> 01:57:21.700
Maybe.

01:57:21.700 --> 01:57:22.940
Makes you feel like,

01:57:24.460 --> 01:57:25.540
because what makes a body?

01:57:25.540 --> 01:57:27.100
A body is a thing that

01:57:29.500 --> 01:57:32.500
can disappear, like has a permanence.

01:57:32.500 --> 01:57:33.900
I mean, there's certain characteristics

01:57:33.900 --> 01:57:37.480
that you kind of associate to a physical object.

01:57:38.460 --> 01:57:41.340
So I think what I'm referring to,

01:57:42.500 --> 01:57:45.660
and I think this gets messy because now we have

01:57:45.660 --> 01:57:49.380
all these new virtual worlds and AR and stuff,

01:57:49.420 --> 01:57:51.020
and I think it gets messy,

01:57:51.020 --> 01:57:53.620
but there's research showing that something on a screen,

01:57:53.620 --> 01:57:54.740
on a traditional screen,

01:57:54.740 --> 01:57:58.260
and something that is moving in your physical space,

01:57:58.260 --> 01:57:59.980
that that has a very different effect

01:57:59.980 --> 01:58:02.500
on how your brain perceives it even.

01:58:04.020 --> 01:58:08.580
So, I mean, I have a sense that we can do that

01:58:08.580 --> 01:58:09.740
in a virtual world.

01:58:09.740 --> 01:58:10.580
Probably.

01:58:10.580 --> 01:58:13.780
Like when I've used VR, I jump around like an idiot

01:58:13.780 --> 01:58:16.140
because I think something's gonna hit me.

01:58:16.140 --> 01:58:18.380
And even if a video game on a 2D screen

01:58:18.380 --> 01:58:19.580
is compelling enough,

01:58:19.580 --> 01:58:21.980
like the thing that's immersive about it

01:58:21.980 --> 01:58:24.180
is I kind of put myself into that world.

01:58:25.420 --> 01:58:28.820
Those, the objects you're interacting with,

01:58:28.820 --> 01:58:30.820
Call of Duty, things you're shooting,

01:58:30.820 --> 01:58:32.140
they're kind of,

01:58:33.340 --> 01:58:36.060
I mean, your imagination fills the gaps and it becomes real.

01:58:36.060 --> 01:58:38.700
Like it pulls your mind in when it's well done.

01:58:38.700 --> 01:58:41.940
So it really depends what's shown on the 2D screen.

01:58:41.940 --> 01:58:43.060
Yeah.

01:58:43.060 --> 01:58:45.060
Yeah, I think there's a ton of different factors

01:58:45.060 --> 01:58:46.740
and there's different types of embodiment.

01:58:46.740 --> 01:58:49.900
Like you can have embodiment in a virtual world.

01:58:49.900 --> 01:58:52.860
You can have an agent that's simply text-based,

01:58:52.860 --> 01:58:54.620
which has no embodiment.

01:58:54.620 --> 01:58:57.580
So I think there's a whole spectrum of factors

01:58:57.580 --> 01:59:00.100
that can influence how much you engage with something.

01:59:00.100 --> 01:59:02.900
Yeah, I wonder, I always wondered if you can have like

01:59:04.620 --> 01:59:07.380
an entity living in a computer.

01:59:07.380 --> 01:59:08.940
It's okay, this is gonna be dark.

01:59:08.940 --> 01:59:10.860
I haven't always wondered about this.

01:59:10.860 --> 01:59:12.700
So it's gonna make it sound like I keep thinking

01:59:12.700 --> 01:59:14.180
about this kind of stuff.

01:59:14.180 --> 01:59:17.340
No, but like, this is almost like Black Mirror,

01:59:17.340 --> 01:59:21.700
but the entity that's convinced

01:59:22.820 --> 01:59:25.940
or is able to convince you that it's being tortured

01:59:25.940 --> 01:59:29.140
inside the computer and needs your help to get out.

01:59:29.140 --> 01:59:30.500
Something like this.

01:59:30.500 --> 01:59:33.700
That becomes, to me, suffering is one of the things

01:59:33.700 --> 01:59:37.220
that make you empathize with, like we're not good at,

01:59:37.220 --> 01:59:41.900
as you've discussed in the physical form,

01:59:41.900 --> 01:59:43.740
like holding a robot upside down,

01:59:44.260 --> 01:59:46.180
giving really good examples about that

01:59:46.180 --> 01:59:47.500
and discussing that.

01:59:47.500 --> 01:59:51.660
I think suffering is a really good catalyst for empathy.

01:59:52.900 --> 01:59:57.260
And I just feel like we can project embodiment

01:59:57.260 --> 01:59:58.940
on a virtual thing if it's capable

01:59:58.940 --> 02:00:01.540
of certain things like suffering.

02:00:01.540 --> 02:00:02.900
Yeah. So I always wonder.

02:00:02.900 --> 02:00:03.860
I think that's true.

02:00:03.860 --> 02:00:06.180
And I think that's what happened with the Lambda thing.

02:00:06.180 --> 02:00:09.940
Not that, none of the transcript was about suffering,

02:00:09.940 --> 02:00:14.700
but it was about having the capacity for suffering

02:00:14.700 --> 02:00:17.860
in human emotion that convinced the engineer

02:00:17.860 --> 02:00:19.340
that this thing was sentient.

02:00:19.340 --> 02:00:22.980
And it's basically the plot of Ex Machina.

02:00:22.980 --> 02:00:23.820
True.

02:00:23.820 --> 02:00:26.500
Have you ever made a robot like scream in pain?

02:00:26.500 --> 02:00:27.340
Have I?

02:00:27.340 --> 02:00:28.700
No, but have you seen that?

02:00:28.700 --> 02:00:29.580
Did someone?

02:00:31.460 --> 02:00:34.620
Oh yeah, no, they actually made a Roomba scream

02:00:34.620 --> 02:00:36.060
whenever it hit a wall.

02:00:36.060 --> 02:00:38.020
Yeah, I programmed that myself as well.

02:00:38.020 --> 02:00:38.860
Yeah?

02:00:38.900 --> 02:00:39.900
I'm inspired by that, yeah.

02:00:39.900 --> 02:00:40.740
Do you still have it?

02:00:40.740 --> 02:00:44.020
Oh, sorry, it hit a wall, I didn't.

02:00:44.020 --> 02:00:44.980
Whenever it bumped into something,

02:00:44.980 --> 02:00:46.300
it would scream in pain.

02:00:46.300 --> 02:00:50.980
No, so I had, the way I programmed the Roombas

02:00:50.980 --> 02:00:53.940
is when I kick it, so contact between me

02:00:53.940 --> 02:00:55.620
and the robot is when it screams.

02:00:55.620 --> 02:00:56.460
Really?

02:00:57.380 --> 02:00:59.940
Okay, and you were inspired by that?

02:00:59.940 --> 02:01:01.660
Yeah, I guess I misremembered the video.

02:01:01.660 --> 02:01:03.860
I saw the video a long, long time ago,

02:01:03.860 --> 02:01:06.100
or maybe heard somebody mention it,

02:01:06.100 --> 02:01:08.580
and it's an easy thing to program.

02:01:09.340 --> 02:01:11.820
I haven't run those Roombas for over a year now,

02:01:11.820 --> 02:01:15.460
but yeah, it was, my experience with it was that

02:01:17.340 --> 02:01:21.660
it's like they quickly become,

02:01:22.820 --> 02:01:27.820
like you remember them, you miss them,

02:01:28.380 --> 02:01:30.540
like they're real living beings.

02:01:30.540 --> 02:01:34.900
So the capacity to suffer is a really powerful thing.

02:01:34.900 --> 02:01:35.740
Yeah.

02:01:35.740 --> 02:01:38.100
Even then that, I mean, it was kind of hilarious.

02:01:38.100 --> 02:01:40.540
It was just a random recording of screaming

02:01:40.540 --> 02:01:45.140
from the internet, but it's still as weird.

02:01:45.140 --> 02:01:46.860
There's a thing you have to get right

02:01:46.860 --> 02:01:49.100
based on the interaction, like the latency.

02:01:53.620 --> 02:01:56.540
There is a realistic aspect of how you should scream

02:01:56.540 --> 02:01:58.700
relative to when you get hurt,

02:01:58.700 --> 02:02:01.500
like it should correspond correctly.

02:02:01.500 --> 02:02:05.060
Like if you kick it really hard, it should scream louder?

02:02:05.060 --> 02:02:07.740
No, it should scream at the appropriate time, not like.

02:02:07.740 --> 02:02:08.580
Oh, I see.

02:02:08.580 --> 02:02:09.420
Not like five minutes later.

02:02:09.420 --> 02:02:10.620
One second later, right?

02:02:10.620 --> 02:02:15.260
Like there's a timing when you get like,

02:02:15.260 --> 02:02:18.580
I don't know, when you run your foot

02:02:18.580 --> 02:02:20.780
into the side of a table or something,

02:02:20.780 --> 02:02:23.620
there's a timing there, the dynamics you have to get right

02:02:23.620 --> 02:02:25.540
for the actual screaming,

02:02:25.540 --> 02:02:30.100
because the Roomba in particular doesn't,

02:02:30.100 --> 02:02:35.100
so I was, the sensors don't,

02:02:35.980 --> 02:02:37.340
it doesn't know about pain.

02:02:38.340 --> 02:02:39.180
See? What?

02:02:40.940 --> 02:02:44.060
I'm sorry to say, Roomba doesn't understand pain.

02:02:44.060 --> 02:02:48.300
So you have to correctly map the sensors,

02:02:48.300 --> 02:02:51.780
the timing to the production of the sound.

02:02:51.780 --> 02:02:53.940
But when you get that somewhat right,

02:02:53.940 --> 02:02:56.780
it starts, it's a weird, it's a really weird feeling,

02:02:56.780 --> 02:02:58.780
and you actually feel like a bad person.

02:02:58.780 --> 02:03:00.780
Aw. Yeah.

02:03:00.780 --> 02:03:04.860
So, but it's, it makes you think

02:03:04.860 --> 02:03:07.660
because that, with all the ways that we talked about,

02:03:07.660 --> 02:03:09.700
that could be used to manipulate you.

02:03:09.700 --> 02:03:10.540
Oh, for sure.

02:03:10.540 --> 02:03:11.780
In a good and bad way.

02:03:11.780 --> 02:03:14.100
So the good way is like you can form a connection

02:03:14.100 --> 02:03:17.420
with a thing in a bad way that you can form a connection

02:03:17.420 --> 02:03:21.500
in order to sell you products that you don't want.

02:03:21.500 --> 02:03:23.940
Yeah, or manipulate you politically

02:03:23.940 --> 02:03:26.580
or many nefarious things.

02:03:26.580 --> 02:03:29.900
You tweeted, we're about to be living in the movie Her,

02:03:29.900 --> 02:03:31.260
except instead of,

02:03:31.260 --> 02:03:33.060
obviously I've researched your tweets,

02:03:33.060 --> 02:03:34.460
like they're like Shakespeare.

02:03:34.980 --> 02:03:36.860
We're about to be living in the movie Her,

02:03:36.860 --> 02:03:40.220
except instead of about love, it's gonna be about,

02:03:40.220 --> 02:03:43.900
what did I say, the chat bot being subtly racist

02:03:43.900 --> 02:03:47.380
and the question whether it's ethical for companies

02:03:47.380 --> 02:03:48.780
to charge for software upgrades.

02:03:48.780 --> 02:03:49.620
Yeah.

02:03:49.620 --> 02:03:52.700
So can we break that down?

02:03:52.700 --> 02:03:54.100
What do you mean by that?

02:03:54.100 --> 02:03:54.940
Yeah.

02:03:54.940 --> 02:03:56.780
Obviously some of it is humor.

02:03:56.780 --> 02:03:57.620
Yes.

02:03:57.620 --> 02:03:58.780
Well, kind of.

02:03:58.780 --> 02:04:02.940
I am like, oh, it's so weird to be in this space

02:04:02.940 --> 02:04:06.700
where I'm so worried about this technology

02:04:06.700 --> 02:04:09.700
and also so excited about it at the same time.

02:04:09.700 --> 02:04:12.860
But the really like I haven't,

02:04:12.860 --> 02:04:17.420
I'd gotten a little bit jaded and then with GPT-3

02:04:17.420 --> 02:04:19.420
and then the Lambda transcript,

02:04:19.420 --> 02:04:23.900
I was like re-energized,

02:04:23.900 --> 02:04:25.940
but have also been thinking a lot about

02:04:29.100 --> 02:04:32.580
what are the ethical issues that are gonna come up?

02:04:33.180 --> 02:04:34.260
And I think some of the things that companies

02:04:34.260 --> 02:04:36.420
are really gonna have to figure out is

02:04:36.420 --> 02:04:38.900
obviously algorithmic bias is a huge

02:04:38.900 --> 02:04:40.500
and known problem at this point.

02:04:40.500 --> 02:04:45.500
Like even the new image generation tools like Dali,

02:04:46.980 --> 02:04:49.820
where they've clearly put in a lot of effort

02:04:49.820 --> 02:04:52.500
to make sure that if you search for people,

02:04:52.500 --> 02:04:54.740
it gives you a diverse set of people, et cetera.

02:04:54.740 --> 02:04:57.540
Like even that one, people have already found numerous

02:04:57.540 --> 02:05:02.180
like ways that it just kind of regurgitates biases

02:05:02.180 --> 02:05:03.540
of things that it finds on the internet.

02:05:03.540 --> 02:05:05.540
Like how if you search for success,

02:05:05.540 --> 02:05:07.140
it gives you a bunch of images of men.

02:05:07.140 --> 02:05:08.300
If you search for sadness,

02:05:08.300 --> 02:05:10.100
it gives you a bunch of images of women.

02:05:10.100 --> 02:05:14.180
So I think that this is like the really tricky one

02:05:14.180 --> 02:05:15.780
with these voice agents

02:05:15.780 --> 02:05:17.860
that companies are gonna have to figure out.

02:05:17.860 --> 02:05:20.740
And that's why it's subtly racist and not overtly,

02:05:20.740 --> 02:05:22.300
because I think they're gonna be able to solve

02:05:22.300 --> 02:05:24.300
the overt thing and then with the subtle stuff,

02:05:24.300 --> 02:05:25.980
it's gonna be really difficult.

02:05:25.980 --> 02:05:29.140
And then I think the other thing is gonna be,

02:05:29.140 --> 02:05:33.700
yeah, like people are gonna become so emotionally attached

02:05:33.700 --> 02:05:38.620
to artificial agents with this complexity of language,

02:05:38.620 --> 02:05:41.940
with a potential embodiment factor that,

02:05:41.940 --> 02:05:44.580
I mean, there's a paper at WeRobot this year

02:05:44.580 --> 02:05:47.940
written by roboticists about how to deal with the fact

02:05:47.940 --> 02:05:51.980
that robots die and looking at it as an ethical issue

02:05:51.980 --> 02:05:54.580
because it impacts people.

02:05:54.580 --> 02:05:57.620
And I think there's gonna be way more issues than just that.

02:05:57.620 --> 02:06:01.460
Like I think the tweet was software upgrades, right?

02:06:01.460 --> 02:06:04.580
Like how much is it okay to charge for something like that

02:06:04.580 --> 02:06:07.620
if someone is deeply emotionally invested

02:06:07.620 --> 02:06:09.060
in this relationship?

02:06:10.540 --> 02:06:13.060
Oh, the ethics of that is interesting,

02:06:13.060 --> 02:06:17.180
but there's also the practical funding mechanisms.

02:06:17.180 --> 02:06:19.340
Like you mentioned with Aibo, the dog,

02:06:19.340 --> 02:06:21.300
in theory, there's a subscription.

02:06:22.300 --> 02:06:23.540
Yeah, the new Aibo.

02:06:23.540 --> 02:06:25.700
So the old Aibo from the 90s,

02:06:25.700 --> 02:06:27.140
people got really attached to.

02:06:27.140 --> 02:06:29.380
And in Japan, they're still having like funerals

02:06:29.380 --> 02:06:32.180
and Buddhist temples for the Aibos that can't be repaired

02:06:32.180 --> 02:06:34.540
because people really viewed them

02:06:34.540 --> 02:06:36.100
as part of their families.

02:06:36.100 --> 02:06:37.540
So we're talking about robot dogs.

02:06:37.540 --> 02:06:38.860
Robot dogs, the Aibo, yeah,

02:06:38.860 --> 02:06:42.140
the original like famous robot dog that Sony made,

02:06:42.140 --> 02:06:45.060
came out in the 90s, got discontinued,

02:06:45.060 --> 02:06:47.100
having funerals for them in Japan.

02:06:47.100 --> 02:06:48.780
Now they have a new one.

02:06:48.780 --> 02:06:49.860
The new one is great.

02:06:49.860 --> 02:06:50.860
I have one at home.

02:06:50.860 --> 02:06:51.820
It's like-

02:06:51.820 --> 02:06:53.300
It's $3,000, how much is it?

02:06:53.300 --> 02:06:54.820
I think it's 3000 bucks.

02:06:54.860 --> 02:06:58.700
And then after a few years, you have to start paying,

02:06:58.700 --> 02:07:01.820
I think it's like 300 a year for a subscription service

02:07:01.820 --> 02:07:02.980
for cloud services.

02:07:02.980 --> 02:07:04.420
And the cloud services,

02:07:07.220 --> 02:07:08.220
I mean, it's a lot,

02:07:09.220 --> 02:07:11.460
the dog is more complex than the original

02:07:11.460 --> 02:07:12.780
and it has a lot of cool features

02:07:12.780 --> 02:07:15.500
and it can remember stuff and experiences and it can learn.

02:07:15.500 --> 02:07:18.420
And a lot of that is outsourced to the cloud.

02:07:18.420 --> 02:07:20.700
And so you have to pay to keep that running,

02:07:20.700 --> 02:07:21.540
which makes sense.

02:07:21.540 --> 02:07:24.260
People should pay and people who aren't using it

02:07:24.260 --> 02:07:25.820
shouldn't have to pay.

02:07:25.820 --> 02:07:28.860
But it does raise the interesting question,

02:07:30.100 --> 02:07:33.860
could you set that price to reflect a consumer's willingness

02:07:33.860 --> 02:07:36.220
to pay for the emotional connection?

02:07:36.220 --> 02:07:41.220
So if like, you know that people are really, really attached

02:07:41.900 --> 02:07:45.100
to these things, just like they would be to a real dog,

02:07:46.220 --> 02:07:47.540
could you just start charging more

02:07:47.540 --> 02:07:49.980
because there's like more demand?

02:07:50.060 --> 02:07:51.980
Yeah, I mean, you have to be,

02:07:56.020 --> 02:07:58.940
but that's true for anything that people love, right?

02:07:58.940 --> 02:08:00.940
It is, and it's also true for real dogs.

02:08:00.940 --> 02:08:04.020
Like there's all these new medical services nowadays

02:08:04.020 --> 02:08:07.140
where people will shell out thousands and thousands of dollars

02:08:07.140 --> 02:08:08.580
to keep their pets alive.

02:08:09.420 --> 02:08:11.540
And is that taking advantage of people

02:08:11.540 --> 02:08:14.260
or is that just giving them what they want?

02:08:14.260 --> 02:08:16.180
That's the question.

02:08:16.180 --> 02:08:19.340
Back to marriage, what about all the money

02:08:19.340 --> 02:08:20.900
that it costs to get married

02:08:20.900 --> 02:08:24.820
and then all the money that it costs to get a divorce?

02:08:26.020 --> 02:08:30.300
That feels like a very, that's like a scam.

02:08:30.300 --> 02:08:32.820
I think the society is full of scams that are like-

02:08:32.820 --> 02:08:34.540
Oh, it's such a scam.

02:08:34.540 --> 02:08:35.820
And then we've created,

02:08:35.820 --> 02:08:37.820
the whole wedding industrial complex

02:08:37.820 --> 02:08:40.980
has created all these quote unquote traditions

02:08:40.980 --> 02:08:43.940
that people buy into that aren't even traditions.

02:08:43.940 --> 02:08:48.300
They're just fabricated by marketing, it's awful.

02:08:48.340 --> 02:08:50.700
Well, let me ask you about racist robots.

02:08:50.700 --> 02:08:51.540
Sure.

02:08:51.540 --> 02:08:54.540
Is it up to a company that creates that?

02:08:54.540 --> 02:08:56.820
So we talk about removing bias and so on.

02:08:56.820 --> 02:08:57.660
Yeah.

02:08:57.660 --> 02:09:00.460
And that's a really popular field in AI currently.

02:09:00.460 --> 02:09:01.300
Yeah.

02:09:01.300 --> 02:09:03.860
And a lot of people agree that it's an important field.

02:09:03.860 --> 02:09:06.660
But the question is for like social robotics,

02:09:06.660 --> 02:09:08.580
is should it be up to the company

02:09:08.580 --> 02:09:10.540
to remove the bias of society?

02:09:10.540 --> 02:09:13.540
Well, who else can, oh, to remove the bias of society.

02:09:13.540 --> 02:09:16.420
I guess because there's a lot of people

02:09:16.420 --> 02:09:19.060
that are subtly racist in modern society.

02:09:19.060 --> 02:09:22.220
Like why shouldn't our robots also be subtly racist?

02:09:23.420 --> 02:09:24.820
I mean, that's like,

02:09:24.820 --> 02:09:28.660
why do we put so much responsibility on the robots?

02:09:28.660 --> 02:09:29.500
Because,

02:09:31.580 --> 02:09:32.420
because the robots-

02:09:32.420 --> 02:09:34.780
I'm imagining like a Hitler Roomba.

02:09:36.460 --> 02:09:37.980
I mean, that would be funny,

02:09:37.980 --> 02:09:40.860
but I guess I'm asking a serious question.

02:09:40.860 --> 02:09:41.700
You're Jewish, right?

02:09:41.700 --> 02:09:42.540
You're allowed to make that joke.

02:09:42.540 --> 02:09:44.340
Yes, exactly, I'm allowed to make that joke, yes.

02:09:44.340 --> 02:09:49.340
And I've been nonstop reading about World War II and Hitler.

02:09:50.180 --> 02:09:52.700
I think I'm glad we exist in a world

02:09:52.700 --> 02:09:55.100
where we can just make those jokes.

02:09:55.100 --> 02:09:56.340
That helps deal with it.

02:09:57.220 --> 02:10:00.860
Anyway, it is a serious question of sort of like,

02:10:03.940 --> 02:10:06.460
like it's such a difficult problem to solve.

02:10:06.460 --> 02:10:09.500
Now, of course, like bias and so on,

02:10:09.500 --> 02:10:11.100
like there's low hanging fruit,

02:10:11.100 --> 02:10:13.940
which I think was what a lot of people are focused on.

02:10:14.460 --> 02:10:17.860
But then it becomes like subtle stuff over time.

02:10:17.860 --> 02:10:19.460
And it's very difficult to know.

02:10:19.460 --> 02:10:23.900
Now, if you can also completely remove the personality,

02:10:23.900 --> 02:10:26.380
you can completely remove the personalization.

02:10:26.380 --> 02:10:28.060
You can remove the language aspect,

02:10:28.060 --> 02:10:30.060
which is what I had been arguing,

02:10:30.060 --> 02:10:30.900
because I was like,

02:10:30.900 --> 02:10:32.580
the language is a disappointing aspect

02:10:32.580 --> 02:10:34.420
of social robots anyway.

02:10:34.420 --> 02:10:36.060
But now we're reintroducing that,

02:10:36.060 --> 02:10:39.260
because it's now no longer disappointing.

02:10:39.260 --> 02:10:42.460
So I do think, well,

02:10:42.460 --> 02:10:44.580
let's just start with the promise,

02:10:44.580 --> 02:10:45.780
which I think is very true,

02:10:45.780 --> 02:10:48.300
which is that racism is not a neutral thing,

02:10:48.300 --> 02:10:51.260
but it is the thing that we don't want in our society.

02:10:51.260 --> 02:10:54.460
Like it does not conform to my values.

02:10:54.460 --> 02:10:56.500
So if we agree that racism is bad,

02:10:57.500 --> 02:11:01.500
I do think that it has to be the company,

02:11:01.500 --> 02:11:06.140
because I mean, it might not be possible.

02:11:06.140 --> 02:11:09.820
And companies might have to put out products

02:11:10.820 --> 02:11:11.900
where they're taking risks,

02:11:12.340 --> 02:11:14.620
and they might get slammed by consumers,

02:11:14.620 --> 02:11:15.980
and they might have to adjust.

02:11:15.980 --> 02:11:19.020
I don't know like how this is gonna work in the market.

02:11:19.020 --> 02:11:20.460
I have opinions about how it should work,

02:11:20.460 --> 02:11:22.460
but it is on the company.

02:11:22.460 --> 02:11:25.300
And the danger with robots

02:11:25.300 --> 02:11:27.300
is that they can entrench this stuff.

02:11:27.300 --> 02:11:30.700
It's not like your racist uncle

02:11:30.700 --> 02:11:35.140
who you can have a conversation with,

02:11:35.140 --> 02:11:36.340
and...

02:11:36.340 --> 02:11:38.260
And put things into context maybe with that.

02:11:38.260 --> 02:11:40.540
Yeah, or who might change over time

02:11:40.540 --> 02:11:42.060
with more experience.

02:11:42.060 --> 02:11:46.460
A robot really just like regurgitates things,

02:11:46.460 --> 02:11:50.460
entrenches them, could influence other people.

02:11:50.460 --> 02:11:54.300
And I mean, I think that's terrible.

02:11:54.300 --> 02:11:56.340
Well, I think there's a difficult challenge here

02:11:56.340 --> 02:11:58.860
is because even the premise you started with

02:11:58.860 --> 02:12:01.220
that essentially racism is bad.

02:12:02.540 --> 02:12:04.540
I think we live in a society today

02:12:04.540 --> 02:12:06.700
where the definition of racism

02:12:06.700 --> 02:12:09.380
is different between different people.

02:12:09.420 --> 02:12:12.060
Some people say that it's not enough not to be racist.

02:12:12.060 --> 02:12:14.540
Some people say you have to be anti-racist.

02:12:14.540 --> 02:12:18.540
So you have to have a robot that constantly calls out,

02:12:18.540 --> 02:12:23.540
like calls you out on your implicit racism.

02:12:24.060 --> 02:12:25.660
I would love that.

02:12:25.660 --> 02:12:27.460
I would love that robot.

02:12:27.460 --> 02:12:29.900
But like maybe it sees...

02:12:29.900 --> 02:12:31.060
Well, I don't know if you'd love it

02:12:31.060 --> 02:12:33.380
because maybe you'll see racism

02:12:33.380 --> 02:12:34.740
in things that aren't racist,

02:12:34.740 --> 02:12:37.180
and then you're arguing with the robot.

02:12:37.180 --> 02:12:39.940
Your robot starts calling you racist.

02:12:39.940 --> 02:12:42.020
I'm not exactly sure that...

02:12:42.020 --> 02:12:43.580
I mean, it's a tricky thing.

02:12:43.580 --> 02:12:48.580
I guess I'm saying that the line is not obvious,

02:12:48.620 --> 02:12:51.180
especially in this heated discussion

02:12:51.180 --> 02:12:53.020
where we have a lot of identity politics

02:12:53.020 --> 02:12:56.340
of what is harmful to different groups and so on.

02:12:56.340 --> 02:12:57.180
Yeah.

02:12:57.180 --> 02:13:00.020
It feels like a...

02:13:00.020 --> 02:13:01.860
The broader question here is

02:13:01.860 --> 02:13:06.100
should a social robotics company be solving

02:13:06.100 --> 02:13:09.860
or being part of solving the issues of society?

02:13:09.860 --> 02:13:12.620
Well, okay, I think it's the same question as

02:13:12.620 --> 02:13:16.380
should I as an individual be responsible

02:13:16.380 --> 02:13:19.260
for knowing everything in advance

02:13:19.260 --> 02:13:21.140
and saying all the right things?

02:13:21.140 --> 02:13:26.140
And the answer to that is yes, I am responsible,

02:13:28.300 --> 02:13:30.580
but I'm not gonna get it perfect.

02:13:30.580 --> 02:13:32.780
And then the question is how do we deal with that?

02:13:32.780 --> 02:13:37.460
And so as a person, how I aspire to deal with that is

02:13:39.220 --> 02:13:41.100
when I do inevitably make a mistake

02:13:41.100 --> 02:13:45.780
because I have blind spots and people get angry,

02:13:45.780 --> 02:13:47.380
I don't take that personally

02:13:47.380 --> 02:13:50.580
and I listen to what's behind the anger.

02:13:50.580 --> 02:13:52.460
And it can even happen that like

02:13:52.460 --> 02:13:55.220
maybe I'll tweet something that's well-intentioned

02:13:55.220 --> 02:13:59.820
and one group of people starts yelling at me

02:13:59.820 --> 02:14:01.860
and then I change it the way that they said,

02:14:01.860 --> 02:14:03.940
and then another group of people starts yelling at me,

02:14:03.940 --> 02:14:08.940
which has happened to me actually around.

02:14:08.940 --> 02:14:10.900
In my talks, I talk about robots

02:14:10.900 --> 02:14:12.780
that are used in autism therapy.

02:14:12.780 --> 02:14:15.820
And so whether to say a child with autism

02:14:15.820 --> 02:14:19.180
or an autistic child is super controversial.

02:14:19.180 --> 02:14:21.940
And a lot of autistic people prefer to be referred to

02:14:21.940 --> 02:14:23.820
as autistic people and a lot of parents

02:14:23.820 --> 02:14:27.620
of autistic children prefer child with autism.

02:14:27.620 --> 02:14:29.380
And then they disagree.

02:14:29.380 --> 02:14:32.380
So I've gotten yelled at from both sides

02:14:32.380 --> 02:14:35.540
and I think I'm still, I'm responsible

02:14:35.540 --> 02:14:37.260
even if I can't get it right.

02:14:37.260 --> 02:14:38.220
I don't know if that makes sense.

02:14:38.220 --> 02:14:40.140
Like it's a responsibility thing

02:14:42.380 --> 02:14:44.940
and I can be as well-intentioned as I want

02:14:44.940 --> 02:14:46.380
and I'm still gonna make mistakes

02:14:46.380 --> 02:14:49.700
and that is part of the existing power structures that exist.

02:14:49.700 --> 02:14:51.220
And that's something that I accept.

02:14:51.220 --> 02:14:53.260
And you accept being attacked from both sides

02:14:53.260 --> 02:14:55.260
and grow from it and learn from it.

02:14:55.260 --> 02:14:59.300
But the danger is that after being attacked,

02:15:00.140 --> 02:15:00.980
assuming you don't get canceled,

02:15:00.980 --> 02:15:04.860
AKA completely removed from your ability to tweet,

02:15:07.220 --> 02:15:09.220
you might become jaded

02:15:09.220 --> 02:15:11.700
and not want to talk about autism anymore.

02:15:11.700 --> 02:15:13.460
I don't and I didn't.

02:15:13.460 --> 02:15:15.500
I mean, it's happened to me.

02:15:15.500 --> 02:15:19.380
What I did was I listened to both sides and I chose,

02:15:19.380 --> 02:15:21.700
I tried to get information.

02:15:21.700 --> 02:15:26.700
And then I decided that I was going to use autistic children

02:15:27.620 --> 02:15:29.100
and now I'm moving forward with that.

02:15:29.100 --> 02:15:29.940
Like, I don't know.

02:15:29.940 --> 02:15:30.780
For now, right?

02:15:30.780 --> 02:15:31.620
For now, yeah.

02:15:31.620 --> 02:15:33.100
Until I get updated information

02:15:33.100 --> 02:15:35.060
and I'm never gonna get anything perfect

02:15:35.060 --> 02:15:37.620
but I'm making choices and I'm moving forward

02:15:37.620 --> 02:15:41.580
because being a coward and like just retreating from that,

02:15:42.780 --> 02:15:43.620
I think.

02:15:43.620 --> 02:15:45.020
But see, here's the problem.

02:15:45.020 --> 02:15:47.940
You're a very smart person in an individual,

02:15:47.940 --> 02:15:50.180
a researcher, a thinker, an intellectual.

02:15:50.180 --> 02:15:52.540
So that's the right thing for you to do.

02:15:52.540 --> 02:15:54.620
The hard thing is when as a company,

02:15:54.620 --> 02:15:56.140
imagine you had a big company

02:15:56.140 --> 02:15:57.700
and imagine you had a PR team.

02:15:58.780 --> 02:16:00.180
I said, Kate, like this, you should.

02:16:00.180 --> 02:16:01.620
PR teams we hate.

02:16:01.620 --> 02:16:06.620
Yeah, I mean, just, well, if you hired PR people,

02:16:06.620 --> 02:16:10.180
like obviously they would see that and they'd be like,

02:16:10.180 --> 02:16:12.660
well, maybe don't bring up autism.

02:16:12.660 --> 02:16:14.260
Maybe don't bring up these topics.

02:16:14.260 --> 02:16:17.180
You're getting attacked, it's bad for your brand.

02:16:17.180 --> 02:16:18.820
They'll say the brand word.

02:16:18.820 --> 02:16:22.260
There'll be, you know, if we look at different demographics

02:16:22.260 --> 02:16:23.780
that are inspired by your work,

02:16:23.780 --> 02:16:25.180
I think it's insensitive to them.

02:16:25.180 --> 02:16:26.460
Let's not mention this anymore.

02:16:26.460 --> 02:16:31.020
Like there's this kind of pressure that all of a sudden you,

02:16:31.020 --> 02:16:34.860
or you do suboptimal decisions.

02:16:34.860 --> 02:16:36.660
You take a kind of poll.

02:16:38.380 --> 02:16:41.220
Again, it's looking at the past versus the future,

02:16:41.220 --> 02:16:42.140
all those kinds of things.

02:16:42.140 --> 02:16:44.300
And it becomes difficult.

02:16:44.300 --> 02:16:46.340
In the same way that it's difficult

02:16:46.340 --> 02:16:49.180
for social media companies to figure out like

02:16:49.180 --> 02:16:53.260
who to censor, who to recommend.

02:16:53.260 --> 02:16:55.900
I think this is ultimately a question about leadership,

02:16:55.900 --> 02:16:58.220
honestly, like the way that I see leadership.

02:16:58.220 --> 02:17:02.620
Because right now, the thing that bothers me

02:17:02.620 --> 02:17:04.820
about institutions and a lot of people

02:17:04.820 --> 02:17:09.020
who run current institutions is that their main focus

02:17:09.020 --> 02:17:10.780
is protecting the institution

02:17:10.780 --> 02:17:12.500
or protecting themselves personally.

02:17:12.500 --> 02:17:13.900
That is bad leadership

02:17:13.900 --> 02:17:16.220
because it means you cannot have integrity.

02:17:16.220 --> 02:17:18.900
You cannot lead with integrity.

02:17:18.900 --> 02:17:20.980
And it makes sense because like, obviously,

02:17:21.020 --> 02:17:22.420
if you're the type of leader

02:17:22.420 --> 02:17:25.060
who immediately blows up the institution you're leading,

02:17:25.060 --> 02:17:26.660
then that doesn't exist anymore.

02:17:26.660 --> 02:17:29.380
And maybe that's why we don't have any good leaders anymore

02:17:29.380 --> 02:17:30.860
because they had integrity

02:17:30.860 --> 02:17:35.860
and they didn't put the survival of the institution first.

02:17:36.780 --> 02:17:41.780
But I feel like you have to just to be a good leader,

02:17:43.060 --> 02:17:48.060
you have to be responsible and understand

02:17:48.460 --> 02:17:50.940
that with great power comes great responsibility.

02:17:51.940 --> 02:17:52.940
You have to be humble and you have to listen

02:17:52.940 --> 02:17:54.700
and you have to learn, you can't get defensive

02:17:54.700 --> 02:17:57.980
and you cannot put your own protection before other things.

02:17:57.980 --> 02:18:00.620
Yeah, take risks where you might lose your job,

02:18:01.620 --> 02:18:05.020
you might lose your wellbeing because of,

02:18:09.740 --> 02:18:12.780
in the process of standing for the principles,

02:18:12.780 --> 02:18:14.900
for the things you think are right to do.

02:18:14.900 --> 02:18:18.580
Yeah, based on the things you,

02:18:18.580 --> 02:18:20.500
like based on learning from,

02:18:20.500 --> 02:18:23.580
like listening to people and learning from what they feel.

02:18:23.580 --> 02:18:26.140
And the same goes for the institution, yeah.

02:18:26.140 --> 02:18:27.940
Yeah, but I ultimately actually believe

02:18:27.940 --> 02:18:32.500
that those kinds of companies and countries succeed

02:18:32.500 --> 02:18:34.060
that have leaders like that.

02:18:34.060 --> 02:18:35.500
You should run for president.

02:18:36.940 --> 02:18:38.100
No, thank you.

02:18:38.100 --> 02:18:38.940
Yeah.

02:18:38.940 --> 02:18:39.780
That's maybe the problem,

02:18:39.780 --> 02:18:42.540
like the people who have good ideas about leadership,

02:18:42.540 --> 02:18:46.940
they're like, yeah, this is why I'm not running a company.

02:18:46.940 --> 02:18:48.660
It's been, I think, three years

02:18:48.660 --> 02:18:52.380
since the Jeffrey Epstein controversy at MIT,

02:18:52.380 --> 02:18:53.940
MIT Media Lab.

02:18:53.940 --> 02:18:58.220
Joey Ito, the head of the media lab resigned.

02:18:58.220 --> 02:18:59.740
And I think at that time,

02:18:59.740 --> 02:19:02.700
you wrote an opinion article about it.

02:19:02.700 --> 02:19:05.660
So just looking back a few years have passed,

02:19:05.660 --> 02:19:10.220
what have you learned about human nature

02:19:11.220 --> 02:19:16.220
from the fact that somebody like Jeffrey Epstein

02:19:17.580 --> 02:19:19.460
found his way inside MIT?

02:19:22.260 --> 02:19:23.420
That's a really good question.

02:19:23.420 --> 02:19:25.300
What have I learned about human nature?

02:19:27.060 --> 02:19:31.500
I think, well, there's,

02:19:34.060 --> 02:19:36.940
how did this problem come about?

02:19:36.940 --> 02:19:40.940
And then there's what was the reaction to this problem

02:19:40.940 --> 02:19:43.300
and to it becoming public.

02:19:43.300 --> 02:19:44.900
And in the reaction,

02:19:47.500 --> 02:19:49.860
the things I learned about human nature

02:19:54.220 --> 02:19:58.460
were that sometimes cowards are worse than assholes.

02:20:01.020 --> 02:20:02.220
Wow, I'm really, ugh.

02:20:03.700 --> 02:20:05.980
I mean, that's a really powerful statement.

02:20:05.980 --> 02:20:08.020
I think because the assholes,

02:20:08.020 --> 02:20:11.020
at least you know what you're dealing with.

02:20:11.980 --> 02:20:13.700
They have integrity in a way.

02:20:13.700 --> 02:20:16.140
They're just living out their asshole values.

02:20:16.140 --> 02:20:18.780
And the cowards are the ones that you have to watch out for.

02:20:18.780 --> 02:20:22.660
And this comes back to people protecting themselves

02:20:22.660 --> 02:20:25.060
over doing the right thing.

02:20:27.060 --> 02:20:29.700
They'll throw others under the bus.

02:20:29.700 --> 02:20:31.300
Is there some sense that not enough people

02:20:31.300 --> 02:20:32.500
took responsibility?

02:20:33.500 --> 02:20:34.340
For sure.

02:20:34.340 --> 02:20:39.340
And I mean, I don't wanna sugarcoat at all

02:20:39.580 --> 02:20:41.060
what Joey Ito did.

02:20:41.060 --> 02:20:44.420
I mean, I think it's gross that he took money

02:20:44.420 --> 02:20:45.500
from Jeffrey Epstein.

02:20:45.500 --> 02:20:47.980
I believe him that he didn't know about the bad, bad stuff.

02:20:47.980 --> 02:20:49.380
But I've been in those circles

02:20:49.380 --> 02:20:52.460
with those public intellectual dudes

02:20:52.460 --> 02:20:53.660
that he was hanging out with.

02:20:53.660 --> 02:20:58.660
And any woman in those circles saw 10 zillion red flags.

02:20:58.900 --> 02:21:01.700
The whole environment was so misogynist.

02:21:02.340 --> 02:21:05.260
Like, and so personally,

02:21:05.260 --> 02:21:09.180
because Joey like was a great boss and a great friend,

02:21:10.860 --> 02:21:13.820
I was really disappointed that he ignored that

02:21:13.820 --> 02:21:15.300
in favor of raising money.

02:21:18.180 --> 02:21:23.100
And I think that it was right for him to resign

02:21:23.100 --> 02:21:24.660
in the face of that.

02:21:24.660 --> 02:21:29.100
But one of the things that he did

02:21:29.100 --> 02:21:33.780
that no many others didn't was he came forward about it

02:21:33.780 --> 02:21:35.740
and he took responsibility.

02:21:37.580 --> 02:21:40.980
And all of the people who didn't, I think,

02:21:44.580 --> 02:21:46.020
it's just interesting.

02:21:46.020 --> 02:21:47.700
The other thing I learned about human nature,

02:21:47.700 --> 02:21:49.340
okay, I'm gonna go on a tangent,

02:21:49.340 --> 02:21:50.340
but I'll come back, I promise.

02:21:50.340 --> 02:21:53.700
So I once saw this tweet from someone,

02:21:53.700 --> 02:21:54.700
or it was a Twitter thread,

02:21:54.700 --> 02:21:57.060
from someone who worked at a homeless shelter.

02:21:57.100 --> 02:22:00.460
And he said that when he started working there,

02:22:00.460 --> 02:22:02.820
he noticed that people would often come in

02:22:02.820 --> 02:22:03.740
and use the bathroom,

02:22:03.740 --> 02:22:05.780
and they would just trash the entire bathroom,

02:22:05.780 --> 02:22:07.380
like rip things out of the walls,

02:22:07.380 --> 02:22:08.900
like toilet paper on the ground.

02:22:08.900 --> 02:22:10.660
And he asked someone who had been there longer,

02:22:10.660 --> 02:22:12.260
like, why do they do this?

02:22:12.260 --> 02:22:14.020
Why do the homeless people come in and trash the bathroom?

02:22:14.020 --> 02:22:17.020
And he was told it's because it's the only thing

02:22:17.020 --> 02:22:19.180
in their lives that they have control over.

02:22:20.140 --> 02:22:25.060
And I feel like sometimes when it comes to

02:22:27.060 --> 02:22:29.140
the response, the,

02:22:32.420 --> 02:22:36.140
just the mobbing response that happens

02:22:36.140 --> 02:22:39.980
in the wake of some harm that was caused,

02:22:41.380 --> 02:22:45.100
if you can't target the person who actually caused the harm,

02:22:45.100 --> 02:22:47.540
who was Epstein,

02:22:47.540 --> 02:22:50.140
you will go as many circles out as you can

02:22:50.140 --> 02:22:52.580
until you find the person that you have power over

02:22:52.580 --> 02:22:55.340
and you have control over, and then you will trash that.

02:22:55.340 --> 02:22:57.900
And it makes sense that people do this.

02:22:57.900 --> 02:22:59.940
It's, again, it's a human nature thing.

02:22:59.940 --> 02:23:02.020
Of course you're gonna focus all your energy

02:23:02.020 --> 02:23:04.500
because you feel helpless and enraged,

02:23:04.500 --> 02:23:08.460
and you, and it's unfair, and you have no other power.

02:23:08.460 --> 02:23:10.060
You're gonna focus all of your energy

02:23:10.060 --> 02:23:12.940
on someone who's so far removed from the problem

02:23:12.940 --> 02:23:16.940
that that's not even an efficient solution.

02:23:16.940 --> 02:23:20.900
And the problem is often the first person you find

02:23:20.900 --> 02:23:22.340
is the one that has integrity,

02:23:22.340 --> 02:23:24.740
sufficient integrity to take responsibility.

02:23:24.740 --> 02:23:28.540
Yeah, and it's why my husband always says he's a liberal,

02:23:28.540 --> 02:23:32.300
but he's always like when liberals form a firing squad,

02:23:32.300 --> 02:23:33.900
they stand in a circle,

02:23:33.900 --> 02:23:37.300
because you know that your friends are gonna listen to you

02:23:37.300 --> 02:23:39.100
so you criticize them.

02:23:39.100 --> 02:23:42.420
You're not gonna be able to convince someone across the aisle.

02:23:42.420 --> 02:23:44.940
But see, in that situation, what I had hoped

02:23:45.820 --> 02:23:49.380
is the people in the farther, in that situation,

02:23:49.380 --> 02:23:51.500
any situation of that sort,

02:23:51.500 --> 02:23:54.740
the people that are farther out in the circles,

02:23:56.380 --> 02:24:00.820
stand up and also take some responsibility

02:24:00.820 --> 02:24:02.660
for the broader picture of human nature

02:24:02.660 --> 02:24:05.060
versus specific situation,

02:24:05.060 --> 02:24:09.020
but also take some responsibility,

02:24:11.940 --> 02:24:16.580
but also defend the people involved as flawed,

02:24:16.580 --> 02:24:20.940
not in a like, no, no, no, nothing like this,

02:24:20.980 --> 02:24:21.900
people fucked up.

02:24:21.900 --> 02:24:24.180
Like you said, there's a lot of red flags

02:24:24.180 --> 02:24:27.100
that people just ignored for the sake of money

02:24:27.100 --> 02:24:28.420
in this particular case.

02:24:29.580 --> 02:24:32.820
But also like be transparent and public about it

02:24:32.820 --> 02:24:37.820
and spread the responsibility across large number of people

02:24:38.500 --> 02:24:42.900
such that you learn a lesson from it institutionally.

02:24:42.900 --> 02:24:45.020
Yeah, it was a systems problem.

02:24:45.020 --> 02:24:47.140
It wasn't a one individual problem.

02:24:47.140 --> 02:24:52.140
And I feel like currently because Joey took,

02:24:52.820 --> 02:24:55.060
like resigned because of it,

02:24:55.060 --> 02:24:58.740
or essentially fired, pressured out because of it,

02:25:00.100 --> 02:25:04.740
MIT can pretend like, oh, we didn't know anything.

02:25:04.740 --> 02:25:05.740
It wasn't part-

02:25:05.740 --> 02:25:07.940
Bad leadership again,

02:25:07.940 --> 02:25:11.620
because when you are at the top of an institution

02:25:11.620 --> 02:25:15.300
with that much power and you are complicit in what happened,

02:25:15.300 --> 02:25:18.300
which they were like, come on,

02:25:18.300 --> 02:25:19.980
there's no way that they didn't know

02:25:19.980 --> 02:25:21.380
that this was happening.

02:25:21.380 --> 02:25:26.300
So like to not stand up and take responsibility,

02:25:27.540 --> 02:25:29.220
I think it's bad leadership.

02:25:29.220 --> 02:25:32.380
Do you understand why Epstein was able to,

02:25:34.980 --> 02:25:36.060
outside of MIT,

02:25:36.060 --> 02:25:38.860
he was able to make a lot of friends

02:25:38.860 --> 02:25:41.020
with a lot of powerful people?

02:25:41.020 --> 02:25:42.540
Does that make sense to you?

02:25:42.540 --> 02:25:45.460
Why was he able to get in these rooms,

02:25:45.460 --> 02:25:47.940
befriend these people,

02:25:47.940 --> 02:25:50.740
befriend people that I don't know personally,

02:25:50.740 --> 02:25:55.300
but I think a lot of them indirectly I know

02:25:55.300 --> 02:25:59.460
as being good people, smart people.

02:25:59.460 --> 02:26:02.980
Why would they let Jeffrey Epstein into their office,

02:26:02.980 --> 02:26:04.980
have a discussion with them?

02:26:04.980 --> 02:26:07.740
What do you understand about human nature from that?

02:26:07.740 --> 02:26:12.500
Well, so I never met Epstein or I mean,

02:26:13.500 --> 02:26:15.820
I've met some of the people who interacted with him,

02:26:15.820 --> 02:26:20.140
but I was never like, I never saw him in action.

02:26:20.140 --> 02:26:23.180
I don't know how charismatic he was or what that was,

02:26:23.180 --> 02:26:27.780
but I do think that sometimes the simple answer

02:26:27.780 --> 02:26:29.140
is the more likely one.

02:26:29.140 --> 02:26:31.500
And from my understanding,

02:26:31.500 --> 02:26:34.340
what he would do is he was kind of a social grifter,

02:26:34.340 --> 02:26:37.980
like, you know those people who will,

02:26:37.980 --> 02:26:40.380
you must get this because you're famous.

02:26:40.380 --> 02:26:43.500
You must get people coming to you and being like,

02:26:43.500 --> 02:26:45.780
oh, I know your friends so and so,

02:26:45.780 --> 02:26:48.100
in order to get cred with you.

02:26:50.460 --> 02:26:54.020
I think he just convinced some people

02:26:54.020 --> 02:26:57.740
who were trusted in a network

02:26:57.740 --> 02:27:01.540
that he was a great guy and that whatever,

02:27:01.540 --> 02:27:02.780
I think at that point,

02:27:02.820 --> 02:27:04.820
because at that point he had had like,

02:27:04.820 --> 02:27:08.900
what, a conviction prior, but it was a one-off thing.

02:27:08.900 --> 02:27:11.020
It wasn't clear that there was this other thing

02:27:11.020 --> 02:27:12.540
that was that.

02:27:12.540 --> 02:27:14.460
And most people probably don't check.

02:27:14.460 --> 02:27:15.700
Yeah, and most people don't check,

02:27:15.700 --> 02:27:17.140
like you're at an event, you meet this guy.

02:27:17.140 --> 02:27:18.460
I don't know, maybe people do check

02:27:18.460 --> 02:27:20.100
when they're that powerful and wealthy,

02:27:20.100 --> 02:27:21.860
or maybe they don't, I have no idea, no.

02:27:21.860 --> 02:27:23.700
They're just stupid.

02:27:23.700 --> 02:27:25.740
I mean, they're not like,

02:27:28.140 --> 02:27:30.980
all right, does anyone check anything about me?

02:27:30.980 --> 02:27:33.700
Because I've walked into some of the richest

02:27:33.700 --> 02:27:35.740
and most powerful people in the world,

02:27:35.740 --> 02:27:39.460
and nobody asks questions like, who the fuck is this guy?

02:27:41.620 --> 02:27:43.100
Nobody asks those questions.

02:27:43.100 --> 02:27:44.140
It's interesting.

02:27:45.260 --> 02:27:48.140
I would think there would be more security or something.

02:27:48.140 --> 02:27:49.420
There really isn't.

02:27:49.420 --> 02:27:53.340
I think a lot of it has to do, well, my hope is,

02:27:53.340 --> 02:27:55.300
in my case, has to do with people can sense

02:27:55.300 --> 02:27:56.860
that this is a good person,

02:27:56.860 --> 02:28:00.820
but if that's the case, then they can surely,

02:28:01.660 --> 02:28:05.180
then a human being can use charisma to infiltrate.

02:28:05.180 --> 02:28:06.020
Yeah.

02:28:06.020 --> 02:28:07.500
Just being, just saying the right things.

02:28:07.500 --> 02:28:08.700
And once you have people vouching for you

02:28:08.700 --> 02:28:12.460
within that type of network, like once you, yeah,

02:28:12.460 --> 02:28:14.900
once you have someone powerful vouching for you

02:28:14.900 --> 02:28:18.860
who someone else trusts, then, you know, you're in.

02:28:22.100 --> 02:28:25.620
So how do you avoid something like that?

02:28:25.620 --> 02:28:27.300
If you're at MIT, if you're at Harvard,

02:28:27.300 --> 02:28:29.220
if you're in any of these institutions?

02:28:29.220 --> 02:28:31.700
Well, I mean, first of all, you have to do your homework

02:28:31.700 --> 02:28:33.660
before you take money from someone.

02:28:34.980 --> 02:28:38.500
Like, I think that's required.

02:28:38.500 --> 02:28:40.540
But I think, you know, I think Joey did do his homework.

02:28:40.540 --> 02:28:41.740
I think he did.

02:28:41.740 --> 02:28:44.620
And I think at the time that he took money,

02:28:44.620 --> 02:28:47.380
there was the one conviction and not like the later thing.

02:28:47.380 --> 02:28:50.860
And I think that the story at that time was that

02:28:52.660 --> 02:28:55.580
he didn't know she was underage and blah, blah,

02:28:55.580 --> 02:28:56.860
or whatever, it was a mistake.

02:28:56.860 --> 02:28:59.140
And Joey always believed in redemption for people.

02:29:00.100 --> 02:29:02.420
And that people can change and that they can genuinely

02:29:02.420 --> 02:29:05.300
regret and like, learn and move on.

02:29:05.300 --> 02:29:06.540
And he was a big believer in that.

02:29:06.540 --> 02:29:09.100
So I could totally see him being like,

02:29:09.100 --> 02:29:12.660
well, I'm not gonna exclude him because of this thing.

02:29:12.660 --> 02:29:14.900
And because other people are vouching for him.

02:29:16.420 --> 02:29:19.900
And just to be clear, we're now talking about

02:29:19.900 --> 02:29:22.500
the set of people who I think Joey belonged to,

02:29:22.500 --> 02:29:24.540
who did not like go to the island

02:29:24.540 --> 02:29:26.100
and have sex with underage girls.

02:29:26.100 --> 02:29:29.060
Because that's a whole other set of people who like,

02:29:29.980 --> 02:29:32.300
were powerful and like were part of that network

02:29:32.300 --> 02:29:34.420
and who knew and participated.

02:29:34.420 --> 02:29:38.220
And so like, I distinguish between people who got taken in,

02:29:38.220 --> 02:29:40.100
who didn't know that that was happening,

02:29:40.100 --> 02:29:41.580
and people who knew.

02:29:41.580 --> 02:29:43.820
I wonder what the different circles look like.

02:29:43.820 --> 02:29:46.420
So like, people that went to the island

02:29:47.540 --> 02:29:49.540
and didn't do anything, didn't see anything,

02:29:49.540 --> 02:29:50.900
didn't know about anything.

02:29:51.820 --> 02:29:53.580
Versus the people that did something.

02:29:53.580 --> 02:29:56.820
And then there's people who heard rumors maybe.

02:29:56.820 --> 02:29:58.100
And what do you do with rumors?

02:29:58.300 --> 02:30:01.700
Isn't there people that heard rumors

02:30:01.700 --> 02:30:03.620
about Bill Cosby for the longest time?

02:30:04.580 --> 02:30:08.260
For like, for the longest, like whenever that happened,

02:30:08.260 --> 02:30:10.420
like all these people came out of the woodwork,

02:30:10.420 --> 02:30:11.820
like everybody kind of knew.

02:30:13.340 --> 02:30:15.740
I mean, it's like, all right,

02:30:15.740 --> 02:30:17.380
so what are you supposed to do with the rumors?

02:30:17.380 --> 02:30:20.620
Like what, I think the other way to put it

02:30:20.620 --> 02:30:22.300
is red flags, as you were saying.

02:30:22.300 --> 02:30:25.300
Yeah, and like, I can tell you that those circles,

02:30:25.340 --> 02:30:28.220
like there were red flags without me even hearing

02:30:28.220 --> 02:30:30.060
any rumors about anything ever.

02:30:30.060 --> 02:30:34.900
Like I was already like, there are not a lot of women here,

02:30:34.900 --> 02:30:36.220
which is a bad sign.

02:30:37.740 --> 02:30:40.020
Isn't there a lot of places where there's not a lot of women

02:30:40.020 --> 02:30:42.580
and that doesn't necessarily mean it's a bad sign?

02:30:42.580 --> 02:30:44.700
There are, if it's like a pipeline problem

02:30:44.700 --> 02:30:49.220
where it's like, I don't know,

02:30:49.220 --> 02:30:51.900
technology law clinic that only gets like male lawyers

02:30:51.900 --> 02:30:56.900
because there's not a lot of women applicants in the pool.

02:30:56.980 --> 02:30:58.980
But there's some aspect of this situation

02:30:58.980 --> 02:31:00.780
that like there should be more women here.

02:31:00.780 --> 02:31:03.140
Oh yeah, yeah.

02:31:05.740 --> 02:31:10.740
You've, actually I'd love to ask you about this

02:31:11.180 --> 02:31:15.700
because you have strong opinions about Richard Stallman.

02:31:18.100 --> 02:31:20.460
Is that, do you still have those strong opinions?

02:31:20.500 --> 02:31:23.260
Look, all I need to say is that he met my friend

02:31:23.260 --> 02:31:25.020
who's a law professor.

02:31:25.020 --> 02:31:25.940
Yeah.

02:31:25.940 --> 02:31:28.260
She shook his hand and he licked her arm

02:31:28.260 --> 02:31:29.460
from wrist to elbow

02:31:29.460 --> 02:31:31.980
and it certainly wasn't appropriate at that time.

02:31:33.260 --> 02:31:38.180
What about if you're like an incredibly weird person?

02:31:38.180 --> 02:31:41.100
Okay, that's a good question because obviously

02:31:41.100 --> 02:31:45.340
there's a lot of neurodivergence at MIT and everywhere

02:31:45.340 --> 02:31:48.420
and obviously like we need to accept

02:31:48.420 --> 02:31:49.900
that people are different,

02:31:49.900 --> 02:31:52.900
that people don't understand social conventions the same way.

02:31:52.900 --> 02:31:54.740
But one of the things that I've learned

02:31:54.740 --> 02:31:59.460
about neurodivergence is that women are often

02:32:02.300 --> 02:32:05.500
expected or taught to mask their neurodivergence

02:32:05.500 --> 02:32:10.460
and kind of fit in and men are accommodated and excused.

02:32:10.460 --> 02:32:15.220
And I don't think that being neurodivergent

02:32:15.220 --> 02:32:17.220
gives you a license to be an asshole.

02:32:17.820 --> 02:32:21.980
You can be a weird person and you can still learn

02:32:21.980 --> 02:32:24.780
that it's not okay to lick someone's arm.

02:32:24.780 --> 02:32:26.700
Yeah, it's a balance.

02:32:26.700 --> 02:32:29.340
Women should be allowed to be a little weirder

02:32:29.340 --> 02:32:31.060
and men should be less weird.

02:32:31.060 --> 02:32:34.780
Because I think there's, because you're one of the people

02:32:34.780 --> 02:32:37.060
I think tweeting that what made me,

02:32:37.060 --> 02:32:38.900
because I wanted to talk to Richard Stallman

02:32:38.900 --> 02:32:43.820
on the podcast about, because I didn't have a context,

02:32:43.820 --> 02:32:44.900
because I wanted to talk to him

02:32:44.900 --> 02:32:49.620
because he's free software, he's very weird

02:32:49.620 --> 02:32:52.660
in interesting good ways in the world of computer science.

02:32:53.700 --> 02:32:57.220
He's also weird in that when he gives a talk,

02:32:57.220 --> 02:33:00.580
he'll be like picking at his feet

02:33:00.580 --> 02:33:03.180
and eating the skin off his feet, right?

02:33:03.180 --> 02:33:05.380
He's known for these extremely kind of,

02:33:06.860 --> 02:33:07.980
how else do you put it?

02:33:07.980 --> 02:33:09.340
I don't know how to put it.

02:33:09.340 --> 02:33:13.140
But then there was something that happened to him

02:33:13.140 --> 02:33:16.220
in conversations on this thread related to Epstein,

02:33:17.060 --> 02:33:21.340
which I was torn about because I felt,

02:33:21.340 --> 02:33:24.820
it's similar to Joy Ito's,

02:33:24.820 --> 02:33:27.100
I felt he was maligned,

02:33:29.100 --> 02:33:32.060
people were looking for somebody to get angry at.

02:33:32.060 --> 02:33:34.620
So he was inappropriate, but the,

02:33:37.220 --> 02:33:39.340
I didn't like the cowardice more.

02:33:39.340 --> 02:33:44.220
I set aside his situation and we could discuss it,

02:33:44.220 --> 02:33:46.580
but the cowardice on MIT's part,

02:33:46.580 --> 02:33:48.060
and this is me saying it,

02:33:48.060 --> 02:33:50.780
about the way they treated that whole situation.

02:33:50.780 --> 02:33:52.820
Oh, they're always cowards about how they treat anything.

02:33:52.820 --> 02:33:54.340
They just try to make the problem go away.

02:33:54.340 --> 02:33:57.180
Yeah, so it was about, yeah,

02:33:57.180 --> 02:33:58.020
exactly making the conversation.

02:33:58.020 --> 02:34:01.340
That said, I think he should have left the mailing list.

02:34:01.340 --> 02:34:03.620
He shouldn't have been part of the mailing list.

02:34:03.620 --> 02:34:05.900
Well, that's probably true also.

02:34:05.900 --> 02:34:09.780
But I think what bothered me,

02:34:09.780 --> 02:34:12.420
what always bothers me in these mailing list situations

02:34:12.420 --> 02:34:13.940
or Twitter situations,

02:34:13.940 --> 02:34:18.940
like if you say something that's hurtful to people

02:34:19.340 --> 02:34:20.820
or makes people angry,

02:34:20.820 --> 02:34:22.660
and then people start yelling at you,

02:34:24.340 --> 02:34:26.220
maybe they shouldn't be yelling.

02:34:26.220 --> 02:34:28.740
Maybe they are yelling because again,

02:34:28.740 --> 02:34:31.540
you're the only point of power they have.

02:34:31.540 --> 02:34:34.500
Maybe it's okay that you're yelling,

02:34:34.500 --> 02:34:39.500
whatever it is, it's your response to that that matters.

02:34:40.020 --> 02:34:43.220
And I think that I just have a lot of respect for people

02:34:45.300 --> 02:34:49.140
who can say, oh, people are angry.

02:34:50.060 --> 02:34:51.100
There's a reason they're angry.

02:34:51.100 --> 02:34:52.500
Let me find out what that reason is

02:34:52.500 --> 02:34:53.780
and learn more about it.

02:34:54.860 --> 02:34:56.460
It doesn't mean that I'm wrong.

02:34:56.460 --> 02:34:58.140
It doesn't mean that I'm bad.

02:34:58.140 --> 02:35:00.220
It doesn't mean that I'm ill-intentioned,

02:35:00.220 --> 02:35:02.420
but why are they angry?

02:35:02.420 --> 02:35:03.700
I wanna understand.

02:35:03.980 --> 02:35:06.540
And then once you understand,

02:35:06.540 --> 02:35:09.980
you can respond again with integrity and say,

02:35:09.980 --> 02:35:12.100
actually I stand by what I said, here's why.

02:35:12.100 --> 02:35:14.460
Or you can say, actually I listened

02:35:14.460 --> 02:35:16.100
and here's some things I learned.

02:35:17.220 --> 02:35:19.980
That's the kind of response I wanna see from people.

02:35:19.980 --> 02:35:22.340
And people like Stalman do not respond that way.

02:35:22.340 --> 02:35:25.700
They just like go into battle.

02:35:25.700 --> 02:35:28.780
Right, like where it's obvious you didn't listen.

02:35:28.780 --> 02:35:30.460
Yeah, no interest in listening.

02:35:30.460 --> 02:35:32.460
Honestly, that's to me as bad

02:35:32.460 --> 02:35:34.300
as the people who just apologize

02:35:34.300 --> 02:35:36.940
just because they are trying to make the problem go away.

02:35:36.940 --> 02:35:37.940
Of course.

02:35:37.940 --> 02:35:41.220
Right, so like both are bad.

02:35:41.220 --> 02:35:43.540
A good apology has to include

02:35:43.540 --> 02:35:46.300
understanding what you did wrong.

02:35:46.300 --> 02:35:48.620
And in part, standing up for the things

02:35:48.620 --> 02:35:50.060
you think you did right.

02:35:50.060 --> 02:35:51.660
Yeah, if there are those things, yeah.

02:35:51.660 --> 02:35:54.180
Finding and then, but you have to give,

02:35:55.180 --> 02:35:56.140
you have to acknowledge,

02:35:56.140 --> 02:35:58.820
you have to like give that hard hit to the ego

02:35:58.820 --> 02:36:00.500
that says I did something wrong.

02:36:00.540 --> 02:36:03.220
Yeah, definitely Richard Stalman is not somebody

02:36:03.220 --> 02:36:05.860
who is capable of that kind of thing

02:36:05.860 --> 02:36:08.260
or hasn't given evidence of that kind of thing.

02:36:09.220 --> 02:36:11.660
But that was also, even just your tweet,

02:36:11.660 --> 02:36:13.380
I had to do a lot of thinking like,

02:36:15.260 --> 02:36:17.620
different people from different walks of life

02:36:17.620 --> 02:36:19.740
see red flags in different things.

02:36:19.740 --> 02:36:20.700
Yeah.

02:36:20.700 --> 02:36:24.540
And so, things I find

02:36:25.260 --> 02:36:30.260
as a man, non-threatening and hilarious

02:36:32.020 --> 02:36:33.900
are not necessarily,

02:36:35.300 --> 02:36:36.740
doesn't mean that they're,

02:36:37.780 --> 02:36:41.660
aren't like deeply hurtful to others.

02:36:41.660 --> 02:36:44.140
And I don't mean that in a social justice warrior way,

02:36:44.140 --> 02:36:46.140
but in a real way,

02:36:46.140 --> 02:36:49.260
like people really have different experiences.

02:36:49.260 --> 02:36:52.260
So I have to like really put things into context.

02:36:53.220 --> 02:36:56.740
I have to kind of listen to what people are saying,

02:36:56.740 --> 02:36:58.820
put aside the emotion of what they're,

02:36:58.820 --> 02:37:00.780
emotion with what you're saying,

02:37:00.780 --> 02:37:04.820
and try to keep the facts of their experience

02:37:04.820 --> 02:37:05.860
and learn from it.

02:37:05.860 --> 02:37:07.140
And because it's not just about

02:37:07.140 --> 02:37:08.620
the individual experience either.

02:37:08.620 --> 02:37:10.540
It's not like, oh, you know,

02:37:10.540 --> 02:37:13.860
my friend didn't have a sense of humor about being licked.

02:37:13.860 --> 02:37:17.100
It's that she's been metaphorically licked,

02:37:17.100 --> 02:37:18.940
you know, 57 times that week

02:37:18.940 --> 02:37:21.260
because she's an attractive law professor

02:37:21.300 --> 02:37:22.460
and she doesn't get taken to.

02:37:22.460 --> 02:37:25.300
And so like men walk through the world

02:37:25.300 --> 02:37:29.260
and it's impossible for them to even understand

02:37:29.260 --> 02:37:33.100
what it's like to have a different experience of the world.

02:37:33.100 --> 02:37:35.580
And that's why it's so important to listen to people

02:37:35.580 --> 02:37:37.140
and believe people

02:37:37.140 --> 02:37:39.220
and believe that they're angry for a reason.

02:37:39.220 --> 02:37:40.540
Maybe you don't like their tone.

02:37:40.540 --> 02:37:42.380
Maybe you don't like that they're angry at you.

02:37:42.380 --> 02:37:44.100
Maybe you get defensive about that.

02:37:44.100 --> 02:37:45.900
Maybe you think that they should, you know,

02:37:45.900 --> 02:37:47.140
explain it to you,

02:37:48.380 --> 02:37:50.540
but believe that they're angry for a reason

02:37:50.540 --> 02:37:51.660
and try to understand it.

02:37:51.660 --> 02:37:53.260
Yeah, there's a deep truth there

02:37:54.180 --> 02:37:57.020
and an opportunity for you to become a better person.

02:37:59.740 --> 02:38:01.780
Can I ask you a question?

02:38:01.780 --> 02:38:03.940
Haven't you been doing that for two hours?

02:38:05.940 --> 02:38:06.860
Three hours now.

02:38:08.500 --> 02:38:11.300
Let me ask you about Ghislaine Maxwell.

02:38:11.300 --> 02:38:13.900
She's been saying that she's an innocent victim.

02:38:15.500 --> 02:38:18.060
Is she an innocent victim

02:38:18.060 --> 02:38:23.060
or is she evil and equally responsible like Jeffrey Epstein?

02:38:23.620 --> 02:38:27.620
Now I'm asking far away from any MIT things and more,

02:38:27.620 --> 02:38:29.620
just your sense of the whole situation.

02:38:29.620 --> 02:38:30.860
I haven't been following it

02:38:30.860 --> 02:38:33.180
so I don't know the facts of the situation

02:38:33.180 --> 02:38:37.460
and like what is now like known to be her role in that.

02:38:37.460 --> 02:38:39.500
If I were her, clearly I'm not,

02:38:39.500 --> 02:38:40.340
but if I were her,

02:38:40.340 --> 02:38:42.860
I wouldn't be going around saying I'm an innocent victim.

02:38:42.860 --> 02:38:43.980
I would say,

02:38:46.180 --> 02:38:47.540
maybe she's, I don't know what she's saying.

02:38:47.540 --> 02:38:48.460
Again, like I don't know.

02:38:48.460 --> 02:38:50.820
She was controlled by Jeffrey.

02:38:50.820 --> 02:38:52.700
Is she saying this as part of a legal case

02:38:52.700 --> 02:38:55.780
or is she saying this as like a PR thing?

02:38:55.780 --> 02:38:56.620
Well,

02:38:58.820 --> 02:39:00.940
PR, but it's not just her.

02:39:00.940 --> 02:39:03.220
It's her whole family believes this.

02:39:03.220 --> 02:39:07.060
There's a whole effort that says like that,

02:39:08.540 --> 02:39:09.620
how should I put it?

02:39:09.620 --> 02:39:11.220
I believe they believe it.

02:39:11.220 --> 02:39:12.860
So in that sense, it's not PR.

02:39:14.020 --> 02:39:16.140
I believe the family,

02:39:16.140 --> 02:39:18.260
basically the family is saying that

02:39:19.540 --> 02:39:22.060
she's a good, she's a really good human being.

02:39:22.060 --> 02:39:23.940
Well, I think everyone is a good human being.

02:39:23.940 --> 02:39:25.380
I know it's a controversial opinion,

02:39:25.380 --> 02:39:27.820
but I think everyone

02:39:30.140 --> 02:39:32.460
is a good human being.

02:39:32.460 --> 02:39:35.180
There's no evil people.

02:39:35.180 --> 02:39:39.220
There's people who do bad things

02:39:39.220 --> 02:39:41.420
and who behave in ways that harm others

02:39:41.420 --> 02:39:43.100
and I think we should always hold people

02:39:43.100 --> 02:39:44.620
accountable for that.

02:39:44.620 --> 02:39:45.780
But holding someone accountable

02:39:46.420 --> 02:39:48.340
doesn't mean saying that they're evil.

02:39:48.340 --> 02:39:51.220
Yeah, actually those people usually think

02:39:51.220 --> 02:39:52.900
they're doing good.

02:39:52.900 --> 02:39:55.460
Yeah, I mean, aside from, I don't know,

02:39:55.460 --> 02:39:58.660
maybe sociopaths are specifically trying

02:39:58.660 --> 02:40:02.060
to harm people, but I think most people

02:40:02.060 --> 02:40:04.700
are trying to do their best.

02:40:04.700 --> 02:40:07.140
And if they're not doing their best,

02:40:07.140 --> 02:40:09.060
it's because there's some impediment

02:40:09.060 --> 02:40:10.820
or something in their past.

02:40:11.740 --> 02:40:14.980
So I genuinely don't believe in good and evil people,

02:40:14.980 --> 02:40:18.420
but I do believe in harmful and not harmful actions.

02:40:18.420 --> 02:40:22.140
And so I don't know, I don't care.

02:40:22.140 --> 02:40:23.620
Yeah, she's a good person,

02:40:23.620 --> 02:40:25.860
but if she contributed to harm,

02:40:25.860 --> 02:40:27.660
then she needs to be accountable for that.

02:40:27.660 --> 02:40:28.980
That's my position.

02:40:28.980 --> 02:40:30.420
I don't know what the facts of the matter are.

02:40:30.420 --> 02:40:32.500
It seems like she was pretty close to the situation,

02:40:32.500 --> 02:40:34.420
so it doesn't seem very believable

02:40:34.420 --> 02:40:36.540
that she was a victim, but I don't know.

02:40:36.540 --> 02:40:38.980
I wish I have met Epstein,

02:40:38.980 --> 02:40:40.580
because something tells me he would just be

02:40:40.580 --> 02:40:43.140
a regular person, a charismatic person,

02:40:43.180 --> 02:40:46.940
like anybody else, and that's a very dark reality

02:40:46.940 --> 02:40:49.100
that we don't know which among us,

02:40:50.220 --> 02:40:52.620
what each of us are hiding in the closet.

02:40:54.700 --> 02:40:58.100
That's a really tough thing to deal with,

02:40:58.100 --> 02:41:01.660
because then you can put your trust into some people,

02:41:01.660 --> 02:41:03.660
and they can completely betray that trust

02:41:03.660 --> 02:41:05.380
and in the process destroy you.

02:41:06.460 --> 02:41:09.260
Which there's a lot of people that interacted with Epstein

02:41:10.220 --> 02:41:15.220
that now have to, I mean, if they're not destroyed by it,

02:41:15.540 --> 02:41:20.540
then their whole, like, the ground on which

02:41:20.580 --> 02:41:25.580
they stand ethically has crumbled, at least in part.

02:41:27.380 --> 02:41:31.980
And I'm sure you and I have interacted with people

02:41:31.980 --> 02:41:34.740
without knowing it who are bad people.

02:41:34.740 --> 02:41:36.780
As I always tell my four-year-old,

02:41:36.780 --> 02:41:38.380
people who have done bad things.

02:41:38.540 --> 02:41:39.660
He's always talking about bad guys,

02:41:39.660 --> 02:41:42.260
and I'm trying to move him towards,

02:41:42.260 --> 02:41:44.940
they're just people who make bad choices.

02:41:44.940 --> 02:41:46.980
Yeah, that's really powerful, actually.

02:41:46.980 --> 02:41:48.340
That's really important to remember,

02:41:48.340 --> 02:41:50.340
because that means you have compassion

02:41:50.340 --> 02:41:51.620
towards all human beings.

02:41:53.060 --> 02:41:54.900
Do you have hope for the future of MIT,

02:41:54.900 --> 02:41:57.060
the future of Media Lab in this context?

02:41:57.900 --> 02:41:59.980
So, Dava Newman is now at the helm.

02:41:59.980 --> 02:42:00.820
I'm going to talk to her.

02:42:00.820 --> 02:42:03.060
I talked to her previously, I'll talk to her again.

02:42:03.060 --> 02:42:03.900
She's great.

02:42:03.900 --> 02:42:04.740
I love her.

02:42:04.740 --> 02:42:06.460
Yeah, she's great.

02:42:06.460 --> 02:42:07.300
I don't know if she,

02:42:07.580 --> 02:42:11.060
I don't know if she knew the whole situation

02:42:11.060 --> 02:42:14.700
when she started, because the situation

02:42:14.700 --> 02:42:17.620
went beyond just the Epstein scandal.

02:42:17.620 --> 02:42:20.340
A bunch of other stuff happened at the same time.

02:42:21.180 --> 02:42:22.900
Some of it's not public,

02:42:24.500 --> 02:42:28.500
but what I was personally going through at that time.

02:42:28.500 --> 02:42:30.220
So, the Epstein thing happened,

02:42:30.220 --> 02:42:34.260
I think, was it August or September 2019?

02:42:34.500 --> 02:42:39.500
It was somewhere around late summer in June 2019.

02:42:40.820 --> 02:42:43.180
So, I'm a research scientist at MIT.

02:42:43.180 --> 02:42:45.020
You are too, right?

02:42:45.020 --> 02:42:49.420
And I always have had various supervisors over the years,

02:42:49.420 --> 02:42:51.300
and they've just basically let me do what I want,

02:42:51.300 --> 02:42:52.620
which has been great.

02:42:52.620 --> 02:42:55.420
But I had a supervisor at the time,

02:42:55.420 --> 02:43:00.300
and he called me into his office for a regular check-in.

02:43:00.300 --> 02:43:03.420
In June of 2019, I reported to MIT

02:43:03.460 --> 02:43:08.060
that my supervisor had grabbed me,

02:43:08.060 --> 02:43:12.340
pulled me into a hug, wrapped his arms around my waist,

02:43:12.340 --> 02:43:16.620
and started massaging my hip and trying to kiss me,

02:43:16.620 --> 02:43:18.580
kiss my face, kiss me near the mouth,

02:43:19.500 --> 02:43:21.980
and said literally the words,

02:43:21.980 --> 02:43:24.740
don't worry, I'll take care of your career.

02:43:27.220 --> 02:43:31.700
And that experience was really interesting

02:43:31.700 --> 02:43:35.340
because I was very indignant.

02:43:35.340 --> 02:43:38.780
I was like, he can't do that to me.

02:43:38.780 --> 02:43:39.940
Doesn't he know who I am?

02:43:39.940 --> 02:43:42.220
And I was like, this is the me too era.

02:43:42.220 --> 02:43:44.980
And I naively thought that when I reported that,

02:43:44.980 --> 02:43:46.380
it would get taken care of.

02:43:46.380 --> 02:43:47.340
And then I had to go through

02:43:47.340 --> 02:43:49.100
the whole reporting process at MIT,

02:43:49.100 --> 02:43:52.940
and I learned a lot about how institutions

02:43:52.940 --> 02:43:55.260
really handle those things internally,

02:43:56.260 --> 02:43:59.340
particularly situations where I couldn't provide evidence

02:43:59.340 --> 02:44:00.420
that it happened.

02:44:00.420 --> 02:44:01.660
I had no reason to lie about it,

02:44:01.660 --> 02:44:02.620
but I had no evidence.

02:44:02.620 --> 02:44:06.220
And so I was going through that,

02:44:06.220 --> 02:44:09.460
and that was another experience for me

02:44:09.460 --> 02:44:13.060
where there's so many people in the institution

02:44:14.060 --> 02:44:16.420
who really believe in protecting the institution

02:44:16.420 --> 02:44:17.900
at all costs.

02:44:17.900 --> 02:44:19.140
And there's only a few people

02:44:19.140 --> 02:44:21.380
who care about doing the right thing.

02:44:21.380 --> 02:44:24.740
And one of them resigned.

02:44:24.740 --> 02:44:27.700
Now there's even less of them left, so.

02:44:28.940 --> 02:44:31.020
So what'd you learn from that?

02:44:31.540 --> 02:44:35.020
Where's the source, if you have hope for this institution

02:44:35.020 --> 02:44:38.300
that I think you love, at least in part?

02:44:39.220 --> 02:44:41.380
I love the idea of MIT.

02:44:41.380 --> 02:44:42.220
I love the idea.

02:44:42.220 --> 02:44:43.220
I love the research body.

02:44:43.220 --> 02:44:44.340
I love a lot of the faculty.

02:44:44.340 --> 02:44:45.900
I love the students.

02:44:45.900 --> 02:44:46.940
I love the energy.

02:44:46.940 --> 02:44:48.100
I love it all.

02:44:48.100 --> 02:44:51.300
I think the administration suffers from the same problems

02:44:51.300 --> 02:44:54.980
as any institutional,

02:44:54.980 --> 02:44:58.420
any leadership of an institution that is large,

02:44:58.420 --> 02:45:03.420
which is that they've become risk-averse,

02:45:04.500 --> 02:45:06.300
like you mentioned.

02:45:06.300 --> 02:45:08.180
They care about PR.

02:45:08.180 --> 02:45:12.540
The only ways to get their attention

02:45:12.540 --> 02:45:14.140
or change their minds about anything

02:45:14.140 --> 02:45:16.340
are to threaten the reputation of the institute

02:45:16.340 --> 02:45:17.700
or to have a lot of money.

02:45:18.780 --> 02:45:21.380
That's the only way to have power at the institute.

02:45:23.220 --> 02:45:26.420
Yeah, I don't think they have a lot of integrity

02:45:26.420 --> 02:45:29.500
or believe in ideas or even have a lot of connection

02:45:29.500 --> 02:45:32.500
to the research body and the people who are really,

02:45:32.500 --> 02:45:33.460
because it's so weird.

02:45:33.460 --> 02:45:36.420
You have this amazing research body of people

02:45:36.420 --> 02:45:37.900
pushing the boundaries of things

02:45:37.900 --> 02:45:41.020
who aren't afraid to, there's the hacker culture,

02:45:42.620 --> 02:45:44.060
and then you have the administration

02:45:44.060 --> 02:45:45.660
and they're really like,

02:45:48.220 --> 02:45:50.340
protect the institution at all costs.

02:45:50.340 --> 02:45:52.260
Yeah, there's a disconnect, right?

02:45:52.260 --> 02:45:53.100
Complete disconnect.

02:45:53.100 --> 02:45:54.340
I wonder if that was always there,

02:45:54.340 --> 02:45:57.580
if it just kinda slowly grows over time,

02:45:57.580 --> 02:45:59.940
a disconnect between the administration and the faculty.

02:45:59.940 --> 02:46:03.340
I think it grew over time is what I've heard.

02:46:03.340 --> 02:46:05.660
I mean, I've been there for 11 years now.

02:46:09.420 --> 02:46:11.140
I don't know if it's gotten worse during my time,

02:46:11.140 --> 02:46:13.940
but I've heard from people who've been there longer

02:46:13.940 --> 02:46:15.540
that it didn't, like,

02:46:15.540 --> 02:46:18.420
MIT didn't used to have a general counsel's office.

02:46:18.420 --> 02:46:20.860
They didn't used to have all of this corporate stuff

02:46:20.860 --> 02:46:23.660
and then they had to create it as they got bigger

02:46:24.340 --> 02:46:28.820
in the era where such things are, I guess, deemed necessary.

02:46:28.820 --> 02:46:30.860
See, I believe in the power of individuals

02:46:30.860 --> 02:46:33.060
to, like, overthrow the thing.

02:46:33.060 --> 02:46:36.820
So, like, just a really good president of MIT

02:46:36.820 --> 02:46:38.900
or certain people in the administration

02:46:38.900 --> 02:46:40.260
can reform the whole thing,

02:46:40.260 --> 02:46:44.540
because the culture is still there of, like,

02:46:44.540 --> 02:46:48.740
I think everybody remembers that MIT

02:46:48.740 --> 02:46:50.980
is about the students and the faculty.

02:46:50.980 --> 02:46:51.820
Do they, though?

02:46:51.820 --> 02:46:53.660
Because, I don't know,

02:46:53.660 --> 02:46:56.060
I've had a lot of conversations that have been shocking

02:46:56.060 --> 02:46:57.660
with, like, senior administration.

02:46:57.660 --> 02:46:59.740
They think the students are children.

02:46:59.740 --> 02:47:01.220
They call them kids.

02:47:01.220 --> 02:47:03.340
It's like, these are the smartest people.

02:47:03.340 --> 02:47:05.820
They're way smarter than you,

02:47:05.820 --> 02:47:07.420
and you're so dismissive of that.

02:47:07.420 --> 02:47:11.940
But those individuals, I'm saying, like, the capacity,

02:47:11.940 --> 02:47:15.380
like, the aura of the place

02:47:15.380 --> 02:47:18.980
still values the students and the faculty.

02:47:18.980 --> 02:47:22.020
Like, I'm not, I'm being awfully poetic about it,

02:47:22.020 --> 02:47:27.020
but what I mean is the administration is the froth

02:47:27.740 --> 02:47:31.020
at the top of the, like, the waves, the surface.

02:47:31.020 --> 02:47:35.860
Like, they can be removed, and new life can be brought in

02:47:35.860 --> 02:47:38.700
that would keep to the spirit of the place.

02:47:38.700 --> 02:47:40.700
Who decides on who to bring in?

02:47:40.700 --> 02:47:42.060
Who's hiring? It's bottom-up.

02:47:42.060 --> 02:47:45.020
Oh, I see, I see.

02:47:45.020 --> 02:47:47.420
But I do think, ultimately,

02:47:47.420 --> 02:47:50.820
especially in the era of social media and so on,

02:47:53.580 --> 02:47:55.700
faculty and students have more and more power.

02:47:55.700 --> 02:47:57.820
Just more and more of a voice, I suppose.

02:47:57.820 --> 02:48:00.340
I hope so, I really do.

02:48:00.340 --> 02:48:02.660
I don't see MIT going away anytime soon,

02:48:02.660 --> 02:48:05.380
and, like, I also don't think it's a terrible place at all.

02:48:05.380 --> 02:48:07.100
Yeah, it's an amazing place, and it's a,

02:48:07.100 --> 02:48:09.660
but there's different trajectories it can take.

02:48:09.660 --> 02:48:10.900
Yeah.

02:48:10.900 --> 02:48:14.020
And, like, and that has to do with a lot of things,

02:48:14.020 --> 02:48:19.020
including, is it stays, even if we talk about robotics,

02:48:22.300 --> 02:48:25.500
it could be the capital of the world in robotics.

02:48:25.500 --> 02:48:29.420
But currently, if you want to be doing the best AI work

02:48:29.420 --> 02:48:33.460
in the world, you're gonna go to Google, or Facebook,

02:48:34.540 --> 02:48:37.060
or Tesla, or Apple, or so on.

02:48:37.060 --> 02:48:39.260
You're not gonna be at MIT.

02:48:40.220 --> 02:48:45.220
And so that has to do, I think that basically has to do

02:48:45.220 --> 02:48:50.220
with not allowing the brilliance

02:48:51.460 --> 02:48:53.020
of the researchers to flourish.

02:48:54.060 --> 02:48:55.340
Yeah, people say it's about money,

02:48:55.340 --> 02:48:56.660
but I don't think it's about that at all.

02:48:56.660 --> 02:49:00.620
Like, sometimes you have more freedom

02:49:00.620 --> 02:49:02.860
and can work on more interesting things in companies.

02:49:02.860 --> 02:49:05.060
That's really where they lose people.

02:49:05.100 --> 02:49:09.780
Yeah, and sometimes the freedom in all ways,

02:49:11.060 --> 02:49:12.940
which is why it's heartbreaking to get, like,

02:49:12.940 --> 02:49:14.300
people like Richard Stallman,

02:49:14.300 --> 02:49:15.900
there's such an interesting line,

02:49:15.900 --> 02:49:18.940
because, like, Richard Stallman's a gigantic weirdo

02:49:19.860 --> 02:49:22.900
that crossed lines he shouldn't have crossed, right?

02:49:22.900 --> 02:49:25.620
But we don't want to draw too many lines.

02:49:26.900 --> 02:49:28.100
This is the tricky thing.

02:49:28.100 --> 02:49:30.940
There are different types of lines, in my opinion.

02:49:30.940 --> 02:49:34.580
But yes, your opinion, you have strong lines you hold to,

02:49:34.580 --> 02:49:37.380
but then if administration listens to every line,

02:49:38.260 --> 02:49:40.900
there's also power in drawing a line.

02:49:42.220 --> 02:49:47.220
Like, and there's, it becomes like a little drug.

02:49:47.500 --> 02:49:49.180
You have to find the right balance.

02:49:49.180 --> 02:49:52.740
Licking somebody's arm is never appropriate.

02:49:52.740 --> 02:49:57.740
I think the biggest aspect there is not owning it,

02:49:57.780 --> 02:49:59.180
learning from it, growing from it,

02:49:59.180 --> 02:50:01.780
from the perspective of Stallman or people like that.

02:50:02.700 --> 02:50:05.100
Back when it happened, like, understanding,

02:50:05.100 --> 02:50:06.980
seeing the right, being empathetic,

02:50:06.980 --> 02:50:10.620
seeing the fact that this was, like, totally inappropriate.

02:50:11.660 --> 02:50:14.340
Like, not when that particular act,

02:50:14.340 --> 02:50:16.860
but everything that led up to it, too.

02:50:16.860 --> 02:50:19.020
No, I think there are different kinds of lines.

02:50:19.020 --> 02:50:19.860
I think there are,

02:50:22.060 --> 02:50:25.780
so Stallman crossed lines that essentially

02:50:25.780 --> 02:50:28.620
excluded a bunch of people and created an environment

02:50:28.620 --> 02:50:30.860
where there are brilliant minds

02:50:30.860 --> 02:50:32.900
that we never got the benefit of

02:50:32.900 --> 02:50:36.580
because he made things feel gross

02:50:36.580 --> 02:50:38.660
or even unsafe for people.

02:50:38.660 --> 02:50:40.900
There are lines that you can cross

02:50:40.900 --> 02:50:44.340
where you're challenging an institution to,

02:50:47.100 --> 02:50:49.020
like, I don't think he was intentionally trying

02:50:49.020 --> 02:50:54.020
to cross a line or maybe he didn't care.

02:50:54.420 --> 02:50:56.460
There are lines that you can cross intentionally

02:50:56.460 --> 02:50:59.380
to move something forward or to do the right thing.

02:50:59.380 --> 02:51:00.820
Like, when MIT was like,

02:51:00.820 --> 02:51:03.860
you can't put an all-gender restroom in the media lab

02:51:03.860 --> 02:51:07.060
because, like, something permits whatever,

02:51:07.060 --> 02:51:08.620
and Joey did it anyway.

02:51:08.620 --> 02:51:10.140
That's a line you can cross

02:51:10.140 --> 02:51:11.860
to make things actually better for people,

02:51:11.860 --> 02:51:13.980
and the line you're crossing is some arbitrary,

02:51:13.980 --> 02:51:17.900
stupid rule that people who don't wanna take the risk

02:51:17.900 --> 02:51:21.580
are like, you know what I mean?

02:51:21.580 --> 02:51:23.460
No, ultimately, I think the thing you said

02:51:23.460 --> 02:51:28.460
is cross lines in a way that doesn't alienate others.

02:51:31.340 --> 02:51:33.500
So, for example, me wearing,

02:51:33.500 --> 02:51:37.020
I started for a while wearing a suit often at MIT,

02:51:37.020 --> 02:51:38.780
which sounds counterintuitive,

02:51:38.780 --> 02:51:43.780
but that's actually, people always looked at me weird for that.

02:51:44.460 --> 02:51:46.020
MIT created this culture,

02:51:46.020 --> 02:51:47.860
specifically the people I was working with.

02:51:47.860 --> 02:51:48.940
Like, nobody wears suits.

02:51:48.940 --> 02:51:49.780
Maybe the business school does.

02:51:49.780 --> 02:51:50.620
Yeah, we don't trust the suits.

02:51:50.620 --> 02:51:51.620
We don't trust the suits.

02:51:51.620 --> 02:51:53.580
I was like, fuck you, I'm wearing a suit.

02:51:53.580 --> 02:51:54.900
Nice.

02:51:54.900 --> 02:51:55.740
See, that I like.

02:51:55.740 --> 02:51:57.060
But that's not really hurting anybody, right?

02:51:57.060 --> 02:51:58.540
Exactly.

02:51:58.540 --> 02:52:01.220
It's challenging people's perceptions.

02:52:01.220 --> 02:52:03.540
It's doing something that you wanna do.

02:52:03.540 --> 02:52:04.380
Yeah.

02:52:04.380 --> 02:52:05.980
But it's not hurting people.

02:52:05.980 --> 02:52:08.740
Yeah, and that particular thing was,

02:52:08.740 --> 02:52:10.900
yeah, it was hurting people.

02:52:10.900 --> 02:52:11.740
It's a good line.

02:52:11.740 --> 02:52:13.900
It's a good line to, like,

02:52:16.140 --> 02:52:20.380
hurting, ultimately, the people that you want to flourish.

02:52:20.380 --> 02:52:21.220
Yeah. Yeah.

02:52:22.460 --> 02:52:26.100
You tweeted a picture of a pumpkin spice Greek yogurt

02:52:27.140 --> 02:52:29.900
and asked grounds for divorce.

02:52:29.900 --> 02:52:30.860
Yes, no.

02:52:30.860 --> 02:52:32.420
So let me ask you,

02:52:32.420 --> 02:52:34.940
what's the key to a successful relationship?

02:52:34.940 --> 02:52:35.900
Oh my God.

02:52:35.900 --> 02:52:37.260
A good couples therapist?

02:52:40.020 --> 02:52:42.820
What went wrong with a pumpkin spice Greek yogurt?

02:52:42.820 --> 02:52:43.820
What's exactly wrong?

02:52:43.820 --> 02:52:45.260
Is it the pumpkin?

02:52:45.260 --> 02:52:46.100
Is it the Greek?

02:52:46.100 --> 02:52:46.940
I don't understand.

02:52:46.940 --> 02:52:48.580
I stared at that tweet for a while.

02:52:48.580 --> 02:52:49.580
I grew up in Europe,

02:52:49.580 --> 02:52:53.420
so I don't understand the pumpkin spice in everything craze

02:52:53.420 --> 02:52:55.620
that they do every autumn here.

02:52:55.620 --> 02:52:58.580
Like, I understand that it might be good in some foods,

02:52:58.580 --> 02:53:01.020
but they just put it in everything.

02:53:01.020 --> 02:53:03.180
And it doesn't belong in Greek yogurt.

02:53:05.100 --> 02:53:07.220
I mean, I was just being humorous.

02:53:07.220 --> 02:53:09.780
I ate one of those yogurts and actually tasted pretty good.

02:53:09.780 --> 02:53:10.620
Yeah, exactly.

02:53:11.940 --> 02:53:14.660
I think part of the success of a good marriage

02:53:14.660 --> 02:53:18.700
is, like, giving each other a hard time humorously

02:53:18.700 --> 02:53:19.860
for things like that.

02:53:21.300 --> 02:53:22.900
Is there a broader lesson?

02:53:22.900 --> 02:53:25.580
Because you guys seem to have a really great marriage

02:53:25.580 --> 02:53:26.780
from the external.

02:53:26.780 --> 02:53:29.660
I mean, every marriage looks good from the external.

02:53:29.660 --> 02:53:31.260
Every, I think, yeah.

02:53:33.260 --> 02:53:35.380
That's not true, but yeah, I get it.

02:53:35.380 --> 02:53:36.220
Okay, right.

02:53:36.220 --> 02:53:38.060
That's not true.

02:53:38.060 --> 02:53:40.580
No, but like, relationships are hard.

02:53:40.580 --> 02:53:42.300
Relationships with anyone are hard,

02:53:42.300 --> 02:53:45.340
and especially because people evolve and change,

02:53:45.340 --> 02:53:47.420
and you have to make sure there's space

02:53:47.420 --> 02:53:49.380
for both people to evolve and change together.

02:53:49.380 --> 02:53:52.860
And I think one of the things that I really liked

02:53:52.860 --> 02:53:56.260
about our marriage vows was,

02:53:56.260 --> 02:53:58.460
I remember before we got married,

02:53:58.460 --> 02:54:01.780
Greg, at some point, got kind of nervous,

02:54:01.780 --> 02:54:04.500
and he was like, it's such a big commitment

02:54:04.500 --> 02:54:06.620
to commit to something for life.

02:54:06.620 --> 02:54:09.420
And I was like, we're not committing to this for life.

02:54:09.420 --> 02:54:11.540
And he was like, we're not?

02:54:11.540 --> 02:54:12.940
And I'm like, no.

02:54:12.940 --> 02:54:15.180
We're committing to being part of a team

02:54:15.180 --> 02:54:17.580
and doing what's best for the team.

02:54:17.580 --> 02:54:20.620
If what's best for the team is to break up, we'll break up.

02:54:20.620 --> 02:54:22.580
Like, I don't believe in this,

02:54:22.580 --> 02:54:25.500
like, we have to do this for our whole lives.

02:54:25.500 --> 02:54:28.500
And that really resonated with him too, so, yeah.

02:54:30.900 --> 02:54:32.420
Did you put in the vows?

02:54:32.420 --> 02:54:33.740
Yeah, yeah, that was our vows,

02:54:33.740 --> 02:54:35.820
like that we're just, we're gonna be a team.

02:54:35.820 --> 02:54:37.300
You're a team and do what's right for the team?

02:54:37.300 --> 02:54:38.420
Yeah, yeah.

02:54:40.580 --> 02:54:42.380
That's very like Michael Jordan view.

02:54:45.180 --> 02:54:47.540
Did you guys get like married in the desert,

02:54:47.540 --> 02:54:50.700
like November rain style with slash playing or?

02:54:52.020 --> 02:54:53.860
You don't have to answer that.

02:54:53.860 --> 02:54:55.100
I'm not good at these questions.

02:54:55.100 --> 02:54:56.060
Okay.

02:54:56.060 --> 02:54:57.940
You've brought up marriage like eight times.

02:54:57.940 --> 02:55:01.460
Are you trying to hint something on the podcast?

02:55:01.460 --> 02:55:04.420
I don't, yeah, I have an announcement to make.

02:55:04.420 --> 02:55:05.380
No, what?

02:55:05.380 --> 02:55:06.420
I don't know.

02:55:06.420 --> 02:55:11.420
It just seems like a good metaphor for, why would?

02:55:12.060 --> 02:55:16.300
It felt like a good metaphor for, in a bunch of cases,

02:55:16.300 --> 02:55:20.660
for the marriage industrial complex, I remember that.

02:55:20.660 --> 02:55:23.420
And, oh, people complaining.

02:55:23.420 --> 02:55:27.060
It just seemed like marriage is one of the things

02:55:27.060 --> 02:55:30.260
that always surprises me, because I wanna get married.

02:55:30.260 --> 02:55:31.080
You do?

02:55:31.080 --> 02:55:31.920
Yeah, I do.

02:55:31.920 --> 02:55:33.980
And then I listened to like friends of mine

02:55:33.980 --> 02:55:35.820
that complain, not all.

02:55:35.820 --> 02:55:37.940
I like guys, I really like guys

02:55:37.940 --> 02:55:39.580
that don't complain about their marriage.

02:55:39.580 --> 02:55:41.300
It's such a cheap.

02:55:42.020 --> 02:55:44.220
It's such a cheap release valve.

02:55:44.220 --> 02:55:46.820
That's bitching about anything, honestly.

02:55:46.820 --> 02:55:48.780
That's just like, it's too easy.

02:55:48.780 --> 02:55:52.460
But especially, bitch about the sports team

02:55:52.460 --> 02:55:55.380
or the weather if you want, but about somebody

02:55:55.380 --> 02:55:57.180
that you're dedicating your life to,

02:55:58.300 --> 02:55:59.820
if you bitch about them,

02:56:01.020 --> 02:56:03.980
you're going to see them as a lesser being also.

02:56:03.980 --> 02:56:06.700
You don't think so, but you're going to decrease the value

02:56:06.700 --> 02:56:10.300
you have, I personally believe over time,

02:56:10.300 --> 02:56:13.360
you're not going to appreciate the magic of that person.

02:56:14.420 --> 02:56:15.380
I think, anyway.

02:56:15.380 --> 02:56:18.460
But it's like, I just notice this a lot,

02:56:18.460 --> 02:56:23.100
that people are married and they will whine about,

02:56:23.100 --> 02:56:25.620
like the wife, whatever.

02:56:27.060 --> 02:56:30.500
It's part of the culture to comment in that way.

02:56:30.500 --> 02:56:33.020
I think women do the same thing about the husband.

02:56:33.020 --> 02:56:36.060
He never does this, or he's a goof,

02:56:36.060 --> 02:56:38.780
he's incompetent at this or that, whatever.

02:56:38.780 --> 02:56:39.620
There's a kind of-

02:56:39.660 --> 02:56:44.340
There's those tropes, like, oh, husbands never do acts,

02:56:44.340 --> 02:56:48.020
and wives are, I think those do a disservice to everyone,

02:56:48.020 --> 02:56:50.380
it's just disrespectful to everyone involved.

02:56:50.380 --> 02:56:52.100
Yeah, but it happens.

02:56:52.100 --> 02:56:54.580
I brought that up as an example of something

02:56:54.580 --> 02:56:57.580
that people actually love, but they complain about,

02:56:57.580 --> 02:57:00.500
because for some reason, that's more fun to do,

02:57:00.500 --> 02:57:02.540
is complain about stuff.

02:57:02.540 --> 02:57:05.020
And so that's what with Clippy or whatever, right?

02:57:05.020 --> 02:57:07.940
So you complain about it, but you actually love it.

02:57:07.980 --> 02:57:09.980
It's just a good metaphor that, you know.

02:57:11.740 --> 02:57:13.020
What was I gonna ask you?

02:57:14.460 --> 02:57:15.300
Oh, you,

02:57:18.180 --> 02:57:19.380
your hamster died.

02:57:20.420 --> 02:57:22.740
When I was like, eight.

02:57:22.740 --> 02:57:23.580
You miss her?

02:57:24.620 --> 02:57:25.460
Beige.

02:57:27.420 --> 02:57:30.980
What's the closest relationship you've had with a pet?

02:57:30.980 --> 02:57:31.820
Not the one?

02:57:33.980 --> 02:57:37.820
What pet or robot have you loved the most?

02:57:37.820 --> 02:57:38.860
Most in your life?

02:57:42.020 --> 02:57:46.060
I think my first pet was a goldfish named Bob,

02:57:46.060 --> 02:57:48.460
and he died immediately, and that was really sad.

02:57:50.460 --> 02:57:53.580
I think it was really attached to Bob and Nancy,

02:57:53.580 --> 02:57:55.020
my goldfish.

02:57:55.020 --> 02:57:57.300
We got new Bobs, and then Bob kept dying,

02:57:57.300 --> 02:57:58.900
and we got new Bobs.

02:57:58.900 --> 02:58:00.180
Nancy just kept living.

02:58:02.300 --> 02:58:04.700
So it was very replaceable.

02:58:04.700 --> 02:58:07.140
Yeah, I was young.

02:58:08.780 --> 02:58:10.580
It was easy to.

02:58:10.580 --> 02:58:13.660
Do you think there will be a time when the robot,

02:58:13.660 --> 02:58:14.900
like in the movie Her,

02:58:14.900 --> 02:58:17.860
be something we fall in love with romantically?

02:58:17.860 --> 02:58:18.700
Oh, yeah.

02:58:18.700 --> 02:58:20.100
Oh, for sure.

02:58:20.100 --> 02:58:22.540
Yeah. At scale, like with a lot of people.

02:58:24.300 --> 02:58:27.500
Romantically, I don't know if it's gonna happen at scale.

02:58:27.500 --> 02:58:30.980
I think we talked about this a little bit last time

02:58:30.980 --> 02:58:33.620
on the podcast, too, where I think we're just capable

02:58:33.620 --> 02:58:35.620
of so many different kinds of relationships.

02:58:35.740 --> 02:58:39.300
And actually, part of why I think marriage is so tough

02:58:39.300 --> 02:58:41.140
as a relationship is because we put

02:58:41.140 --> 02:58:44.380
so many expectations on it.

02:58:44.380 --> 02:58:46.740
Your partner has to be your best friend,

02:58:46.740 --> 02:58:48.660
and you have to be sexually attracted to them,

02:58:48.660 --> 02:58:51.180
and they have to be a good co-parent and a good roommate,

02:58:51.180 --> 02:58:55.300
and it's all the relationships at once that have to work.

02:58:56.260 --> 02:58:58.700
But normally, with other people,

02:58:58.700 --> 02:59:00.580
we have one type of relationship,

02:59:00.580 --> 02:59:03.540
or we have a different relationship to our dog,

02:59:03.540 --> 02:59:04.620
then we do to our neighbor,

02:59:04.620 --> 02:59:08.100
then we do to the person, someone, a coworker.

02:59:09.820 --> 02:59:11.780
I think that some people are gonna find

02:59:11.780 --> 02:59:14.660
romantic relationships with robots interesting.

02:59:15.500 --> 02:59:17.020
It might even be a widespread thing,

02:59:17.020 --> 02:59:19.820
but I don't think it's gonna replace

02:59:19.820 --> 02:59:21.340
human romantic relationships.

02:59:21.340 --> 02:59:24.460
I think it's just gonna be a separate type of thing.

02:59:25.940 --> 02:59:27.340
It's gonna be more narrow.

02:59:28.780 --> 02:59:31.140
More narrow, or even just something new

02:59:31.140 --> 02:59:33.220
that we haven't really experienced before.

02:59:33.740 --> 02:59:36.020
Having a crush on an artificial agent

02:59:36.020 --> 02:59:38.620
is a different type of fascination.

02:59:38.620 --> 02:59:39.460
I don't know.

02:59:39.460 --> 02:59:41.780
Do you think people would see that as cheating?

02:59:41.780 --> 02:59:43.300
I think people would.

02:59:43.300 --> 02:59:47.420
Well, I mean, the things that people feel threatened by

02:59:47.420 --> 02:59:50.220
in relationships are very many-fold, so.

02:59:50.220 --> 02:59:53.260
Yeah, that's just an interesting one,

02:59:53.260 --> 02:59:57.820
because maybe they'll be good,

02:59:57.820 --> 03:00:00.580
a little jealousy for the relationship.

03:00:00.580 --> 03:00:03.020
Maybe that'll be part of the couple's therapy,

03:00:03.820 --> 03:00:04.660
or whatever.

03:00:04.660 --> 03:00:06.060
I don't think jealousy.

03:00:06.060 --> 03:00:09.500
I mean, I think it's hard to avoid jealousy,

03:00:09.500 --> 03:00:12.060
but I think the objective is probably to avoid it.

03:00:12.060 --> 03:00:13.500
I mean, some people don't even get jealous

03:00:13.500 --> 03:00:15.100
when their partner sleeps with someone else.

03:00:15.100 --> 03:00:20.100
Like, there's polyamory, and I think there's just

03:00:20.860 --> 03:00:23.180
such a diversity of different ways

03:00:23.180 --> 03:00:26.340
that we can structure relationships or view them

03:00:26.340 --> 03:00:30.060
that this is just gonna be another one that we add.

03:00:30.060 --> 03:00:32.260
You dedicate your book to your dad.

03:00:32.260 --> 03:00:35.420
What did you learn about life from your dad?

03:00:35.420 --> 03:00:40.420
Oh man, my dad is, he's a great listener,

03:00:40.700 --> 03:00:45.700
and he is the best person I know

03:00:46.780 --> 03:00:50.820
at the type of cognitive empathy

03:00:50.820 --> 03:00:53.580
that's perspective-taking.

03:00:53.580 --> 03:00:57.580
So not like emotional, like crying empathy,

03:00:57.580 --> 03:01:01.940
but trying to see someone else's point of view

03:01:01.940 --> 03:01:03.940
and trying to put yourself in their shoes.

03:01:03.940 --> 03:01:07.740
And he really instilled that in me from an early age.

03:01:07.740 --> 03:01:09.940
And then he made me read a ton of science fiction,

03:01:09.940 --> 03:01:13.740
which probably led me down this path.

03:01:13.740 --> 03:01:15.660
Tell you how to be curious about the world

03:01:15.660 --> 03:01:17.060
and how to be open-minded.

03:01:17.060 --> 03:01:18.540
Yeah.

03:01:18.540 --> 03:01:19.380
Last question.

03:01:19.380 --> 03:01:22.940
What role does love play in the human condition?

03:01:22.940 --> 03:01:25.500
Since we've been talking about love and robots,

03:01:26.140 --> 03:01:31.140
how, and you're fascinated by social robotics.

03:01:33.500 --> 03:01:36.340
It feels like all of that operates in the landscape

03:01:36.340 --> 03:01:38.660
of something that we can call love.

03:01:38.660 --> 03:01:39.940
Love?

03:01:39.940 --> 03:01:43.580
Yeah, I think there are a lot of different kinds of love.

03:01:43.580 --> 03:01:46.500
I feel like it's, we need,

03:01:46.500 --> 03:01:48.300
I'm like, don't the Eskimos have

03:01:48.300 --> 03:01:49.780
all these different words for snow?

03:01:49.780 --> 03:01:53.140
We need more words to describe different types

03:01:53.140 --> 03:01:54.820
and kinds of love that we experience.

03:01:54.820 --> 03:01:56.500
But I think love is so important

03:01:56.500 --> 03:01:59.660
and I also think it's not zero sum.

03:01:59.660 --> 03:02:02.220
That's the really interesting thing about love

03:02:02.220 --> 03:02:07.220
is that I had one kid and I loved my first kid

03:02:07.460 --> 03:02:08.940
more than anything else in the world.

03:02:08.940 --> 03:02:10.900
And I was like, how can I have a second kid

03:02:10.900 --> 03:02:12.820
and then love that kid also?

03:02:12.820 --> 03:02:15.420
I'm never gonna love it as much as the first,

03:02:15.420 --> 03:02:16.940
but I love them both equally.

03:02:16.940 --> 03:02:19.340
It just like, my heart expanded.

03:02:19.340 --> 03:02:22.940
And so I think that people who are threatened

03:02:22.940 --> 03:02:27.220
by love towards artificial agents,

03:02:27.220 --> 03:02:30.460
they don't need to be threatened for that reason.

03:02:30.460 --> 03:02:33.180
Artificial agents will just, if done right,

03:02:33.180 --> 03:02:37.220
will just expand your capacity for love.

03:02:37.220 --> 03:02:38.140
I think so.

03:02:39.340 --> 03:02:41.100
I agree, beautifully put.

03:02:41.100 --> 03:02:43.100
Kate, this was awesome.

03:02:43.100 --> 03:02:44.580
I still didn't talk about half the things

03:02:44.580 --> 03:02:45.420
I wanted to talk about,

03:02:45.420 --> 03:02:47.620
but we're already like way over three hours.

03:02:47.620 --> 03:02:48.500
So thank you so much.

03:02:48.500 --> 03:02:50.580
I really appreciate you talking today.

03:02:50.580 --> 03:02:51.420
You're awesome.

03:02:51.420 --> 03:02:52.660
You're an amazing human being,

03:02:53.300 --> 03:02:55.980
a great roboticist, great writer now.

03:02:55.980 --> 03:02:57.340
It's an honor that you would talk with me.

03:02:57.340 --> 03:02:58.180
Thanks for doing it.

03:02:58.180 --> 03:02:59.000
Right back at you.

03:02:59.000 --> 03:02:59.840
Thank you.

03:03:00.620 --> 03:03:03.460
Thanks for listening to this conversation with Kate Darling.

03:03:03.460 --> 03:03:04.820
To support this podcast,

03:03:04.820 --> 03:03:07.620
please check out our sponsors in the description.

03:03:07.620 --> 03:03:11.700
And now let me leave you with some words from Maya Angelou.

03:03:11.700 --> 03:03:15.300
Courage is the most important of all the virtues

03:03:15.300 --> 03:03:16.700
because without courage,

03:03:16.700 --> 03:03:20.340
you can't practice any other virtue consistently.

03:03:20.340 --> 03:03:22.540
Thank you for listening and hope to see you

03:03:22.540 --> 03:03:24.220
next time.


WEBVTT

00:00.000 --> 00:04.120
And it turns out that if you train a planaria and then cut their heads off, the tail will

00:04.120 --> 00:07.720
regenerate a brand new brain that still remembers the original information.

00:07.720 --> 00:12.440
I think planaria hold the answer to pretty much every deep question of life.

00:12.440 --> 00:14.760
For one thing, they're similar to our ancestors.

00:14.760 --> 00:15.760
So they have true symmetry.

00:15.760 --> 00:16.760
They have a true brain.

00:16.760 --> 00:17.760
They're not like earthworms.

00:17.760 --> 00:19.000
They're, you know, they're much more advanced life form.

00:19.000 --> 00:21.960
They have lots of different internal organs, but they're these little, they're about,

00:21.960 --> 00:25.080
you know, maybe two centimeters in the centimeter to two in size.

00:25.080 --> 00:27.480
They have a head and a tail.

00:27.480 --> 00:29.760
And the first thing is planaria are immortal.

00:29.760 --> 00:31.080
So they do not age.

00:31.080 --> 00:32.720
There's no such thing as an old planarian.

00:32.720 --> 00:37.280
So that right there tells you that these theories of thermodynamic limitations of on lifespan

00:37.280 --> 00:38.280
are wrong.

00:38.280 --> 00:41.560
It's not, it's not that well over time of everything degrades, no planaria can keep

00:41.560 --> 00:46.280
it going for probably, you know, how long have they been around 400 million years, right?

00:46.280 --> 00:50.640
So these are the actual, so the planaria in our lab are actually in physical continuity

00:50.640 --> 00:55.120
with planaria that were here 400 million years ago.

00:55.120 --> 01:00.160
The following is a conversation with Michael Levin, one of the most fascinating and brilliant

01:00.160 --> 01:02.700
biologists I've ever talked to.

01:02.700 --> 01:08.520
He and his lab at Tufts University works on novel ways to understand and control complex

01:08.520 --> 01:11.720
pattern formation in biological systems.

01:11.720 --> 01:17.440
Andre Karpathy, a world-class AI researcher, is the person who first introduced me to Michael

01:17.440 --> 01:18.440
Levin's work.

01:18.440 --> 01:24.420
I bring this up because these two people make me realize that biology has a lot to teach

01:24.420 --> 01:30.460
us about AI and AI might have a lot to teach us about biology.

01:30.460 --> 01:32.260
This is the Lex Fridman podcast.

01:32.260 --> 01:35.900
To support it, please check out our sponsors in the description.

01:35.900 --> 01:41.260
And now, dear friends, here's Michael Levin.

01:41.260 --> 01:44.500
Embryogenesis is the process of building the human body from a single cell.

01:44.500 --> 01:49.840
I think it's one of the most incredible things that exists on earth from a single embryo.

01:49.840 --> 01:51.420
So how does this process work?

01:51.420 --> 01:52.420
Yeah, it is.

01:52.420 --> 01:53.660
It is an incredible process.

01:53.660 --> 01:57.780
I think it's maybe the most magical process there is.

01:57.780 --> 02:02.560
I think one of the most fundamentally interesting things about it is that it shows that each

02:02.560 --> 02:07.060
of us takes the journey from so-called just physics to mind, right?

02:07.060 --> 02:12.180
Because we all start life as a single quiescent, unfertilized oocyte, and it's basically

02:12.180 --> 02:13.180
a bag of chemicals.

02:13.180 --> 02:15.820
And you look at that and you say, okay, this is chemistry and physics.

02:15.820 --> 02:19.860
And then nine months and some years later, you have an organism with high level cognition

02:19.860 --> 02:23.700
and preferences and an inner life and so on.

02:23.700 --> 02:28.500
And what embryogenesis tells us is that that transformation from physics to mind is gradual.

02:28.500 --> 02:29.500
It's smooth.

02:29.500 --> 02:34.300
There is no special place where a lightning bolt says, boom, now you've gone from physics

02:34.300 --> 02:35.460
to true cognition.

02:35.460 --> 02:36.460
That doesn't happen.

02:36.460 --> 02:40.500
And so we can see in this process that the whole mystery, the biggest mystery of the

02:40.500 --> 02:43.620
universe basically, how you get mind from matter.

02:43.620 --> 02:45.740
From just physics in quotes.

02:45.740 --> 02:46.740
Yeah.

02:46.740 --> 02:48.820
So where's the magic into the thing?

02:48.820 --> 02:55.760
How do we get from information encoding DNA and make physical reality out of that information?

02:55.760 --> 02:59.460
So one of the things that I think is really important if we're going to bring in DNA

02:59.460 --> 03:05.220
into this picture is to think about the fact that what DNA encodes is the hardware of life.

03:05.220 --> 03:09.400
DNA contains the instructions for the kind of micro level hardware that every cell gets

03:09.400 --> 03:10.400
to play with.

03:10.400 --> 03:14.180
So all the proteins, all the signaling factors, the ion channels, all the cool little pieces

03:14.180 --> 03:16.940
of hardware that cells have, that's what's in the DNA.

03:16.940 --> 03:20.940
The rest of it is in so-called generic laws.

03:20.940 --> 03:22.420
And these are laws of mathematics.

03:22.420 --> 03:23.900
These are laws of computation.

03:23.900 --> 03:29.060
These are laws of physics, of all kinds of interesting things that are not directly in

03:29.060 --> 03:30.380
the DNA.

03:30.380 --> 03:36.260
And that process, I think the reason I always put just physics in quotes is because I don't

03:36.260 --> 03:37.900
think there is such a thing as just physics.

03:37.900 --> 03:42.100
I think that thinking about these things in binary categories like this is physics, this

03:42.100 --> 03:46.140
is true cognition, this is as if it's only faking, these kinds of things, I think that's

03:46.140 --> 03:47.140
what gets us in trouble.

03:47.140 --> 03:50.420
I think that we really have to understand that it's a continuum and we have to work

03:50.420 --> 03:53.780
up the scaling, the laws of scaling, and we can certainly talk about that.

03:53.780 --> 03:56.780
There's a lot of really interesting thoughts to be had there.

03:56.780 --> 04:01.580
So the physics is deeply integrated with the information.

04:01.580 --> 04:03.300
So the DNA doesn't exist on its own.

04:03.300 --> 04:10.540
The DNA is integrated in some sense in response to the laws of physics at every scale.

04:10.540 --> 04:13.540
The laws of the environment it exists in.

04:14.220 --> 04:16.460
Yeah, the environment and also the laws of the universe.

04:16.460 --> 04:23.380
I mean, the thing about the DNA is that once evolution discovers a certain kind of machine,

04:23.380 --> 04:28.500
that if the physical implementation is appropriate, it's sort of, and this is hard to talk about

04:28.500 --> 04:32.660
because we don't have a good vocabulary for this yet, but it's a very kind of platonic

04:32.660 --> 04:40.320
notion that if the machine is there, it pulls down interesting things that you do not have

04:40.520 --> 04:44.040
to evolve from scratch because the laws of physics give it to you for free.

04:44.040 --> 04:48.240
So just as a really stupid example, if you're trying to evolve a particular triangle, you

04:48.240 --> 04:51.160
can evolve the first angle and you evolve the second angle, but you don't need to evolve

04:51.160 --> 04:52.160
the third.

04:52.160 --> 04:53.160
You know what it is already.

04:53.160 --> 04:54.160
Now, why do you know?

04:54.160 --> 04:56.160
That's a gift for free from geometry in a particular space.

04:56.160 --> 04:57.680
You know what that angle has to be.

04:57.680 --> 05:01.520
And if you evolve an ion channel, which is ion channels are basically transistors, right?

05:01.520 --> 05:03.840
They're voltage gated current conductances.

05:03.840 --> 05:07.640
If you evolve that ion channel, you immediately get to use things like truth tables.

05:07.640 --> 05:08.640
You get logic functions.

05:08.760 --> 05:10.240
You don't have to evolve the logic function.

05:10.240 --> 05:11.560
You don't have to evolve a truth table.

05:11.560 --> 05:13.160
It doesn't have to be in the DNA.

05:13.160 --> 05:14.480
You get it for free, right?

05:14.480 --> 05:16.880
And the fact that if you have NAND gates, you can build anything you want.

05:16.880 --> 05:17.880
You get that for free.

05:17.880 --> 05:22.680
All you have to evolve is that first step, that first little machine that enables you

05:22.680 --> 05:24.040
to couple to those laws.

05:24.040 --> 05:26.440
And there's laws of adhesion and many other things.

05:26.440 --> 05:32.280
And this is all that interplay between the hardware that's set up by the genetics and

05:32.280 --> 05:33.720
the software that's built, right?

05:33.720 --> 05:37.880
The physiological software that basically does all the computation and the cognition

05:37.920 --> 05:42.840
and everything else is a real interplay between the information and the DNA and the laws of

05:42.840 --> 05:44.440
physics, of computation and so on.

05:44.640 --> 05:49.520
So is it fair to say, just like this idea that the laws of mathematics are discovered,

05:50.640 --> 05:55.280
they're latent within the fabric of the universe in that same way, the laws of biology are

05:55.280 --> 05:56.240
kind of discovered?

05:56.320 --> 06:00.080
Yeah, I think that's absolutely and it's probably not a popular view, but I think that's

06:00.080 --> 06:01.000
right on the money. Yeah.

06:01.320 --> 06:03.120
I think that's a really deep idea.

06:04.000 --> 06:13.400
Then embryogenesis is the process of revealing, of embodying, of manifesting these

06:13.400 --> 06:16.680
laws. You're not building the laws.

06:17.120 --> 06:20.720
You're just creating the capacity to reveal.

06:20.800 --> 06:25.440
Yes. I think, again, not the standard view of molecular biology by any means, but I

06:25.440 --> 06:26.400
think that's right on the money.

06:26.560 --> 06:29.520
I'll give you a simple example, you know, some of our latest work with the xenobots,

06:29.520 --> 06:33.080
right? So what we've done is to take some skin cells off of an early frog and

06:33.080 --> 06:35.600
embryo and basically ask about their plasticity.

06:35.600 --> 06:39.680
If we give you a chance to sort of reboot your multicellularity in a different context,

06:39.680 --> 06:43.480
what would you do? Because what you might assume by looking at the thing about

06:43.480 --> 06:45.400
embryogenesis is that it's super reliable, right?

06:45.400 --> 06:50.520
It's very robust and that really obscures some of its most interesting features.

06:50.520 --> 06:54.320
We get used to it. We get used to the fact that acorns make oak trees and frog eggs

06:54.320 --> 06:55.880
make frogs. And we say, well, what else is it going to make?

06:55.880 --> 06:57.080
That's what it, you know, that's what it makes.

06:57.080 --> 06:58.040
That's a standard story.

06:58.520 --> 07:04.040
But the reality is, and so you look at these skin cells and you say, well, what do they

07:04.040 --> 07:07.840
know how to do? Well, they know how to be a passive, boring, two-dimensional outer

07:07.840 --> 07:09.960
layer, keeping the bacteria from getting into the embryo.

07:09.960 --> 07:10.760
That's what they know how to do.

07:11.120 --> 07:15.440
Well, it turns out that if you take these skin cells and you remove the rest of the

07:15.440 --> 07:19.440
embryo, so you remove all of the rest of the cells and you say, well, you're by

07:19.440 --> 07:20.560
yourself now, what do you want to do?

07:20.800 --> 07:25.600
So what they do is they form this little, this multi little creature that runs around

07:25.600 --> 07:27.800
the dish. They have all kinds of incredible capacities.

07:27.800 --> 07:29.280
They navigate through mazes.

07:29.280 --> 07:32.640
They have various behaviors that they do both independently and together.

07:33.120 --> 07:38.800
They, they have a, basically they implement von Neumann's dream of self-replication

07:39.040 --> 07:42.200
because if you sprinkle a bunch of loose cells into the dish, what they do is they

07:42.200 --> 07:44.320
run around, they collect those cells into little piles.

07:44.560 --> 07:48.440
They sort of mush them together until those little piles become the next generation of

07:48.440 --> 07:52.440
xenobots. So you've got this machine that builds copies of itself from loose material

07:52.440 --> 07:53.200
in its environment.

07:53.440 --> 07:57.960
None of this are things that you would have expected from the frog genome.

07:57.960 --> 07:59.080
In fact, there's wild type.

07:59.080 --> 07:59.840
The genome is wild type.

07:59.840 --> 08:01.080
There's nothing wrong with their genetics.

08:01.320 --> 08:02.240
Nothing has been added.

08:02.240 --> 08:04.320
No nanomaterials, no genomic editing, nothing.

08:04.680 --> 08:08.520
And so what we have done there is engineered by subtraction.

08:08.520 --> 08:12.040
What you've read, what you've done is you removed the other cells that normally

08:12.040 --> 08:15.960
basically bully these cells into being skin cells and you find out that what they

08:15.960 --> 08:20.280
really want to do is, is to be this, they want their default behaviors to be a

08:20.320 --> 08:25.040
xenobot. But in vivo, in the embryo, they get told to be skinned by these other cell

08:25.040 --> 08:29.560
types. And so, so now, so now here comes this, this really interesting question that

08:29.560 --> 08:34.360
you just posed. When you ask where does the form of the tadpole and the frog come

08:34.360 --> 08:38.200
from, the standard answer is, well, it's, it's, it's a selection.

08:38.280 --> 08:42.040
So over, over millions of years, right, it's been shaped to, to produce the specific

08:42.040 --> 08:44.280
body with that's fit for froggy environments.

08:44.840 --> 08:46.480
Where does the shape of the xenobot come from?

08:46.800 --> 08:47.960
There's never been any xenobots.

08:47.960 --> 08:49.720
There's never been selection to be a good xenobot.

08:50.040 --> 08:52.960
These cells find themselves in the new environment in 48 hours.

08:53.000 --> 08:57.840
They figure out how to be an entirely different protoorganism with new capacities

08:57.840 --> 08:59.240
like kinematic self-replication.

08:59.240 --> 09:00.880
That's not how frogs or tadpoles replicate.

09:01.120 --> 09:03.600
We've made it impossible for them to replicate their normal way.

09:03.880 --> 09:06.000
Within a couple of days, these guys find a new way of doing it.

09:06.040 --> 09:07.800
That's not done anywhere else in the biosphere.

09:08.000 --> 09:11.600
Well, actually let's step back and define what are xenobots.

09:12.320 --> 09:16.000
So a xenobot is a self-assembling little protoorganism.

09:16.320 --> 09:17.840
It's also a biological robot.

09:18.040 --> 09:19.600
Those things are not distinct.

09:19.720 --> 09:20.960
It's a member of both classes.

09:21.680 --> 09:23.240
How much is it biology?

09:23.240 --> 09:24.680
How much is that robot?

09:25.400 --> 09:28.880
At this point, most of it is biology because what we're doing is we're

09:28.880 --> 09:34.120
discovering natural behaviors of these, of these, of the cells and also

09:34.120 --> 09:34.960
of the cell collectives.

09:35.240 --> 09:38.760
Now, one of the really important parts of this was that we're working together

09:38.760 --> 09:41.800
with Josh Bongard's group at University of Vermont, they're computer

09:41.800 --> 09:47.320
scientists do AI and they've basically been able to use an evolutionary, a

09:47.320 --> 09:51.200
simulated evolution approach to ask, how can we manipulate these cells?

09:51.200 --> 09:53.240
Give them signals, not rewire their DNA.

09:53.240 --> 09:55.320
So not hardware, but experiences signals.

09:55.560 --> 09:56.840
So can we remove some cells?

09:56.840 --> 09:57.760
Can we add some cells?

09:57.760 --> 10:00.480
Can we poke them in different ways to get them to do other things?

10:00.720 --> 10:03.320
So in the future, there's going to be, you know, we're, we're now, and this

10:03.320 --> 10:06.680
is, this is future on published work, but we're doing all sorts of interesting

10:06.680 --> 10:08.600
ways to reprogram them to new behaviors.

10:08.880 --> 10:11.400
But before you can start to reprogram these things, you have to understand

10:11.400 --> 10:13.560
what their innate capacities are.

10:13.880 --> 10:14.120
Okay.

10:14.120 --> 10:19.800
So that means engineering programming, you're engineering them in the future.

10:20.040 --> 10:24.920
And in some sense, the, the definition of a robot is something you in part

10:24.920 --> 10:28.440
engineer versus evolve.

10:28.760 --> 10:34.560
I mean, um, it's such a fuzzy definition anyway, in some sense, many of the

10:34.560 --> 10:37.000
organisms within our body are kinds of robots.

10:37.040 --> 10:37.760
Yes, yes.

10:38.000 --> 10:43.520
And I think robots is a weird line cause it's, we tend to see robots as

10:43.520 --> 10:46.760
the other, I think there will be a time in the future when there's going to be

10:47.600 --> 10:51.080
something akin to the civil rights movements for robots, but we'll talk

10:51.080 --> 10:53.000
about that later, perhaps anyway.

10:53.040 --> 10:57.680
Um, so how do you, can we just linger on it?

10:57.680 --> 10:59.120
How do you build a xenobot?

10:59.320 --> 11:00.720
What are we talking about here?

11:01.600 --> 11:08.880
From, from whence does it start and how does it become the glorious xenobot?

11:09.000 --> 11:09.280
Yeah.

11:09.560 --> 11:12.720
So just to take one step back, one of the things that, um, a lot of

11:12.720 --> 11:17.560
people, uh, get stuck on is they say, well, uh, you know, engineering requires

11:17.720 --> 11:22.480
new, uh, DNA circuits or it requires new nanomaterials, you know, what the

11:22.480 --> 11:26.920
thing is we are now moving from old school engineering, which used passive

11:26.920 --> 11:27.920
materials, right?

11:27.920 --> 11:30.520
That things that, you know, wood metal, things like this, that basically the

11:30.520 --> 11:32.760
only thing you could depend on is that they were going to keep their shape.

11:32.800 --> 11:33.120
That's it.

11:33.120 --> 11:34.000
They don't do anything else.

11:34.000 --> 11:37.440
You it's on you as an engineer to make them do everything they're going to do.

11:37.440 --> 11:40.400
And then there were active materials and now computation materials.

11:40.400 --> 11:41.360
This is a whole new era.

11:41.360 --> 11:42.840
These are agential materials.

11:43.120 --> 11:45.800
This is your, you're now collaborating with your substrate because

11:45.800 --> 11:47.080
your material has an agenda.

11:47.280 --> 11:49.960
These cells have, you know, billions of years of evolution.

11:49.960 --> 11:50.480
They have goals.

11:50.480 --> 11:51.200
They have preferences.

11:51.200 --> 11:52.840
They're not just going to sit where you put them.

11:52.880 --> 11:56.640
That's hilarious that you have to talk your material into keeping its shape.

11:56.640 --> 11:56.960
That's it.

11:56.960 --> 11:57.880
That is exactly right.

11:58.280 --> 11:59.160
That is exactly right.

11:59.480 --> 12:00.000
Stay there.

12:00.320 --> 12:03.640
It's like getting a bunch of cats or something and trying to

12:03.640 --> 12:04.920
organize the shape out of them.

12:04.960 --> 12:05.400
It's funny.

12:05.400 --> 12:08.440
We're on the same page here because in a paper, this is, this is currently, um,

12:08.520 --> 12:10.520
uh, just been accepted in nature by engineering.

12:10.720 --> 12:14.520
One of the figures I have is building a tower out of Legos versus dogs.

12:14.680 --> 12:14.920
Right.

12:14.920 --> 12:15.040
Yeah.

12:15.040 --> 12:16.080
So think about the difference, right?

12:16.080 --> 12:20.040
If you build out of Legos, you have full control over where it's going to go.

12:20.320 --> 12:23.560
But if somebody knocks it over, it's game over with the dogs.

12:23.600 --> 12:25.200
You cannot just come and stack them.

12:25.200 --> 12:26.120
They're not going to stay that way.

12:26.240 --> 12:28.640
But the good news is that if you train them, then somebody

12:28.640 --> 12:29.880
knocks it over, they'll get right back up.

12:30.120 --> 12:30.960
So it's all right.

12:30.960 --> 12:33.400
So as an engineer, what you really want to know is what can they

12:33.400 --> 12:35.040
depend on this thing to do, right?

12:35.040 --> 12:38.000
That's really, you know, a lot of people have definitions of robots as far as

12:38.040 --> 12:41.280
what they're made of or how they got here, you know, design versus evolve, whatever.

12:41.520 --> 12:42.640
I don't think any of that is useful.

12:42.640 --> 12:46.760
I think, I think as an engineer, what you want to know is how much can I depend

12:46.760 --> 12:49.320
on this thing to do when I'm not around to micromanage it?

12:49.320 --> 12:53.120
What level of, uh, what level of dependency can I, can I give this thing?

12:53.120 --> 12:54.280
How much agency does it have?

12:54.520 --> 12:56.280
Which then tells you what techniques do you use?

12:56.280 --> 12:57.480
So do you use micromanagement?

12:57.480 --> 12:58.720
Like you put everything where it goes.

12:58.880 --> 12:59.680
Do you train it?

12:59.840 --> 13:01.080
Do you give it signals?

13:01.080 --> 13:02.600
Do you try to convince it to do things, right?

13:02.600 --> 13:04.440
How much, you know, how intelligent is your substrate?

13:04.680 --> 13:07.800
And so now we're moving into this, uh, into this area where you're, you're,

13:07.800 --> 13:09.520
you're working with agential materials.

13:09.680 --> 13:10.640
That's a collaboration.

13:10.640 --> 13:12.600
That's not, that's not old, old style.

13:12.720 --> 13:13.720
What's the word you're using?

13:13.760 --> 13:14.440
Agential?

13:14.480 --> 13:14.960
Agential.

13:15.040 --> 13:15.200
Yeah.

13:15.200 --> 13:15.720
What's that mean?

13:15.800 --> 13:16.240
Agency.

13:16.240 --> 13:17.360
It comes from the word agency.

13:17.360 --> 13:21.840
So, so basically the material has agency, meaning that it has some, some level

13:21.840 --> 13:26.600
of obviously not human level, but some level of, uh, preferences, goals, memories,

13:26.600 --> 13:30.200
ability to remember things, to compute into the future, meaning anticipate,

13:30.440 --> 13:32.920
um, you know, when you're working with cells, they have all of that to

13:33.640 --> 13:34.480
various degrees.

13:34.840 --> 13:39.200
Is that empowering or limiting having material that has a mind of its own?

13:39.280 --> 13:39.800
Literally.

13:39.920 --> 13:40.840
I think it's both, right?

13:40.840 --> 13:45.400
So it raises difficulties because it means that if you, if you're using the old

13:45.400 --> 13:49.400
mindset, which is a linear, um, kind of extrapolation of what's going to happen,

13:49.560 --> 13:54.160
you're going to be surprised and shocked all the time because biology, uh, does

13:54.160 --> 13:56.480
not do what we linearly expect materials to do.

13:56.920 --> 13:59.160
On the other hand, it's massively liberating.

13:59.240 --> 14:03.200
And so in the following way, I've argued that advances in regenerative medicine

14:03.280 --> 14:07.640
require us to take advantage of this because what it means is that you can get

14:07.640 --> 14:10.400
the material to do things that you don't know how to micromanage.

14:10.560 --> 14:12.080
So just as a simple example, right?

14:12.080 --> 14:16.800
If you, if you, you had a rat and, uh, you wanted this rat to do a circus trick,

14:16.800 --> 14:20.840
put a ball in the little hoop, you can do it the micromanagement way, which is try

14:20.840 --> 14:23.280
to control every neuron and try to play the thing like a puppet, right?

14:23.280 --> 14:24.720
And maybe someday that'll be possible.

14:24.720 --> 14:26.320
Maybe, or you can train the rat.

14:26.720 --> 14:30.320
And this is why humanity for thousands of years before we knew any neuroscience,

14:30.320 --> 14:32.800
we had no idea what's behind, what's between the ears of any animal.

14:32.960 --> 14:36.280
We were able to train these animals because once you recognize the level

14:36.280 --> 14:40.160
of agency of a certain system, you can use appropriate techniques.

14:40.160 --> 14:43.080
If you know the currency of motivation, reward and punishment, you know, how

14:43.080 --> 14:46.240
smart it is, you know, what kinds of things it likes to do, you are searching

14:46.240 --> 14:50.560
a much more, much smoother, much nicer problem space than if you try to

14:50.560 --> 14:53.640
micromanage the thing and then regenerative medicine, when you're trying

14:53.640 --> 14:56.800
to get, um, let's say an arm to grow back or an eye to repair or sell birth

14:56.800 --> 15:00.800
defect or something, do you really want to be controlling tens of thousands

15:00.800 --> 15:04.680
of genes at each point to try to micromanage it, or do you want to

15:04.680 --> 15:08.680
find the high level modular controls that say, build an arm here?

15:08.720 --> 15:09.760
You already know how to build an arm.

15:09.760 --> 15:11.080
You did it before and do it again.

15:11.400 --> 15:14.920
So that's, I think it's, it's both, it's both difficult and it challenges

15:14.920 --> 15:18.640
us to develop new ways of engineering and it's, it's hugely empowering.

15:19.280 --> 15:19.480
Okay.

15:19.480 --> 15:23.000
So how do you do, I mean, maybe sticking with the metaphor of dogs

15:23.480 --> 15:30.080
and cats, uh, I presume you have to figure out the, find the dogs and, uh,

15:30.240 --> 15:37.240
dispose of the cats, um, because, you know, it's like the old herding cats is an issue.

15:37.240 --> 15:39.360
So you may be able to train dogs.

15:40.080 --> 15:44.480
I suspect you will not be able to train cats or if you do, you're

15:44.480 --> 15:45.840
never going to be able to trust them.

15:46.160 --> 15:52.440
So is there a way to figure out which material is amenable to herding?

15:52.640 --> 15:55.760
Is it in the lab work or is it in simulation?

15:56.120 --> 16:00.560
Right now it's largely in the lab because we are, our simulations do not capture

16:00.600 --> 16:03.960
yet the most interesting and powerful things about biology.

16:03.960 --> 16:08.600
So the simulation does what we're pretty good at simulating our, um, feed

16:08.600 --> 16:10.880
forward emergent types of things, right?

16:10.880 --> 16:15.080
So cellular automata, if you have simple rules and you sort of roll those forward

16:15.080 --> 16:18.720
for every, every agent or every cell in the simulation and complex things happen,

16:18.720 --> 16:21.720
you know, ant colony or, um, algorithms, things like that, we're, we're, we're

16:21.720 --> 16:22.920
good at that and that's, and that's fine.

16:22.920 --> 16:26.840
The difficulty with all of that is that it's incredibly hard to reverse.

16:26.960 --> 16:28.680
So this is a really hard inverse problem, right?

16:28.680 --> 16:31.080
If you look at a bunch of termites and they make a, you know, a thing with a

16:31.080 --> 16:33.800
single chimney and you say, well, I like it, but I'd like two chimneys.

16:34.560 --> 16:36.800
How do you change the rules of behavior for each termite?

16:36.800 --> 16:37.880
So they make two chimneys, right?

16:37.880 --> 16:41.200
Or, or if you say, here are a bunch of cells that are creating this kind of

16:41.200 --> 16:43.360
organism, I don't think that's optimal.

16:43.360 --> 16:45.160
I'd like to, to repair that birth defect.

16:45.360 --> 16:48.560
How do you control all the, all the individual low level rules, right?

16:48.560 --> 16:51.440
All the protein interactions and everything else, rolling it back from

16:51.440 --> 16:55.160
the anatomy that you want to the low level hardware rules is in general,

16:55.160 --> 16:57.800
intractable, it's a, it's an inverse problem that's generally not soluble.

16:58.440 --> 17:02.480
So, um, right now it's mostly in the lab because what we need to do is we need

17:02.480 --> 17:04.880
to understand how biology uses top down controls.

17:05.120 --> 17:09.960
So the idea is not, not bottom up emergence, but the idea of, um, things

17:09.960 --> 17:14.120
like a goal directed, uh, test operate exit kinds of loops where, where it's

17:14.120 --> 17:18.240
basically an error minimization function over a new space is not a space of gene

17:18.240 --> 17:20.080
expression, but for example, a space of anatomy.

17:20.480 --> 17:23.880
So just as a simple example, if you have, um, you have a salamander and it's got

17:23.880 --> 17:28.280
an arm, you can, you can amputate that arm anywhere along the length, it will

17:28.280 --> 17:30.480
grow exactly what's needed and then it stops.

17:30.480 --> 17:33.000
That's the most amazing thing about regeneration is that it stops.

17:33.000 --> 17:33.880
It knows when to stop.

17:34.080 --> 17:34.720
When does it stop?

17:34.720 --> 17:37.200
It stops when a correct salamander arm has been completed.

17:37.440 --> 17:38.440
So that tells you that's right.

17:38.440 --> 17:42.960
That's a, that's a, uh, a means ends kind of analysis where it has to

17:42.960 --> 17:45.440
know what the correct limb is supposed to look like, right?

17:45.440 --> 17:47.920
So it has a way to ascertain the current shape.

17:47.920 --> 17:51.040
It has a way to measure that Delta from, from what shape it's supposed to be.

17:51.240 --> 17:54.240
And then it will keep taking actions, meaning remodeling and growing and

17:54.240 --> 17:55.520
everything else until that's complete.

17:55.800 --> 17:58.560
So once you know that, and we've taken advantage of this in the lab to do some,

17:58.560 --> 18:02.080
some really wild things with, with both planaria and frog embryos and so on.

18:02.480 --> 18:05.760
Once you know that, um, you can start playing with that, uh,

18:05.800 --> 18:06.920
with that homeostatic cycle.

18:07.080 --> 18:09.880
You can ask, for example, well, how does it remember what the correct shape is?

18:09.880 --> 18:10.960
And can we mess with that memory?

18:11.080 --> 18:13.720
Can we give it a false memory of what the shape should be and let the cells

18:13.720 --> 18:16.960
build something else, or can we mess with the measurement apparatus, right?

18:17.000 --> 18:22.400
So it gives you, it gives you those kinds of, so, so, so the idea is to basically

18:22.480 --> 18:28.200
appropriate a lot of the, um, approaches and concepts from cognitive neuroscience

18:28.200 --> 18:32.280
and behavioral science into things that, uh, previously were taken to be dumb

18:32.280 --> 18:35.760
materials and you know, you get yelled at in class if you, if you, for being

18:35.760 --> 18:38.680
anthropomorphic, if you said, well, my cells want to do this and my cells want

18:38.680 --> 18:39.000
to do that.

18:39.320 --> 18:42.160
And I think, I think that's a, that's a major mistake that leaves a ton of

18:42.160 --> 18:43.320
capabilities on the table.

18:43.480 --> 18:47.840
So thinking about biologic systems as things that have memory, have almost

18:47.840 --> 18:57.560
something like cognitive ability, but I mean, how incredible is it, you know,

18:57.560 --> 19:02.320
that the salamander arm is being rebuilt, not with a dictator.

19:03.200 --> 19:05.120
It's kind of like the cellular automata system.

19:05.200 --> 19:07.520
All the individual workers are doing their own thing.

19:08.000 --> 19:13.000
So where's that, uh, top down signal that does the control coming from?

19:13.000 --> 19:14.760
Like, how can you find it?

19:15.280 --> 19:16.680
Like, why does it stop growing?

19:16.840 --> 19:18.280
How does it know the shape?

19:18.560 --> 19:20.040
How does it have memory of the shape?

19:20.400 --> 19:23.960
And how does it tell everybody to be like, whoa, slow down, we're done.

19:24.240 --> 19:29.520
So the first thing to think about, I think, is that there are no examples

19:29.680 --> 19:34.080
anywhere of, of a central dictator because in this, in this kind of science,

19:34.080 --> 19:36.480
because everything is made of parts.

19:36.520 --> 19:41.800
And so we, we, even though we, we feel as a unified central sort of intelligence

19:41.800 --> 19:45.520
and kind of point of, of cognition, we are a bag of neurons, right?

19:45.520 --> 19:47.680
We all intelligence is collective intelligence.

19:47.840 --> 19:51.960
There's this, this is important to kind of think about because a lot of people

19:51.960 --> 19:55.760
think, okay, there's real intelligence like me, and then there's collective

19:55.760 --> 19:59.400
intelligence, which is ants and flocks of birds and, you know, termites and

19:59.400 --> 20:02.720
things like that, and, and, you know, and, and, and maybe it's appropriate to

20:02.720 --> 20:06.200
think of them as a, as a, as a, as an individual, and maybe it's not a lot of

20:06.200 --> 20:08.720
people are skeptical about, about that and so on, but it's got, you've got to

20:08.720 --> 20:11.640
realize that we are not, there's, there's no such thing as this like

20:11.640 --> 20:13.520
indivisible diamond of intelligence.

20:13.520 --> 20:15.920
That's like this one central thing that's not made of parts.

20:15.920 --> 20:16.840
We are all made of parts.

20:17.240 --> 20:22.360
And so if, if you believe the, which I think is, is, is hard to, to get around

20:22.360 --> 20:26.200
that, that, that we in fact have a centralized set of goals and preferences

20:26.200 --> 20:27.840
and we plan and we do things and so on.

20:28.320 --> 20:31.760
You are already committed to the fact that a collection of cells is able to do

20:31.760 --> 20:33.560
this because we are a collection of cells.

20:33.600 --> 20:34.440
There's no getting around that.

20:34.840 --> 20:38.120
In our case, what we do is we navigate the three dimensional world and we have

20:38.120 --> 20:40.280
behavior is blowing my mind right now.

20:40.320 --> 20:41.920
Cause we are just a collection of cell.

20:41.920 --> 20:42.240
Oh yeah.

20:42.400 --> 20:42.640
Yeah.

20:42.840 --> 20:50.320
So when I'm moving this arm, I feel like I'm the central dictator of that action,

20:50.760 --> 20:52.560
but there's a lot of stuff going on.

20:52.800 --> 20:57.800
Like every, all, all the cells here are collaborating in some interesting way.

20:58.000 --> 21:00.560
They're getting signal from the central nervous system.

21:01.080 --> 21:03.840
Well, even the central nervous system is, is misleadingly named

21:03.840 --> 21:06.160
because it isn't really central.

21:06.160 --> 21:09.080
Again, it's, it's what, it's just a bunch of cells.

21:09.080 --> 21:10.160
I mean, all of it, right.

21:10.280 --> 21:14.000
There are no, there are no singular indivisible intelligences anywhere.

21:14.240 --> 21:18.400
We are all every, every example that we've ever seen is, is a collective of some,

21:18.440 --> 21:20.240
of something it's just that we're used to it.

21:20.240 --> 21:20.920
We're used to that.

21:20.920 --> 21:23.200
You know, we're used to, okay, this thing is kind of a single thing, but it's

21:23.200 --> 21:26.280
really not, you zoom in, you know what you see, you see a bunch of cells running around.

21:26.280 --> 21:31.120
And so is there some unifying, I mean, we're jumping around, but that something

21:31.160 --> 21:37.320
that you look as the, the, the bioelectrical signal versus the biochemical,

21:38.440 --> 21:47.200
the, um, the chemistry, the electricity, maybe the life is in that versus the cells.

21:47.840 --> 21:53.480
It's the, uh, there's, there's an orchestra playing and, uh, the

21:53.480 --> 21:56.640
resulting music is the dictator.

21:57.200 --> 21:58.320
That's not bad.

21:58.360 --> 22:01.200
Um, Dennis, that's Dennis Noble's, uh, kind of view of things.

22:01.200 --> 22:04.320
He has, he has two really good books where he talks about this musical analogy.

22:04.320 --> 22:04.480
Right.

22:04.480 --> 22:06.600
So, so I think that's, that's, that's, I like it.

22:06.720 --> 22:07.480
Um, I like it.

22:07.840 --> 22:08.600
Is it wrong?

22:08.600 --> 22:10.440
No, I don't think it's, no, I don't think it's wrong.

22:10.480 --> 22:12.240
Um, I don't, I don't think it's wrong.

22:12.240 --> 22:18.200
I think, I think the important thing about it is that we have to come to grips with

22:18.200 --> 22:25.200
the fact that a true, a true proper cognitive intelligence can still be made of parts.

22:25.200 --> 22:26.960
Those things are, and in fact, it has to be.

22:27.000 --> 22:29.560
And I, I think it's a real shame, but I see this all the time.

22:30.200 --> 22:33.320
When you have, uh, when you have a collective like this, whether it be a

22:33.320 --> 22:37.480
collective, a group of robots or, um, you know, a collection of cells or neurons

22:37.480 --> 22:42.920
or whatever, as soon as, as soon as we gain some insight into how it works, right.

22:43.000 --> 22:45.920
Meaning that, oh, I see in order to take you this action, here's the information

22:45.920 --> 22:49.760
that got processed via this chemical mechanism or whatever, immediately people

22:49.760 --> 22:52.120
say, oh, well then that's not real cognition.

22:52.120 --> 22:53.080
That's just physics.

22:53.280 --> 22:57.040
I think this is, this is fundamentally flawed because if you zoom into anything,

22:57.040 --> 22:57.760
what are you going to see?

22:57.760 --> 22:59.240
Of course, you're just going to see physics.

22:59.240 --> 23:00.400
What else could be underneath, right?

23:00.400 --> 23:01.320
That's not going to be fairy dust.

23:01.320 --> 23:04.080
It's going to be physics and chemistry, but that doesn't take away from the

23:04.080 --> 23:07.920
magic of the fact that there are certain ways to arrange that physics and chemistry

23:07.920 --> 23:12.520
and in particular, the bioelectricity, which, which I like a lot, uh, to give

23:12.520 --> 23:17.520
you an emergent collective with goals and preferences and memories and

23:17.520 --> 23:20.640
anticipations that do not belong to any of the subunits.

23:20.920 --> 23:23.920
So I think what we're getting into here, and we can talk about how this

23:23.920 --> 23:25.440
happens during embryogenesis and so on.

23:25.720 --> 23:30.600
What we're getting into is the origin of the set of a self with a big, with a

23:30.600 --> 23:34.040
capital S, so we ourselves, there are many other kinds of selves and we can

23:34.040 --> 23:36.880
tell some really interesting stories about where selves come from and

23:36.880 --> 23:38.080
how they become unified.

23:38.560 --> 23:38.760
Yeah.

23:38.760 --> 23:42.960
Is this the first, or at least humans tend to think that this is the, the

23:42.960 --> 23:48.840
level of which the self with a capital S is first born, but, uh, and we really

23:48.840 --> 23:54.720
don't want to see, um, human civilization or earth itself as one living organism.

23:54.840 --> 23:55.080
Yeah.

23:55.440 --> 23:57.120
That's very uncomfortable to us.

23:57.200 --> 23:57.560
It is.

23:57.720 --> 23:58.000
Yeah.

23:58.240 --> 23:59.920
But is, um, yeah.

23:59.920 --> 24:01.040
Where's the self born?

24:01.200 --> 24:02.760
We have to grow up past that.

24:02.800 --> 24:06.120
So what I like to do is, uh, well, I'll tell you two quick stories about that.

24:06.120 --> 24:07.600
Uh, I like to roll backwards.

24:07.600 --> 24:11.120
So, so as opposed to, so if you start and you say, okay, here's a paramecium

24:11.120 --> 24:13.800
and you see it, um, you know, it's a single cell organism, you see it doing

24:13.800 --> 24:17.520
various things and people will say, okay, I'm sure there's some chemical story

24:17.520 --> 24:18.880
to be told about how it's doing it.

24:18.880 --> 24:20.280
So that's not true cognition.

24:20.280 --> 24:20.440
Right.

24:20.440 --> 24:21.480
And people will argue about that.

24:22.080 --> 24:23.440
I like to work it backwards.

24:23.440 --> 24:27.120
I say, let's, let's, let's agree that you and I, as, as we sit here are

24:27.160 --> 24:30.040
examples of true cognition, if anything, is if there's anything that's true

24:30.040 --> 24:31.760
cognition, we are, we are examples of it.

24:32.120 --> 24:33.360
Now let's just roll back slowly.

24:33.360 --> 24:33.520
Right.

24:33.520 --> 24:36.520
So you roll back to the time when you were a small child and used to doing whatever.

24:36.880 --> 24:39.960
And then just sort of day by day, you roll, you roll back and eventually you

24:39.960 --> 24:42.000
become more or less that paramecium.

24:42.000 --> 24:44.320
And then, and then you sort of even below that, right.

24:44.320 --> 24:46.000
As a, as an unfertilized OSA.

24:46.360 --> 24:52.440
So it's no one has to my knowledge, no one has come up with any convincing

24:52.680 --> 24:57.280
discrete step at which my cognitive powers disappear, right?

24:57.360 --> 25:00.240
It just doesn't, the biology doesn't offer any specific step.

25:00.240 --> 25:02.880
It's, it's incredibly smooth and slow and continuous.

25:03.200 --> 25:07.720
And so I think this idea that it just sort of magically shows up at one point.

25:07.720 --> 25:11.320
And then, and then, you know, humans have true selves that don't exist elsewhere.

25:11.320 --> 25:14.440
I think it runs against everything we know about evolution, everything we

25:14.440 --> 25:15.680
know about developmental biology.

25:15.680 --> 25:17.440
These are all slow continuum.

25:17.840 --> 25:21.400
And the other really important story I want to tell is where embryos come from.

25:21.400 --> 25:22.400
So think about this for a second.

25:23.040 --> 25:24.000
Amniote embryo.

25:24.000 --> 25:27.560
So this is humans, birds, and so on, mammals and birds and so on.

25:27.760 --> 25:30.040
Imagine a flat disc of cells.

25:30.080 --> 25:31.560
So there's maybe 50,000 cells.

25:32.040 --> 25:35.640
And in that, so when you get an egg from a, from a fertilized, let's

25:35.640 --> 25:37.440
let's say you buy a fertilized egg from a farm, right?

25:37.640 --> 25:43.320
That, that egg will, will have about 50,000 cells in a, in a flat disc.

25:43.320 --> 25:45.000
It looks like a little, little tiny little frisbee.

25:45.680 --> 25:51.920
And in that flat disc, what'll happen is there'll be one, one set of cells will

25:52.640 --> 25:56.520
become, will become special and it will tell all the other cells, I'm, I'm

25:56.520 --> 25:58.280
going to be the head, you guys don't be the head.

25:58.520 --> 26:00.800
And so it'll amplify symmetry, breaking amplification.

26:00.800 --> 26:01.560
You get one embryo.

26:01.560 --> 26:04.040
There's a, there's a, you know, there's some neural tissue and some other stuff forms.

26:04.800 --> 26:08.880
Now, now you say, okay, I had one egg and one embryo and there you go.

26:08.880 --> 26:09.560
What else could it be?

26:09.840 --> 26:13.320
Well, the reality is, and I used to, I did all of this as a grad student.

26:13.320 --> 26:18.040
If you, if you take a little needle and you make a scratch in that blastoderm in

26:18.040 --> 26:21.080
that, in that disc, such that the cells can't talk to each other for a while, it

26:21.080 --> 26:22.840
heals up, but for a while, they can't talk to each other.

26:23.200 --> 26:28.560
What'll happen is that both regions will decide that they can be the embryo and

26:28.560 --> 26:29.240
there will be two of them.

26:29.240 --> 26:31.920
And then when they heal up, they become conjoint twins and you can make two,

26:31.960 --> 26:33.400
you can make three, you can make lots.

26:33.640 --> 26:39.640
So the question of how many cells are in there cannot be answered until it's

26:39.640 --> 26:41.320
actually played all the way through.

26:41.320 --> 26:43.080
It isn't necessarily that there's just one.

26:43.080 --> 26:43.640
There can be many.

26:43.880 --> 26:47.600
So what you have is you have this medium, this, this undifferentiated, I'm sure

26:47.600 --> 26:50.880
there's a, there's a psychological, um, version of this somewhere that I don't

26:50.880 --> 26:53.800
know the proper terminology, but you have this, you have this list, like

26:53.960 --> 26:58.440
ocean of potentiality, you have these thousands of cells and some number of

26:58.440 --> 27:01.960
individuals are going to be formed out of it, usually one, sometimes zero,

27:01.960 --> 27:07.600
sometimes several, and they form out of these cells because a region of these

27:07.600 --> 27:12.320
cells organizes into a collective that will have goals, goals that individual

27:12.320 --> 27:16.200
cells don't have, for example, uh, make a limb, make an eye, how many eyes?

27:16.240 --> 27:17.040
Well, exactly two.

27:17.200 --> 27:18.920
So individual cells don't know what an eye is.

27:18.920 --> 27:21.320
They don't know how many eyes you're supposed to have, but the collective does.

27:21.520 --> 27:25.040
The collective has goals and memories and anticipations that the individual cells

27:25.040 --> 27:29.720
don't and that, that the establishment of that boundary with its own, um, ability

27:29.720 --> 27:31.960
to maintain, to, to pursue certain goals.

27:32.240 --> 27:34.640
That's the origin of, of selfhood.

27:35.640 --> 27:43.040
But I, is that goal in there somewhere where they always destined?

27:43.080 --> 27:45.080
Like, are they discovering that goal?

27:45.480 --> 27:50.160
Like where the hell did evolution, um, discover this when you went from the

27:50.200 --> 27:55.640
prokaryotes to eukaryotic cells, and then they started making groups.

27:55.640 --> 28:00.000
And when you make a certain group, you make a, you make it sound.

28:00.000 --> 28:04.680
And it's such a tricky thing to try to understand.

28:05.160 --> 28:09.480
You make it sound like this cells didn't get together and came up with a goal,

28:10.080 --> 28:17.440
but the very act of them getting together revealed the goal that was always there.

28:17.440 --> 28:19.560
There was always that potential for that goal.

28:19.760 --> 28:22.280
So the first thing to say is that, uh, there are way more questions

28:22.280 --> 28:23.440
here than, than certainties.

28:23.440 --> 28:23.640
Okay.

28:23.640 --> 28:27.280
So everything I'm telling you is, is cutting edge developing, you know, stuff.

28:27.280 --> 28:29.960
So, so it's not as if any of us know the answer to this, but, but

28:29.960 --> 28:31.520
here's, here's, here's my opinion on this.

28:32.040 --> 28:35.840
I think what evolution, I don't think that evolution produces

28:36.160 --> 28:38.040
solutions to specific problems.

28:38.040 --> 28:39.400
In other words, specific environments.

28:39.400 --> 28:42.080
Like here's a frog that can live well in a froggy environment.

28:42.680 --> 28:47.240
I think what evolution produces is problem solving machines that, that

28:47.240 --> 28:49.960
will, that will solve problems in different spaces, so not just

28:49.960 --> 28:52.480
three-dimensional space, this goes back to what we were talking about before.

28:52.480 --> 28:56.760
We, the, the brain is a, evolutionarily a late development.

28:57.120 --> 29:01.160
It's a system that is able to, to pursue goals in three-dimensional

29:01.160 --> 29:02.640
space by giving commands to muscles.

29:02.920 --> 29:04.040
Where did that system come from?

29:04.040 --> 29:07.280
That system evolved from a much more ancient evolutionarily, much more

29:07.280 --> 29:12.680
ancient system where collections of cells gave, uh, instructions to, for

29:12.680 --> 29:17.160
cell behaviors, meaning cells move to, to, to divide, to, to die, to, um,

29:17.200 --> 29:21.000
change into different cell types to navigate more for space, the space of

29:21.000 --> 29:23.160
anatomies, the space of all possible anatomies.

29:23.560 --> 29:27.120
And before that cells were navigating transcriptional space, which is a

29:27.120 --> 29:28.720
space of all possible gene expressions.

29:28.720 --> 29:30.120
And before that metabolic space.

29:30.360 --> 29:36.120
So what evolution has done, I think is, is, is, is produced hardware that is

29:36.120 --> 29:39.800
very good at navigating different spaces using a bag of tricks, right?

29:39.800 --> 29:42.600
Which, which I'm sure many of them, we can steal for autonomous vehicles

29:42.600 --> 29:43.640
and robotics and various things.

29:44.200 --> 29:47.960
And what happens is that, um, they navigate these spaces without a whole

29:47.960 --> 29:49.600
lot of commitment to what the space is.

29:49.600 --> 29:51.240
In fact, they don't know what the space is, right?

29:51.280 --> 29:53.280
We are all brains in a vat, so to speak.

29:53.480 --> 29:55.920
Every cell does not know, right?

29:55.920 --> 29:59.640
Every cell is some other name, some other cells, external environment, right?

29:59.760 --> 30:03.400
So where does the, with that border between you, you and the outside world,

30:03.400 --> 30:04.880
you don't really know where that is, right?

30:04.880 --> 30:07.360
Every, every collection of cell has to figure that out from scratch.

30:08.080 --> 30:12.280
And the fact that evolution requires all of these things to figure out what they

30:12.280 --> 30:15.600
are, what effectors they have, what sensors they have, where does it make

30:15.600 --> 30:18.320
sense to draw a boundary between me and the outside world, the fact that you

30:18.320 --> 30:19.800
have to build all that from scratch.

30:19.800 --> 30:24.160
This autopoiesis is what defines the border of a self.

30:24.440 --> 30:28.360
Now, biology uses like a, um, a multi-scale, um, a multi-scale competency

30:28.360 --> 30:31.160
architecture, meaning that every level has goals.

30:31.280 --> 30:35.360
So, so molecular networks have goals, cells have goals, tissues, organs,

30:35.720 --> 30:40.800
colonies, uh, and, and it's the interplay of all of those that, uh, that enable

30:40.800 --> 30:42.760
biology to solve problems in new ways.

30:42.760 --> 30:44.920
For example, in xenobots and various other things.

30:45.520 --> 30:50.920
Um, this is, you know, uh, it's, it's exactly as you said, in many ways, the

30:50.920 --> 30:53.440
cells are discovering new ways of being.

30:53.840 --> 30:56.400
But at the same time, evolution certainly shapes all this.

30:56.400 --> 31:00.400
So, so evolution is very good at this agential bioengineering, right?

31:00.400 --> 31:04.520
When, when evolution is discovering a new way of being an animal, an animal

31:04.520 --> 31:08.000
or a plant or something, sometimes it's by changing the hardware, you know,

31:08.000 --> 31:11.840
protein, changing proteins, protein structure and so on, but much of the

31:11.840 --> 31:14.840
time it's not by changing the hardware, it's by changing the signals that the

31:14.840 --> 31:15.760
cells give to each other.

31:16.000 --> 31:18.800
It's doing what we as engineers do, which is try to convince the cells

31:18.800 --> 31:21.760
to do various things by using signals, experiences, stimuli.

31:21.920 --> 31:23.040
That's what biology does.

31:23.040 --> 31:27.120
It has to, because it's not dealing with a blank slate every time as, as, you

31:27.120 --> 31:30.800
know, if you're evolution and you're trying to, um, uh, make, make a, make an

31:30.800 --> 31:34.720
organism, you're not dealing with a passive material that is fresh and you

31:34.720 --> 31:37.200
have to specify it already wants to do certain things.

31:37.200 --> 31:41.000
So the easiest way to do that search, to find whatever is going to be adaptive

31:41.240 --> 31:45.680
is to find the signals that are going to convince cells to do various things.

31:45.760 --> 31:46.040
Right.

31:46.840 --> 31:50.240
Your sense is that evolution operates both in the software and the hardware

31:50.960 --> 31:54.680
and it's just easier and more efficient to operate in the software.

31:55.120 --> 31:55.400
Yes.

31:55.400 --> 31:58.360
And I should also say, I don't think the distinction is sharp.

31:58.400 --> 32:01.280
In other words, I think it's a continuum, but I think we can, but I think

32:01.280 --> 32:05.760
it's a meaningful distinction where you can make changes to a particular protein.

32:05.760 --> 32:08.920
And now the enzymatic function is different and it metabolizes differently

32:08.920 --> 32:12.720
and whatever, and that will have implications for fitness, or you can

32:12.720 --> 32:18.840
change the huge amount of information in the genome that isn't structural at all.

32:18.840 --> 32:22.720
It's, it's, uh, it's signaling it's when and how do cells say certain things

32:22.720 --> 32:25.880
to each other, and that can have massive changes as far as how

32:25.880 --> 32:26.840
it's going to solve problems.

32:27.040 --> 32:30.640
I mean, this idea of multi-hierarchical competence architecture, which

32:30.640 --> 32:32.680
is incredible to think about.

32:33.200 --> 32:38.200
So this hierarchy that evolution builds, I don't know who's responsible

32:38.200 --> 32:45.480
for this, I also see the incompetence of bureaucracies of humans when they get

32:45.480 --> 32:46.000
together.

32:47.280 --> 32:49.720
So how the hell does evolution build this?

32:50.120 --> 32:55.040
Where at every level, only the best get to stick around.

32:55.040 --> 32:58.560
They somehow figure out how to do their job without knowing the bigger picture.

32:59.680 --> 33:04.360
And then there's like the bosses that do the bigger thing somehow, or that you

33:04.360 --> 33:10.440
can now abstract away the small group of cells as a, as an organ or something.

33:10.440 --> 33:16.040
And then that organ does something bigger in the context of the full body or

33:16.040 --> 33:16.800
something like this.

33:17.920 --> 33:19.200
How is that built?

33:19.520 --> 33:23.840
Is there some intuition you can kind of provide of how that's constructed?

33:23.840 --> 33:27.960
That, that hierarchical competence architecture.

33:28.680 --> 33:31.640
I love that competence, just the word competence is pretty cool in this

33:31.640 --> 33:33.760
context because everybody's good at their job somehow.

33:34.240 --> 33:35.480
Yeah, no, it's really key.

33:35.480 --> 33:39.800
And the other nice thing about competency is that, so, so my, my central belief in

33:39.800 --> 33:43.960
all of this is that engineering is the right perspective on all of this stuff

33:43.960 --> 33:48.600
because it gets you away from subjective terms.

33:48.600 --> 33:52.240
You know, people talk about sentience and this and that, those things very hard to

33:52.240 --> 33:52.920
define there.

33:52.960 --> 33:54.520
People argue about them philosophically.

33:54.920 --> 34:00.360
I think that engineering terms like competency, like, you know, pursuit of

34:00.360 --> 34:05.480
goals, right, all of these things are, are empirically incredibly useful because

34:05.480 --> 34:06.360
you know it when you see it.

34:06.520 --> 34:11.480
And if it helps you build, right, if I, if I can pick the right level, I say this

34:11.480 --> 34:15.840
thing has, I believe this is X level of like, competency, I think it's like a

34:15.840 --> 34:20.080
thermostat, or I think it's like a better thermostat, or I think it's a, you know,

34:21.760 --> 34:24.160
various other kinds of, you know, there's many, many different kinds of complex

34:24.160 --> 34:24.520
systems.

34:24.840 --> 34:28.960
If that helps me to control and, and predict and build such systems, then,

34:29.000 --> 34:29.880
then that's all there is to say.

34:29.880 --> 34:31.240
There's no more philosophy to argue about.

34:31.600 --> 34:34.480
So, so I like competency in that way because you can quantify, you could, you

34:34.480 --> 34:36.920
have to, in fact, you have to, you have to make a client competent at what.

34:37.240 --> 34:40.440
And then, or if I say, if I tell you it has a goal, the question is what's the

34:40.440 --> 34:41.360
goal and how do you know?

34:41.560 --> 34:44.560
And I say, well, because every time I deviated from this particular state,

34:44.680 --> 34:46.320
that's what it spends energy to get back to.

34:46.320 --> 34:48.800
That's the goal and we can quantify it and we can be objective about it.

34:49.440 --> 34:53.160
So, so, so the, the, we're not used to thinking about this.

34:53.160 --> 34:56.240
I, I give a talk sometimes called why don't robots get cancer, right?

34:56.480 --> 34:59.200
And the reason robots don't get cancer is because generally speaking, with a

34:59.200 --> 35:02.440
few exceptions, our, our architectures have been, you've got a bunch of dumb

35:02.440 --> 35:07.200
parts and you hope that if you put them together, the, the, the, the overlying

35:07.200 --> 35:09.720
machine will have some intelligence and do something rather, right?

35:09.720 --> 35:11.120
But the individual parts don't, don't care.

35:11.120 --> 35:11.840
They don't have an agenda.

35:12.320 --> 35:13.480
Biology isn't like that.

35:13.480 --> 35:19.960
Every level has an agenda and the final outcome is the result of cooperation and

35:19.960 --> 35:22.200
competition, both within and across levels.

35:22.480 --> 35:25.800
So for example, during embryogenesis, your tissues and organs are competing

35:25.800 --> 35:28.600
with each other and it's actually a really important part of development.

35:28.600 --> 35:30.000
There's a reason they compete with each other.

35:30.000 --> 35:33.520
They're not all just, you know, sort of helping each other.

35:33.520 --> 35:36.560
They're also competing for, for information, for metabolic, for

35:36.560 --> 35:38.600
limited and metabolic constraints.

35:39.280 --> 35:42.880
But to get back to your, your, your other point, which is, you know, which is,

35:42.880 --> 35:46.880
which is, this seems like really efficient and good and so on compared

35:46.880 --> 35:50.920
to some of our human efforts, we also have to keep in mind that what happens

35:50.960 --> 35:57.120
here is that each level bends the option space for the level beneath so

35:57.120 --> 36:00.920
that your parts, basically they don't see the, the, the geometry.

36:00.920 --> 36:05.400
So, so I'm, I'm using, um, and I, and I, and I think, uh, I take this seriously,

36:05.680 --> 36:09.560
uh, terminology from, from like, um, from like relativity, right?

36:09.560 --> 36:11.120
Where, where the space is literally bent.

36:11.360 --> 36:14.760
So the option space is deformed by the higher level so that the lower

36:14.760 --> 36:17.680
levels, all they really have to do is go down their concentration gradient.

36:17.680 --> 36:20.560
They don't have to, in fact, they don't, they can't know what the big picture is.

36:20.920 --> 36:24.360
But if you bend the space just right, if they do what locally seems right,

36:24.560 --> 36:25.960
they end up doing your bidding.

36:25.960 --> 36:28.800
They end up doing things that are optimal in the, in the higher space.

36:29.080 --> 36:35.080
Conversely, because the components are good at getting their job done, you as

36:35.080 --> 36:38.920
the higher level don't need to, to try to compute all the low level controls.

36:39.040 --> 36:40.280
All you're doing is bending the space.

36:40.280 --> 36:42.120
You don't know or care how they're going to do it.

36:42.400 --> 36:43.640
Give you a super simple example.

36:43.640 --> 36:48.560
And, um, in the tadpole, we found that, okay, so, so tadpoles need to become frogs

36:48.560 --> 36:51.680
and to become, to go from a tadpole head to a frog head, you have to rearrange the

36:51.680 --> 36:53.280
face, so the eyes have to move forward.

36:53.280 --> 36:55.560
The jaws have to come out, the nostrils move, like everything moves.

36:56.360 --> 36:59.720
It used to be thought that because all tadpoles look the same and all frogs

36:59.720 --> 37:02.280
look the same, if you just remember, if every piece just moves in the right

37:02.280 --> 37:04.880
direction, the right amount, then you get your, you get your frog, right?

37:05.240 --> 37:06.560
So we decided to test.

37:06.560 --> 37:09.440
We, I had this hypothesis that I thought, I thought actually the system's

37:09.440 --> 37:10.600
probably more intelligent than that.

37:10.600 --> 37:11.280
So what did we do?

37:11.640 --> 37:13.880
We made what we call Picasso tadpoles.

37:14.080 --> 37:15.480
So these are, so everything is scrambled.

37:15.480 --> 37:17.640
So the eyes are on the back of the head, the jaws are off to the side.

37:17.640 --> 37:18.400
Everything is scrambled.

37:18.560 --> 37:19.240
Well, guess what they make?

37:19.240 --> 37:23.120
They make pretty normal frogs because all the different things move around in

37:23.120 --> 37:27.720
novel paths configurations until they get to the correct froggy, sort of frog

37:27.720 --> 37:28.840
face configuration, then they stop.

37:29.320 --> 37:32.480
So, so the thing about that is now imagine evolution, right?

37:32.480 --> 37:38.480
So, so you make some sort of mutation and it does, like every mutation, it does many

37:38.480 --> 37:41.600
things, so, so, so something good comes of it, but also it moves your

37:41.600 --> 37:43.040
mouth off to the side, right?

37:43.240 --> 37:47.560
Now, if, if, if there wasn't this multi-scale competency, you can see where this is going.

37:47.560 --> 37:50.440
If there wasn't this multi-scale competency, the organism would be dead.

37:50.440 --> 37:53.960
Your fitness is zero because you can't eat and you would never get to explore the

37:53.960 --> 37:56.280
other beneficial consequences of that mutation.

37:56.280 --> 37:59.240
You'd have to wait until you find some other way of doing it without moving the

37:59.240 --> 37:59.440
mouth.

37:59.440 --> 38:00.080
That's really hard.

38:00.400 --> 38:03.080
So, so the fitness landscape would be incredibly rugged.

38:03.080 --> 38:04.240
Evolution would take forever.

38:04.480 --> 38:08.960
The reason it works, one of the reasons it works so well is because you do that.

38:09.120 --> 38:09.600
No worries.

38:09.600 --> 38:12.240
The mouth will find its way where, where it belongs, right?

38:12.320 --> 38:13.400
So now you get to explore.

38:13.400 --> 38:16.600
So, so what that means is that all of these mutations that otherwise would be

38:16.600 --> 38:22.320
deleterious are now neutral because the competency of the parts make up for all

38:22.320 --> 38:22.960
kinds of things.

38:22.960 --> 38:27.360
So all the noise of development, all the variability in the environment, all these

38:27.360 --> 38:29.920
things, the competency of the parts makes up for it.

38:30.440 --> 38:33.440
So the, so, so that's all, that's all fantastic, right?

38:33.440 --> 38:34.280
That's all, that's all great.

38:34.880 --> 38:37.840
The only other thing to remember when we compare this to human efforts is this.

38:38.400 --> 38:42.160
Every component has its own goals in various spaces, usually with very little

38:42.160 --> 38:44.800
regard for the welfare of the other levels.

38:44.960 --> 38:49.880
So, so as a simple example, you know, um, you, as a, as a complex system, um, you

38:49.880 --> 38:53.080
will go out and you will do, you know, jujitsu or whatever, you'll have some,

38:53.080 --> 38:55.800
go, you have to go rock climbing and scrape a bunch of cells off your hands.

38:56.040 --> 38:57.480
And then you're happy as a system, right?

38:57.480 --> 39:00.480
You come back and you've, you've accomplished some goals and you're really happy.

39:00.640 --> 39:01.360
Those cells are dead.

39:01.440 --> 39:01.920
They're gone.

39:02.040 --> 39:02.360
Right.

39:02.400 --> 39:03.520
Did you think about those cells?

39:03.560 --> 39:04.040
Not really.

39:04.040 --> 39:04.200
Right.

39:04.200 --> 39:05.440
You had some, you had some bruising.

39:05.440 --> 39:07.120
You're a selfish SOB.

39:07.360 --> 39:07.760
That's it.

39:07.880 --> 39:12.040
And so, and so that's the thing to remember is that, um, you know, and we

39:12.080 --> 39:15.960
know this from, from history is that, is that just being a collective isn't enough

39:16.240 --> 39:20.440
because, uh, what the goals of that collective will be relative to the

39:20.440 --> 39:23.280
welfare of the individual parts is a massively open question.

39:23.280 --> 39:24.720
The ends justify the means.

39:24.720 --> 39:26.880
I'm telling you, Stalin was onto something.

39:27.320 --> 39:29.440
No, uh, but we can, exactly.

39:29.440 --> 39:34.840
That's the danger of, uh, uh, for us humans, we have to construct ethical

39:34.840 --> 39:41.720
systems under which we don't take seriously the full mechanism of biology

39:41.720 --> 39:45.160
and apply it to the way the world functions, which is, which is an

39:45.200 --> 39:52.680
interesting line we've drawn the world that built us is the one we reject in

39:52.680 --> 39:58.160
some sense, when we construct human societies, the idea that this country

39:58.160 --> 40:03.240
was founded on that all men are created equal, that's such a fascinating idea.

40:03.880 --> 40:09.280
That's like, uh, you're fighting against nature and saying, well, there's

40:09.280 --> 40:15.640
something bigger here than, um, a hierarchical competency architecture.

40:16.200 --> 40:19.560
Uh, but there's so many interesting things you said.

40:19.560 --> 40:24.520
So from an algorithmic perspective, the act of bending the option space,

40:26.960 --> 40:31.800
that's really, that's really profound because if you look at the way AI

40:31.800 --> 40:37.120
systems are built today, there's a big system, like I said, with robots and

40:37.120 --> 40:41.680
as a goal and he gets better and better at optimizing that goal at accomplishing

40:41.680 --> 40:47.840
that goal, but if biology built a hierarchical system where everything

40:47.880 --> 40:53.080
is doing computation and everything is accomplishing the goal, not only that.

40:54.080 --> 41:01.680
It's kind of dumb, you know, with the, uh, with the limited, with a bent option

41:01.680 --> 41:03.120
space is just doing the thing.

41:03.120 --> 41:05.440
That's the easiest thing for it in some sense.

41:06.400 --> 41:12.560
And somehow that allows you to have, uh, turtles on top of turtles, literally

41:12.680 --> 41:18.040
dumb systems on top of dumb systems that as a whole creates something incredibly smart.

41:18.560 --> 41:18.840
Yeah.

41:18.880 --> 41:24.200
I mean, every system is, has some degree of intelligence in its own problem domain.

41:24.400 --> 41:29.240
So, so cells will have problems they're trying to solve in physiological

41:29.240 --> 41:30.800
space and transcriptional space.

41:30.800 --> 41:33.960
And then I can give you some, some cool examples of that, but the collective

41:33.960 --> 41:36.840
is trying to solve problems in anatomical space, right.

41:36.840 --> 41:39.920
And forming a, you know, a creature and growing your blood vessels and so on.

41:40.320 --> 41:44.960
And then the collective, the, the, the, the whole body is solving yet other problems.

41:44.960 --> 41:47.880
They may be in social space and linguistic space and three-dimensional space.

41:48.200 --> 41:51.840
And, and who knows, you know, the group might be solving problems in, in, um, you

41:51.840 --> 41:53.680
know, I don't know, some sort of financial space or something.

41:54.000 --> 42:00.120
So one of the major differences with, with most, um, uh, with most, uh,

42:00.160 --> 42:05.640
AIs today is, is a, the, the kind of flatness of the architecture, but also

42:05.640 --> 42:12.040
of the fact that they are constructed from outside their, their borders and

42:12.040 --> 42:16.480
they're, you know, so, so if you, so to a large extent, and of course there

42:16.480 --> 42:20.080
are counter examples now, but, but to a large extent, our technology had been

42:20.080 --> 42:21.880
such that you create a machine or a robot.

42:22.080 --> 42:24.280
It knows what its sensors are.

42:24.360 --> 42:26.040
It knows what its effectors are.

42:26.040 --> 42:28.280
It knows the boundary between it and the outside world.

42:28.280 --> 42:29.680
All of this is given from the outside.

42:30.280 --> 42:32.280
Biology constructs this from scratch.

42:32.280 --> 42:37.280
Now the best example of this, that, that, uh, originally, uh, in, in robotics was

42:37.280 --> 42:40.840
actually Josh Bongard's work in 2006, where he made these, these robots that

42:40.840 --> 42:42.600
did not know their shape to start with.

42:42.720 --> 42:45.520
So like a baby, they sort of floundered around, they made some hypotheses.

42:45.520 --> 42:47.360
Well, I did this and I moved in this way.

42:47.440 --> 42:51.160
Well, maybe I'm a whatever, maybe I have wheels or maybe I have six legs or whatever.

42:51.160 --> 42:51.320
Right.

42:51.320 --> 42:53.120
And they would make a model and then eventually it would crawl around.

42:53.440 --> 42:54.840
So that's, I mean, that's really good.

42:54.840 --> 42:58.160
That's part of the autopoiesis, but we can go a step further and some people are

42:58.200 --> 43:02.160
doing this and then we're sort of working on some of this too, is this idea that

43:02.160 --> 43:05.160
let's even go back further, you don't even know what sensors you have.

43:05.160 --> 43:07.560
You don't know where you end in the outside world begins.

43:07.760 --> 43:10.960
All you have is, is, uh, certain things like active inference, meaning you're

43:10.960 --> 43:12.640
trying to minimize surprise, right?

43:12.920 --> 43:14.320
You have some metabolic constraints.

43:14.320 --> 43:15.560
You don't have all the energy you need.

43:15.560 --> 43:18.080
You don't have all the time in the world to, to, to think about everything

43:18.080 --> 43:18.680
you want to think about.

43:18.920 --> 43:22.440
So that means that you can't afford to be a micro, um, reductionist.

43:22.440 --> 43:25.280
You know, all this data coming in, you have to coarse grain it and say, I'm

43:25.280 --> 43:27.360
going to take all this stuff and I'm going to call that a cat.

43:27.520 --> 43:28.200
I'm going to take all this.

43:28.200 --> 43:29.640
I'm going to call that the edge of the table.

43:29.640 --> 43:30.520
I don't want to fall off of.

43:30.760 --> 43:32.600
And I don't want to know anything about the micro states.

43:32.600 --> 43:35.720
What I want to know is what is the optimal way to cut up my world?

43:35.800 --> 43:37.440
And by the way, this thing over here, that's me.

43:37.680 --> 43:40.680
And the reason that's me is because I have more control over this than I have

43:40.680 --> 43:41.800
over any of this other stuff.

43:42.000 --> 43:43.360
And so now you can begin to write.

43:43.520 --> 43:46.720
So that's self-construction at that, that figuring out making models of the

43:46.720 --> 43:49.800
outside world and then turning that inwards and starting to make a model of

43:49.800 --> 43:54.360
yourself, right, which immediately starts to get into issues of, of agency and

43:54.360 --> 43:59.960
control because in order to, if, if you are under metabolic constraints, meaning

43:59.960 --> 44:02.680
you don't have the energy, right, that all the energy in the world, you have to

44:02.680 --> 44:07.600
be efficient that immediately forces you to start telling stories about coarse

44:07.600 --> 44:09.560
grained agents that do things, right?

44:09.560 --> 44:12.480
You don't have the energy to like Laplace's demon, you know, calculate

44:12.480 --> 44:15.520
every, every possible state that's going to happen.

44:15.760 --> 44:19.480
You have to, you have to coarse grain and you have to say that is the kind of

44:19.640 --> 44:22.840
creature that does things, either things that I avoid or things that I will go

44:22.840 --> 44:24.880
towards, that's a mate or food or whatever, whatever it's going to be.

44:25.360 --> 44:30.360
And so right at the base of a simple, very simple organism, starting to

44:30.360 --> 44:38.320
make models of agents doing things, that is the origin of models of free will

44:38.320 --> 44:38.920
basically, right?

44:38.920 --> 44:42.320
Because you see the world around you as having agency and then you turn that on

44:42.320 --> 44:45.640
yourself and you say, wait, I have agency too, I can, I do things, right?

44:45.880 --> 44:47.800
And, and then you make decisions about what you're going to do.

44:47.800 --> 44:54.280
So all of this one, one model is to view all of those kinds of things as being

44:54.280 --> 44:59.480
driven by that early need to determine what you are and to do so, and to then

44:59.480 --> 45:03.240
take actions in the most energetically efficient space possible, right?

45:03.240 --> 45:07.600
So free will emerges when you try to simplify, tell a nice

45:07.600 --> 45:09.200
narrative about your environment.

45:09.240 --> 45:10.640
I think that's very plausible.

45:10.680 --> 45:11.040
Yeah.

45:11.720 --> 45:13.560
You think free will is an illusion.

45:14.560 --> 45:18.200
So, so you're kind of implying that it's a useful hack.

45:19.280 --> 45:20.600
Well, I'll say two things.

45:20.640 --> 45:24.600
The first thing is, I think, I think it's very plausible to say that any

45:24.600 --> 45:29.960
organism that self or any agent that self, whether it's biological or not, any

45:29.960 --> 45:35.040
agent that self constructs under energy constraints is going to believe in

45:35.040 --> 45:38.360
free will, we'll, we'll get to whether it has free will momentarily, but, but I

45:38.360 --> 45:41.640
think, but I think what, what it definitely drives is a view of yourself

45:41.720 --> 45:43.400
and the outside world as an agential view.

45:43.400 --> 45:44.600
I think that's inescapable.

45:44.640 --> 45:47.560
So that's true for even primitive organisms.

45:47.680 --> 45:48.120
I think so.

45:48.280 --> 45:51.400
I think that's now, now they don't have, now, obviously you have to scale down,

45:51.400 --> 45:51.560
right?

45:51.560 --> 45:56.120
So, so, so, so they don't have the kinds of complex metacognition that we have, so

45:56.120 --> 45:59.320
they can do long-term planning and thinking about free will and so, and so on.

45:59.600 --> 46:02.880
But, but the sense of agency is really useful to accomplish

46:02.880 --> 46:04.920
those tasks, simple or complicated.

46:04.960 --> 46:05.320
That's right.

46:05.360 --> 46:08.920
In, in all kinds of spaces, not just in obvious three-dimensional space.

46:09.080 --> 46:13.960
I mean, we're very good at the thing is humans are very good at detecting agency

46:14.360 --> 46:18.720
of, of like medium sized objects moving at medium speeds in the three-dimensional

46:18.720 --> 46:19.320
world, right?

46:19.320 --> 46:21.880
We see a bowling ball and we see a mouse and we immediately know what the

46:21.880 --> 46:22.600
difference is, right?

46:22.600 --> 46:25.760
And how we're going to mostly things you can eat or get eaten by.

46:25.880 --> 46:26.480
Yeah, yeah.

46:26.560 --> 46:28.080
That's our, that's our training set, right?

46:28.080 --> 46:31.200
From the time you're little, your training set is a visual data on, on this,

46:31.200 --> 46:33.000
this like little chunk of your experience.

46:33.280 --> 46:37.480
But imagine if, imagine if, uh, from the time that we were born, we had

46:37.520 --> 46:39.720
innate senses of your blood chemistry.

46:39.720 --> 46:42.120
If you could feel your blood chemistry, the way you can see, right?

46:42.120 --> 46:45.080
You had a high bandwidth connection and you could feel your blood chemistry and

46:45.080 --> 46:48.360
you could see, uh, you could sense all the things that your organs were doing.

46:48.360 --> 46:52.160
So your pancreas, your liver, all the things, if, if we had that, you, we

46:52.160 --> 46:55.240
would be very good at detecting intelligence in physiological space.

46:55.440 --> 46:59.560
We would know the level of intelligence that our various organs were deploying

46:59.560 --> 47:03.080
to deal with things that were coming to anticipate the stimuli to, you know, but,

47:03.080 --> 47:04.400
but we're just terrible at that.

47:04.400 --> 47:07.200
We don't, in fact, in fact, people don't even, you know, you talk about

47:07.200 --> 47:09.640
intelligence to these other papers spaces and a lot of people think that's

47:09.640 --> 47:13.000
just crazy because, because all we're, all we know is motion.

47:13.120 --> 47:14.720
We do have access to that information.

47:14.720 --> 47:19.640
So it's actually possible that, uh, so evolution could, if we wanted to

47:19.640 --> 47:24.080
construct an organism that's able to perceive the flow of blood through your

47:24.080 --> 47:30.520
body, the way you see an old friend and say, yo, what's up, how's the wife and

47:30.520 --> 47:35.080
the kids, uh, in that same way you would see the, you would feel like a

47:35.080 --> 47:36.520
connection to the liver.

47:36.840 --> 47:37.080
Yeah.

47:37.320 --> 47:37.560
Yeah.

47:37.680 --> 47:40.880
I think, you know, maybe other people's liver and not just your own, cause you

47:40.880 --> 47:42.280
don't have access to other people's.

47:42.880 --> 47:45.920
Not yet, but you could imagine some really interesting connection, right?

47:45.920 --> 47:49.760
But like sexual selection, like, Ooh, that girl's got a nice liver.

47:50.240 --> 47:53.120
Well, that's like the way her blood flows.

47:53.600 --> 47:57.080
The, the dynamics of the blood, uh, is very interesting.

47:57.080 --> 47:57.800
It's novel.

47:57.840 --> 48:01.160
I've never seen one of those, but you know, that's, that's exactly what we're

48:01.160 --> 48:05.000
trying to half-ass when we, when we, um, uh, judge judgment of, of beauty by

48:05.000 --> 48:06.240
facial symmetry and so on.

48:06.400 --> 48:09.520
That's a, that's a half-assed assessment of exactly that, of exactly that.

48:09.600 --> 48:12.520
Because if your cells could not cooperate enough to keep your, your

48:12.520 --> 48:15.560
organism symmetrical, you know, you can make some inferences about

48:15.560 --> 48:16.360
what else is wrong, right?

48:16.360 --> 48:18.880
Like that's a, that's a very, you know, that's a very basic.

48:19.080 --> 48:19.680
Interesting.

48:19.680 --> 48:19.920
Yeah.

48:19.920 --> 48:24.160
So that in some deep sense, actually, that is what we're doing.

48:24.160 --> 48:32.680
We're trying to infer how, uh, health, we use the word healthy, but basically

48:32.920 --> 48:39.040
how functional is this biological system I'm looking at so I can hook up

48:39.040 --> 48:41.040
with that one and make offspring.

48:41.600 --> 48:41.840
Yeah.

48:41.880 --> 48:42.040
Yeah.

48:42.040 --> 48:44.960
Well, what kind of hardware might their genomics give me that

48:44.960 --> 48:46.400
that might be useful in the future?

48:46.520 --> 48:50.120
I wonder why evolution didn't give us, um, higher resolution signal.

48:50.560 --> 48:55.320
Like why the whole peacock thing with the feathers, it doesn't seem.

48:56.720 --> 48:59.680
It's a very low bandwidth signal for sexual selection.

48:59.720 --> 49:02.240
I'm gonna, and I'm not an expert on this stuff, but

49:03.160 --> 49:06.800
well, no, but I'll take a stab at the reason.

49:06.840 --> 49:09.160
I think that it's because it's an arms race.

49:09.480 --> 49:11.840
You see, you don't want everybody to know everything about you.

49:12.200 --> 49:15.640
So I think that as much as, as much as, and in fact, there's

49:15.640 --> 49:19.760
another interesting part of this arms race, which is the thing about this.

49:19.800 --> 49:24.640
Uh, the, the, the most adaptive, evolvable system is one that has

49:24.640 --> 49:27.720
the most level of top-down control, right?

49:27.840 --> 49:31.920
If it's really easy to say to a bunch of cells, make another finger

49:32.080 --> 49:35.680
versus, okay, here's 10,000 gene expression changes that you need to

49:35.680 --> 49:37.000
do to make it, to change your finger, right?

49:37.200 --> 49:40.520
The, the, the, the, the system with good top-down control that has

49:40.520 --> 49:43.320
memory and when we need to get back to that, by the way, that's a question

49:43.320 --> 49:47.720
I neglected to answer about where the memory is and so on, um, a system that

49:47.720 --> 49:51.520
uses all of that is really highly evolveable and that's fantastic, but

49:51.520 --> 49:55.720
guess what, it's also highly, um, the subject of hijacking by parasites, by,

49:56.040 --> 49:59.440
uh, by, by, by cheaters of various kinds, by conspecifics.

49:59.600 --> 50:03.280
Like we, we found that, um, and then that, that goes back to the story of

50:03.280 --> 50:06.400
the pattern memory in these planaria, there's a bacterium that lives on these

50:06.400 --> 50:10.480
planaria, that bacterium has an input into how many heads the worm is going

50:10.480 --> 50:14.480
to have because it's hijacks that, that control system and it's able to make

50:14.480 --> 50:18.080
a chemical that basically interfaces with the system that calculates how

50:18.080 --> 50:19.920
many heads you're supposed to have, and they can have two, and they can

50:19.920 --> 50:20.680
make them have two heads.

50:21.000 --> 50:24.000
And so you can imagine that if you are too, so you want to be understandable

50:24.000 --> 50:26.360
for your own parts to understand each other, but you don't want to be too

50:26.360 --> 50:28.760
understandable because you'll be too easily controllable.

50:29.000 --> 50:33.360
And so I think that, that, um, my guess is that, that, um, that, that, that,

50:33.400 --> 50:37.880
that opposing pressure keeps this from being a super high bandwidth kind of

50:37.880 --> 50:40.360
thing where we can just look at somebody and know, you know, everything about them.

50:40.400 --> 50:43.360
So it's a kind of biological game of Texas Hold'em.

50:43.840 --> 50:47.600
You're showing some cards and you're hiding other cards and this part of it

50:47.600 --> 50:51.200
and there's bluffing and there's, and all of that, and then there's probably

50:51.640 --> 50:54.320
whole species that would do way too much bluffing.

50:54.320 --> 50:55.640
That's probably where peacocks fall.

50:56.320 --> 51:02.480
There's a, there's a book that I don't remember if I read or if I, if I wrote,

51:02.640 --> 51:07.840
if I read summaries of the book, but it's about the evolution of beauty and birds.

51:07.880 --> 51:08.760
Where is that from?

51:09.200 --> 51:11.480
Is that a book or does Richard Dawkins talk about it?

51:11.480 --> 51:16.800
But basically there's some species start to like over select for beauty, not

51:16.800 --> 51:19.400
over select, they just some reason select for beauty.

51:19.400 --> 51:21.280
There is a case to be made.

51:21.320 --> 51:23.120
Actually now I'm starting to remember.

51:23.280 --> 51:28.720
I think Darwin himself made a case that you can select based on beauty alone.

51:29.560 --> 51:34.400
So that beauty, there's a point where B doesn't represent some

51:34.400 --> 51:36.120
underlying biological truth.

51:36.320 --> 51:39.560
You start to select for, for beauty itself.

51:39.560 --> 51:43.360
And I think the deep question is there is some, is there some

51:43.360 --> 51:46.160
evolutionary value to beauty?

51:46.720 --> 51:52.720
But it's an interesting kind of thought that this, can we deviate

51:52.720 --> 51:57.560
completely from the deep biological truth to actually appreciate some kind

51:57.560 --> 52:00.200
of the, the summarization in itself?

52:01.200 --> 52:02.240
Let me get back to memory.

52:02.240 --> 52:03.920
Cause this is a really interesting idea.

52:04.160 --> 52:09.360
Um, how do a collection of cells remember anything?

52:09.400 --> 52:12.280
How do biological systems remember anything?

52:12.800 --> 52:16.200
How is that akin to the kind of memory we think of humans as having

52:16.720 --> 52:18.600
within our big cognitive engine?

52:18.920 --> 52:19.200
Yeah.

52:19.600 --> 52:22.680
One of the ways to start thinking about bioelectricity is to ask

52:22.680 --> 52:27.960
ourselves, where did neurons and all these cool tricks that the brain uses

52:27.960 --> 52:33.240
to, uh, run these amazing problem solving abilities on and basically

52:33.240 --> 52:34.920
an electrical network, right?

52:34.920 --> 52:35.760
Where did that come from?

52:35.760 --> 52:37.760
They didn't just evolve, you know, up here out of nowhere, it

52:37.760 --> 52:38.880
must've evolved from something.

52:39.240 --> 52:44.240
And what it evolved from was a much more ancient ability of cells to form

52:44.240 --> 52:47.320
networks, to solve other kinds of problems, for example, to navigate

52:47.320 --> 52:49.200
more for space, to control the body shape.

52:49.680 --> 52:54.880
And so all of the components of, uh, of neurons, so, so ion channels,

52:54.920 --> 52:58.200
um, uh, neurotransmitter machinery, electrical synapses, all this

52:58.200 --> 53:01.080
stuff is way older than brains, way older than neurons, in fact,

53:01.080 --> 53:02.320
older than multicellularity.

53:02.800 --> 53:04.400
And so it was already there.

53:04.400 --> 53:05.960
Even, even bacterial biofilms.

53:05.960 --> 53:09.440
There's some beautiful work from UCSD on, on, on brain-like

53:09.440 --> 53:11.000
dynamics and bacterial biofilms.

53:11.240 --> 53:15.040
So evolution figured out very early on that electrical networks are

53:15.080 --> 53:18.160
amazing at having memories, at integrating information across distance,

53:18.480 --> 53:21.600
at different kinds of optimization tasks, you know, image recognition

53:21.600 --> 53:23.800
and so on long before there were brains.

53:24.360 --> 53:26.760
Can you actually just step back and we'll return to it.

53:27.200 --> 53:28.680
What is bioelectricity?

53:28.960 --> 53:30.160
What is biochemistry?

53:30.160 --> 53:32.880
What is, what are electrical networks?

53:33.360 --> 53:40.080
I think a lot of the biology community focuses on the chemicals as

53:40.080 --> 53:43.120
the signaling mechanisms that make the whole thing work.

53:43.600 --> 53:49.960
You have, I think to a large degree, uniquely, maybe you can correct me on

53:49.960 --> 53:56.480
that, have focused on the bioelectricity, which is using electricity for signaling.

53:57.120 --> 54:00.800
There's also probably mechanical, like knocking on the door.

54:02.200 --> 54:06.440
Uh, so what, what, what's the difference and what's an electrical network?

54:06.600 --> 54:06.840
Yeah.

54:07.160 --> 54:10.080
So I want to make sure and kind of give credit where credit is due.

54:10.080 --> 54:16.200
So, so as far back as 1903 and probably, um, late 1800s already, people were

54:16.200 --> 54:19.440
thinking about the importance of electrical, um, phenomena in, in life.

54:19.440 --> 54:23.040
So I'm for sure not the first person to stress the importance of electricity.

54:23.440 --> 54:27.160
Um, people, there were, there were waves of research in the, in the thirties,

54:27.200 --> 54:32.080
um, in the forties, and then again, in the kind of, uh, seventies, eighties

54:32.080 --> 54:35.160
and nineties of, of sort of the pioneers of bioelectricity, who did

54:35.160 --> 54:36.440
some amazing work on all this.

54:36.440 --> 54:41.360
I think, I think what, what we've done that's new is to step away from this idea

54:41.360 --> 54:44.640
that, and I'll describe what, what the bioelectricity is a step away from the

54:44.640 --> 54:47.720
idea that, well, here's another piece of physics that you need to keep track of

54:47.720 --> 54:51.960
to understand physiology and development and to really start looking at this as

54:51.960 --> 54:56.320
saying, no, this is a, a privileged computational layer that gives you

54:56.320 --> 54:59.360
access to the actual cognition of the tissue of basal cognition.

54:59.360 --> 55:03.200
So, so merging that, that developmental biophysics with ideas and cognition

55:03.200 --> 55:05.320
of computation and so on, I think, I think that's what we've done.

55:05.360 --> 55:05.720
That's new.

55:06.040 --> 55:08.920
But people have been talking about bioelectricity for a really long time.

55:08.920 --> 55:10.400
And, and, and so I'll, so I'll define that.

55:10.680 --> 55:15.120
So, um, what happens is that, uh, if you have, uh, if you have a single cell,

55:15.160 --> 55:19.520
cell has a membrane in that membrane are proteins called ion channels.

55:19.600 --> 55:22.920
And those proteins allow charged molecules, potassium, sodium, chloride,

55:23.160 --> 55:25.960
to go in and out under certain circumstances.

55:26.360 --> 55:31.040
And when there's an imbalance of, uh, of those ions, there becomes a

55:31.040 --> 55:32.720
voltage gradient across that membrane.

55:32.960 --> 55:37.440
And so all cells, all living cells try to hold a particular kind of voltage,

55:37.680 --> 55:39.280
uh, difference across the membrane.

55:39.280 --> 55:40.760
And they spend a lot of energy to do so.

55:41.360 --> 55:45.160
When you now, now, so, so that's, that's, that's a single cell.

55:45.800 --> 55:49.200
When you have multiple cells, the cells sitting next to each other, they can

55:49.200 --> 55:52.920
communicate their voltage state to each other via a number of different ways.

55:52.920 --> 55:55.440
But one of them is the single, the gap junction, which is basically

55:55.440 --> 55:57.960
like a little submarine hatch that just kind of docs, right.

55:58.240 --> 56:01.560
And the ions from one side can flow to the other side and vice versa.

56:02.200 --> 56:07.280
So isn't it incredible that this evolved, isn't, isn't that wild?

56:07.680 --> 56:09.000
Cause that didn't exist.

56:09.680 --> 56:10.120
Correct.

56:10.160 --> 56:11.520
This had to be, this had to be evolved.

56:11.520 --> 56:12.720
And it had to be invented.

56:12.720 --> 56:13.120
That's right.

56:13.240 --> 56:16.080
So somebody invented electricity in the, in the ocean.

56:16.240 --> 56:17.360
When did this get invented?

56:17.440 --> 56:17.920
Yeah.

56:17.960 --> 56:21.200
So, so, I mean, it's, it is, it is incredible.

56:21.240 --> 56:23.840
Um, the guy who discovered gap junctions, Werner Lowenstein, I

56:23.840 --> 56:25.440
visited him, he was, he was really old.

56:25.760 --> 56:27.440
Human being, he discovered them.

56:27.440 --> 56:32.640
Cause who really discovered them live probably 4 billion years ago.

56:32.960 --> 56:35.200
So you're, you're give credit where credit is due.

56:35.200 --> 56:39.080
I'm just saying he rediscovered, he rediscovered gap junctions.

56:39.280 --> 56:44.360
But, um, when I visited him in Woods Hole, uh, maybe 20 years ago now, uh,

56:44.440 --> 56:47.480
he told me that he was writing and unfortunately he passed away.

56:47.480 --> 56:49.200
And I think this, this book never got written.

56:49.400 --> 56:52.120
He was writing a book on, on gap junctions and consciousness.

56:52.480 --> 56:55.040
And I think, I think it would have been a, uh, an incredible book

56:55.040 --> 56:56.680
because, because gap junctions are magic.

56:56.680 --> 56:57.920
I'll explain why in a minute.

56:58.480 --> 57:02.600
Uh, what happens is that just imagine the, the thing about both these

57:02.600 --> 57:05.760
ion channels and these gap junctions is that many of them are

57:05.760 --> 57:07.160
themselves voltage sensitive.

57:08.000 --> 57:10.640
So that's a voltage sensitive current conductance.

57:10.640 --> 57:11.360
That's a transistor.

57:11.720 --> 57:15.960
And as soon as you've invented one immediately, you now get access to,

57:16.120 --> 57:20.600
from, from this platonic space of, of mathematical truths, you get access

57:20.600 --> 57:22.720
to all of the cool things that transistors do.

57:23.040 --> 57:27.200
So now when you have a network of cells, not only do they, do they talk to each

57:27.200 --> 57:29.840
other, but they can send messages to each other and the differences

57:29.840 --> 57:31.040
of voltage can propagate.

57:31.240 --> 57:34.640
Now to neuroscientists, this is old hat because you see this in the brain, right?

57:34.640 --> 57:38.320
There's action potentials that, you know, the electricity, um, you can, you can,

57:38.360 --> 57:41.240
uh, they have, they have these awesome movies where you can take a zebra, like

57:41.240 --> 57:45.240
a transparent, uh, uh, animal, like a zebra fish, you can literally look down

57:45.240 --> 57:48.400
and you can see all the, all the firings as the fish is like making decisions

57:48.400 --> 57:49.480
about what to eat and things like this.

57:49.480 --> 57:49.600
Right.

57:49.600 --> 57:50.000
It's amazing.

57:50.200 --> 57:53.040
Well, your whole body is doing that all the time, just much slower.

57:53.400 --> 57:57.120
So there are very few things that neurons do that other cells, that all

57:57.120 --> 57:58.200
the cells in your body don't do.

57:58.240 --> 58:01.640
They all, they all do very similar things, just on a much slower timescale.

58:01.880 --> 58:05.600
And whereas your brain is thinking about how to solve problems in three

58:05.600 --> 58:09.240
dimensional space, um, the cells in an embryo are thinking about how to

58:09.240 --> 58:11.040
solve problems in anatomical space.

58:11.200 --> 58:13.640
They're trying to have memories like, Hey, how many fingers are we supposed to have?

58:13.680 --> 58:14.640
Well, how many do we have now?

58:14.720 --> 58:16.160
What do we do to get from here to there?

58:16.360 --> 58:18.120
That's the kind of problems they're thinking about.

58:18.600 --> 58:21.880
And the reason that gap junctions are magic is imagine, right.

58:21.880 --> 58:25.440
From the, from the, from the earliest, um, from the earliest time.

58:26.920 --> 58:28.240
I'm here are two cells.

58:28.280 --> 58:30.280
This cell, uh, how can they communicate?

58:30.280 --> 58:34.240
Well, well, the simple version is this cell could send a chemical, a

58:34.240 --> 58:37.160
chemical signal, it floats over and it hits a receptor on this cell, right?

58:37.720 --> 58:39.040
Because it comes from outside.

58:39.040 --> 58:41.320
This cell can very easily tell that that came from outside.

58:41.680 --> 58:43.640
It's this is whatever information is coming.

58:43.640 --> 58:44.720
That's not my information.

58:44.720 --> 58:46.200
I think that information is coming from the outside.

58:46.200 --> 58:47.760
So I can, I can trust it.

58:47.840 --> 58:48.680
I can ignore it.

58:48.720 --> 58:51.280
I can do various things with it, whatever, but I know it comes from the outside.

58:51.680 --> 58:54.520
Now imagine instead that you have two cells with a gap junction between them.

58:54.760 --> 58:55.480
Something happens.

58:55.480 --> 58:56.600
Let's say the cell gets poked.

58:56.640 --> 59:00.280
There's a calcium spike, the calcium spike or whatever small molecule

59:00.280 --> 59:02.880
signal propagates through the gap junction to this cell.

59:03.400 --> 59:05.840
There's no ownership metadata on that signal.

59:06.040 --> 59:09.560
This cell does not know now that it's didn't, that it came from outside

59:09.560 --> 59:12.240
because it looks exactly like its own memories would have looked like

59:12.240 --> 59:14.280
of being, of being, of whatever had happened, right?

59:14.760 --> 59:19.200
So gap junctions, to some extent, wipe ownership information on data,

59:19.400 --> 59:23.520
which means that if I can't, if, if you and I are sharing memories and we can't

59:23.520 --> 59:26.960
quite tell who the memories belong to, that's the beginning of a mind melt.

59:27.160 --> 59:31.480
That's the beginning of a scale up of cognition from here's me and here's you

59:31.480 --> 59:32.920
to know now there's just us.

59:33.040 --> 59:36.080
So they enforce a collective intelligence gap junctions.

59:36.080 --> 59:36.360
That's right.

59:36.600 --> 59:37.000
It helps.

59:37.000 --> 59:37.560
It's the beginning.

59:37.560 --> 59:39.400
It's not the whole story by any means, but it's the start.

59:39.640 --> 59:43.800
Where's state stored of the system?

59:43.920 --> 59:47.680
So there are some, is it in part in the gap junctions themselves?

59:47.680 --> 59:48.960
Is it in the cells?

59:49.600 --> 59:52.240
There are many, many layers to this as always in biology.

59:52.240 --> 59:55.360
So there are, um, uh, chemical networks.

59:55.480 --> 59:57.920
So for example, gene regulatory networks, right.

59:57.920 --> 01:00:01.040
Which, which, or, or basically any kind of chemical pathway where different

01:00:01.040 --> 01:00:04.080
chemicals activate and repress each other, they can store memories.

01:00:04.240 --> 01:00:06.520
So in a dynamical system sense, they can store memories.

01:00:06.520 --> 01:00:09.840
They can, they can get into stable states that are hard to pull them out of.

01:00:09.840 --> 01:00:10.080
Right.

01:00:10.080 --> 01:00:12.560
So that's, that becomes, once they get in, that's a memory, a permanent

01:00:12.560 --> 01:00:14.960
memory of some or semi-permanent memory of something that's happened.

01:00:15.400 --> 01:00:17.560
There are cytoskeletal structures, right.

01:00:17.560 --> 01:00:21.800
That are physically, they store, they store memories in physical, um, configuration.

01:00:22.280 --> 01:00:28.040
There are, uh, electrical memories like flip-flops where there is no physical, right.

01:00:28.040 --> 01:00:32.240
So, so if you look, I show my students, this example is a flip-flop and the

01:00:32.240 --> 01:00:37.520
reason that it stores a zero or one is not because some, some, uh, piece of

01:00:37.520 --> 01:00:41.160
the hardware moved it's because there's a, there's a cycling of the current

01:00:41.160 --> 01:00:42.080
in one side of the thing.

01:00:42.240 --> 01:00:46.440
If I come over and I hold, um, you know, I hold the other side to, uh, to a

01:00:46.440 --> 01:00:50.040
high voltage for, for, you know, a brief period of time, it flips over and now

01:00:50.040 --> 01:00:53.600
it's here, but the hard, none of the hardware moved, the information is in

01:00:53.600 --> 01:00:54.920
a stable dynamical sense.

01:00:54.920 --> 01:00:58.000
And if you were to X-ray the thing, you couldn't tell me if it was zero or one.

01:00:58.120 --> 01:00:59.680
Cause all you would see is where the hardware is.

01:00:59.680 --> 01:01:01.800
You wouldn't see the energetic state of the system.

01:01:02.120 --> 01:01:05.960
So there are also, so there are bioelectrical states that are held in that

01:01:05.960 --> 01:01:10.180
exact way, like, like, like volatile RAM, basically, like in the, in the electrical

01:01:10.180 --> 01:01:15.060
state, it's very akin to the different ways that memory stored in a computer.

01:01:15.940 --> 01:01:18.780
So there's RAM, there's hard drives.

01:01:18.780 --> 01:01:19.940
You can make that mapping, right?

01:01:20.100 --> 01:01:24.220
So I think the interesting thing is that based on the biology, we can

01:01:24.340 --> 01:01:28.500
have a more sophisticated, you know, I think we can revise some of our, some

01:01:28.500 --> 01:01:32.340
of our, um, computer engineering methods, because there are some interesting

01:01:32.340 --> 01:01:35.380
things that biology does, we haven't done yet, but, but you can, but that

01:01:35.380 --> 01:01:36.900
map, but that mapping is not bad.

01:01:36.900 --> 01:01:38.420
I mean, I think it works in many ways.

01:01:38.460 --> 01:01:38.820
Yeah.

01:01:38.820 --> 01:01:42.660
I wonder, cause I mean, the way we build computers at the root of computer

01:01:42.660 --> 01:01:46.740
science is the idea of proof of correctness, the way we program things

01:01:46.740 --> 01:01:53.940
to be perfect, reliable, you know, this idea of resilience and robustness

01:01:53.940 --> 01:01:56.500
to unknown conditions is not as important.

01:01:56.940 --> 01:01:58.660
So that's what biology is really good at.

01:01:58.980 --> 01:02:03.860
So I don't know what kind of systems, I don't know how we go from a computer

01:02:03.860 --> 01:02:05.860
to a biological system in the future.

01:02:06.180 --> 01:02:06.460
Yeah.

01:02:06.620 --> 01:02:10.900
I think that, you know, you know, the thing about biology, like is all about

01:02:11.140 --> 01:02:14.980
making really important decisions really quickly on very limited information.

01:02:15.140 --> 01:02:16.540
I mean, that's what biology is all about.

01:02:16.540 --> 01:02:18.180
You have to act, you have to act now.

01:02:18.220 --> 01:02:21.500
The stakes are very high and you don't know most of what you

01:02:21.500 --> 01:02:22.500
need to know to be perfect.

01:02:22.500 --> 01:02:25.860
And so there's not even an attempt to be, to be perfect or to get it right.

01:02:25.900 --> 01:02:31.060
In any sense, there are just, uh, things like active inference, minimize surprise,

01:02:31.300 --> 01:02:35.140
optimize, uh, some, some efficiency and, and, and some things like this.

01:02:35.500 --> 01:02:37.540
Not that that guides the whole, the whole business.

01:02:37.620 --> 01:02:43.300
I mentioned to, uh, offline that, um, somebody who's a fan of your work is

01:02:43.300 --> 01:02:50.420
Andre Capati and he's, uh, amongst many things also, uh, writes occasionally

01:02:50.420 --> 01:02:53.620
a great blog and he came up with this idea.

01:02:53.860 --> 01:03:00.500
I don't know if he coined the term, but of software 2.0, uh, where the programming

01:03:00.540 --> 01:03:05.620
is done in the space of configuring these, uh, artificial neural networks.

01:03:06.380 --> 01:03:10.220
Is there some sense in which that would be the future of programming for us

01:03:10.220 --> 01:03:20.260
humans where we're less doing like Python, like programming and more, um, how

01:03:20.260 --> 01:03:21.540
would you, how would that look like?

01:03:21.580 --> 01:03:28.220
But basically doing the hyper parameters of something akin to a biological system

01:03:28.900 --> 01:03:33.980
and watching it go and keeping adjusting it and creating some kind of feedback

01:03:33.980 --> 01:03:34.940
loop within the system.

01:03:34.940 --> 01:03:36.140
So correct itself.

01:03:36.700 --> 01:03:42.460
And then we watch it over time, accomplish the goals we wanted to accomplish.

01:03:42.460 --> 01:03:47.340
Is that kind of the, the dream of the, the dogs that you describe in the nature paper?

01:03:47.380 --> 01:03:47.580
Yeah.

01:03:47.980 --> 01:03:48.340
Yeah.

01:03:48.340 --> 01:03:54.020
I mean, that's what you just painted is a, is a very good description of our

01:03:54.460 --> 01:03:57.740
efforts at regenerative medicine as a kind of somatic psychiatry.

01:03:58.020 --> 01:04:02.020
So the idea is that you're not, you know, you're not trying to micromanage.

01:04:02.020 --> 01:04:05.540
I mean, think about the limitations of, of, of a lot of the medicines today,

01:04:05.820 --> 01:04:12.020
we try to interact down at the level of pathways, right?

01:04:12.020 --> 01:04:13.580
So, so we're trying to micromanage it.

01:04:14.340 --> 01:04:15.740
What the, what's the problem?

01:04:15.740 --> 01:04:20.100
Well, one problem is that for almost every medicine, other than antibiotics,

01:04:20.660 --> 01:04:23.300
once you stop it, the problem comes right back.

01:04:23.300 --> 01:04:24.220
You haven't fixed anything.

01:04:24.220 --> 01:04:25.180
You were addressing symptoms.

01:04:25.180 --> 01:04:27.780
You weren't actually curing anything again, except for antibiotics.

01:04:28.380 --> 01:04:29.860
Uh, that's one problem.

01:04:29.860 --> 01:04:32.860
The other problem is you have massive amount of side effects because you were

01:04:32.860 --> 01:04:35.380
trying to interact at the lowest level.

01:04:35.540 --> 01:04:36.140
It's right.

01:04:36.140 --> 01:04:38.980
It's like, I'm going to, you know, I'm going to, I'm going to try to program

01:04:38.980 --> 01:04:43.620
this computer by changing the, uh, the melting point of copper, like, maybe you

01:04:43.620 --> 01:04:47.020
can do things that way, but my God, it's hard to, to, to program at the, right.

01:04:47.020 --> 01:04:47.980
At the, at the hardware level.

01:04:48.460 --> 01:04:52.620
So what, what I think we're, we're, we're starting to understand is that.

01:04:53.220 --> 01:04:57.300
And by the way, this goes back to what you were saying before about, uh, that

01:04:57.300 --> 01:04:59.300
we could have access to our internal state, right?

01:04:59.300 --> 01:05:00.860
So people who practice that kind of stuff, right?

01:05:00.860 --> 01:05:05.340
So yoga and biofeedback and those, those are all the people that uniformly will

01:05:05.340 --> 01:05:08.100
say things like, well, the body has an intelligence and this and that, right?

01:05:08.340 --> 01:05:11.660
Like those two sets overlap perfectly because, because that's exactly right.

01:05:11.660 --> 01:05:14.860
Because once you, once you start thinking about it that way, you realize

01:05:14.860 --> 01:05:18.580
that the better locus of control is not always at the lowest level.

01:05:18.580 --> 01:05:21.260
This is why we don't all program with a soldering iron, right?

01:05:21.500 --> 01:05:26.300
We, we, we take advantage of, of the high level intelligences that are there,

01:05:26.460 --> 01:05:29.820
which means trying to figure out, okay, which of your tissues can learn, what can

01:05:29.820 --> 01:05:34.220
they learn, uh, why, you know, why is it that, um, certain drugs stop working

01:05:34.220 --> 01:05:36.660
after you take them for a while with this habituation, right?

01:05:36.660 --> 01:05:40.660
And so can we understand habituation, sensitization, associative learning,

01:05:41.340 --> 01:05:43.940
and these kinds of things in chemical pathways, we're going to have a

01:05:43.940 --> 01:05:45.780
completely different way, I think.

01:05:46.340 --> 01:05:49.140
Um, we're going to have a completely different way of, of using drugs

01:05:49.180 --> 01:05:53.260
and of medicine in general, when we start focusing on, um, the goal states

01:05:53.260 --> 01:05:56.940
and on the intelligence of our subsystems, as opposed to treating everything

01:05:56.940 --> 01:05:59.980
as if the only path was micromanagement from chemistry upwards.

01:06:00.300 --> 01:06:03.460
Well, can you speak to this idea of somatic psychiatry?

01:06:03.940 --> 01:06:05.340
What are somatic cells?

01:06:05.340 --> 01:06:10.940
How do they form networks that use bioelectricity to have memory

01:06:10.940 --> 01:06:11.860
and all those kinds of things?

01:06:11.860 --> 01:06:12.060
Yeah.

01:06:12.500 --> 01:06:14.180
What are somatic cells like basics here?

01:06:14.220 --> 01:06:17.500
Somatic cells just means the cells of your body so much as means body, right?

01:06:17.500 --> 01:06:20.620
So somatic cells are just the, I'm not even specifically making a distinction

01:06:20.620 --> 01:06:22.660
between somatic cells and stem cells or anything like that.

01:06:22.660 --> 01:06:25.820
I mean, basically all the cells in your body, not just neurons, but all

01:06:25.820 --> 01:06:30.180
the cells in your body, they form electrical networks during embryogenesis,

01:06:30.180 --> 01:06:35.220
during regeneration, what those networks are doing in part is processing

01:06:35.220 --> 01:06:39.580
information about what our current shape is and what the goal shape is.

01:06:39.780 --> 01:06:40.540
Now, how do I know this?

01:06:41.100 --> 01:06:43.180
Because I can give you a couple of examples.

01:06:43.180 --> 01:06:47.260
One, one example is when we started studying this, we said, okay, here's

01:06:47.260 --> 01:06:49.180
a, here's a planarian, a planarian is a flatworm.

01:06:49.540 --> 01:06:51.300
It has one head and one tail normally.

01:06:51.700 --> 01:06:54.980
And the amazing, the several amazing things about planaria, but basically

01:06:54.980 --> 01:06:58.780
they, they kind of, I think, I think planaria hold the answer to pretty

01:06:58.780 --> 01:07:00.580
much every deep question of life.

01:07:01.060 --> 01:07:03.460
For one thing, they're similar to our ancestors.

01:07:03.460 --> 01:07:04.620
So they're, they have true symmetry.

01:07:04.620 --> 01:07:05.340
They have a true brain.

01:07:05.340 --> 01:07:06.220
They're not like earthworms.

01:07:06.220 --> 01:07:07.940
They're, you know, they're much more advanced life form.

01:07:08.140 --> 01:07:10.620
They have lots of different internal organs, but they, these little, they're

01:07:10.620 --> 01:07:13.660
about, you know, maybe two centimeters in the centimeter to two in size.

01:07:14.620 --> 01:07:15.740
They have a head and a tail.

01:07:16.220 --> 01:07:19.940
And the first thing is planaria are immortal, so they do not age.

01:07:19.940 --> 01:07:21.380
There's no such thing as an old planarian.

01:07:21.580 --> 01:07:24.300
So that right there tells you that these theories of thermodynamic

01:07:24.860 --> 01:07:26.700
limitations of on lifespan are wrong.

01:07:26.700 --> 01:07:29.620
It's not, it's not that well over time of everything degrades.

01:07:29.620 --> 01:07:33.460
No planaria can keep it going for probably, you know, how long have

01:07:33.460 --> 01:07:34.820
they been around 400 million years.

01:07:34.980 --> 01:07:35.180
Right.

01:07:35.180 --> 01:07:39.060
So these are the actual, so the planaria in our lab are actually in physical

01:07:39.060 --> 01:07:41.700
continuity with planaria that were here 400 million years ago.

01:07:42.180 --> 01:07:45.780
So there's planaria that have lived that long, essentially.

01:07:46.140 --> 01:07:47.780
What does it mean, physical continuity?

01:07:47.820 --> 01:07:50.780
Because, because what they do is they split in half, the way they

01:07:50.780 --> 01:07:52.340
reproduce is they split in half.

01:07:52.540 --> 01:07:56.020
So, so the planaria, the back, the backend grabs the petri dish, the front

01:07:56.020 --> 01:07:58.380
end takes off and they, they rip themselves in half.

01:07:58.460 --> 01:08:04.980
But isn't, isn't it some sense that we're like, you are a physical continuation?

01:08:04.980 --> 01:08:08.020
Yes, except that, except that we go through a bottleneck of one cell,

01:08:08.060 --> 01:08:10.940
which is the egg, they do not, I mean, they can, there are certain planaria.

01:08:11.020 --> 01:08:11.420
Got it.

01:08:11.420 --> 01:08:15.620
So we go through a very ruthless compression process and they don't.

01:08:15.620 --> 01:08:18.500
Yes, like an auto encoder, you know, sort of squashed down to

01:08:18.500 --> 01:08:19.580
one cell and then back out.

01:08:19.900 --> 01:08:22.380
These, these guys just tear themselves in half.

01:08:22.380 --> 01:08:24.060
And then each, and then, and so the other amazing thing

01:08:24.060 --> 01:08:25.020
about them is they regenerate.

01:08:25.300 --> 01:08:26.700
So you can cut them into pieces.

01:08:26.780 --> 01:08:30.340
The record is I think 276 or something like that by Thomas Hunt Morgan.

01:08:30.740 --> 01:08:33.500
Uh, and each piece regrows a perfect little worm.

01:08:33.900 --> 01:08:37.500
They know exactly, every piece knows exactly what's missing, what needs to happen.

01:08:37.780 --> 01:08:42.900
Uh, in fact, in fact, if you chop it in half, as it grows the other half, uh, the

01:08:42.900 --> 01:08:47.060
original, the original tissue shrinks so that when the new tiny head shows up,

01:08:47.060 --> 01:08:47.780
they're proportional.

01:08:48.060 --> 01:08:49.980
So it keeps, it keeps perfect proportion.

01:08:50.140 --> 01:08:51.820
If you, if you starve them, they shrink.

01:08:51.820 --> 01:08:53.260
If you feed them again, they expand.

01:08:53.500 --> 01:08:56.580
They, their control, their anatomical control is, is, is just insane.

01:08:56.580 --> 01:08:58.980
Somebody cut them into over 200 pieces.

01:08:59.020 --> 01:08:59.740
Yeah, yeah, yeah.

01:08:59.740 --> 01:09:00.580
Thomas Hunt Morgan did.

01:09:00.700 --> 01:09:01.860
Hashtag science.

01:09:01.900 --> 01:09:02.100
Yep.

01:09:02.180 --> 01:09:02.580
Amazing.

01:09:02.620 --> 01:09:02.820
Yeah.

01:09:02.820 --> 01:09:03.420
And maybe more.

01:09:03.420 --> 01:09:04.820
I mean, they didn't have antibiotics back then.

01:09:04.820 --> 01:09:06.140
I bet he lost some due to infection.

01:09:06.180 --> 01:09:07.620
I bet, I bet it's actually more than that.

01:09:07.620 --> 01:09:08.860
You could, I bet you could do more than that.

01:09:08.900 --> 01:09:10.140
Humans can't do that.

01:09:11.820 --> 01:09:12.620
Well, yes.

01:09:12.700 --> 01:09:18.060
I mean, again, true, except that at the embryonic level, that's the thing.

01:09:18.060 --> 01:09:18.260
Right.

01:09:18.260 --> 01:09:21.980
So, so I tell it when I talk about this, I said, just remember that is as amazing

01:09:21.980 --> 01:09:25.620
as it is to grow a whole planarian from a tiny fragment, half of the human

01:09:25.620 --> 01:09:28.420
population can grow a full body from one cell, right?

01:09:28.580 --> 01:09:32.940
So, so development is really, you can look at development as a, as a, just

01:09:32.940 --> 01:09:34.300
an example of regeneration.

01:09:34.900 --> 01:09:35.220
Yeah.

01:09:35.220 --> 01:09:39.340
To think, we'll talk about regenerative medicine, but there's some sense

01:09:39.340 --> 01:09:45.660
of what would be like that warm in like 500 years where I can just go regrow a hand.

01:09:46.100 --> 01:09:46.620
Yep.

01:09:46.660 --> 01:09:48.340
I was given time.

01:09:48.340 --> 01:09:51.420
It takes time to grow large things, but yeah, I think so.

01:09:51.660 --> 01:09:54.020
I think you can probably, why not accelerate?

01:09:54.340 --> 01:09:56.380
Oh, biology takes its time.

01:09:56.900 --> 01:09:59.140
I'm not going to say anything is impossible, but I don't know of a

01:09:59.140 --> 01:10:00.540
way to accelerate these processes.

01:10:00.540 --> 01:10:01.340
I think it's possible.

01:10:01.340 --> 01:10:04.180
I think we are going to be regenerative, but I don't know of a way

01:10:04.180 --> 01:10:04.980
to make it faster.

01:10:04.980 --> 01:10:08.620
I could just think people from a few centuries from now would be like, well,

01:10:08.620 --> 01:10:13.780
they have to, they used to have to wait a week for the hand to regrow.

01:10:14.260 --> 01:10:18.420
It's like when the microwave was invented, you can, you can toast here.

01:10:18.900 --> 01:10:21.900
Um, what's that called when you put a cheese on a toast?

01:10:22.020 --> 01:10:26.700
Um, but it's delicious is all I know.

01:10:26.980 --> 01:10:29.300
I'm blanking and you, and you, all right.

01:10:29.300 --> 01:10:33.340
So, uh, planaria, why were we talking about the magical planaria that they

01:10:33.340 --> 01:10:34.420
have the mystery of life?

01:10:34.420 --> 01:10:34.620
Yeah.

01:10:34.620 --> 01:10:36.780
So, so the reason we're talking about planaria is not only are they

01:10:36.780 --> 01:10:39.820
immortal, okay, not only do they regenerate every part of the body.

01:10:40.260 --> 01:10:43.500
Uh, they do, they generally don't get cancer, right?

01:10:43.780 --> 01:10:45.740
So, which we can talk about why that's important.

01:10:46.020 --> 01:10:46.500
They're smart.

01:10:46.500 --> 01:10:47.860
They can learn things so you can train them.

01:10:48.220 --> 01:10:51.780
And it turns out that if you train a planarian and then cut their heads off,

01:10:52.020 --> 01:10:54.820
the tail will regenerate a brand new brain that still remembers

01:10:54.820 --> 01:10:55.860
the original information.

01:10:56.140 --> 01:10:58.820
Do they have a bioelectrical network going on or no?

01:10:59.060 --> 01:10:59.340
Yes.

01:10:59.380 --> 01:11:02.660
So their somatic cells are forming a network.

01:11:02.660 --> 01:11:04.580
And that's, that's what you mean by true brain.

01:11:04.780 --> 01:11:06.700
Well, what's the requirement for a true brain?

01:11:06.780 --> 01:11:08.020
I like everything else.

01:11:08.020 --> 01:11:11.860
It's a continuum, but, but, but a true brain has certain characteristics as far

01:11:11.860 --> 01:11:16.060
as the density, like a localized density of neurons that guides behavior in the

01:11:16.060 --> 01:11:17.980
head, exactly, exactly.

01:11:17.980 --> 01:11:21.220
If you cut their head off, uh, the, the tail doesn't have, that doesn't do

01:11:21.220 --> 01:11:24.580
anything, it just sits there until the new brain is, is, is, you know, until a

01:11:24.580 --> 01:11:27.420
new brain regenerates, they have all the same neurotransmitters that you and I

01:11:27.420 --> 01:11:30.540
have, but here's why, here's what we're talking about them in this, in this

01:11:30.540 --> 01:11:31.140
context.

01:11:31.500 --> 01:11:32.380
So here's your planaria.

01:11:32.380 --> 01:11:34.380
You cut off the head, you cut off the tail, you have a middle fragment.

01:11:34.660 --> 01:11:36.660
That middle fragment has to make one head and one tail.

01:11:36.900 --> 01:11:39.820
How does it know how many of each to make and where do they go?

01:11:39.820 --> 01:11:40.780
How come it doesn't switch?

01:11:40.780 --> 01:11:41.060
How come?

01:11:41.060 --> 01:11:41.260
Right.

01:11:41.780 --> 01:11:47.100
So, so we did a very simple thing and we said, okay, let's, let's make the

01:11:47.100 --> 01:11:51.660
hypothesis that there's a somatic electrical network that remembers the

01:11:51.660 --> 01:11:54.540
correct pattern and that what it's doing is, is recalling that memory

01:11:54.540 --> 01:11:55.540
and building to that pattern.

01:11:55.900 --> 01:11:59.700
So what we did was we used a, um, a way to visualize electrical

01:11:59.700 --> 01:12:01.140
activity in these cells, right?

01:12:01.140 --> 01:12:03.620
It's a, it's a, it's a variant of what people are used to look for

01:12:03.620 --> 01:12:04.500
electricity in the brain.

01:12:04.980 --> 01:12:08.460
And we saw that it has a, that that fragment has a very, very

01:12:08.460 --> 01:12:10.220
particular, um, electrical pattern.

01:12:10.220 --> 01:12:10.980
You can literally see it.

01:12:11.180 --> 01:12:14.700
Once, once we developed the technique, it has a very particular electrical

01:12:14.700 --> 01:12:18.300
pattern that shows you where the head and the tail goes, right?

01:12:18.300 --> 01:12:19.180
You can, you can just see it.

01:12:19.500 --> 01:12:22.980
And then we said, okay, well now let's test the idea that that's a memory that

01:12:22.980 --> 01:12:25.940
actually controls where the head and the tail goes, let's change that pattern.

01:12:25.940 --> 01:12:27.660
So basically incept the false memory.

01:12:27.940 --> 01:12:30.100
And so what you can do is you can do that in many different ways.

01:12:30.100 --> 01:12:34.380
One way is with, um, drugs, the target ion channels to say, and so you pick

01:12:34.380 --> 01:12:37.700
these drugs and you say, okay, I'm going to do it so that instead of, so that

01:12:37.700 --> 01:12:40.780
instead of this one head, one tail pat electrical pattern, you have a two

01:12:40.780 --> 01:12:41.860
headed pattern, right?

01:12:41.860 --> 01:12:44.340
You're just editing the electrical information in the, in the network.

01:12:44.740 --> 01:12:45.500
When you do that, guess what?

01:12:45.500 --> 01:12:47.260
The cells build, they build a two headed worm.

01:12:47.700 --> 01:12:50.340
And the coolest thing about it now, no, no genetic changes.

01:12:50.340 --> 01:12:51.220
So we haven't touched the genome.

01:12:51.220 --> 01:12:53.900
The genome is totally wild type, but the amazing thing about it is that when

01:12:53.900 --> 01:12:58.420
you take these two headed animals and you cut them into pieces again, some

01:12:58.420 --> 01:13:00.660
of those pieces will continue to make two headed animals.

01:13:01.540 --> 01:13:05.220
So, so that information, that, that memory, that, that electrical circuit,

01:13:05.220 --> 01:13:08.500
not only does it hold the information for how many heads, not only does it

01:13:08.500 --> 01:13:11.940
use that information to tell the cells what to do to regenerate, but it stores

01:13:11.940 --> 01:13:15.420
it once you've reset it, it keeps, and we can go back, we can take a two headed

01:13:15.420 --> 01:13:17.340
animal and put it back to one headed.

01:13:17.740 --> 01:13:20.700
So now imagine, so there's a couple of interesting things here that, um, that

01:13:20.700 --> 01:13:23.380
have implications for understanding what genomes and things like that.

01:13:24.020 --> 01:13:25.580
Imagine I take this two headed animal.

01:13:26.540 --> 01:13:29.020
Oh, and by the way, when they reproduce, when they tear themselves in half,

01:13:29.100 --> 01:13:30.580
you still get two headed animals.

01:13:30.780 --> 01:13:32.980
So imagine I take them and I throw them in the Charles river over here.

01:13:33.140 --> 01:13:35.140
So a hundred years later, some scientists come along and they

01:13:35.140 --> 01:13:37.900
scoop up some samples and they go, oh, there's a single headed

01:13:37.900 --> 01:13:38.980
form and a two headed form.

01:13:39.140 --> 01:13:39.380
Wow.

01:13:39.380 --> 01:13:40.380
A speciation event.

01:13:40.380 --> 01:13:40.700
Cool.

01:13:40.700 --> 01:13:43.180
Let's sequence the genome and see why, what happened.

01:13:43.380 --> 01:13:44.220
Genomes are identical.

01:13:44.220 --> 01:13:45.180
It's nothing wrong with the genome.

01:13:45.460 --> 01:13:48.580
So if you ask the question, how does, so, so this goes back to your very first

01:13:48.580 --> 01:13:50.340
question is where do body plans come from, right?

01:13:50.940 --> 01:13:53.380
How does the planarian know how many heads it's supposed to have?

01:13:53.700 --> 01:13:57.940
Now it's interesting because you could say DNA, but what happened, what,

01:13:57.940 --> 01:14:02.300
what, as it turns out, the DNA produces a piece of hardware that by

01:14:02.300 --> 01:14:06.580
default says one head, the way that when you turn on a calculator, by

01:14:06.580 --> 01:14:08.380
default, it's a zero every single time, right?

01:14:08.380 --> 01:14:10.660
When you turn it on, it says zero, but it's a programmable

01:14:10.660 --> 01:14:11.820
calculator as it turns out.

01:14:11.980 --> 01:14:15.260
So once you've changed that next time, it won't say zero.

01:14:15.260 --> 01:14:16.020
It'll say something else.

01:14:16.020 --> 01:14:16.700
And the same thing here.

01:14:16.700 --> 01:14:18.620
So you can make, you can make one headed, two headed.

01:14:18.620 --> 01:14:19.740
You can make no headed worms.

01:14:19.980 --> 01:14:21.860
We've done some other things along these lines, some other

01:14:21.860 --> 01:14:23.100
really weird constructs.

01:14:24.020 --> 01:14:26.700
So, so this, this, this, this question of, right.

01:14:26.740 --> 01:14:27.860
So again, it's really important.

01:14:27.860 --> 01:14:30.100
The, the, the hardware software distinction is really important

01:14:30.540 --> 01:14:34.460
because the hardware is essential because without proper hardware, you're

01:14:34.460 --> 01:14:37.180
never going to get to the right physiology of having that memory.

01:14:37.460 --> 01:14:40.620
But once you have it, it doesn't fully determine what the

01:14:40.620 --> 01:14:41.700
information is going to be.

01:14:41.740 --> 01:14:44.780
You can have other information in there and it's reprogrammable by us, by

01:14:44.780 --> 01:14:48.020
bacteria, by various parasites, probably things like that.

01:14:48.420 --> 01:14:51.100
The other amazing thing about these planaries, think about this.

01:14:51.620 --> 01:14:55.580
Most animals, when we get a mutation in our bodies, our children don't inherit it.

01:14:55.620 --> 01:14:55.860
Right.

01:14:55.860 --> 01:14:58.580
So you could go on, you could run around for 50, 60 years, getting

01:14:58.580 --> 01:15:01.180
mutations, your children don't have those mutations because we

01:15:01.180 --> 01:15:02.100
go through the egg stage.

01:15:02.340 --> 01:15:04.900
Planaria tear themselves in half and that's how they reproduce.

01:15:05.180 --> 01:15:09.020
So for 400 million years, they keep every mutation that they've had

01:15:09.020 --> 01:15:10.500
that doesn't kill the cell that it's in.

01:15:10.820 --> 01:15:13.940
So when you look at these planaria, their bodies are what's called mix

01:15:13.940 --> 01:15:16.740
diploid, meaning that every cell might have a different number of chromosomes.

01:15:16.860 --> 01:15:17.660
They look like a tumor.

01:15:17.660 --> 01:15:21.220
If you look at the, the, the, the, the, the genome is an incredible mess

01:15:21.220 --> 01:15:22.500
because they accumulate all this stuff.

01:15:22.900 --> 01:15:27.580
And yet the, their body structure is, they are the best regenerators on the planet.

01:15:27.580 --> 01:15:30.660
Their anatomy is rock solid, even though their genome is all kinds of crap.

01:15:31.020 --> 01:15:33.100
So this is a kind of a scandal, right?

01:15:33.100 --> 01:15:36.980
That, you know, when we learn that, well, you know, what are genomes to what?

01:15:36.980 --> 01:15:37.980
Genomes determine your body.

01:15:38.100 --> 01:15:38.260
Okay.

01:15:38.260 --> 01:15:41.460
Why is the animal with the worst genome have the best anatomical control, the

01:15:41.460 --> 01:15:43.500
most cancer resistant, the most regenerative, right?

01:15:43.980 --> 01:15:47.460
Really, we're just beginning to start to understand this relationship

01:15:47.460 --> 01:15:51.100
between the genomically determined hardware and, and, and by the way, just

01:15:51.100 --> 01:15:55.220
as of, as of a couple of months ago, I think I now somewhat understand why this

01:15:55.220 --> 01:15:57.980
is, but it's really, it's really a major, you know, major puzzle.

01:15:58.380 --> 01:16:03.580
I mean, that really throws a wrench into the whole nature versus nurture.

01:16:04.420 --> 01:16:10.020
Cause you usually associate electricity with the, with the nurture

01:16:11.340 --> 01:16:13.020
and the hardware with the nature.

01:16:13.500 --> 01:16:17.300
And it's, there's just this weird integrated mess.

01:16:17.540 --> 01:16:19.300
That propagates to generations.

01:16:19.380 --> 01:16:20.820
Yeah, it's much more fluid.

01:16:20.820 --> 01:16:21.980
It's much more complex.

01:16:22.380 --> 01:16:26.420
Um, you can, you can imagine what's, what's happening here is just, just

01:16:26.460 --> 01:16:29.580
imagine the evolution of a, of a, of a, of an animal like this, that, that

01:16:29.580 --> 01:16:32.060
multi-scaleless goes back to this multi-scale competency, right?

01:16:32.700 --> 01:16:36.820
Imagine that you have two, two, two, you have, you have an animal that, um, that

01:16:36.820 --> 01:16:40.020
where it's, it's tissues have some degree of multi-scale competency.

01:16:40.020 --> 01:16:43.700
So for example, if the, like, like we saw in the tadpole, you know, if you put an eye

01:16:43.700 --> 01:16:45.780
on its tail, they can still see out of that eye, right?

01:16:45.820 --> 01:16:47.820
That the, you know, there's all, there's incredible plasticity.

01:16:48.180 --> 01:16:51.980
So if you have an animal and it comes up for selection and, uh, the fitness is quite

01:16:51.980 --> 01:16:57.540
good, evolution doesn't know whether the fitness is good because the genome was

01:16:57.540 --> 01:17:01.100
awesome or because the genome was kind of junky, but, but the competency made

01:17:01.100 --> 01:17:01.940
up for it, right?

01:17:01.940 --> 01:17:03.300
And things kind of ended up good.

01:17:03.580 --> 01:17:07.260
So what that means is that the more competency you have, the harder it is

01:17:07.260 --> 01:17:09.020
for selection to pick the best genomes.

01:17:09.220 --> 01:17:10.780
It hides information, right?

01:17:10.980 --> 01:17:15.340
And so that means that, uh, so, so what happens, you know, evolution starts,

01:17:15.380 --> 01:17:18.940
basically starts all the start, all the hard work is being done to increase the

01:17:18.940 --> 01:17:21.820
competency because it's harder and harder to see the genomes.

01:17:22.140 --> 01:17:25.820
And so I think in planaria, what happened is that there's this runaway phenomenon

01:17:25.820 --> 01:17:30.060
where all the effort went into the algorithm such that we know you got a

01:17:30.060 --> 01:17:33.060
crappy genome, we can't keep, we can't clean up the genome, we can't keep track

01:17:33.060 --> 01:17:37.380
of it, so what's going to happen is what survives are the algorithms that can

01:17:37.380 --> 01:17:39.660
create a great worm, no matter what the genome is.

01:17:39.860 --> 01:17:43.540
So everything went into the algorithm and which, which of course, then reduces

01:17:43.540 --> 01:17:45.820
the pressure on keeping a, you know, keeping a clean genome.

01:17:46.540 --> 01:17:47.740
So this idea of, right.

01:17:47.740 --> 01:17:51.100
And to, and different animals have this in different, to different levels, but

01:17:51.100 --> 01:17:56.020
this idea of, of putting energy into an algorithm that does not overtrain on

01:17:56.020 --> 01:17:57.700
priors, right, it can't assume.

01:17:57.700 --> 01:18:00.820
I mean, I think biology is this way in general, evolution doesn't take the

01:18:00.820 --> 01:18:05.260
past too seriously because it makes these basically problem-solving machines

01:18:05.300 --> 01:18:09.220
as opposed to like exactly what, you know, to, to, to deal with exactly

01:18:09.220 --> 01:18:10.020
what happened last time.

01:18:10.220 --> 01:18:10.460
Yeah.

01:18:10.460 --> 01:18:12.740
Problem-solving versus memory recall.

01:18:13.300 --> 01:18:15.740
So a little memory, but a lot of problem-solving.

01:18:15.780 --> 01:18:16.260
I think so.

01:18:16.300 --> 01:18:16.540
Yeah.

01:18:16.580 --> 01:18:17.460
In many cases, yeah.

01:18:17.460 --> 01:18:18.660
Problem-solving.

01:18:22.340 --> 01:18:25.740
I mean, it's incredible that those kinds of systems are able to be constructed.

01:18:26.540 --> 01:18:30.660
Um, especially how much they contrast with the way we build problem-solving

01:18:30.660 --> 01:18:32.380
systems in the AI world.

01:18:33.140 --> 01:18:40.100
Um, back to Xenobots, I'm not sure if we ever described how Xenobots are built,

01:18:40.100 --> 01:18:45.260
but you have a paper titled biological robots, perspectives on an emerging

01:18:45.420 --> 01:18:49.580
interdisciplinary field and the beginning you, uh, you mentioned that the

01:18:49.580 --> 01:18:52.300
word Xenobots is like controversial.

01:18:52.300 --> 01:18:55.420
Did you guys get in trouble for using Xenobots or what?

01:18:55.420 --> 01:18:57.100
Do people not like the word Xenobots?

01:18:57.420 --> 01:19:01.460
Are you trying to be provocative with the word Xenobots versus biological

01:19:01.460 --> 01:19:02.100
robots?

01:19:02.540 --> 01:19:03.020
I don't know.

01:19:03.100 --> 01:19:05.580
Is there some drama that we should be aware of?

01:19:05.980 --> 01:19:06.980
There's a little bit of drama.

01:19:07.020 --> 01:19:13.460
Uh, I think, I think the drama is basically related to people, um, having

01:19:13.460 --> 01:19:16.340
very fixed ideas about what terms mean.

01:19:16.660 --> 01:19:21.660
And I think in many cases, these ideas are completely out of date with,

01:19:21.740 --> 01:19:23.660
with where science is now.

01:19:23.940 --> 01:19:27.540
And for sure they're, they're out of date with what's going to be.

01:19:27.580 --> 01:19:31.340
I mean, these, these, these concepts, uh, are not going to

01:19:31.340 --> 01:19:32.740
survive the next couple of decades.

01:19:32.780 --> 01:19:36.740
So if you ask a person and including, um, you know, a lot of people in biology

01:19:36.780 --> 01:19:39.700
who kind of want to keep a sharp distinction between biologicals and

01:19:39.700 --> 01:19:40.260
robots, right?

01:19:40.340 --> 01:19:41.100
So what's a robot?

01:19:41.100 --> 01:19:43.300
Well, a robot, it comes out of a factory.

01:19:43.300 --> 01:19:44.220
It's made by humans.

01:19:44.220 --> 01:19:44.980
It is boring.

01:19:44.980 --> 01:19:47.460
It is a meaning that you can predict everything it's going to do.

01:19:47.740 --> 01:19:50.740
It's made of metal and certain other inorganic materials, living

01:19:50.740 --> 01:19:51.740
organisms are magical.

01:19:51.740 --> 01:19:53.260
They, they, they arise right then.

01:19:53.260 --> 01:19:53.540
So on.

01:19:53.540 --> 01:19:54.500
So there's these distinctions.

01:19:54.500 --> 01:20:00.300
I think these, these distinctions, I think were never good, but, uh, they're

01:20:00.300 --> 01:20:02.460
going to be completely useless going forward.

01:20:02.460 --> 01:20:05.260
And so part of, there's a couple of papers that that's one paper.

01:20:05.260 --> 01:20:07.220
And there's another one that Josh Bongard and I wrote where we

01:20:07.220 --> 01:20:09.260
really attack the terminology.

01:20:09.660 --> 01:20:15.260
And we say these binary categories are based on very, um, non-essential

01:20:15.260 --> 01:20:19.740
kind of surface, uh, limitations of, of technology and imagination

01:20:19.740 --> 01:20:22.140
that were true before, but they've got to go.

01:20:22.340 --> 01:20:23.860
And so, and so we call them xenobots.

01:20:23.860 --> 01:20:27.660
So, so Zeno for Xenopus Lavis where this is, it's the frog that, that

01:20:27.660 --> 01:20:31.300
these guys are made of, but we think it's an example of, of, of, of a

01:20:31.300 --> 01:20:36.700
biobot technology, because ultimately if we, if we under, once we understand

01:20:36.740 --> 01:20:42.140
how to, uh, communicate and manipulate, um, the inputs to these cells, we

01:20:42.140 --> 01:20:45.820
will be able to get them to build whatever we want them to build.

01:20:45.820 --> 01:20:46.860
And that's robotics, right?

01:20:46.860 --> 01:20:50.260
It's, it's the rational construction of machines that have useful purposes.

01:20:50.380 --> 01:20:53.180
I absolutely think that this is a robotics platform,

01:20:53.180 --> 01:20:54.860
whereas some biologists don't.

01:20:55.340 --> 01:21:00.740
But it's built in a way that, uh, all the different components are

01:21:00.740 --> 01:21:02.060
doing their own computation.

01:21:02.100 --> 01:21:05.020
So in a way that we've been talking about, so you're trying to do top

01:21:05.020 --> 01:21:06.940
down control on that biological system.

01:21:07.100 --> 01:21:09.860
And in the future, all of this will, will, will merge together because of

01:21:09.860 --> 01:21:12.620
course, at some point we're going to throw in synthetic biology circuits,

01:21:12.620 --> 01:21:15.540
right, new, new, um, you know, new transcriptional circuits to

01:21:15.540 --> 01:21:16.420
get them to do new things.

01:21:16.420 --> 01:21:19.300
Of course we'll throw some of that in, but we specifically stayed away from

01:21:19.300 --> 01:21:22.540
all of that because in the first few papers, and there's some more coming

01:21:22.540 --> 01:21:26.860
down the pike that are I think going to be pretty, pretty dynamite, um, that,

01:21:26.900 --> 01:21:30.660
uh, we want to show what the native cells are made of, because what happens is,

01:21:30.780 --> 01:21:33.500
you know, if you engineer the heck out of them, right, if we were to put in

01:21:33.500 --> 01:21:36.820
new, you know, new transcription factors and some new metabolic machinery and

01:21:36.820 --> 01:21:40.380
whatever, people will say, well, okay, you engineered this and you made it,

01:21:40.380 --> 01:21:41.300
do whatever and fine.

01:21:42.060 --> 01:21:47.540
I wanted to show, uh, and, and the whole team, uh, wanted to show the

01:21:47.580 --> 01:21:50.060
plasticity and the intelligence in the biology.

01:21:50.220 --> 01:21:54.460
What does it do that's surprising before you even start manipulating

01:21:54.460 --> 01:21:55.380
the hardware in that way?

01:21:56.060 --> 01:21:56.540
Yeah.

01:21:56.580 --> 01:21:59.660
Don't try to, uh, over control the thing.

01:21:59.660 --> 01:22:03.500
Let it flourish the full beauty of the biological system.

01:22:03.500 --> 01:22:06.940
Why is Xenopus, Levis, Levis, how do you pronounce it?

01:22:07.220 --> 01:22:08.380
Frog Xenopus, Levis.

01:22:08.380 --> 01:22:08.580
Yeah.

01:22:08.740 --> 01:22:08.900
Yeah.

01:22:08.900 --> 01:22:12.460
It's a very popular, it's been used since I think the fifties.

01:22:12.500 --> 01:22:16.180
Uh, it's just very convenient because you can, you, you know, we, we keep

01:22:16.180 --> 01:22:18.660
the adults in this, in this, uh, very fine frog habitat.

01:22:18.660 --> 01:22:21.140
They lay eggs, they lay tens of thousands of eggs at a time.

01:22:21.580 --> 01:22:24.780
Um, the eggs develop right in front of your eyes.

01:22:24.780 --> 01:22:28.860
It's the most magical thing you can, you can see because normally, you

01:22:28.860 --> 01:22:31.380
know, if you were to deal with mice or rabbits or whatever, you don't see

01:22:31.380 --> 01:22:34.100
the early stages, because everything's inside the mother, everything's in

01:22:34.100 --> 01:22:35.260
a petri dish at room temperature.

01:22:35.420 --> 01:22:38.340
So you just, you, you have an egg, it's fertilized and you can just watch

01:22:38.340 --> 01:22:41.300
it divide and divide and divide and on all the organs forming, you just see it.

01:22:41.620 --> 01:22:45.340
And at that point, um, the community has, has developed lots of different

01:22:45.340 --> 01:22:49.700
tools for understanding what's going on and also for, for manipulating it.

01:22:49.700 --> 01:22:49.820
Right.

01:22:49.820 --> 01:22:53.420
So it's, it's people use it for, um, you know, for understanding birth defects

01:22:53.420 --> 01:22:55.300
and neurobiology and cancer immunology.

01:22:55.300 --> 01:22:58.980
Also you get the whole, uh, embryo genesis in the petri dish.

01:23:00.340 --> 01:23:01.620
That's so cool to watch.

01:23:01.660 --> 01:23:02.620
Is there videos of this?

01:23:02.700 --> 01:23:02.980
Oh yeah.

01:23:03.060 --> 01:23:06.300
Yeah, yeah, there's, yeah, there's, there's amazing videos on online.

01:23:06.460 --> 01:23:08.460
I mean, mammalian embryos are super cool too.

01:23:08.460 --> 01:23:11.180
For example, monozygotic twins are what happens when you cut

01:23:11.180 --> 01:23:12.340
a mammalian embryo in half.

01:23:12.660 --> 01:23:14.060
You don't get two half bodies.

01:23:14.060 --> 01:23:16.780
You get two perfectly normal bodies because it's a regeneration event.

01:23:17.020 --> 01:23:17.300
Right.

01:23:17.300 --> 01:23:19.980
A development is just, uh, is just the kind of regeneration really.

01:23:20.020 --> 01:23:21.820
And why this particular frog?

01:23:22.340 --> 01:23:25.260
It's just, uh, cause they were doing in the fifties and.

01:23:25.780 --> 01:23:31.180
It breeds well in, um, you know, in, in, it's easy to raise in the laboratory

01:23:31.460 --> 01:23:35.340
and, uh, it's very prolific and all the tools basically for decades,

01:23:35.340 --> 01:23:36.540
people have been developing tools.

01:23:36.660 --> 01:23:39.260
There's other pieces of some people use other frogs, but I have to say,

01:23:39.340 --> 01:23:40.660
this is, this is, this is important.

01:23:40.860 --> 01:23:43.820
Xenobots are fundamentally not anything about frogs.

01:23:43.820 --> 01:23:47.180
So, um, I, I can't say too much about this because it's not published

01:23:47.180 --> 01:23:50.540
in peer reviewed yet, but we've made Xenobots out of other things

01:23:50.540 --> 01:23:51.700
that have nothing to do with frogs.

01:23:51.700 --> 01:23:53.340
It's, this is not a frog phenomenon.

01:23:53.340 --> 01:23:54.020
This is it.

01:23:54.020 --> 01:23:57.060
We started with frog because it's so convenient, but this, this,

01:23:57.060 --> 01:23:58.860
this plasticity is not a fraud.

01:23:58.860 --> 01:24:00.700
You know, it's not related to the fact that they're frogs.

01:24:01.340 --> 01:24:02.580
What happens when you kiss it?

01:24:02.580 --> 01:24:03.700
Does it turn to a prince?

01:24:03.820 --> 01:24:04.100
No.

01:24:04.180 --> 01:24:05.460
Or princess, which way?

01:24:05.540 --> 01:24:06.260
Uh, prince.

01:24:06.260 --> 01:24:06.500
Yeah.

01:24:06.500 --> 01:24:07.260
Prince prince.

01:24:07.260 --> 01:24:07.420
Yeah.

01:24:07.460 --> 01:24:10.020
Uh, that's an experiment that I don't believe we've done.

01:24:10.100 --> 01:24:12.020
And if we have, I don't want to collaborate.

01:24:12.020 --> 01:24:14.980
I can, I can take on the lead on that effort.

01:24:15.380 --> 01:24:15.740
Okay.

01:24:15.740 --> 01:24:16.100
Cool.

01:24:16.140 --> 01:24:19.140
Uh, how does this house coordinate?

01:24:19.460 --> 01:24:21.820
Let's focus in on just the embryo genesis.

01:24:22.460 --> 01:24:27.580
So there's one cell, so it divides, doesn't have to be very careful

01:24:27.620 --> 01:24:32.380
about what each cell starts doing once they divide.

01:24:33.100 --> 01:24:37.620
And like when there's three of them, it's like the co-founders or whatever,

01:24:37.860 --> 01:24:42.300
like slow down, you're responsible for this.

01:24:42.580 --> 01:24:46.500
When do they become specialized and how do they coordinate that specialization?

01:24:46.540 --> 01:24:49.940
So, so this is the basic science of developmental biology.

01:24:49.940 --> 01:24:51.860
There's a lot known about all of that.

01:24:51.860 --> 01:24:56.220
But, um, but I'll tell you what I think is kind of the most important part,

01:24:56.220 --> 01:25:02.260
which is yes, it's very important who does what, however, because going back

01:25:02.300 --> 01:25:07.220
to this issue of why I made this claim that, um, biology doesn't take the

01:25:07.220 --> 01:25:08.300
past too seriously.

01:25:08.300 --> 01:25:12.820
And what I mean by that is it doesn't assume that everything is the way it's

01:25:12.820 --> 01:25:14.060
expected to be, right?

01:25:14.300 --> 01:25:15.380
And here's an example of that.

01:25:15.740 --> 01:25:18.580
Um, this was, this was done, this was, this was an old experiment going back

01:25:18.580 --> 01:25:22.980
to the forties, but, um, basically imagine imagine it's a new salamander

01:25:23.300 --> 01:25:25.780
and it's got these little two tubules that go to the kidneys, right?

01:25:25.780 --> 01:25:28.140
This little tube, take a cross section of that tube.

01:25:28.140 --> 01:25:31.420
You see eight to 10 cells that have cooperated to make this little

01:25:31.420 --> 01:25:32.540
tube and cross section, right?

01:25:33.020 --> 01:25:38.700
So one amazing, one amazing thing you can do is, um, you can, you can mess

01:25:38.700 --> 01:25:42.260
with the very early cell division to make the cells gigantic, bigger.

01:25:42.260 --> 01:25:43.580
You can, you can make them different sizes.

01:25:43.580 --> 01:25:44.980
You can force them to be different sizes.

01:25:45.540 --> 01:25:50.100
So if you make the cells different sizes, the whole nude is still the same size.

01:25:50.300 --> 01:25:53.460
So if you take a cross section through the, through that tubule, instead of

01:25:53.460 --> 01:25:56.140
eight to 10 cells, you might have four or five, or you might have, you know,

01:25:56.180 --> 01:26:02.100
three, until you make the cell so enormous that one single cell wraps

01:26:02.100 --> 01:26:06.900
around itself and, and gives you that same large scale structure, but a

01:26:06.900 --> 01:26:08.500
completely different molecular mechanism.

01:26:08.660 --> 01:26:12.300
So now instead of cell to cell communication to make a tubule, instead

01:26:12.300 --> 01:26:15.780
of that, it's one cell using the cytoskeleton to bend itself around.

01:26:16.020 --> 01:26:19.380
So think about what that means in the service of a large scale, it

01:26:19.380 --> 01:26:20.660
talked about top-down control, right?

01:26:20.660 --> 01:26:23.900
In the service of a large scale anatomical feature, different

01:26:23.900 --> 01:26:25.540
molecular mechanisms get called up.

01:26:25.820 --> 01:26:28.700
So now think about this, you're, you're, you're, you're a nude

01:26:28.700 --> 01:26:29.780
cell and trying to make an embryo.

01:26:30.420 --> 01:26:33.700
If you had a fixed idea of who was supposed to do what, it'd be screwed

01:26:33.700 --> 01:26:35.140
because now your cells are gigantic.

01:26:35.140 --> 01:26:35.780
Nothing would work.

01:26:36.020 --> 01:26:40.860
The, there's an incredible tolerance for changes in the size of the parts

01:26:40.980 --> 01:26:42.540
and the amount of DNA in those parts.

01:26:42.820 --> 01:26:44.060
Um, all sorts of stuff.

01:26:44.060 --> 01:26:46.420
You can, you can, the life is highly interoperable.

01:26:46.420 --> 01:26:49.180
You can put electrodes in there and you can put weird nanomaterials.

01:26:49.180 --> 01:26:49.740
It still works.

01:26:50.220 --> 01:26:53.540
It's, it's, uh, this is that problem solving action, right?

01:26:53.540 --> 01:26:56.460
It's able to do what it needs to do, even when circumstances change.

01:26:56.740 --> 01:27:00.420
That is, you know, uh, the hallmark of intelligence, right?

01:27:00.420 --> 01:27:03.220
William James defined intelligence as the ability to get to the

01:27:03.220 --> 01:27:04.500
same goal by different means.

01:27:04.660 --> 01:27:07.660
That's this, you get to the same goal by completely different means.

01:27:07.940 --> 01:27:11.060
And so, so, so why am I bringing this up is just to say that, yeah, it's

01:27:11.060 --> 01:27:14.580
important for the cells to do the right stuff, but they have incredible

01:27:14.580 --> 01:27:18.460
tolerances for things not being what you expect and to still get their job done.

01:27:18.740 --> 01:27:24.140
So if you're, you know, um, all of these things are not hardwired, there

01:27:24.140 --> 01:27:25.660
are organisms that might be hardwired.

01:27:25.660 --> 01:27:29.620
For example, the nematode C elegans in that organism, every cell is

01:27:29.620 --> 01:27:32.940
numbered, meaning that every C elegans has exactly the same number of cells

01:27:32.940 --> 01:27:33.900
as every other C elegans.

01:27:33.900 --> 01:27:34.740
They're all in the same place.

01:27:34.740 --> 01:27:35.260
They all divide.

01:27:35.260 --> 01:27:38.780
There's literally a map of how it works that in that, in that sort of system,

01:27:39.060 --> 01:27:43.140
it's, it's, it's much more cookie cutter, but, but most, most organisms

01:27:43.180 --> 01:27:45.020
are incredibly plastic in that way.

01:27:45.740 --> 01:27:49.380
Is there something particularly magical to you about the whole

01:27:49.380 --> 01:27:51.380
developmental biology process?

01:27:52.660 --> 01:27:54.180
Uh, is there something you could say?

01:27:54.180 --> 01:27:55.460
Cause you just said it.

01:27:55.660 --> 01:27:57.900
They're very good at accomplishing the goal of the job.

01:27:57.900 --> 01:28:03.220
They need to do the competency thing, but you get freaking organism for one cell.

01:28:04.260 --> 01:28:12.420
It's like, uh, I mean, it's very hard, hard to intuit that whole process to even

01:28:12.460 --> 01:28:14.940
think about reverse engineering that process.

01:28:15.020 --> 01:28:15.300
Right.

01:28:15.340 --> 01:28:19.820
Very hard to the point where I often just imagine I, I sometimes ask my

01:28:19.820 --> 01:28:21.140
students to do this thought experiment.

01:28:21.140 --> 01:28:24.500
Imagine you were, you were shrunk down to the, to the scale of a single cell

01:28:24.700 --> 01:28:26.980
and you were in the middle of an embryo and you were looking around at what's

01:28:26.980 --> 01:28:28.500
going on and the cells running around.

01:28:28.500 --> 01:28:30.900
Some cells are dying at the, you know, every time you look, it's kind of a

01:28:30.900 --> 01:28:33.180
different number of cells for most organisms and so on.

01:28:33.660 --> 01:28:37.820
I think that if you didn't know what embryonic development was, you would

01:28:37.820 --> 01:28:40.980
have no clue that what you're seeing is always going to make the same thing.

01:28:40.980 --> 01:28:43.020
Nevermind knowing what that, what that is.

01:28:43.180 --> 01:28:46.340
Nevermind being able to say, even with full genomic information, being able

01:28:46.340 --> 01:28:47.420
to say, what the hell are they building?

01:28:47.420 --> 01:28:48.620
We have no way to do that.

01:28:48.940 --> 01:28:52.780
But, but just even to guess that, wow, the, the, the outcome of all this

01:28:52.780 --> 01:28:56.820
activity is it's always going to be, it's always going to build the same thing.

01:28:56.940 --> 01:29:02.580
The imperative to create the final you as you are now is there already.

01:29:03.220 --> 01:29:06.380
So you can, you would, so she started from the same embryo.

01:29:06.380 --> 01:29:08.820
You would create a very similar organism.

01:29:09.820 --> 01:29:15.020
Yeah, except for cases like the xenobots, when you give them a different

01:29:15.020 --> 01:29:18.140
environment, they come up with a different way to be adaptive in that environment.

01:29:18.380 --> 01:29:24.300
But overall, I mean, so, so I think, so I think to kind of summarize it, I

01:29:24.300 --> 01:29:29.700
think what evolution is really good at is creating hardware that has a

01:29:29.700 --> 01:29:34.620
very stable baseline mode, meaning that left to its own devices, it's very

01:29:34.620 --> 01:29:38.580
good at doing the same thing, but it has a bunch of problem solving capacity

01:29:38.580 --> 01:29:41.820
such that if any, if any assumptions don't hold, if your cells are a weird

01:29:41.820 --> 01:29:44.860
size or you get the wrong number of cells, or there's a, you know, somebody

01:29:44.860 --> 01:29:49.220
stuck in electrode halfway through the body, whatever, it will still get

01:29:49.220 --> 01:29:51.140
most of what it needs to do done.

01:29:52.380 --> 01:29:55.900
You've talked about the magic and the power of biology here.

01:29:56.460 --> 01:29:59.940
If we look at the human brain, how special is the brain in this context?

01:30:00.380 --> 01:30:04.340
You're kind of minimizing the importance of the brain or lessening.

01:30:04.340 --> 01:30:08.380
It's, uh, we think of all the special computation happens in the brain.

01:30:08.740 --> 01:30:11.220
Everything else is like the help.

01:30:11.820 --> 01:30:15.420
You're kind of saying that the whole thing is, the whole thing is doing

01:30:15.420 --> 01:30:20.660
computation, uh, but nevertheless, how special is the human brain in

01:30:20.660 --> 01:30:22.940
this full context of biology?

01:30:23.300 --> 01:30:23.580
Yeah.

01:30:23.620 --> 01:30:27.740
I mean, look, there's no getting away from the fact that the human brain allows

01:30:27.740 --> 01:30:30.220
us to do things that we could not do without it.

01:30:30.220 --> 01:30:32.060
You can say the same thing about the liver.

01:30:33.220 --> 01:30:36.060
Yeah, no, this is, this is true.

01:30:36.060 --> 01:30:39.180
And so, and so, you know, I, my goal is not, no, you're right.

01:30:39.220 --> 01:30:43.100
My goal is just being polite to the brain right now, being a politician.

01:30:43.140 --> 01:30:46.020
Like, listen, everybody has, everybody has a role.

01:30:46.180 --> 01:30:46.420
Yeah.

01:30:46.500 --> 01:30:47.700
It's very important role.

01:30:47.700 --> 01:30:48.060
That's right.

01:30:48.100 --> 01:30:51.620
We have to acknowledge the importance of the brain, you know, there are more

01:30:51.620 --> 01:30:56.220
than enough people who are, um, cheerleading the brain, right?

01:30:56.220 --> 01:31:00.740
So, so I, I don't feel like, uh, nothing I say is going to reduce people's

01:31:00.740 --> 01:31:02.100
excitement about the human brain.

01:31:02.100 --> 01:31:05.220
And so, so I, I emphasize other credit.

01:31:05.660 --> 01:31:07.100
I don't think it gets too much credit.

01:31:07.100 --> 01:31:08.700
I think other things don't get enough credit.

01:31:08.900 --> 01:31:11.820
I think the brain is, is the, the human brain is incredible

01:31:11.820 --> 01:31:12.860
and special and all that.

01:31:13.220 --> 01:31:14.900
I think other things need more credit.

01:31:14.940 --> 01:31:19.860
And, and I also think that this, and I'm sort of this way about everything.

01:31:19.860 --> 01:31:22.020
I don't like binary categories about almost anything.

01:31:22.020 --> 01:31:22.820
I like a continuum.

01:31:22.820 --> 01:31:28.260
And the thing about the human brain is that by, by, by accepting that as, as

01:31:28.260 --> 01:31:34.140
some kind of an important category or essential, um, essential thing, we end

01:31:34.180 --> 01:31:37.180
up with all kinds of weird pseudo problems and conundrum.

01:31:37.180 --> 01:31:41.020
So for example, uh, when we talk about it, you know, if you don't want to

01:31:41.020 --> 01:31:46.540
talk about, um, uh, uh, ethics and other, other things like that, uh, and, and

01:31:46.540 --> 01:31:50.300
what, you know, this, this idea that surely if we look out into the universe,

01:31:50.300 --> 01:31:54.420
surely we don't believe that this human brain is the only way to be sentient.

01:31:54.420 --> 01:31:54.620
Right.

01:31:54.620 --> 01:31:58.180
So surely we don't, you know, and to have high level cognition, I just, I can't

01:31:58.180 --> 01:32:01.780
even wrap my mind around this, this idea that that is the only way to do it.

01:32:01.980 --> 01:32:05.500
No doubt there are other architectures made by, made of completely different

01:32:05.500 --> 01:32:07.180
principles that achieve the same thing.

01:32:07.700 --> 01:32:11.660
And once we believe that, then that tells us something important.

01:32:11.660 --> 01:32:16.540
It tells us that things that are not quite human brains or chimeras of

01:32:16.540 --> 01:32:20.380
human brains and other tissue or human brains or other kinds of brains and

01:32:20.380 --> 01:32:24.100
novel configurations or things that are sort of brains, but not really, or

01:32:24.100 --> 01:32:29.140
plants or embryos or whatever might also have important cognitive status.

01:32:29.460 --> 01:32:31.300
So that's the only thing.

01:32:31.300 --> 01:32:34.580
I think we have to be really careful about treating the human brain as if it

01:32:34.580 --> 01:32:38.260
was some kind of like sharp binary category, you know, you are, or you

01:32:38.260 --> 01:32:40.340
aren't, I don't believe that exists.

01:32:40.860 --> 01:32:47.620
So when we look out at all the beautiful variety of semi biological architectures

01:32:47.620 --> 01:32:52.980
out there in the universe, how many, how many intelligent alien civilizations

01:32:52.980 --> 01:32:54.220
do you think are out there?

01:32:55.380 --> 01:32:58.380
Boy, I have no expertise in that whatsoever.

01:32:58.780 --> 01:32:59.580
You haven't met any.

01:32:59.820 --> 01:33:02.820
I, I have met the ones we've made.

01:33:03.020 --> 01:33:04.500
I think that, I mean, exactly.

01:33:04.500 --> 01:33:10.060
In some sense with synthetic biology, are you not creating aliens?

01:33:10.100 --> 01:33:13.860
I absolutely think so because, because look, all of life, all of

01:33:13.860 --> 01:33:19.180
state, all standard model systems are an N of one course of evolution on earth.

01:33:19.260 --> 01:33:19.580
Right.

01:33:19.820 --> 01:33:25.900
And trying to make conclusions about biology from looking at life on earth is

01:33:25.900 --> 01:33:28.900
like testing your theory on the same data that generated it.

01:33:28.900 --> 01:33:31.180
It's all, it's all kind of like locked in.

01:33:31.500 --> 01:33:39.100
So we absolutely have to create novel, uh, examples that have no history on earth.

01:33:39.100 --> 01:33:42.940
That don't, you know, xenobots have no history of selection to be a good xenobot.

01:33:42.940 --> 01:33:45.380
The cells have selection for various things, but, but the

01:33:45.380 --> 01:33:46.940
xenobot itself never existed before.

01:33:46.940 --> 01:33:50.140
And so we can make chimeras, you know, we make, um, frog a lot of others,

01:33:50.140 --> 01:33:51.660
you know, sort of half frog have axolotl.

01:33:51.860 --> 01:33:53.700
You can make all sorts of high brats, right?

01:33:53.700 --> 01:33:56.500
Constructions of living tissue with robots and whatever.

01:33:56.740 --> 01:34:00.460
We need to be making these things until we find actual aliens, because

01:34:00.460 --> 01:34:04.540
otherwise we're just looking at an N of one set of examples, all kinds of frozen

01:34:04.540 --> 01:34:06.060
accidents of evolution and so on.

01:34:06.260 --> 01:34:09.140
We need to go beyond that to really understand biology.

01:34:09.180 --> 01:34:13.060
But we're still, even when you're doing synthetic biology, you're locked

01:34:13.140 --> 01:34:19.500
in to the basic components of, um, the way biology is done on this earth.

01:34:20.020 --> 01:34:20.260
Yeah.

01:34:20.300 --> 01:34:20.540
Right.

01:34:20.540 --> 01:34:20.860
Yeah.

01:34:20.860 --> 01:34:25.820
And also, and the, and also the basic constraints of the environment, even

01:34:25.820 --> 01:34:29.060
artificial environments that construct in the lab are tied up to the environment.

01:34:29.500 --> 01:34:32.100
I mean, what do you, okay.

01:34:32.100 --> 01:34:40.340
Let's say there is, I mean, what I think is there's a nearly infinite number of

01:34:40.340 --> 01:34:42.660
intelligent civilizations living or dead out there.

01:34:43.140 --> 01:34:49.340
Um, if you pick one out of the box, what do you think it would look like?

01:34:50.340 --> 01:34:57.140
So in, when you think about synthetic biology or creating synthetic organisms,

01:34:58.820 --> 01:35:01.940
how hard is it to create something that's very different?

01:35:03.100 --> 01:35:06.500
Yeah, I think it's very hard to create something that's very different, right?

01:35:06.500 --> 01:35:12.500
It's, um, uh, we are just locked in both, both, both, uh, experimentally

01:35:12.500 --> 01:35:14.300
and in terms of our imagination, right?

01:35:14.420 --> 01:35:15.340
It's very hard.

01:35:15.460 --> 01:35:18.500
And you also emphasize several times that the idea of shape.

01:35:18.660 --> 01:35:18.980
Yeah.

01:35:19.740 --> 01:35:23.100
The individual cell get together with other cells and they kind of,

01:35:24.020 --> 01:35:25.580
they're going to build a shape.

01:35:25.620 --> 01:35:28.660
So it's shape and function, but shape is a critical thing.

01:35:28.700 --> 01:35:28.980
Yeah.

01:35:29.620 --> 01:35:30.740
So here I'll take a stab.

01:35:30.740 --> 01:35:34.500
I mean, I, I agree with you to whatever extent that we can say anything.

01:35:34.500 --> 01:35:37.860
I do think that there's a, you know, probably an infinite number of, of

01:35:37.860 --> 01:35:41.500
different, uh, different, um, uh, architectures with, with that are with

01:35:41.500 --> 01:35:43.180
interest in cognitive properties out there.

01:35:43.540 --> 01:35:45.300
Uh, what can we say about them?

01:35:45.340 --> 01:35:49.900
I think that, um, the only things that are going, I don't, I don't think

01:35:49.900 --> 01:35:54.020
we can rely on any of the typical stuff, you know, carbon based, none of that.

01:35:54.020 --> 01:35:57.340
Like I think all of that is just, um, you know, us being, having,

01:35:57.340 --> 01:36:02.340
having a lack of imagination, but I think the things that, um, are going

01:36:02.340 --> 01:36:07.460
to be universal, if anything is, are things, for example, driven by resource

01:36:07.460 --> 01:36:11.700
limitation, the fact that you are fighting a hostile world and you have

01:36:11.700 --> 01:36:14.940
to draw a boundary between yourself and the world somewhere, the fact that

01:36:14.980 --> 01:36:16.500
boundary is not given to you by anybody.

01:36:16.500 --> 01:36:19.940
You have to, you have to assume it, you know, uh, estimate it yourself.

01:36:20.260 --> 01:36:23.700
And the fact that you have to coarse grain your experience and the fact that

01:36:23.700 --> 01:36:25.340
you're going to try to minimize surprise.

01:36:25.340 --> 01:36:27.780
And the fact that like, these, these are the things that I think are

01:36:27.780 --> 01:36:31.260
fundamental about biology, none of the, you know, the facts about the genetic

01:36:31.260 --> 01:36:34.140
code or even the fact that we have genes or, or the biochemistry of it.

01:36:34.140 --> 01:36:37.220
I don't think any of those things are fundamental, but, um, it's going to be

01:36:37.220 --> 01:36:40.060
a lot more about the information and about the creation of the self, the

01:36:40.100 --> 01:36:45.620
fact that, so in my, in my framework, selves are demarcated by, uh, the scale

01:36:45.620 --> 01:36:46.900
of the goals that they can pursue.

01:36:47.100 --> 01:36:50.900
So from little tiny local goals to like massive, you know, planetary scale goals

01:36:50.900 --> 01:36:54.260
for, for certain humans, um, and everything and everything in between.

01:36:54.260 --> 01:36:57.580
So you can draw this like cognitive light cone about that, that determines

01:36:57.580 --> 01:37:00.500
the, the scale of the goals you could possibly pursue.

01:37:00.740 --> 01:37:05.220
I think those kinds of frameworks, uh, like that, like active inference and so

01:37:05.220 --> 01:37:08.500
on are going to be universally applicable, but, but none of the other

01:37:08.540 --> 01:37:10.580
things that are, that are typically, um, discussed.

01:37:11.420 --> 01:37:12.900
Quick pause, Dean of Bath and Break.

01:37:13.940 --> 01:37:16.180
We were just talking about, uh, you know, aliens and all that.

01:37:16.180 --> 01:37:19.020
And it's a funny thing, which is, I didn't know if you've seen them, there's

01:37:19.020 --> 01:37:22.500
a kind of debate that goes on about cognition and plants and what can you

01:37:22.500 --> 01:37:24.900
say about different kinds of computation and cognition and plants.

01:37:25.260 --> 01:37:26.500
And I always, I always look at that stuff.

01:37:26.500 --> 01:37:30.460
I'm like, if, if you're weirded out by cognition and plants, you're

01:37:30.460 --> 01:37:32.140
not ready for exobiology, right?

01:37:32.140 --> 01:37:35.780
If, if, if, you know, something that's that similar here on earth is already

01:37:35.820 --> 01:37:39.100
like freaking you out, then I think there's going to be all kinds of

01:37:39.100 --> 01:37:41.980
cognitive life out there that we're going to have a really hard time recognizing.

01:37:42.980 --> 01:37:50.180
I think robots will help us like expand our mind about cognition, either that

01:37:50.220 --> 01:37:58.780
or the word like Xenobots, so, and they maybe becomes the same thing is, you

01:37:58.780 --> 01:38:05.180
know, really when the human engineers, the thing at least in part, and then

01:38:05.180 --> 01:38:08.820
is able to achieve some kind of cognition that's different than what

01:38:08.820 --> 01:38:14.060
you're used to, then you start to understand like, Oh, you know, every

01:38:14.060 --> 01:38:16.100
living organisms capable of cognition.

01:38:16.140 --> 01:38:19.940
Oh, I need to kind of broaden my understanding of what cognition is.

01:38:20.100 --> 01:38:24.820
But do you think plants, um, like when you, when you eat them, are they screaming?

01:38:25.380 --> 01:38:26.260
I don't know about screaming.

01:38:26.260 --> 01:38:29.460
I think you have to, what I think when I eat a salad, yeah, good.

01:38:29.620 --> 01:38:30.060
Yeah.

01:38:30.220 --> 01:38:33.580
I think you have to scale down the expectations in terms of, right.

01:38:33.700 --> 01:38:36.180
So probably they're not screaming in the way that we would be screaming.

01:38:36.180 --> 01:38:41.180
However, there's plenty of data on plants being able to, um, to do

01:38:41.180 --> 01:38:44.220
anticipation and certain kinds of memory and, and, and so on.

01:38:44.700 --> 01:38:49.300
Um, I think, you know, what you just said about robots, uh, I hope you're right.

01:38:49.300 --> 01:38:52.420
And I hope that's, but, but there's two, there's two ways that people can take that.

01:38:52.420 --> 01:38:52.580
Right.

01:38:52.580 --> 01:38:55.860
So one way is exactly what you just said to try to kind of expand their, expand

01:38:55.860 --> 01:38:57.980
their, their, their, their notions for that category.

01:38:58.380 --> 01:39:05.300
The other way people often go is, uh, they just sort of define the term is if, if,

01:39:05.500 --> 01:39:08.580
if it's not a natural product, it's, it's just faking, right.

01:39:08.580 --> 01:39:10.940
It's not really intelligence if it was made by somebody else,

01:39:11.100 --> 01:39:12.620
because it's that same, it's the same thing.

01:39:12.620 --> 01:39:13.980
They can see how it's done.

01:39:14.260 --> 01:39:16.300
And once you see how it's, it's like a magic trick.

01:39:16.300 --> 01:39:19.380
When you see how it's done, it's not as fun anymore.

01:39:19.780 --> 01:39:22.220
And, and, and I think people have a real tendency for that.

01:39:22.220 --> 01:39:24.860
And they sort of, which, which I find really strange in the sense that if

01:39:24.860 --> 01:39:30.820
somebody said to me, we have this, this, this sort of blind, like, like, uh,

01:39:30.980 --> 01:39:35.260
a hill climbing search and then, and then we have a really smart team of engineers.

01:39:35.620 --> 01:39:39.580
Which one do you think is going to produce a system that has good intelligence?

01:39:39.780 --> 01:39:43.220
I think it's really weird to say that it only comes from the blind search, right?

01:39:43.220 --> 01:39:46.260
It can't be done by people who, by the way, can also use evolutionary

01:39:46.260 --> 01:39:48.540
techniques if they want to, but also rational design.

01:39:48.740 --> 01:39:51.980
I think it's really weird to say that, um, real intelligence

01:39:52.020 --> 01:39:53.780
only comes from natural evolution.

01:39:54.460 --> 01:39:55.260
So I hope you're right.

01:39:55.260 --> 01:39:56.980
I hope people take it in the other, the other way.

01:39:57.060 --> 01:39:59.100
But there, there's a nice shortcut.

01:39:59.100 --> 01:40:05.220
So I work with leg of robots a lot now for my, for my own, um, personal pleasure.

01:40:05.820 --> 01:40:13.660
Uh, not in that way, internet, uh, so, uh, the four legs and, uh, one of the things

01:40:13.660 --> 01:40:20.700
that changes my experience of the robots a lot is, um, when I can't understand

01:40:20.700 --> 01:40:23.380
why I did a certain thing and there's a lot of ways to engineer that.

01:40:24.500 --> 01:40:27.980
Meet the person that created a software that runs it.

01:40:28.900 --> 01:40:32.220
There's a lot of ways for me to build that software in such a way that I don't

01:40:32.340 --> 01:40:36.180
exactly know why it did a certain basic decision.

01:40:36.860 --> 01:40:40.700
Of course, as an engineer, you can go in and start to look at logs.

01:40:40.700 --> 01:40:45.140
You can log all kinds of data, sensory data, the, the decisions you made, you

01:40:45.140 --> 01:40:49.060
know, all the outputs in your networks and so on, but I also try to really

01:40:49.100 --> 01:40:54.580
experience that surprise and that really experience as another person would that

01:40:54.580 --> 01:40:56.060
totally doesn't know how it's built.

01:40:56.740 --> 01:41:02.780
And I think the magic is there and not knowing how it works that I think

01:41:02.780 --> 01:41:10.460
biology does that for you through the layers of abstraction, because nobody

01:41:10.460 --> 01:41:15.980
really knows what's going on inside the biological, like each one component is

01:41:15.980 --> 01:41:17.820
clueless about the big picture.

01:41:18.340 --> 01:41:22.100
I think there's actually really cheap systems that can, that can illustrate

01:41:22.100 --> 01:41:26.300
that kind of thing, which is even like, um, you know, uh, fractals, right?

01:41:26.540 --> 01:41:32.220
Like you have a very small short formula in Z and you see it and there's no magic.

01:41:32.220 --> 01:41:34.980
You're just going to crank through, you know, Z squared plus C, whatever.

01:41:34.980 --> 01:41:36.020
You're just going to crank through it.

01:41:36.340 --> 01:41:41.220
But the result of it is this incredibly rich, beautiful image, right?

01:41:41.220 --> 01:41:45.540
That, that, that just like, wow, all of that was in this like 10 character

01:41:45.540 --> 01:41:47.300
long string, like amazing.

01:41:47.500 --> 01:41:52.340
So the fact that you can, you can know everything there is to know about the

01:41:52.340 --> 01:41:55.660
details and the process and all the parts and everything, like there's literally

01:41:55.660 --> 01:41:57.260
no magic of any kind there.

01:41:57.620 --> 01:42:02.100
And yet the outcome is something that you would never have expected.

01:42:02.100 --> 01:42:06.340
And it's just, it just, you know, is incredibly rich and complex and beautiful.

01:42:06.340 --> 01:42:08.100
So there's a lot of that.

01:42:08.820 --> 01:42:13.380
You write the, you work on developing conceptual frameworks for understanding

01:42:13.420 --> 01:42:14.860
unconventional cognition.

01:42:14.860 --> 01:42:17.180
So the kind of thing we've been talking about, I just

01:42:17.180 --> 01:42:19.580
like the term unconventional cognition.

01:42:20.620 --> 01:42:23.940
And you want to figure out how to detect, study and communicate with a thing.

01:42:24.580 --> 01:42:28.620
You've already mentioned a few examples, but what is unconventional cognition?

01:42:28.940 --> 01:42:33.140
Is it as simply as everything else outside of what we define usually as

01:42:33.140 --> 01:42:37.540
cognition, cognitive science, the stuff going on between our ears, or is there

01:42:37.540 --> 01:42:42.740
some deeper way to get at the fundamentals of what is cognition?

01:42:43.700 --> 01:42:47.500
Yeah, I think like, and I'm certainly not the only person who works in

01:42:47.500 --> 01:42:50.220
unconventional, unconventional cognition.

01:42:50.380 --> 01:42:51.700
So it's the term used.

01:42:52.140 --> 01:42:54.700
Yeah, that's one that I, so, so I've coined a number of weird terms, but

01:42:54.700 --> 01:42:56.740
that's not one of mine, like that, that's an existing thing.

01:42:56.740 --> 01:43:00.060
So, so for example, somebody like Andy Adamatsky, who I don't know if you've,

01:43:00.180 --> 01:43:02.980
if you've had him on, if you haven't, you should, he's a, he's a, he's a,

01:43:03.020 --> 01:43:04.660
he's a very interesting guy.

01:43:05.060 --> 01:43:08.820
He's a computer scientist and he does unconventional cognition and slime molds

01:43:08.820 --> 01:43:12.460
and all kinds of weird, he's a real weird, weird cat, really interesting.

01:43:12.860 --> 01:43:15.900
Anyway, so, so that's, you know, it's a bunch of terms that I've come up with,

01:43:15.900 --> 01:43:16.740
but that's not one of mine.

01:43:16.740 --> 01:43:23.180
So I think like many terms, that one is, is really defined by the times, meaning

01:43:23.180 --> 01:43:26.660
that unconventional cognitive things, things that are unconventional cognition

01:43:26.660 --> 01:43:30.180
today are not going to be considered unconventional cognition at some point.

01:43:30.620 --> 01:43:32.900
Uh, it's one of those, it's one of those things.

01:43:33.340 --> 01:43:38.380
And so it's, you know, it's, it's, it's this, it's this really deep question

01:43:38.380 --> 01:43:45.100
of how do you recognize, communicate with, um, um, classify cognition when you

01:43:45.100 --> 01:43:47.820
cannot rely on the typical milestones.

01:43:47.940 --> 01:43:48.220
Right.

01:43:48.300 --> 01:43:52.340
So, so typical, um, you know, again, if you stick with the, with the, uh, the

01:43:52.340 --> 01:43:55.740
history of life on earth, like these, these exact model systems, you would say,

01:43:55.740 --> 01:43:58.780
ah, here's a particular structure of the brain and this one has fewer of those.

01:43:58.780 --> 01:44:01.580
And this one has a bigger frontal cortex and this one, right.

01:44:01.580 --> 01:44:04.420
So these are, these are landmarks that, that we're, that we're used to.

01:44:04.740 --> 01:44:08.620
And, and, and it allows us to make very, very kind of rapid judgments about things.

01:44:09.180 --> 01:44:12.940
But if you can't rely on that either because you're looking at a synthetic

01:44:12.940 --> 01:44:17.100
history thing or, or an engineered thing or an alien thing, then what do you do?

01:44:17.180 --> 01:44:17.460
Right.

01:44:17.460 --> 01:44:19.540
How do you, and so, and so that's what I'm really interested in.

01:44:19.540 --> 01:44:24.460
I'm interested in mind in all of its possible implementations, not just the

01:44:24.460 --> 01:44:27.740
obvious ones that we know from, from looking at brains here on earth.

01:44:29.220 --> 01:44:33.220
Whenever I think about something like unconventional cognition, I think

01:44:33.220 --> 01:44:37.300
about cellular automata, I'm just kept captivated by the beauty of the thing.

01:44:38.460 --> 01:44:45.140
The fact that from simple little, uh, objects, you can create some such

01:44:45.140 --> 01:44:50.940
beautiful complexity that very quickly you forget about the individual objects

01:44:50.940 --> 01:44:56.060
and you see the things that it creates as its own organisms that blows my mind

01:44:56.100 --> 01:45:03.460
every time, like honestly, I could full-time just eat mushrooms and watch

01:45:03.460 --> 01:45:08.340
cellular Tom and doesn't don't even have to do mushrooms, uh, just, just cellular

01:45:08.340 --> 01:45:14.940
Tom, but it feels like, I mean, from the engineer perspective, I love when a very

01:45:14.940 --> 01:45:19.660
simple system captures something really powerful because then you can study that

01:45:19.660 --> 01:45:24.060
system to understand something fundamental about complexity, about life on earth.

01:45:25.020 --> 01:45:27.500
Anyway, how do I communicate with a thing?

01:45:28.100 --> 01:45:33.860
If a cellular automata can, can, uh, do cognition, if a plant can do cognition,

01:45:34.380 --> 01:45:40.340
if, uh, a xenobot can do cognition, how do I like whisper in its ear and, and,

01:45:40.340 --> 01:45:43.700
and get an answer back to, how do I have a conversation?

01:45:43.820 --> 01:45:44.100
Yeah.

01:45:44.420 --> 01:45:46.980
Um, well, how do I have a xenobot on a podcast?

01:45:47.700 --> 01:45:51.500
It's a really, a really interesting line of investigation that, that, that,

01:45:51.620 --> 01:45:52.380
that, that opens up.

01:45:52.380 --> 01:45:53.940
I mean, I mean, we've, we've thought about this.

01:45:53.940 --> 01:45:55.180
So you need a few things.

01:45:55.180 --> 01:45:57.420
You need, you need to understand the space in which they live.

01:45:57.740 --> 01:46:02.060
So, uh, what, not, not just the physical modality, like, can they see,

01:46:02.060 --> 01:46:03.140
like, can they feel vibration?

01:46:03.140 --> 01:46:05.100
I mean, that's important, of course, because that's how you deliver your

01:46:05.100 --> 01:46:08.900
message, but, but not just, not just the ideas for a communication medium, not,

01:46:08.900 --> 01:46:12.180
not just the physical medium, but what is saliency, right?

01:46:12.180 --> 01:46:14.940
So, so what are these, what, what are important to this?

01:46:14.980 --> 01:46:16.420
What's important to this system?

01:46:16.860 --> 01:46:21.620
And systems have all kinds of different levels of sophistication of what

01:46:21.620 --> 01:46:22.740
you could expect to get back.

01:46:23.020 --> 01:46:26.740
And I think what's, what's really important, I call this, um, the, the

01:46:26.740 --> 01:46:29.980
spectrum of persuadability, which is this, this idea that when you're looking

01:46:29.980 --> 01:46:33.780
at a system, you can't, you can't assume where on the spectrum it is,

01:46:33.980 --> 01:46:35.180
you have to do experiments.

01:46:35.180 --> 01:46:41.220
And so, so, so, so, uh, for example, uh, if you look at, uh, a gene regulatory

01:46:41.220 --> 01:46:44.020
network, which is just a bunch of bunch of, um, nodes that turn each other on

01:46:44.020 --> 01:46:46.940
and off at various rates, you might look at that and you say, wow, there's no

01:46:46.940 --> 01:46:50.860
magic here, I mean, clearly this thing is, uh, is, is as deterministic as it gets.

01:46:50.860 --> 01:46:51.820
It's a piece of hardware.

01:46:51.820 --> 01:46:55.300
The only way we're going to be able to control it is by rewiring it, which is

01:46:55.300 --> 01:46:56.820
the way my molecular biology works, right?

01:46:56.820 --> 01:46:58.220
We can add nodes, remove nodes or whatever.

01:46:58.540 --> 01:47:02.700
Well, so we've done simulations and shown that, um, biological, and now we're

01:47:02.700 --> 01:47:07.260
doing this in the lab, the biological networks like that have, have associative

01:47:07.260 --> 01:47:10.180
memory, so they can actually learn, they can learn from experience, they have,

01:47:10.220 --> 01:47:12.660
they have habituation, they have sensitization, they have associative

01:47:12.660 --> 01:47:15.860
memory, which you wouldn't have known if you assume that they have to be on

01:47:15.860 --> 01:47:16.940
the left side of that spectrum.

01:47:17.340 --> 01:47:21.340
So when you're going to communicate with something and we've even, um, uh,

01:47:21.380 --> 01:47:25.140
Charles Abramson and I've written a paper on, um, uh, behaviorist approaches

01:47:25.140 --> 01:47:28.340
to synthetic organism, meaning that if you're given something, you have no idea

01:47:28.340 --> 01:47:32.380
what it is or what it can do, how do you figure out what its psychology is, what

01:47:32.380 --> 01:47:36.140
its level is, what does it, and so, and so we literally lay out a set of protocols

01:47:36.220 --> 01:47:38.820
starting with the simplest things and then moving up to more complex things

01:47:39.020 --> 01:47:41.500
where you can make no assumptions about what this thing can do, right?

01:47:41.500 --> 01:47:43.580
Just from you, you have to start and you'll find out.

01:47:44.220 --> 01:47:47.020
So, so when you're going to, so, so here's a simple, I mean, here's one

01:47:47.020 --> 01:47:48.060
way to communicate with something.

01:47:48.060 --> 01:47:50.740
If you can train it, that's a way of communicating.

01:47:51.020 --> 01:47:54.740
So if you can provide, if you can figure out what the currency of reward of, of

01:47:54.740 --> 01:47:58.780
positive and negative reinforcement is, right, and you can get it to do something

01:47:58.780 --> 01:48:02.820
it wasn't doing before based on experiences you've given it, you have taught

01:48:02.820 --> 01:48:06.420
it one thing, you have communicated one thing that, that such and such an action

01:48:06.420 --> 01:48:08.780
is good, some other action is not good.

01:48:08.780 --> 01:48:12.380
That's, that's like a basic atom of, of a primitive atom of communication.

01:48:12.700 --> 01:48:18.860
What about in some sense, if it gets you to do something you haven't done before,

01:48:19.180 --> 01:48:20.380
is it answering back?

01:48:20.540 --> 01:48:21.900
Yeah, most, most certainly.

01:48:21.900 --> 01:48:23.660
And then there's, there's, I've seen cartoons.

01:48:23.660 --> 01:48:26.980
I think maybe Gary Larson or somebody had had a cartoon of these, of these

01:48:26.980 --> 01:48:30.380
rats in the maze and the one rat, you know, assist to the other, he'll get this

01:48:30.380 --> 01:48:33.540
every time, every time I walk over here, he starts scribbling in that on the, you

01:48:33.540 --> 01:48:35.020
know, on the clipboard that he has.

01:48:35.020 --> 01:48:35.500
It's awesome.

01:48:35.660 --> 01:48:42.260
If we step outside ourselves and really measure how much, like,

01:48:42.300 --> 01:48:47.180
if I, if I actually measure how much I've changed because of my interaction

01:48:47.180 --> 01:48:52.420
with certain cellular automata, I mean, you really have to take that into

01:48:52.420 --> 01:48:56.660
consideration about like, well, these things are changing you too.

01:48:57.180 --> 01:49:01.900
I know, you know how it works and so on, but you're being changed by the thing.

01:49:02.420 --> 01:49:02.940
Absolutely.

01:49:02.940 --> 01:49:06.020
I think, I think I read, I don't know any details, but I think I read

01:49:06.020 --> 01:49:10.780
something about how, how wheat and other things have domesticated humans in terms

01:49:10.780 --> 01:49:14.100
of, right, but by their properties change the way that the human

01:49:14.100 --> 01:49:15.500
behavior and societal structures.

01:49:15.500 --> 01:49:19.580
And that sense cats are running the world.

01:49:20.340 --> 01:49:25.820
Cause they, they've took over the, so first of all, so first they, while not

01:49:25.820 --> 01:49:30.100
giving a shit about humans clearly with ever, with, with every ounce of their

01:49:30.100 --> 01:49:37.100
being, they've somehow got just millions and millions of humans to, to, to, to

01:49:37.140 --> 01:49:38.740
take them home and feed them.

01:49:39.540 --> 01:49:44.020
And then not only the physical space that they take over, they took over

01:49:44.020 --> 01:49:48.420
the digital space, they dominate the internet in terms of cuteness, in

01:49:48.420 --> 01:49:49.660
terms of me mobility.

01:49:50.580 --> 01:49:55.740
And so they're, they're like, they got themselves literally inside the memes

01:49:55.740 --> 01:49:58.300
that become viral and spread on the internet.

01:49:59.100 --> 01:50:01.580
And they're the ones that are probably controlling humans.

01:50:01.580 --> 01:50:02.420
That's my theory.

01:50:02.620 --> 01:50:05.460
Another, that's a follow-up paper after the frog kissing.

01:50:05.780 --> 01:50:06.300
Okay.

01:50:06.940 --> 01:50:12.700
I mean, you mentioned sentience and consciousness.

01:50:13.460 --> 01:50:18.940
Uh, you have a paper titled generalizing frameworks for sentience

01:50:18.940 --> 01:50:20.940
beyond natural species.

01:50:22.420 --> 01:50:31.340
So beyond normal cognition, if we look at sentience and consciousness, and I

01:50:31.340 --> 01:50:35.100
wonder if you draw an interesting distinction between those two, uh,

01:50:35.140 --> 01:50:42.580
elsewhere outside of humans and, uh, maybe outside of earth, you think

01:50:42.580 --> 01:50:47.860
aliens are have sentience and if they do, how do we think about it?

01:50:47.900 --> 01:50:51.100
So when you have this framework, what is this paper?

01:50:51.100 --> 01:50:54.060
What is, what is the way you propose to think about sentience?

01:50:54.580 --> 01:50:58.540
Yeah, that, that, that particular paper was, it was a very short commentary

01:50:58.540 --> 01:51:00.820
on another paper that was written about crabs.

01:51:00.820 --> 01:51:02.060
It was a really good paper on them.

01:51:02.420 --> 01:51:07.100
Uh, crabs and various, like a, like a rubric of, uh, of, uh, different types

01:51:07.100 --> 01:51:09.740
of behaviors that, uh, that could be applied to different creatures and

01:51:09.740 --> 01:51:10.820
they're trying to apply it to crabs.

01:51:10.820 --> 01:51:15.940
And so, um, I, I've consciousness, we can talk about if you want, but

01:51:15.940 --> 01:51:17.060
it's a whole separate kettle of fish.

01:51:17.060 --> 01:51:21.340
I, I, I almost never talk about crabs in this case.

01:51:21.340 --> 01:51:21.620
Yes.

01:51:22.460 --> 01:51:24.660
I almost never talk about consciousness per se.

01:51:24.660 --> 01:51:27.540
I've said very, very little about it, but we can, we can talk about it if you want.

01:51:27.780 --> 01:51:31.980
Mostly what I talk about is, is cognition because I think that that's

01:51:31.980 --> 01:51:37.020
much easier to deal with in a, um, kind of rigorous experimental, experimental way.

01:51:37.380 --> 01:51:43.380
I think that, um, all of these, all of these terms have, uh, you know, sentience

01:51:43.380 --> 01:51:49.420
and, and, and so on have different definitions and I fundamentally, I think

01:51:49.420 --> 01:51:54.900
that people can, as long as they specify what they mean ahead of time, um, I think

01:51:54.940 --> 01:51:56.380
people can define them in various ways.

01:51:56.620 --> 01:52:02.300
The one, the, the, the only thing that I really kind of insist on is that the

01:52:02.300 --> 01:52:07.540
right way to think about all this stuff is, is, is an energy from an engineering

01:52:07.540 --> 01:52:12.380
perspective, what does it help me to, to control, predict, and, uh, to, and

01:52:12.380 --> 01:52:13.820
does it help me do my next experiment?

01:52:14.020 --> 01:52:17.660
So, so, so, so that's, that's not a universal perspective.

01:52:17.660 --> 01:52:23.260
So some people have, uh, philosophical kind of underpinnings and those are primary.

01:52:23.300 --> 01:52:26.660
And if anything runs against that, then it must automatically be wrong.

01:52:26.780 --> 01:52:31.540
So, so some people will say, I don't care what else, if your theory says to me

01:52:31.540 --> 01:52:36.540
that thermostats have little tiny goals, I'm not, I'm not, I'm not, so that's it.

01:52:36.540 --> 01:52:39.580
I just like, that's my philosophical, you know, preconception that like

01:52:39.580 --> 01:52:41.180
thermostats do not have goals and that's it.

01:52:41.540 --> 01:52:43.340
So, um, so that's one way of doing it.

01:52:43.340 --> 01:52:44.260
And some people do it that way.

01:52:44.300 --> 01:52:45.700
I do not do it that way.

01:52:45.700 --> 01:52:49.420
And I think that we can, if we can't, I don't think we can know much of anything

01:52:49.420 --> 01:52:51.020
from an art, from a philosophical armchair.

01:52:51.060 --> 01:52:55.300
I think that all of these theories and ways of doing things, standard fall

01:52:55.580 --> 01:52:59.460
based on just, w w just basically one set of criteria, does it help you

01:52:59.500 --> 01:53:01.060
run a rich research program?

01:53:01.100 --> 01:53:01.460
That's it.

01:53:02.020 --> 01:53:05.260
I agree with you totally, but so forget philosophy.

01:53:05.580 --> 01:53:08.380
What about the poetry of ambiguity?

01:53:08.380 --> 01:53:13.020
What about at the limits of the things you can engineer using terms that are,

01:53:13.740 --> 01:53:20.820
that can be defined in multiple ways and living within that uncertainty in order

01:53:20.820 --> 01:53:24.980
to play with words until something lands that you can engineer.

01:53:25.300 --> 01:53:27.860
I mean, that's to me where consciousness sits currently.

01:53:28.180 --> 01:53:32.180
Nobody really understands the, the, the hard problem of consciousness, the

01:53:32.180 --> 01:53:36.980
subject, what it feels like, because it really feels like it feels like something

01:53:36.980 --> 01:53:41.260
to be this biological system, this conglomerate of a bunch of cells in this

01:53:41.260 --> 01:53:46.420
hierarchy of competencies feels like something and I feel like one thing.

01:53:46.900 --> 01:53:56.460
And is that just, is that just the, as a side effect of a complex system or is

01:53:56.460 --> 01:54:02.460
there something more that humans have, or is there something more that any biological

01:54:02.460 --> 01:54:08.620
system has some kind of magic, some kind of, not just a sense of agency, but a

01:54:08.620 --> 01:54:11.940
real sense with a capital letter S of agency.

01:54:12.140 --> 01:54:12.380
Yeah.

01:54:13.260 --> 01:54:13.980
Ah, boy.

01:54:14.140 --> 01:54:14.300
Yeah.

01:54:14.300 --> 01:54:15.020
That's a deep question.

01:54:15.380 --> 01:54:17.980
Is there room for poetry and engineering or no?

01:54:18.100 --> 01:54:19.580
No, there, there definitely is.

01:54:19.580 --> 01:54:24.380
And a lot of the poetry comes in when we realize that none of the categories we

01:54:24.380 --> 01:54:26.140
deal with are sharp as we think they are.

01:54:26.140 --> 01:54:26.340
Right.

01:54:26.340 --> 01:54:31.140
And so, and so in the, you know, in the different areas of, of, of all the

01:54:31.140 --> 01:54:33.860
spectra are where a lot of the poetry sits.

01:54:34.140 --> 01:54:37.540
I have many new theories about things, but I, in fact, do not have a good

01:54:37.540 --> 01:54:39.700
theory about consciousness that I plan to trot out.

01:54:39.700 --> 01:54:43.140
So, and you almost don't see it as useful for your current work to

01:54:44.140 --> 01:54:45.020
I think it will come.

01:54:45.100 --> 01:54:47.100
I have some thoughts about it, but I don't feel like they're going to

01:54:47.100 --> 01:54:48.860
move the needle yet on, on that.

01:54:48.860 --> 01:54:52.140
But you want to ground in, in, in engineering always.

01:54:52.740 --> 01:54:58.420
So, well, I mean, I don't, so, so, so, so if we really tackle consciousness per

01:54:58.420 --> 01:55:03.140
se, in terms of the heart problem, I don't, I don't, that, that isn't necessarily

01:55:03.140 --> 01:55:05.100
going to be groundable in engineering, right?

01:55:05.100 --> 01:55:09.620
That, that aspect of cognition is, but actual consciousness per se, you

01:55:09.620 --> 01:55:12.580
know, for first person perspective, I'm not sure that that's groundable in

01:55:12.580 --> 01:55:17.060
engineering and I think specifically what's different about, what's different

01:55:17.060 --> 01:55:18.580
about it is there's, there's a couple of things.

01:55:18.580 --> 01:55:19.820
So, so let's, you know, here we go.

01:55:19.820 --> 01:55:22.620
I'll say, I'll say a couple of things about, about consciousness.

01:55:22.620 --> 01:55:28.580
One, one thing is that what makes it different is that for every other

01:55:28.860 --> 01:55:34.420
type aspect of science, when we think about having a correct or a good theory

01:55:34.460 --> 01:55:39.820
of it, we have some idea of what format that theory makes predictions in.

01:55:40.020 --> 01:55:42.140
So whether those be numbers or whatever.

01:55:42.140 --> 01:55:43.300
We have some idea.

01:55:43.340 --> 01:55:44.380
We may not know the answer.

01:55:44.380 --> 01:55:47.740
We may not have the theory, but we know that when we get the theory, here's

01:55:47.740 --> 01:55:51.180
what it's going to output and then we'll know if it's right or wrong for actual

01:55:51.180 --> 01:55:54.980
consciousness, not behavior, not neural correlates, but actual first person

01:55:54.980 --> 01:55:59.140
consciousness, if we had a correct theory of consciousness or even a good

01:55:59.140 --> 01:56:03.020
one, what the hell would, what, what format would, would it make predictions

01:56:03.020 --> 01:56:07.580
in, right, because, because all the things that we know about basically boil

01:56:07.580 --> 01:56:09.220
down to observable behaviors.

01:56:09.540 --> 01:56:15.300
So the only thing I can think of when I think about that is, is, is what it will

01:56:15.300 --> 01:56:20.020
be poetry or it'll be, it'll be, it'll be something to, um, if, if I ask you,

01:56:20.020 --> 01:56:22.700
okay, you've got a great theory of consciousness and here's this, here's

01:56:22.700 --> 01:56:25.140
this creature, maybe it's a natural, maybe it's an engineer one, whatever.

01:56:25.580 --> 01:56:29.900
And I want you to tell me what your theory says about this, this, this

01:56:29.940 --> 01:56:35.180
being, um, uh, what it's like to be this being the only thing I can imagine you

01:56:35.180 --> 01:56:40.100
giving me is some piece of art, a poem or, or, or something that once I've taken

01:56:40.100 --> 01:56:46.740
it in, I share, uh, I, I, I now have a similar state as whatever that's, that's

01:56:46.740 --> 01:56:48.020
about as good as I can come up with.

01:56:48.100 --> 01:56:53.420
Well, it's possible that once you have a good understanding of consciousness, it

01:56:53.420 --> 01:56:55.860
would be mapped to some things that are more measurable.

01:56:56.220 --> 01:57:03.820
So for example, it's possible that a conscious being is one that's able to

01:57:03.860 --> 01:57:04.660
suffer.

01:57:06.140 --> 01:57:09.100
So you start to look at pain and suffering.

01:57:09.420 --> 01:57:18.020
You can start to connect it closer to things that you can measure that in

01:57:18.020 --> 01:57:25.100
terms of how they reflect themselves in behavior and problem solving and, uh,

01:57:25.140 --> 01:57:27.380
creation and attainment of goals.

01:57:27.380 --> 01:57:32.060
For example, which I think suffering is one of the, you know, life is suffering.

01:57:32.140 --> 01:57:38.020
It's one of the, one of the big aspects of the, the human condition.

01:57:38.940 --> 01:57:44.860
And so if consciousness is somehow a, maybe at least a catalyst for suffering,

01:57:46.300 --> 01:57:48.460
you could start to get like echoes of it.

01:57:48.460 --> 01:57:52.300
And you start, you start to see like the actual effects of consciousness on

01:57:52.300 --> 01:57:55.620
behavior, that it's not just about subjective experience.

01:57:55.620 --> 01:58:00.660
It's like, it's really deeply integrated in the problem solving, uh, decision

01:58:00.660 --> 01:58:03.060
making of a system, uh, something like this.

01:58:03.060 --> 01:58:08.820
But also it's possible that we realize this is not a philosophical statement.

01:58:09.260 --> 01:58:11.220
Philosophers can write their books.

01:58:11.260 --> 01:58:12.140
I welcome it.

01:58:12.580 --> 01:58:16.460
Um, you know, I, I take the Turing test really seriously.

01:58:16.460 --> 01:58:22.420
I, I don't know why people really don't like it when, uh, a robot convinces

01:58:22.420 --> 01:58:23.980
you that it's intelligent.

01:58:24.180 --> 01:58:26.500
I think that's a really incredible accomplishment.

01:58:27.100 --> 01:58:30.180
And there's some deep sense in which that is intelligence.

01:58:30.980 --> 01:58:33.700
If it looks like it's intelligent, it is intelligent.

01:58:34.260 --> 01:58:42.060
And I think there's some deep aspect of, um, a system that appears to be

01:58:42.060 --> 01:58:49.500
conscious in some deep sense, it is conscious, um, it for these, for me,

01:58:49.500 --> 01:58:54.180
we'll have to consider that possibility and a system that appears to be

01:58:54.180 --> 01:58:57.740
conscious is an engineering challenge.

01:58:58.620 --> 01:59:00.580
Yeah, I don't disagree with any of that.

01:59:00.580 --> 01:59:04.300
I mean, especially intelligence, I think is a publicly observable thing.

01:59:04.340 --> 01:59:08.740
Uh, I, I, and, and I mean, you know, science fiction has dealt with this

01:59:08.740 --> 01:59:13.420
for a century or more, much more, maybe, uh, this idea that when you are

01:59:13.420 --> 01:59:17.380
confronted with something that just doesn't meet any of your typical

01:59:17.380 --> 01:59:21.180
assumptions, so you can't look in the skull and say, oh, well, there's that

01:59:21.180 --> 01:59:23.300
frontal cortex, so then I guess we're good, right?

01:59:23.300 --> 01:59:26.180
If it's, if it's, you know, so, so this thing lands on your front lawn

01:59:26.180 --> 01:59:29.180
and this, you know, with the little door opens and something trundles out

01:59:29.180 --> 01:59:32.940
and it's sort of like, um, you know, kind of shiny and aluminum looking.

01:59:32.940 --> 01:59:35.580
And it, and it hands you this, uh, you know, it hands you this poem that

01:59:35.580 --> 01:59:38.780
it wrote while it was on, you know, flying over and how happy it is to meet

01:59:38.780 --> 01:59:41.460
you, like what are, what's going to be your criteria, right?

01:59:41.460 --> 01:59:44.300
For whether, whether you get to take it apart and see what makes it tick or

01:59:44.300 --> 01:59:47.140
whether you have to, you know, be nice to it and, and whatever, right?

01:59:47.140 --> 01:59:51.660
Like all the, all the criteria that we have now and, you know, the people are

01:59:51.660 --> 01:59:54.580
using, and as you said, a lot of people are down on the touring test and things

01:59:54.660 --> 01:59:58.140
like this, but, but what else have we got, you know, because measuring, measuring

01:59:58.140 --> 02:00:00.540
the cortex size isn't going to, isn't going to cut it, right?

02:00:00.540 --> 02:00:01.740
In the broader scheme of things.

02:00:02.140 --> 02:00:06.740
So, uh, I think this is, it's, it's a wide open, it's a wide open problem

02:00:07.060 --> 02:00:10.380
that, right, that, that we, you know, our, our solution to the problem of other

02:00:10.380 --> 02:00:12.620
minds, it's very simplistic, right?

02:00:12.620 --> 02:00:16.180
We, we give each other credit for having minds just because we sort of on a, you

02:00:16.180 --> 02:00:19.140
know, on an anatomical level, we're pretty similar and then so that's good enough.

02:00:19.380 --> 02:00:21.060
But how far, how far is that going to go?

02:00:21.620 --> 02:00:23.180
So I think that's really primitive.

02:00:23.300 --> 02:00:26.140
So, um, yeah, I think, I think it's a major unsolved problem.

02:00:26.180 --> 02:00:33.740
It's a really challenging, um, direction of thought to the human race, uh, that

02:00:33.740 --> 02:00:35.940
you talked about, like embodied minds.

02:00:36.300 --> 02:00:41.940
If you start to think that other things, other than humans have minds, that's

02:00:41.940 --> 02:00:48.540
really challenging because all men are created equal starts, starts being like,

02:00:48.580 --> 02:00:55.700
all right, well, we should probably treat not just cows with respect, but like

02:00:55.780 --> 02:01:03.420
plants and not just plants, but, uh, some kind of organized conglomerates

02:01:03.420 --> 02:01:05.460
of cells in a Petri dish.

02:01:06.220 --> 02:01:10.540
In fact, some of the work we're doing, like you're doing and the whole

02:01:10.540 --> 02:01:12.500
community of science is doing with biology.

02:01:12.500 --> 02:01:15.860
People might be like, we were really mean to viruses.

02:01:16.340 --> 02:01:16.620
Yeah.

02:01:17.380 --> 02:01:19.660
I mean, yeah, I, the thing is you're right.

02:01:19.660 --> 02:01:23.060
And, and I get, I get, I certainly get phone calls about people complaining

02:01:23.060 --> 02:01:28.660
about frog skin and so on, but I think we have to separate the sort of deep

02:01:28.660 --> 02:01:31.060
philosophical aspects of versus what actually happened.

02:01:31.060 --> 02:01:34.820
So what actually happens on earth is that people with exactly the

02:01:34.820 --> 02:01:39.260
same anatomical structure kill each other, you know, on a daily basis.

02:01:39.300 --> 02:01:39.580
Right.

02:01:39.820 --> 02:01:44.660
So, so, so it, I think it's clear that simply knowing that something else is

02:01:44.660 --> 02:01:49.860
equally or maybe more, uh, cognitive or conscious than you are is, is not a

02:01:49.860 --> 02:01:52.980
guarantee of, of, of kind behavior that, that, that much we know of.

02:01:53.300 --> 02:01:56.580
So then, and so then, then we look at a commercial farming of

02:01:56.580 --> 02:01:58.180
mammals and various other things.

02:01:58.180 --> 02:02:02.580
And so, so I think on a practical basis, long before we get to worrying

02:02:02.580 --> 02:02:07.980
about, um, things like frog skin, we have to ask ourselves, why are we,

02:02:08.300 --> 02:02:11.700
uh, what, what can we do about the way that we've been behaving towards

02:02:11.700 --> 02:02:15.260
creatures, which we know for a fact, because of our similarities are,

02:02:15.260 --> 02:02:19.140
are basically just like us, you know, that's kind of a whole other social thing.

02:02:19.140 --> 02:02:22.980
But, but, but fundamentally, you know, of course you're absolutely right in that.

02:02:23.460 --> 02:02:25.340
We, we are also thinking about this.

02:02:25.340 --> 02:02:28.900
We are on this planet in some way, incredibly lucky.

02:02:28.900 --> 02:02:34.220
It's just dumb luck that we really only have one dominant species.

02:02:34.220 --> 02:02:35.820
It didn't have to work out that way.

02:02:35.980 --> 02:02:39.860
So you could easily imagine that there could be a planet somewhere with more

02:02:39.860 --> 02:02:44.020
than one equally or maybe near equally intelligent species.

02:02:44.420 --> 02:02:48.140
And then, uh, but, but then they may not look at anything like each other.

02:02:48.140 --> 02:02:48.340
Right?

02:02:48.340 --> 02:02:52.340
So there may be multiple ecosystems where there are, uh, things of, of, of

02:02:52.340 --> 02:02:54.100
similar, uh, to human like intelligence.

02:02:54.300 --> 02:02:58.060
And then you'd have all kinds of issues about, you know, how do you, how do you

02:02:58.060 --> 02:03:01.420
relate to them when they're physically not like you at all, but yet, yet, you

02:03:01.420 --> 02:03:05.140
know, in terms of behavior and culture and whatever, it's pretty obvious that

02:03:05.140 --> 02:03:08.500
they've got as, you know, as much on the ball as you have, or maybe imagine,

02:03:08.540 --> 02:03:12.820
imagine that there was another, um, group of beings that was like on average, you

02:03:12.820 --> 02:03:15.020
know, 40 IQ points lower, right?

02:03:15.020 --> 02:03:17.500
Like, like we're just, we're pretty lucky in many ways.

02:03:17.500 --> 02:03:20.300
We, you know, we don't really have, even though we, we sort of, you know, we

02:03:20.300 --> 02:03:24.180
still act badly in many ways, but, but, but the fact is, you know, all humans

02:03:24.180 --> 02:03:27.420
are more or less in the, like in the same, that same range, but didn't have

02:03:27.420 --> 02:03:28.220
to work out that way.

02:03:28.820 --> 02:03:34.380
Well, but I think that's part of the way life works on earth, maybe human

02:03:34.420 --> 02:03:41.220
civilization works is it seems like we want us ourselves to be quite similar.

02:03:42.060 --> 02:03:45.900
And then within that, you know, what everybody's about the same relatively

02:03:45.900 --> 02:03:49.100
IQ intelligence, problem solving capabilities, even physical

02:03:49.100 --> 02:03:55.500
characteristics, but then we'll find some aspect of that that's different.

02:03:56.100 --> 02:04:03.220
And that seems to be like, I mean, it's, it's really dark to say, but that

02:04:03.220 --> 02:04:10.900
seems to be the, um, not even a bug, but like a feature of the early

02:04:10.900 --> 02:04:12.660
development of human civilization.

02:04:12.660 --> 02:04:17.140
You pick the other, your tribe versus the other tribe being you war.

02:04:17.140 --> 02:04:22.260
It's a kind of evolution, evolution in the space of, of memes, a space of

02:04:22.260 --> 02:04:25.340
ideas, I think, and you war with each other.

02:04:25.620 --> 02:04:28.740
So we're very good at finding the other, even when the

02:04:28.740 --> 02:04:30.300
characteristics are really the same.

02:04:30.540 --> 02:04:36.180
And that's, I don't know what that, I mean, I'm sure so many of these things

02:04:36.260 --> 02:04:38.420
echo in the biological world in some way.

02:04:39.100 --> 02:04:43.300
There's a fun, um, experiment that, uh, I did, my, my, my son actually came up

02:04:43.300 --> 02:04:46.780
with this and we, we did, um, uh, a biology unit together.

02:04:46.780 --> 02:04:47.700
He would use the homeschool.

02:04:47.700 --> 02:04:49.140
And so we did this a couple of years ago.

02:04:49.140 --> 02:04:52.060
We did this thing where imagines you get this slime mold, right?

02:04:52.060 --> 02:04:56.260
Pfizer and polysulfolam and it grows on, um, uh, on a, uh, on a Petri dish of

02:04:56.300 --> 02:04:59.820
agar, and it sort of spreads out and it's, it's, it's, it's a single cell,

02:04:59.860 --> 02:05:01.820
but you know, protist, but it's like this giant thing.

02:05:02.180 --> 02:05:04.740
And so you put down a piece of oat and it wants to go get the oat and it

02:05:04.740 --> 02:05:05.940
sort of grows towards the oat.

02:05:06.220 --> 02:05:09.660
So what you do is you take a razor blade and you just, you just separate the

02:05:09.660 --> 02:05:12.540
piece of the whole culture that's growing towards the, the, oh, you just

02:05:12.540 --> 02:05:13.260
kind of separate it.

02:05:13.620 --> 02:05:17.540
And so now think about, think about the interesting decision-making calculus

02:05:17.540 --> 02:05:22.460
for that little piece, I can, I can go get the oat and therefore I won't have

02:05:22.460 --> 02:05:24.740
to share those nutrients with this giant mass over there.

02:05:24.740 --> 02:05:27.420
So the, so the nutrients per unit volume is going to be amazing.

02:05:27.420 --> 02:05:28.260
So I should go eat the oat.

02:05:29.100 --> 02:05:33.060
But if I first rejoin, because Pfizer once you cut it has the ability to join

02:05:33.060 --> 02:05:37.820
back up, if I first rejoin, then that whole calculus becomes impossible

02:05:37.860 --> 02:05:39.380
because there is no more me anymore.

02:05:39.380 --> 02:05:42.260
There's just we, and then, and then we will go eat this thing, right?

02:05:42.540 --> 02:05:45.660
So, so this interesting, you know, this, this, you can imagine a kind of game

02:05:45.660 --> 02:05:49.900
theory where the number of agents isn't fixed and that it's not just cooperate

02:05:49.900 --> 02:05:52.540
or defect, but it's actually merge and, and, and whatever, right?

02:05:52.940 --> 02:05:56.380
So that kind of, that, that computation, how does it do that decision-making?

02:05:56.780 --> 02:05:57.340
Yeah.

02:05:57.340 --> 02:05:58.340
So, so, so that, right.

02:05:58.340 --> 02:05:59.860
So, so it's, it's, it's really interesting.

02:05:59.860 --> 02:06:03.180
And so, and so empirically what we found is that it, it tends to merge first.

02:06:03.460 --> 02:06:06.260
It tends to merge first and then the whole thing goes, but, but it's really

02:06:06.260 --> 02:06:09.340
interesting that, that, that, that, that calculus, like, do we even have, I mean,

02:06:09.340 --> 02:06:12.500
I'm not an expert in the economic game theory and all that, but maybe there's a

02:06:12.500 --> 02:06:15.940
calendar needs some sort of hyperbolic discounting or something, but, but maybe,

02:06:16.140 --> 02:06:21.460
you know, this idea that the, the actions you take not only change your payoff,

02:06:21.860 --> 02:06:24.580
but they change who or what you are.

02:06:24.740 --> 02:06:27.700
And that you may not, you, you could take an action after which you don't

02:06:27.700 --> 02:06:31.140
exist anymore, or you are radically changed, or you are merged with somebody

02:06:31.140 --> 02:06:34.980
else, like that's, you know, as far as I know, that's a whole, you know, we,

02:06:34.980 --> 02:06:38.780
we're still missing a formalism for even knowing how to, how to model any of that.

02:06:39.260 --> 02:06:42.980
Do you see evolution, by the way, as a process that applies here on earth?

02:06:42.980 --> 02:06:45.180
Or is it some, where did evolution come from?

02:06:45.900 --> 02:06:47.460
Yeah, I, yeah.

02:06:47.500 --> 02:06:51.900
So, so this thing that from the very origin of life that took us to today,

02:06:51.900 --> 02:06:53.340
what, what, what the heck is that?

02:06:54.100 --> 02:06:59.620
I think evolution is inevitable in the sense that if you combine and, and, and

02:06:59.620 --> 02:07:03.980
basically, I think one of the most useful things that was done in early computing,

02:07:04.100 --> 02:07:07.580
I guess in the sixties, it started was, was evolutionary computation and just

02:07:07.580 --> 02:07:15.020
showing how, how simple it is that if you have, if you have imperfect heredity and

02:07:15.020 --> 02:07:18.420
competition together, those two things with three things, right?

02:07:18.420 --> 02:07:22.540
So heredity, imperfect heredity and competition or selection, those three

02:07:22.540 --> 02:07:26.060
things, and that's it now, now, now you're, you're off through the races, right?

02:07:26.340 --> 02:07:29.660
And so that can be, it's not just on earth because it can be done in the

02:07:29.660 --> 02:07:32.780
computer, it can be done in chemical systems, it can be done in, you know,

02:07:32.780 --> 02:07:36.220
Lee Smolin says it's, it works in on, on, you know, cosmic scales.

02:07:36.700 --> 02:07:43.580
So I think that, that kind of thing is incredibly pervasive and, and, and, and

02:07:43.940 --> 02:07:45.500
general, it's a general feature of life.

02:07:45.940 --> 02:07:50.380
It's, it's interesting to think about, you know, the, the standard, uh, the

02:07:50.380 --> 02:07:53.580
standard thought about this is that it's, uh, it's blind, right?

02:07:53.580 --> 02:07:56.780
Meaning that the, the, the intelligence of the process is zero, it's stumbling

02:07:56.780 --> 02:08:03.140
around, and I think that back in the day when the options, when the options were,

02:08:03.420 --> 02:08:07.580
it's dumb like machines or it's smart like humans, then of course the scientists

02:08:07.580 --> 02:08:10.460
went in this direction because nobody wanted creationism and said, okay, it's

02:08:10.460 --> 02:08:11.580
got to be like completely blind.

02:08:11.820 --> 02:08:13.100
I'm not actually sure, right?

02:08:13.100 --> 02:08:16.660
Because, because I, I think that, um, I think that everything is a continuum.

02:08:16.980 --> 02:08:20.940
And I think that it doesn't have to be smart with foresight like us, but it

02:08:20.940 --> 02:08:22.900
doesn't have to be completely blind either.

02:08:22.900 --> 02:08:24.900
I think there may be aspects of it.

02:08:24.980 --> 02:08:28.900
And in particular, this kind of multi-scale competency might give it a

02:08:28.900 --> 02:08:33.500
little bit of look ahead maybe, or a little bit of, um, problem solving sort

02:08:33.500 --> 02:08:37.180
of baked in, but, but, but that's going to be completely different in

02:08:37.180 --> 02:08:38.460
different, in different systems.

02:08:39.260 --> 02:08:40.620
I do think, I do think it's general.

02:08:40.620 --> 02:08:41.660
I don't think it's just on earth.

02:08:41.660 --> 02:08:43.420
I think it's a very fundamental thing.

02:08:43.860 --> 02:08:47.540
And it does seem to have a kind of direction that is taking us.

02:08:48.300 --> 02:08:52.020
That's somehow perhaps is defined by the environment itself.

02:08:53.020 --> 02:08:58.220
It feels like we're headed towards something like we're playing out a

02:08:58.220 --> 02:09:02.140
script that was just like a single cell defines the entire organism.

02:09:02.220 --> 02:09:02.460
Yeah.

02:09:02.940 --> 02:09:08.500
It feels like from the origin of earth itself, it's playing out a kind of

02:09:08.500 --> 02:09:12.140
script that we can't really go any other way.

02:09:12.580 --> 02:09:16.220
I mean, so this is very controversial and I don't know the answer, but people

02:09:16.220 --> 02:09:19.980
have, people have argued that this is called, uh, you know, sort of

02:09:19.980 --> 02:09:21.620
rewinding the tape of life, right?

02:09:21.820 --> 02:09:26.140
And some people have argued, I think, I think Conway Morris maybe has argued

02:09:26.140 --> 02:09:30.660
that it is that there's a deep attractor, for example, to human, to the

02:09:30.660 --> 02:09:35.180
human, um, uh, kind of, uh, structure in that, in that if you were to rewind

02:09:35.180 --> 02:09:36.940
it again, you'd basically get more or less the same thing.

02:09:37.140 --> 02:09:39.900
And then other people have argued that, no, it's, it's incredibly sensitive

02:09:39.900 --> 02:09:44.140
to frozen accidents and that once certain stochastic decisions are made

02:09:44.140 --> 02:09:45.700
downstream, everything is going to be different.

02:09:45.900 --> 02:09:46.340
I don't know.

02:09:46.340 --> 02:09:46.740
I don't know.

02:09:46.740 --> 02:09:51.620
You know, we're, we're very bad at predicting, uh, attractors in the

02:09:51.620 --> 02:09:54.100
space of complex systems, generally speaking, right?

02:09:54.100 --> 02:09:54.500
We don't know.

02:09:54.500 --> 02:09:58.460
So may, so maybe evolution on earth has these deep attractors that no matter

02:09:58.460 --> 02:10:02.420
what has happened, it pretty much would likely to end up there or maybe not.

02:10:02.900 --> 02:10:03.260
I don't know.

02:10:03.460 --> 02:10:09.380
What's a really difficult idea to imagine that if you ran earth a million times,

02:10:09.740 --> 02:10:11.940
500,000 times, you would get Hitler.

02:10:14.020 --> 02:10:16.100
Like, we don't like to think like that.

02:10:16.100 --> 02:10:21.740
We think like, because at least maybe in America, you'd like to think that

02:10:21.740 --> 02:10:24.100
individual decisions can change the world.

02:10:24.580 --> 02:10:29.660
And if individual decisions could change the world, then surely any

02:10:29.660 --> 02:10:33.180
perturbation results in a totally different trajectory.

02:10:33.580 --> 02:10:39.740
But maybe there's a, in this competency hierarchy, it's a self-correcting

02:10:39.740 --> 02:10:44.020
system that is just ultimately, there's a bunch of chaos that ultimately is

02:10:44.020 --> 02:10:47.260
leading towards something like a super intelligent artificial intelligence

02:10:47.260 --> 02:10:50.500
system that answers 42.

02:10:51.700 --> 02:10:57.660
I mean, there, there might be a kind of imperative for life that it's headed to.

02:10:58.380 --> 02:11:03.940
And we're too focused on our day-to-day life of getting coffee and snacks and

02:11:04.340 --> 02:11:12.060
having sex and getting a promotion at work, not to see the big imperative of life

02:11:12.060 --> 02:11:14.100
on earth that it's headed towards something.

02:11:14.740 --> 02:11:17.860
Yeah, maybe, maybe, I don't, it's, it's, it's difficult.

02:11:17.860 --> 02:11:24.260
I think one of the things that's important about, um, it was Chimerica

02:11:24.500 --> 02:11:29.540
by engineer technologies, all of those things are that we have to start developing

02:11:29.540 --> 02:11:34.780
a better science of predicting the cognitive goals of, of composite system.

02:11:34.780 --> 02:11:36.580
So we're just not very good at it, right?

02:11:36.580 --> 02:11:41.180
We, we don't know, uh, if, if, if, if I create a composite system and this

02:11:41.180 --> 02:11:45.300
could be internet of things or swarm robotics or a cellular, cellular swarm

02:11:45.300 --> 02:11:49.740
or whatever, what is the emergent intelligence of this thing?

02:11:49.780 --> 02:11:51.340
First of all, what level is it going to be at?

02:11:51.380 --> 02:11:54.740
And if it has goal directed capacity, what are the goals going to be?

02:11:55.180 --> 02:11:57.780
Like, we are just not very good at predicting that yet.

02:11:58.100 --> 02:12:04.820
And I think that, uh, it's, it's, um, it's a, it's a existential level,

02:12:04.900 --> 02:12:08.820
uh, need for us to be able to, because we're building these things all the time, right?

02:12:08.820 --> 02:12:13.020
We're building, we're building both physical structures like swarm robotics, and

02:12:13.020 --> 02:12:18.260
we're building, uh, uh, social financial structures and so on with very little

02:12:18.260 --> 02:12:21.900
ability to, uh, predict what sort of autonomous goals that system is going

02:12:21.900 --> 02:12:23.420
to have of which we are now cogs.

02:12:23.820 --> 02:12:24.660
And so, right.

02:12:24.660 --> 02:12:27.940
So, so learning, learning to predict and control those things is going to be critical.

02:12:28.100 --> 02:12:31.340
So we've, so, so in fact, so, so if you're right, and there is some kind of

02:12:31.340 --> 02:12:36.260
attractor to evolution, it would be nice to know what that is and then to make a

02:12:36.260 --> 02:12:39.020
rational decision of whether we're going to go along or we're going to pop out of

02:12:39.020 --> 02:12:41.860
it or try to pop out of it, because there's no guarantee.

02:12:41.860 --> 02:12:44.140
I mean, that's, that's, that's the other, you know, kind of important thing.

02:12:44.460 --> 02:12:49.060
A lot of people, I get a lot of complaints, uh, from, from people email me and say,

02:12:49.060 --> 02:12:52.140
uh, you know, what you're doing, uh, it isn't natural, you know?

02:12:52.420 --> 02:12:56.980
And I'll say, look, natural, that'd be nice if, if somebody was making sure that

02:12:56.980 --> 02:13:01.980
natural was, was, was matched up to our values, but no one's doing that by, you

02:13:01.980 --> 02:13:03.860
know, evolution optimizes for biomass.

02:13:03.940 --> 02:13:04.300
That's it.

02:13:04.500 --> 02:13:05.500
Nobody's optimizing it.

02:13:05.500 --> 02:13:07.020
It's not optimizing for your happiness.

02:13:07.020 --> 02:13:10.420
It's, I don't think necessarily it's optimizing for, for, for intelligence or

02:13:10.420 --> 02:13:11.500
fairness or any of that stuff.

02:13:11.980 --> 02:13:16.180
I'm going to find that person that emailed you beat them up, take their

02:13:16.180 --> 02:13:21.180
place, um, steal everything they own and say, now we're down.

02:13:21.180 --> 02:13:21.940
This is natural.

02:13:21.940 --> 02:13:22.500
This is natural.

02:13:22.500 --> 02:13:23.100
Yeah, exactly.

02:13:23.100 --> 02:13:26.980
Because, because it comes from, it comes from an, from an old worldview where

02:13:27.140 --> 02:13:31.300
you could assume that whatever is natural, that that's probably for the best.

02:13:31.300 --> 02:13:34.220
And I think we're long out of that garden of, of Eden kind of view.

02:13:34.540 --> 02:13:35.980
So I think we can do better.

02:13:35.980 --> 02:13:37.660
We, I think we, and we have to, right.

02:13:37.940 --> 02:13:41.260
Natural just isn't great for, for a lot of, uh, a lot of life forms.

02:13:41.860 --> 02:13:45.580
What are some cool synthetic organisms that you, you think about, you dream

02:13:45.580 --> 02:13:48.100
about when you think about embodied mind?

02:13:48.660 --> 02:13:49.700
What do you imagine?

02:13:50.180 --> 02:13:51.260
What do you hope to build?

02:13:51.460 --> 02:13:51.940
Yeah.

02:13:52.020 --> 02:13:57.820
On a practical level, what I really hope to do is to gain enough of an understanding

02:13:57.820 --> 02:14:04.220
of the embodied intelligence of the organs and tissues such that we can achieve a

02:14:04.220 --> 02:14:09.500
radically different regenerative medicine so that we can say basically, and I think

02:14:09.500 --> 02:14:13.380
about it as, um, uh, you know, in terms of like, okay, can you, what's the, what's

02:14:13.380 --> 02:14:18.420
the, uh, uh, what's the goal, uh, kind of, uh, and, and, and game for this whole

02:14:18.420 --> 02:14:22.180
thing to, to me, the end game is something that you would call an anatomical compiler.

02:14:22.380 --> 02:14:25.500
So the idea is you would sit down in front of the computer and you would draw.

02:14:25.940 --> 02:14:30.940
The, the body or the organ that you wanted, not, not molecular details, but

02:14:30.940 --> 02:14:31.900
like, yeah, this is what I want.

02:14:31.900 --> 02:14:35.500
I want a six legged, uh, you know, frog with a propeller on top, or I want, I

02:14:35.500 --> 02:14:38.020
want a heart that looks like this, or I want a leg that looks like this.

02:14:38.300 --> 02:14:42.940
And what it would do if we knew what we were doing is put out, uh, it could

02:14:42.940 --> 02:14:47.380
convert that anatomical description into a set of stimuli that would have to be

02:14:47.380 --> 02:14:50.660
given to cells to convince them to build exactly that thing, right?

02:14:50.860 --> 02:14:53.380
I probably won't live to see it, but I think it's achievable.

02:14:53.780 --> 02:14:59.180
And I think what that, if, if we can have that, then that is basically the solution

02:14:59.180 --> 02:15:04.420
to all of medicine except for infectious disease, so birth defects, right?

02:15:04.420 --> 02:15:07.100
Traumatic injury, cancer, aging, degenerative disease.

02:15:07.340 --> 02:15:10.380
If we knew how to tell cells what to build, all of those things go away.

02:15:10.700 --> 02:15:15.860
So those things go away and the, um, uh, positive, uh, feedback spiral of

02:15:15.860 --> 02:15:20.780
economic costs, where all of the advances are increasingly more heroic and

02:15:20.780 --> 02:15:24.260
expensive interventions of a sinking ship when you're like 90 and, and, and

02:15:24.260 --> 02:15:25.700
so on, right, all of that goes away.

02:15:25.700 --> 02:15:28.980
Because basically instead of trying to fix you up as you, as you degrade,

02:15:29.260 --> 02:15:33.020
you, you, um, you progressively regenerate, you know, you, you apply

02:15:33.020 --> 02:15:35.180
the regenerative medicine early before things degrade.

02:15:35.660 --> 02:15:39.380
So I think that that'll have massive economic impacts over what we're trying

02:15:39.380 --> 02:15:43.100
to do now, which is not at all, you know, sustainable and, uh, and that,

02:15:43.100 --> 02:15:46.860
that's what I hope, I hope that, I hope that we get, so, so to me, yes, the

02:15:46.860 --> 02:15:51.260
xenobots will be doing useful things, cleaning up the environment, cleaning

02:15:51.260 --> 02:15:54.820
out, you know, your, or, you know, your joints and all that kind of stuff, but

02:15:54.820 --> 02:16:01.660
more important than that, I think we can use these, uh, synthetic systems to try

02:16:01.660 --> 02:16:06.500
to understand, to, to, to, to develop a science of detecting and manipulating

02:16:06.540 --> 02:16:09.980
the goals of collective intelligences of cells specifically for regenerative

02:16:09.980 --> 02:16:13.900
medicine, and then sort of beyond that, if we, you know, sort of think further

02:16:13.900 --> 02:16:17.540
beyond that, what I hope is that kind of like what you said, all of this drives

02:16:17.620 --> 02:16:24.660
a reconsideration of how we formulate, um, ethical norms because this old school,

02:16:24.660 --> 02:16:29.060
so, so, so in the olden days, what you could do is, as you were confronted

02:16:29.060 --> 02:16:31.340
with something new, so you could, so you could tap on it, right?

02:16:31.460 --> 02:16:34.140
And if you heard a metallic clanging sound, you'd said, ah, fine, right?

02:16:34.140 --> 02:16:37.060
So you could conclude it was made in a factory, I could take it apart,

02:16:37.140 --> 02:16:37.900
I can do whatever, right?

02:16:37.900 --> 02:16:42.020
If you did that and you got sort of a squishy, uh, kind of warm sensation,

02:16:42.020 --> 02:16:44.820
you'd say, ah, I need to be, you know, more or less nice to it and whatever.

02:16:44.980 --> 02:16:47.020
That's not going to be feasible.

02:16:47.020 --> 02:16:50.100
It was never really feasible, but it was good enough because we didn't have any,

02:16:50.260 --> 02:16:52.540
we didn't know any better that needs to go.

02:16:52.740 --> 02:16:59.020
And I think that, uh, by, by breaking down those artificial barriers someday

02:16:59.020 --> 02:17:04.420
we can try to build a, uh, a system of, of, of ethical norms that does not rely

02:17:04.420 --> 02:17:08.500
on these completely contingent facts of, of, of our earthly history, but on

02:17:08.500 --> 02:17:12.620
something much, much deeper that, you know, really, um, takes, takes agency

02:17:12.620 --> 02:17:16.300
and, and, and, uh, the capacity to suffer and all that takes that seriously.

02:17:17.140 --> 02:17:20.700
The capacity to suffer and the deep questions I would ask of a system is

02:17:20.700 --> 02:17:22.540
can I eat it and can I have sex with it?

02:17:23.100 --> 02:17:28.020
Um, which is the, the two fundamental tests of again, the human condition.

02:17:28.700 --> 02:17:36.540
Uh, so I can basically do what Dolly does that's in the, in the physical space.

02:17:36.900 --> 02:17:41.580
So print out like a 3d print Pepe the frog with a propeller head,

02:17:42.220 --> 02:17:45.780
propeller hat, uh, is the, is the dream.

02:17:46.140 --> 02:17:47.580
Well, yes and no.

02:17:47.580 --> 02:17:51.060
I mean, I want to get away from the 3d printing thing because that will be

02:17:51.060 --> 02:17:53.420
available for some things much earlier.

02:17:53.420 --> 02:17:56.980
I mean, we can already do bladders and ears and things like that because

02:17:56.980 --> 02:17:58.380
it's micro level control, right?

02:17:58.380 --> 02:18:01.340
When you 3d print, you are in charge of where every cell goes.

02:18:01.340 --> 02:18:03.820
And for some things that, you know, for, for like this thing, they had that, I

02:18:03.820 --> 02:18:06.420
think 20 years ago, or maybe earlier than that, you could do that.

02:18:06.700 --> 02:18:10.980
So yeah, I would like to emphasize the Dolly part where you provide a few words

02:18:11.260 --> 02:18:12.980
and it generates a painting.

02:18:13.340 --> 02:18:20.380
So here you say, I want a frog with these features, and then it would go

02:18:20.380 --> 02:18:24.780
direct a complex biological system to construct something like that.

02:18:24.980 --> 02:18:25.220
Yeah.

02:18:25.420 --> 02:18:29.540
The main magic would be, I mean, I think from, from looking at the Dolly and so

02:18:29.540 --> 02:18:32.780
on, it looks like the first part is kind of solved now where you go from, from

02:18:32.780 --> 02:18:35.980
the words to the image, like that seems more or less solved.

02:18:36.460 --> 02:18:38.500
The next step is really hard.

02:18:38.660 --> 02:18:41.860
This is what keeps things like CRISPR and genomic editing and so on.

02:18:41.860 --> 02:18:42.140
It's good.

02:18:42.140 --> 02:18:48.020
It's what limits all the, uh, uh, impacts for, for regenerative medicine, because

02:18:48.500 --> 02:18:51.620
going back to, okay, this is the knee joint that I want, or this is the eye

02:18:51.660 --> 02:18:55.380
that I want now, what genes do I edit to make that happen, right?

02:18:55.380 --> 02:18:57.420
Going back in that direction is really hard.

02:18:57.620 --> 02:19:00.780
So instead of that, it's going to be, okay, I understand how to motivate cells

02:19:00.780 --> 02:19:02.060
to build particular structures.

02:19:02.220 --> 02:19:05.220
Can I rewrite the memory of what they think they're supposed to be building

02:19:05.340 --> 02:19:08.420
such that then I can, you know, take my hands off the wheel and let them,

02:19:08.500 --> 02:19:09.420
let them do their thing.

02:19:09.820 --> 02:19:13.740
So some of that is experiment, but some of that may be AI can help too, just

02:19:13.740 --> 02:19:17.820
like with protein folding, this is exactly the problem that protein folding,

02:19:18.380 --> 02:19:25.900
uh, in, in the most simple, uh, medium tried and has solved with the alpha

02:19:25.900 --> 02:19:32.900
fold, which is how does the sequence of letters result in this three

02:19:32.900 --> 02:19:37.100
dimensional shape and you have to, um, I guess it didn't solve it because you

02:19:37.100 --> 02:19:42.220
have to, if you say I want this shape, how do I then have a sequence of letters?

02:19:42.820 --> 02:19:43.340
Yeah.

02:19:43.620 --> 02:19:45.820
The reverse engineering step was really tricky.

02:19:46.220 --> 02:19:46.540
It is.

02:19:46.540 --> 02:19:50.980
I think, I think where, where, and we're doing some of this now is, is to, uh,

02:19:51.220 --> 02:19:56.540
use AI to try and, uh, build actionable models of the intelligence of the

02:19:56.540 --> 02:20:00.780
cellular collectives, so try to help us and help us gain models that, that, that,

02:20:01.060 --> 02:20:02.980
um, and, and we've had some success in this.

02:20:02.980 --> 02:20:07.780
So we, we did something like this for, um, uh, for, you know, for repairing,

02:20:07.820 --> 02:20:10.140
uh, uh, birth defects of the brain in frog.

02:20:10.140 --> 02:20:14.420
We've done some of this for, um, normalizing melanoma, uh, where you can

02:20:14.420 --> 02:20:19.860
really start to use AI to make models of how would I impact this thing if I

02:20:19.860 --> 02:20:22.100
wanted to, given all the complexities, right.

02:20:22.100 --> 02:20:26.220
And, and, and given all the, uh, the, the, the controls that it, that it,

02:20:26.220 --> 02:20:27.020
that it knows how to do.

02:20:27.340 --> 02:20:33.180
So when you say regenerative medicine, so we talked about creating biological

02:20:33.180 --> 02:20:39.820
organisms, but if you regrow a hand, that information is already there, right?

02:20:39.820 --> 02:20:42.500
The biological system has that information.

02:20:43.180 --> 02:20:46.580
So how does regenerative medicine work today?

02:20:46.620 --> 02:20:47.780
How do you hope it works?

02:20:47.780 --> 02:20:49.140
What's the hope there?

02:20:49.220 --> 02:20:49.460
Yeah.

02:20:49.860 --> 02:20:50.260
Yeah.

02:20:50.380 --> 02:20:51.660
How do you make it happen?

02:20:52.220 --> 02:20:55.100
Well, today there's a set of popular approaches.

02:20:55.100 --> 02:20:56.500
So, so one is 3d printing.

02:20:56.620 --> 02:20:59.500
So the idea is I'm going to make a scaffold of the thing that I want.

02:20:59.500 --> 02:21:01.820
I'm going to seed it with cells and then, and then there it is, right.

02:21:01.820 --> 02:21:03.940
So kind of direct, and then that works for certain things.

02:21:03.940 --> 02:21:06.620
You can make a bladder that way or an ear, something like that.

02:21:07.180 --> 02:21:11.580
Um, the other, the other idea is, is some sort of stem cell transplant.

02:21:11.660 --> 02:21:15.700
So the idea is if we, uh, if we put in stem cells with appropriate factors, we

02:21:15.700 --> 02:21:19.900
can get them to generate certain kinds of neurons for certain diseases and so on.

02:21:20.300 --> 02:21:24.180
All of those things are good for relatively simple structures.

02:21:24.540 --> 02:21:29.420
But when you want an eye or a hand or something else, I think in this, maybe

02:21:29.420 --> 02:21:33.700
an unpopular opinion, I think the only hope we have in any reasonable kind of

02:21:33.700 --> 02:21:39.300
timeframe is to understand how the thing was motivated to get made in the first place.

02:21:39.580 --> 02:21:43.460
So what is it that, that made those cells in the, in the beginning, create a

02:21:43.460 --> 02:21:49.260
particular arm with a particular, uh, set of sizes and shapes and number of fingers

02:21:49.260 --> 02:21:52.060
and all that, and why is it that a salamander can keep losing theirs and keep

02:21:52.060 --> 02:21:57.820
regrowing theirs and a planarian can do the same even more so to me, uh, kind of

02:21:57.860 --> 02:22:02.700
ultimate regenerative medicine was when you can tell the cells to build whatever

02:22:02.700 --> 02:22:04.500
it is you need them to build, right.

02:22:04.700 --> 02:22:07.540
And so the, so that we can all be like planaria, basically,

02:22:07.660 --> 02:22:12.300
do you have to start at the very beginning or can you, um, do a

02:22:12.300 --> 02:22:14.620
shortcut because we're growing a hand.

02:22:15.380 --> 02:22:17.220
You already got the whole organism.

02:22:17.220 --> 02:22:17.460
Yeah.

02:22:18.220 --> 02:22:19.300
So here's what we've done, right?

02:22:19.300 --> 02:22:22.180
So, so we've, we've more or less solved that in frogs.

02:22:22.180 --> 02:22:25.100
So frogs, unlike salamanders do not regenerate their legs as adults.

02:22:25.420 --> 02:22:32.220
And so, so, uh, we've shown that with a very, um, uh, kind of simple intervention.

02:22:32.220 --> 02:22:36.580
So what we do is there's two things you need to, uh, you need to have a signal

02:22:36.580 --> 02:22:38.140
that tells the cells what to do.

02:22:38.140 --> 02:22:39.420
And then you need some way of delivering it.

02:22:39.420 --> 02:22:42.260
And so this has worked together with, um, with David Kaplan and I

02:22:42.260 --> 02:22:44.140
should do a, um, uh, disclosure here.

02:22:44.140 --> 02:22:47.260
We have a company called more for pseudocals and spit spinoff where we're

02:22:47.260 --> 02:22:51.220
trying to, uh, to address, uh, regenerate, you know, limb regeneration.

02:22:51.260 --> 02:22:54.220
So we've solved it in the frog and we're now in, uh, trials in mice.

02:22:54.260 --> 02:22:56.900
So now we're going to, we're in mammals now and I can't say anything

02:22:56.900 --> 02:22:58.460
about how it's going, but the frog thing is solved.

02:22:58.740 --> 02:23:01.940
So what you do is, um, after you have a little frog loose

02:23:01.940 --> 02:23:03.580
Skywalker with every growing hand.

02:23:03.620 --> 02:23:04.820
Yeah, basically, basically.

02:23:04.820 --> 02:23:05.020
Yeah.

02:23:05.060 --> 02:23:05.260
Yeah.

02:23:05.260 --> 02:23:07.700
So what you do is we did, we did with legs instead of forearms.

02:23:07.700 --> 02:23:10.740
And what you do is, um, after amputation, normally they, they don't regenerate.

02:23:11.020 --> 02:23:12.820
You put on a wearable bioreactor.

02:23:12.860 --> 02:23:16.060
So it's this thing that, um, that goes on and, uh, Dave Kaplan

02:23:16.060 --> 02:23:17.220
does lab makes these things.

02:23:17.700 --> 02:23:21.060
And inside it's a, it's a very controlled, um, environment.

02:23:21.140 --> 02:23:25.180
It is a silk gel that carries, uh, some drugs, for example, ion channel drugs.

02:23:25.620 --> 02:23:29.420
And what you're doing is you're saying to these cells, you should

02:23:29.500 --> 02:23:31.380
regrow what normally goes here.

02:23:31.660 --> 02:23:35.580
So, uh, that whole thing is on for 24 hours.

02:23:35.860 --> 02:23:37.660
Then you take it off and you don't touch the leg again.

02:23:37.660 --> 02:23:40.420
This is really important because what we're not looking for is a set

02:23:40.420 --> 02:23:43.900
of micromanagement, uh, you know, printing or controlling the cells.

02:23:44.060 --> 02:23:46.580
We want to trigger, we want to, we want to interact with it early

02:23:46.580 --> 02:23:48.180
on and then not touch it again.

02:23:48.220 --> 02:23:50.420
Because, because we don't know how to make a frog leg, but the

02:23:50.420 --> 02:23:51.620
frog knows how to make a frog leg.

02:23:52.020 --> 02:23:57.060
So 24 hours, 18 months of leg growth after that, without us touching it again.

02:23:57.300 --> 02:23:58.820
And after 18 months, you get a pretty good leg.

02:23:59.260 --> 02:24:02.740
That kind of shows this proof of concept that early on when the cells, right

02:24:02.740 --> 02:24:05.420
after injury, when they're first making a decision about what they're going to do,

02:24:05.420 --> 02:24:06.660
you can, you can impact them.

02:24:06.900 --> 02:24:09.980
And once they've decided to make a leg, they don't need you after that.

02:24:09.980 --> 02:24:11.180
They can do their own thing.

02:24:11.700 --> 02:24:13.700
So that's an approach that we're now taking.

02:24:13.860 --> 02:24:15.340
What about cancer suppression?

02:24:15.940 --> 02:24:17.180
That's something you mentioned earlier.

02:24:17.620 --> 02:24:20.060
How can all of these ideas help with cancer suppression?

02:24:20.260 --> 02:24:23.220
So let's, let's go back to the beginning and ask what, what, what cancer is.

02:24:23.420 --> 02:24:27.660
So I think, um, you know, asking why there's cancer is the wrong question.

02:24:27.660 --> 02:24:30.420
I think the right question is why is there ever anything but cancer?

02:24:30.620 --> 02:24:34.700
So, so in the normal state, you have a bunch of cells that are all

02:24:34.700 --> 02:24:36.700
cooperating towards a large scale goal.

02:24:37.100 --> 02:24:40.700
If that process of cooperation breaks down and you've got a cell that is

02:24:40.700 --> 02:24:44.220
isolated from that electrical network, that lets you remember what the big goal

02:24:44.220 --> 02:24:48.460
is, you revert back to your unicellular lifestyle as far as nothing about that

02:24:48.460 --> 02:24:50.180
border between self and world, right?

02:24:50.180 --> 02:24:52.740
Normally when all these cells are connected by gap junctions into an

02:24:52.740 --> 02:24:56.700
electrical network, they are all one self, right?

02:24:57.180 --> 02:25:01.820
Meaning that, um, their goals, they have these large tissue level goals and so on.

02:25:02.060 --> 02:25:05.940
As soon as a cell is disconnected from that, the self is tiny, right?

02:25:05.940 --> 02:25:10.740
And so at that point, and so, so people, a lot of people model cancer cells

02:25:10.740 --> 02:25:11.980
as being more selfish and all that.

02:25:11.980 --> 02:25:12.740
They're not more selfish.

02:25:12.740 --> 02:25:14.860
They're equally selfish is just that their self is smaller.

02:25:15.140 --> 02:25:16.300
Normally the self is huge.

02:25:16.300 --> 02:25:17.460
Now they got tiny little cells.

02:25:17.660 --> 02:25:19.500
Now, what are the goals of tiny little cells?

02:25:19.500 --> 02:25:22.620
Well, proliferate and migrate to wherever life is good.

02:25:22.620 --> 02:25:24.900
And that's metastasis, that's proliferation and metastasis.

02:25:25.300 --> 02:25:29.980
So, so one thing we found and people have noticed years ago that when cells

02:25:29.980 --> 02:25:33.700
convert to cancer, the first thing they see is they close the gap junctions.

02:25:34.140 --> 02:25:37.300
And it's a lot like, I think it's a lot like that experiment with the slime

02:25:37.300 --> 02:25:41.580
mold where until you close that gap junction, you can't even entertain the

02:25:41.580 --> 02:25:44.420
idea of leaving the collective because there is no you at that point, right?

02:25:44.420 --> 02:25:46.900
Your mind melded with this, with this whole other network.

02:25:47.100 --> 02:25:51.420
But as soon as the gap junction is closed, now the boundary between you now,

02:25:51.460 --> 02:25:53.940
now the rest of the body is just outside environment to you.

02:25:54.060 --> 02:25:56.860
You're just a, you're just a unicellular organism and the

02:25:56.860 --> 02:25:58.020
rest of the body's environment.

02:25:58.380 --> 02:26:04.140
So, so we, so we studied this process and we worked out a way to artificially

02:26:04.140 --> 02:26:08.460
control the bioelectric state of these cells to physically force them to

02:26:08.460 --> 02:26:09.740
remain in that network.

02:26:09.940 --> 02:26:14.820
And so then, then what that, what that means is that nasty mutations like

02:26:14.820 --> 02:26:18.180
KRAS and things like that, these really tough oncogenic mutations that cause

02:26:18.220 --> 02:26:25.620
tumors, if you, if you do them and then, but then, but then artificially control

02:26:25.620 --> 02:26:30.420
the bioelectrics, you, you, you, you greatly reduce tumor genesis or, or

02:26:30.460 --> 02:26:33.940
normalize cells that had already begun to convert you, basically they go back

02:26:33.940 --> 02:26:34.860
to being normal cells.

02:26:35.260 --> 02:26:38.500
And so this is another, much like with the planaria, this is another way in

02:26:38.500 --> 02:26:43.500
which the bioelectric state kind of dominates what the, what the genetic state

02:26:43.500 --> 02:26:43.660
is.

02:26:43.780 --> 02:26:47.020
So if you sequence the, the, you know, if you sequence the nucleic acid, you'll

02:26:47.020 --> 02:26:50.180
see the KRAS mutation, you'll say, ah, well, that's going to be a tumor, but

02:26:50.180 --> 02:26:52.980
there isn't a tumor because, because bioelectrically you've kept the cells

02:26:52.980 --> 02:26:56.340
connected and they're just working on making nice skin and kidneys and

02:26:56.340 --> 02:26:56.860
whatever else.

02:26:57.420 --> 02:27:01.900
So, so we've started moving that to, to, to human glioblastoma cells and we're

02:27:01.900 --> 02:27:05.780
hoping for, you know, a patient in the future interaction with patients.

02:27:07.460 --> 02:27:12.460
So is this one of the possible ways in which we may quote, cure cancer?

02:27:12.860 --> 02:27:13.340
I think so.

02:27:13.460 --> 02:27:14.100
Yeah, I think so.

02:27:14.100 --> 02:27:16.660
I think, I think the actual cure, I mean, there are other

02:27:16.660 --> 02:27:18.260
technology, you know, immune therapy.

02:27:18.260 --> 02:27:20.060
I think it's a great technology.

02:27:20.820 --> 02:27:23.740
Chemotherapy I don't think is a good, is a good technology.

02:27:23.740 --> 02:27:25.260
I think we've got to get out, get off of that.

02:27:25.540 --> 02:27:27.660
So chemotherapy just kills cells.

02:27:28.140 --> 02:27:28.420
Yeah.

02:27:28.660 --> 02:27:32.700
Well, chemotherapy hopes to kill more of the tumor cells than of your cells.

02:27:32.780 --> 02:27:33.140
That's it.

02:27:33.140 --> 02:27:33.980
It's a fine balance.

02:27:33.980 --> 02:27:36.900
The problem is the cells are very similar because they are your cells.

02:27:37.300 --> 02:27:42.580
And so if you don't have a very tight way of distinguishing between them, then

02:27:43.300 --> 02:27:45.980
the toll that chemo takes on the rest of the body is just unbelievable.

02:27:46.380 --> 02:27:49.340
Immunotherapy tries to get the immune system to do some of the work.

02:27:49.340 --> 02:27:49.700
Exactly.

02:27:50.020 --> 02:27:50.220
Yeah.

02:27:50.380 --> 02:27:53.420
I think that's potentially a very good, a very good approach.

02:27:54.180 --> 02:27:58.940
If, if the immune system can be taught to recognize enough of, of the cancer

02:27:58.940 --> 02:28:00.140
cells, that that's a pretty good approach.

02:28:00.140 --> 02:28:03.380
But I, but I think, but I think our approach is in a way more fundamental

02:28:03.900 --> 02:28:08.460
because if you can, if you can keep the cells harnessed towards organ level

02:28:08.460 --> 02:28:12.620
goals, as opposed to individual cell goals, then nobody will be making a

02:28:12.620 --> 02:28:14.260
tumor or metastasizing and so on.

02:28:15.260 --> 02:28:17.380
So we've been living through a pandemic.

02:28:18.500 --> 02:28:22.900
What do you think about viruses in this full, beautiful biological

02:28:22.900 --> 02:28:24.260
context we've been talking about?

02:28:24.260 --> 02:28:26.100
Are they beautiful to you?

02:28:26.100 --> 02:28:28.260
Are they terrifying?

02:28:29.060 --> 02:28:35.580
Uh, also maybe, uh, let's say, are they, since we've been discriminating

02:28:35.580 --> 02:28:37.540
this whole conversation, are they living?

02:28:38.540 --> 02:28:40.020
Are they embodied minds?

02:28:40.820 --> 02:28:42.700
Embodied minds that are assholes.

02:28:43.660 --> 02:28:46.660
As far as I know, and I haven't been able to find this paper again, but, but

02:28:46.660 --> 02:28:50.860
somewhere I saw in the last couple of months, there was some, there was some

02:28:50.860 --> 02:28:53.780
papers showing an example of a virus that actually had physiology.

02:28:53.780 --> 02:28:55.300
So there was some, something was going on.

02:28:55.300 --> 02:28:57.580
I think proton flux or something on the virus itself.

02:28:57.980 --> 02:29:02.140
But, but barring that, uh, generally speaking, viruses are very passive.

02:29:02.140 --> 02:29:03.820
They don't do anything by themselves.

02:29:03.820 --> 02:29:09.220
And so I don't see any particular reason to attribute much of a mind to them.

02:29:09.260 --> 02:29:16.020
I think, um, you know, uh, they represent a way to hijack other minds for

02:29:16.020 --> 02:29:18.380
sure, like, like cells and other things.

02:29:18.700 --> 02:29:20.380
But that's an interesting interplay though.

02:29:20.700 --> 02:29:25.780
If they're hijacking other minds, you know, the way we're, we were talking

02:29:25.780 --> 02:29:29.100
about living organisms, that they can interact with each other and have a,

02:29:29.660 --> 02:29:35.100
uh, alter each other's trajectory by having interacted.

02:29:35.140 --> 02:29:41.100
I mean, there, that's, that's a deep, meaningful connection

02:29:41.100 --> 02:29:42.420
between a virus and a cell.

02:29:43.100 --> 02:29:46.740
And I think both are transformed by the experience.

02:29:46.740 --> 02:29:48.660
And so in that sense, both are living.

02:29:49.740 --> 02:29:50.060
Yeah.

02:29:50.100 --> 02:29:50.380
Yeah.

02:29:50.540 --> 02:29:55.700
Uh, you know, the whole category that I, um, I, the, this question of what's

02:29:55.700 --> 02:29:59.180
living and what's not living, I really, um, I'm not sure I, and I know there's

02:29:59.180 --> 02:30:01.820
people that work on this and I want to, I don't want to piss anybody off, but,

02:30:02.060 --> 02:30:06.900
but, um, I have not found that particularly useful as, as to try and make

02:30:06.900 --> 02:30:09.620
that a binary, um, kind of a distinction.

02:30:09.820 --> 02:30:14.860
I think level of cognition is very interesting as a, as a continuum, but,

02:30:14.860 --> 02:30:18.460
but living and non-living, you know, I don't, I really know what to do with that.

02:30:18.460 --> 02:30:21.540
I don't, I don't know what you do next after, after making that distinction.

02:30:21.580 --> 02:30:24.780
That's why I make the very binary distinction.

02:30:24.780 --> 02:30:26.660
Can I have sex with it or not?

02:30:26.940 --> 02:30:28.140
Can I eat it or not?

02:30:28.220 --> 02:30:30.220
Those, cause there's, those are actionable, right?

02:30:30.220 --> 02:30:30.420
Yeah.

02:30:30.620 --> 02:30:33.580
Well, I think that's a critical point that you brought up because how you

02:30:33.580 --> 02:30:37.580
relate to something is really what this is all about, right?

02:30:37.580 --> 02:30:39.740
As an engineer, how do I control it?

02:30:39.820 --> 02:30:41.500
But maybe I shouldn't be controlling it.

02:30:41.500 --> 02:30:44.500
Maybe I should be, you know, uh, can I have a relationship with it?

02:30:44.500 --> 02:30:46.260
Should I be listening to its advice?

02:30:46.260 --> 02:30:49.820
Like, like all the way from, you know, I need to take it apart all the

02:30:49.820 --> 02:30:51.980
way to, uh, I better do what it says.

02:30:51.980 --> 02:30:54.380
Cause it seems to be pretty smart and everything in between, right?

02:30:54.380 --> 02:30:55.980
That's really what we're asking about.

02:30:56.660 --> 02:30:57.060
Yeah.

02:30:57.060 --> 02:30:59.580
We need to understand our relationship to it.

02:30:59.580 --> 02:31:02.700
We're searching for that relationship, but even in the most trivial senses,

02:31:03.700 --> 02:31:06.060
you came up with a lot of interesting terms.

02:31:07.340 --> 02:31:10.740
We've mentioned some of them, uh, a gentle material.

02:31:10.780 --> 02:31:12.180
That's a really interesting one.

02:31:13.060 --> 02:31:17.860
That's a really interesting one for the future of computation and artificial

02:31:17.860 --> 02:31:19.940
intelligence and computer science and all of that.

02:31:21.060 --> 02:31:23.620
There's also, let me go through some of them.

02:31:23.940 --> 02:31:28.860
If they spark some interesting thought for you, there's teleophobia,

02:31:29.020 --> 02:31:32.700
the unwanted fear of erring on the side of, uh, too much agency

02:31:32.700 --> 02:31:34.260
when considering a new system.

02:31:34.980 --> 02:31:35.300
Yeah.

02:31:35.300 --> 02:31:36.740
I mean, that's the opposite.

02:31:36.780 --> 02:31:40.780
I mean, being afraid of maybe anthropomorphizing the thing.

02:31:40.980 --> 02:31:43.060
This will get some people ticked off, I think.

02:31:43.060 --> 02:31:48.420
But, but I don't think, I think, I think the whole notion of anthropomorphizing

02:31:48.860 --> 02:31:54.060
is a holdover from an, from a pre-scientific age where humans were

02:31:54.060 --> 02:31:56.300
magic and everything else wasn't magic.

02:31:56.540 --> 02:32:01.140
And you were anthropomorphizing when you dared suggest that, uh, something

02:32:01.140 --> 02:32:02.700
else has some features of humans.

02:32:02.980 --> 02:32:04.980
And I think we need to be way beyond that.

02:32:05.340 --> 02:32:10.500
And this, this issue of anthropomorphizing, I think is, um, it's a cheap,

02:32:10.540 --> 02:32:11.420
it's a cheap charge.

02:32:11.420 --> 02:32:15.620
I don't think it, it, it holds any water at all other than when somebody

02:32:15.620 --> 02:32:19.940
makes a cognitive claim, I think all cognitive claims are engineering claims, really.

02:32:20.100 --> 02:32:23.740
So when somebody says this thing knows, or this thing hopes, or this thing

02:32:23.780 --> 02:32:27.460
wants, or this thing predicts, all you can say is fabulous.

02:32:27.500 --> 02:32:32.100
Give me the engineering protocol that you've derived using that hypothesis.

02:32:32.500 --> 02:32:34.340
And we will see if this thing helps us or not.

02:32:34.340 --> 02:32:37.140
And then, and then we can, you know, then we can make a rational decision.

02:32:37.620 --> 02:32:42.940
I also like anatomical compiler, a future system representing the long-term end

02:32:42.940 --> 02:32:48.100
game of the science of morphogenesis that reminds us how far away from

02:32:48.100 --> 02:32:49.420
true understanding we are.

02:32:49.780 --> 02:32:54.380
Someday you will be able to sit in front of an anatomical computer, specify

02:32:54.380 --> 02:32:58.300
the shape of the animal or a plant that you want, and it will convert that shape

02:32:58.420 --> 02:33:03.060
specification to a set of stimuli that will have to be given to cells to build

02:33:03.060 --> 02:33:09.300
exactly that shape, no matter how weird it ends up being, you have total control.

02:33:09.860 --> 02:33:15.420
Just imagine the possibility for memes in the physical space.

02:33:15.740 --> 02:33:20.540
One of the glorious accomplishments of human civilizations is memes in digital space.

02:33:21.260 --> 02:33:24.220
Now this could create memes in physical space.

02:33:24.820 --> 02:33:27.660
I am both excited and terrified by that possibility.

02:33:28.340 --> 02:33:29.460
Cognitive light cone.

02:33:29.460 --> 02:33:34.060
I think we also talked about the outer boundary in space and time of the

02:33:34.060 --> 02:33:36.500
largest goal a given system can work towards.

02:33:37.900 --> 02:33:41.820
Is this kind of like shaping the set of options?

02:33:42.300 --> 02:33:44.100
It's a little different than options.

02:33:44.620 --> 02:33:51.420
It's really focused on, so back in this, I first came up with this back in 2018,

02:33:51.420 --> 02:33:55.300
I want to say we had a, there was a conference, a Templeton conference where

02:33:55.300 --> 02:33:58.460
they challenged us to come up with frameworks.

02:33:58.460 --> 02:34:01.860
And I think actually it's the here, it's the diverse intelligence community.

02:34:01.860 --> 02:34:02.980
That summer Institute.

02:34:02.980 --> 02:34:04.180
Yeah, they had a summer Institute.

02:34:04.180 --> 02:34:06.620
But the logo is the B with some circuits.

02:34:06.700 --> 02:34:08.460
Yeah, it's got different, different life forms.

02:34:08.460 --> 02:34:12.260
And, you know, so, so, so the whole, the whole program is called diverse

02:34:12.260 --> 02:34:14.500
intelligence and they sort of, they challenged us to come up with a

02:34:14.500 --> 02:34:19.660
framework that was suitable for analyzing different kinds of intelligence together.

02:34:19.660 --> 02:34:19.900
Right.

02:34:20.260 --> 02:34:23.580
Because, because the kinds of things you do to a human are not good with an

02:34:23.580 --> 02:34:25.340
octopus, not good with a plant and so on.

02:34:25.980 --> 02:34:31.660
So, so I started thinking about this and, um, I, I asked myself, what, uh,

02:34:32.020 --> 02:34:35.780
what do all cognitive agents, no matter what their provenance, no matter what

02:34:35.780 --> 02:34:40.540
their, um, uh, uh, architecture is, what, what, what do cognitive agents have in

02:34:40.540 --> 02:34:44.020
common and it seems to me that what they have in common is some degree of

02:34:44.020 --> 02:34:45.580
competency to pursue a goal.

02:34:45.820 --> 02:34:47.620
And so what you can do then is you can draw.

02:34:47.620 --> 02:34:50.180
And so what I, what I, what I ended up drawing was this thing that it's kind

02:34:50.180 --> 02:34:55.700
of like a, like a backwards, um, and Kowski cone diagram where all of space

02:34:55.700 --> 02:34:59.860
is collapsed into one axis and then, and then here, and then time is this axis.

02:35:00.180 --> 02:35:02.420
And then what you can do is you can draw for any creature.

02:35:03.140 --> 02:35:07.740
You can, you can semi quantitatively estimate what are the, what are the

02:35:07.780 --> 02:35:12.900
spatial and temporal goals that it can, that it's capable of pursuing.

02:35:12.900 --> 02:35:18.620
So for example, if you are a tick and all you can, uh, all you really are able

02:35:18.620 --> 02:35:22.660
to pursue is maximum or, or a bacterium in the maximizing the level of some,

02:35:22.820 --> 02:35:24.420
some chemical in your vicinity, right?

02:35:24.420 --> 02:35:25.060
That's all you've got.

02:35:25.060 --> 02:35:27.740
It's a tiny little like on then, then you're a simple system,

02:35:27.740 --> 02:35:28.740
like a tick or a bacteria.

02:35:29.260 --> 02:35:36.060
If you are something like a dog, well, you've got some ability to, um, uh, to

02:35:36.060 --> 02:35:39.220
care about some, some spatial regions, some temporal, you know, you can, you

02:35:39.220 --> 02:35:40.540
can remember a little bit backwards.

02:35:40.540 --> 02:35:44.340
You can, you can predict a little bit forward, but you're never ever going

02:35:44.340 --> 02:35:47.340
to care about what happens in the next town over four weeks from now.

02:35:47.380 --> 02:35:50.740
It just, it's just as far as we know, it's just impossible for that kind of

02:35:50.740 --> 02:35:54.740
architecture, if you're a human, you might be working towards world

02:35:54.740 --> 02:35:56.260
peace long after you're dead, right?

02:35:56.260 --> 02:36:00.660
So you might have a planetary scale goal that's, that's enormous, right?

02:36:00.660 --> 02:36:03.500
And, and so, and, and then there may be, um, there may be other greater

02:36:03.540 --> 02:36:07.340
intelligence is somewhere that can care in the linear range about numbers of

02:36:07.340 --> 02:36:10.180
creatures that, you know, some sort of Buddha like a character that can like

02:36:10.180 --> 02:36:13.060
care about everybody's welfare, like really care the way that we can't.

02:36:13.580 --> 02:36:18.340
Um, and so, and so that it's, it's not a, it's not a mapping of what you can

02:36:18.340 --> 02:36:19.940
sense, how far you can sense, right?

02:36:19.940 --> 02:36:21.900
It's not a mapping of where, how far you can act.

02:36:22.100 --> 02:36:25.300
It's a mapping of how big are the goals you are capable of

02:36:25.300 --> 02:36:26.860
envisioning and working towards.

02:36:27.180 --> 02:36:32.900
And I think that enables you to put, um, uh, uh, synthetic, uh, kinds of

02:36:32.900 --> 02:36:38.860
constructs, AIs, aliens, um, swarms, whatever on the same diagram, because,

02:36:38.900 --> 02:36:41.060
because we're not talking about what you're made of or how you got here.

02:36:41.060 --> 02:36:44.740
We're talking about what are the, what are the, the, the, the size and complexity

02:36:44.740 --> 02:36:46.300
of the goals towards, which you can work.

02:36:47.260 --> 02:36:50.140
Is there any other terms that pop into mind that are interesting?

02:36:51.580 --> 02:36:52.300
Trying to remember this.

02:36:52.500 --> 02:36:53.900
I have a list of them somewhere on my website.

02:36:53.900 --> 02:36:54.900
Target morphology.

02:36:54.900 --> 02:36:55.100
Yeah.

02:36:55.740 --> 02:36:56.900
Definitely check it out.

02:36:56.900 --> 02:36:58.740
More of a, more of a pseudocall.

02:36:59.060 --> 02:36:59.900
I like that one.

02:37:00.340 --> 02:37:01.380
I on a pseudocall.

02:37:02.100 --> 02:37:02.460
Yeah.

02:37:02.500 --> 02:37:02.700
Yeah.

02:37:02.700 --> 02:37:06.100
I mean, those, those, those refer to different types of interventions

02:37:06.100 --> 02:37:07.460
in the regenerative medicine space.

02:37:07.660 --> 02:37:12.100
So more for pseudocall is something that, uh, it's a kind of, uh, intervention

02:37:12.100 --> 02:37:14.660
that really targets the cells.

02:37:15.460 --> 02:37:17.860
Decision-making process about what they're going to build.

02:37:18.060 --> 02:37:20.620
And I on a pseudocall is there like that, but more focused

02:37:20.620 --> 02:37:21.820
specifically on the bioelectrics.

02:37:21.820 --> 02:37:24.220
I mean, there's also, of course, biochemical biomechanical, who

02:37:24.220 --> 02:37:28.660
knows what else, you know, maybe optical kinds of signaling systems there as well.

02:37:28.860 --> 02:37:30.660
Target morphology is, is interesting.

02:37:30.660 --> 02:37:36.860
It really, uh, it's designed to, um, capture this idea that it's not

02:37:36.860 --> 02:37:40.340
just feed forward emergence and oftentimes in biology, I mean, of course

02:37:40.340 --> 02:37:44.340
that happens too, but, but in many cases in biology, the system is specifically

02:37:44.340 --> 02:37:48.100
working towards a target in anatomical morphous space, right?

02:37:48.100 --> 02:37:49.580
It's a, it's a navigation task.

02:37:50.180 --> 02:37:56.540
These kinds of problem solving, um, can be, um, uh, uh, uh, uh, uh, uh, formalized

02:37:56.540 --> 02:38:00.140
as navigation tasks and that they're really going towards a particular region.

02:38:00.300 --> 02:38:00.700
How do you know?

02:38:00.700 --> 02:38:02.380
Because you deviate them and then they go back.

02:38:03.580 --> 02:38:11.220
Let me ask you, because you've really challenged a lot of ideas in biology

02:38:11.260 --> 02:38:16.700
in the work you do probably because, uh, some of your rebelliousness comes

02:38:16.700 --> 02:38:20.300
from the fact that you came from a different field of computer engineering.

02:38:20.900 --> 02:38:25.340
Uh, but could you give advice to young people today in high school or college

02:38:25.340 --> 02:38:32.460
that are trying to pave their life story, whether it's in science or elsewhere,

02:38:32.460 --> 02:38:37.980
how they can have a career that can be proud of, or a life that can be proud of advice.

02:38:38.460 --> 02:38:40.860
Boy, it's dangerous to give advice because things change so fast.

02:38:40.860 --> 02:38:45.500
But, uh, one central thing I can say moving up, uh, and through academia

02:38:45.500 --> 02:38:48.780
and whatnot, you will be surrounded by really smart people.

02:38:49.220 --> 02:38:55.060
And what you need to do is be very careful at distinguishing specific

02:38:55.620 --> 02:38:59.220
critique versus kind of meta, um, meta advice.

02:38:59.220 --> 02:39:03.580
And what I mean by that is if, if somebody really smart and successful

02:39:03.900 --> 02:39:09.220
and, uh, obviously, uh, competent is giving you specific, um, critiques

02:39:09.220 --> 02:39:11.260
on what you've done, that's gold.

02:39:11.260 --> 02:39:14.380
That's an opportunity to hone your craft, to get better at what you're doing,

02:39:14.380 --> 02:39:15.980
to learn, um, to find your mistakes.

02:39:15.980 --> 02:39:16.580
Like that's great.

02:39:17.340 --> 02:39:22.500
If they are telling you what you ought to be studying, how you ought to approach

02:39:22.500 --> 02:39:26.380
things, what is the right way to think about things, you should

02:39:26.620 --> 02:39:28.100
probably ignore most of that.

02:39:28.460 --> 02:39:34.100
And the reason I make that distinction is that a lot of really, um, really

02:39:34.100 --> 02:39:38.860
successful people are very well calibrated on their own ideas and they,

02:39:38.940 --> 02:39:42.380
on, in their own field and their own, you know, sort of, uh, area, and they

02:39:42.380 --> 02:39:45.060
know exactly what works and what doesn't and what's good and what's bad,

02:39:45.460 --> 02:39:47.260
but they're not calibrated on your ideas.

02:39:47.540 --> 02:39:51.620
And so, so, uh, the things they will, they will say, oh, you know, this is

02:39:51.620 --> 02:39:53.780
a dumb idea, don't do this and you shouldn't do that.

02:39:54.100 --> 02:39:59.060
That stuff is generally, uh, worse than, worse than useless.

02:39:59.060 --> 02:40:03.740
It can be very, very, um, uh, demoralizing and, and, and, and really limiting.

02:40:03.860 --> 02:40:08.860
And so, so what I say to people is read very broadly, work really hard,

02:40:08.860 --> 02:40:09.820
know what you're talking about.

02:40:09.820 --> 02:40:14.260
Take all specific criticism as a, as a, um, as an opportunity to improve what

02:40:14.260 --> 02:40:18.540
you're doing and then completely ignore everything else, because I just tell

02:40:18.540 --> 02:40:23.220
you from like, from, from my own experience, um, most of what I consider

02:40:23.220 --> 02:40:27.100
to be interesting and useful things that we've done, very smart people have said,

02:40:27.140 --> 02:40:28.180
this is a terrible idea.

02:40:28.180 --> 02:40:29.340
Don't, don't, don't do that.

02:40:29.340 --> 02:40:32.660
Don't, you know, just, um, yeah, I think, I think we, we just don't know.

02:40:32.660 --> 02:40:36.500
We, we have no idea beyond, beyond our own, like at best we know what we ought

02:40:36.500 --> 02:40:36.980
to be doing.

02:40:36.980 --> 02:40:38.940
We very rarely know what anybody else should be doing.

02:40:39.420 --> 02:40:39.820
Yeah.

02:40:39.820 --> 02:40:44.900
And their ideas, their perspective has been also calibrated, not just on their

02:40:44.900 --> 02:40:51.060
field and specific situation, but also on a state of that field at a particular

02:40:51.060 --> 02:40:52.100
time in the past.

02:40:52.580 --> 02:40:57.580
So there's not many people in this world that are able to achieve revolutionary

02:40:57.580 --> 02:40:59.380
success multiple times in their life.

02:40:59.700 --> 02:41:04.740
So whenever you say somebody very smart, usually what that means is somebody who

02:41:04.740 --> 02:41:10.420
smart, who achieved a success at certain point in their life and people often get

02:41:10.420 --> 02:41:15.060
stuck in that place where they found success to be constantly challenging your

02:41:15.060 --> 02:41:16.700
worldview is a very difficult thing.

02:41:16.980 --> 02:41:19.540
Um, so yeah.

02:41:19.540 --> 02:41:25.980
And also at the same time, probably if a lot of people tell that's the weird thing

02:41:25.980 --> 02:41:31.300
about life, if a lot of people tell you that something is stupid or is not going

02:41:31.300 --> 02:41:34.500
to work, that either means it's stupid.

02:41:34.500 --> 02:41:40.060
It's not going to work or it's actually a great opportunity to do something new.

02:41:41.540 --> 02:41:43.300
And you don't know which one it is.

02:41:43.540 --> 02:41:45.700
And it's probably equally likely to be either.

02:41:45.700 --> 02:41:50.620
If not, well, I don't know the probabilities, um, depends how lucky you are.

02:41:50.620 --> 02:41:52.820
It depends how brilliant you are, but you don't know.

02:41:52.820 --> 02:41:55.540
And so you can't take that advice as actual data.

02:41:55.580 --> 02:41:58.780
Yeah, you have to, um, you have to, and this is, this is kind of hard and fuzzy.

02:41:59.020 --> 02:42:04.980
Hard to describe and fuzzy, but I'm a firm believer that you have to, uh,

02:42:05.020 --> 02:42:06.140
build up your own intuition.

02:42:06.460 --> 02:42:07.580
So over time, right?

02:42:07.580 --> 02:42:10.820
You have to take your own risks that seem like they make sense to you and then

02:42:10.820 --> 02:42:15.940
learn from that and build up so that you can trust your own gut about what's a

02:42:15.940 --> 02:42:19.060
good idea, even when, and then sometimes you'll make mistakes and they'll turn

02:42:19.060 --> 02:42:20.220
out to be a dead end and that's fine.

02:42:20.220 --> 02:42:20.980
That's, that's science.

02:42:20.980 --> 02:42:26.740
But, but, um, you know, what I tell my students is, is, uh, life is hard and

02:42:26.740 --> 02:42:29.340
science is, is, is hard and you're going to sweat and bleed and everything.

02:42:29.700 --> 02:42:34.820
And you should be doing that for, uh, ideas that, that, that really fire

02:42:34.820 --> 02:42:40.220
you up inside and, and, um, you know, and, and, and really don't let, uh, kind

02:42:40.220 --> 02:42:45.020
of the, uh, the, the common denominator of, of, um, standardized approaches

02:42:45.020 --> 02:42:46.380
to things, uh, slow you down.

02:42:47.300 --> 02:42:50.820
So you mentioned planaria being in some sense, immortal, uh, what's

02:42:50.820 --> 02:42:52.460
the role of death in life?

02:42:53.300 --> 02:42:55.660
What's the role of death in this whole process?

02:42:55.660 --> 02:43:01.620
We have, is it, uh, when you look at biological systems, is death an important

02:43:01.620 --> 02:43:08.420
feature, especially as you climb up the hierarchy of, uh, competency?

02:43:08.980 --> 02:43:10.260
Boy, that's an interesting question.

02:43:10.300 --> 02:43:19.060
Um, I think that, uh, it's certainly a factor that promotes change and turnover

02:43:19.060 --> 02:43:25.060
and, uh, an opportunity to do something different the next time, um, for a larger

02:43:25.060 --> 02:43:28.260
scale system, so apoptosis, you know, it's re it's really interesting.

02:43:28.260 --> 02:43:30.300
I mean, death is really interesting in a number of ways.

02:43:30.300 --> 02:43:33.700
One is like, you can think about like, what was the first thing to die?

02:43:34.300 --> 02:43:36.060
You know, that's, that's an interesting question.

02:43:36.060 --> 02:43:38.100
What was the first creature that you could say actually died?

02:43:38.780 --> 02:43:42.820
It's a tough, it's a tough thing because we don't have a great definition for it.

02:43:42.820 --> 02:43:48.260
So if you bring, um, a cabbage home and you put it in your fridge, at what point

02:43:48.260 --> 02:43:50.140
are you going to say it's died, right?

02:43:50.140 --> 02:43:55.020
Then so, so that's, it's kind of hard, um, to know there's also, there's

02:43:55.020 --> 02:43:59.540
also, uh, there's, there's, there's one paper in which I talk about this idea

02:43:59.540 --> 02:44:03.860
that, I mean, think about, think about this and imagine that, uh, you have,

02:44:03.900 --> 02:44:06.180
you have a creature, uh, that's aquatic.

02:44:06.180 --> 02:44:10.380
Let's say, let's say it's a, it's a frog or something or a tadpole and the

02:44:10.380 --> 02:44:12.260
animal dies in the, in the pond.

02:44:12.260 --> 02:44:16.940
It dies for whatever reason, most of the cells are still alive.

02:44:17.260 --> 02:44:20.220
So you could imagine that if, when it died, there was some sort of, um,

02:44:20.220 --> 02:44:24.020
breakdown of, of, of, of the, of the connectivity between the cells, a bunch

02:44:24.020 --> 02:44:28.780
of cells crawled off, they could have a life as amoebas, they, some of them

02:44:28.780 --> 02:44:32.420
could join together and become a xenobot and tootle around, right?

02:44:32.620 --> 02:44:35.300
So we know from planaria that there are cells that don't obey the hay

02:44:35.300 --> 02:44:37.300
flick limit and just sort of live, live forever.

02:44:37.300 --> 02:44:40.900
So you could imagine an organism that when the organism dies, it doesn't

02:44:40.900 --> 02:44:44.700
disappear rather the individual cells that are still alive, crawl off and

02:44:44.700 --> 02:44:47.460
have a completely different kind of lifestyle and maybe come back together

02:44:47.460 --> 02:44:48.700
as something else, or maybe they don't.

02:44:49.100 --> 02:44:52.460
So, so all of this I'm sure is happening somewhere on some, on some, on some

02:44:52.460 --> 02:44:57.460
planet. So, so, um, death in any case, I mean, we already kind of knew this

02:44:57.460 --> 02:45:00.540
because the molecules we can, we know that when something dies, the molecules

02:45:00.540 --> 02:45:04.900
go through the ecosystem, but even the cells don't necessarily die at that

02:45:04.900 --> 02:45:07.900
point, they might have another life in a, in a different, uh, in a different way.

02:45:08.060 --> 02:45:09.700
And you can think about something like Gila, right?

02:45:09.700 --> 02:45:13.820
The Gila cell line, you know, that has this, that's had this incredible life.

02:45:13.860 --> 02:45:16.980
Uh, there are way more Gila cells now than there ever been, than there,

02:45:17.020 --> 02:45:18.580
than there were when, when she was alive.

02:45:18.620 --> 02:45:21.940
It seems like as the organism become more and more complex, like if you

02:45:21.940 --> 02:45:26.460
look at the mammals, their relationship with death becomes more and more complex.

02:45:27.180 --> 02:45:33.300
So the survival imperative starts becoming interesting and humans are

02:45:33.300 --> 02:45:39.940
arguably the first species that have invented the fear of death, the

02:45:40.020 --> 02:45:41.620
understanding that you're going to die.

02:45:41.660 --> 02:45:48.180
Let's put it this way, like a long, so not like instinctual, like I need to run

02:45:48.180 --> 02:45:51.540
away from the thing that's going to eat me, but starting to

02:45:51.540 --> 02:45:53.740
contemplate the finiteness of life.

02:45:53.780 --> 02:45:54.060
Yeah.

02:45:54.580 --> 02:45:58.220
I mean, one thing, so, so one thing about the human, um, light cognitive

02:45:58.220 --> 02:46:03.220
light cone is that for the first, as far as we know, for the first time, you

02:46:03.220 --> 02:46:06.380
might have goals that are longer than your life span that are not achievable.

02:46:06.420 --> 02:46:06.780
Right.

02:46:06.820 --> 02:46:09.540
So if you're, if you were, let's say, and I don't know if this is true, but

02:46:09.540 --> 02:46:13.260
if, if you're a goldfish and you have a 10 minute attention span, I'm not sure

02:46:13.260 --> 02:46:15.740
if that's true, but let's say, let's say there's some organism with a, with a

02:46:15.740 --> 02:46:18.100
short, um, you know, kind of cognitive light cone that way.

02:46:19.060 --> 02:46:22.100
All of your goals are potentially achievable because you're probably

02:46:22.100 --> 02:46:23.220
going to live the next 10 minutes.

02:46:23.220 --> 02:46:25.420
So whatever goals you have, they are totally achievable.

02:46:25.660 --> 02:46:28.860
If you're a human, you could have all kinds of goals that are guaranteed

02:46:28.860 --> 02:46:31.300
not achievable because they just take too long, like guaranteed.

02:46:31.300 --> 02:46:32.060
You're not going to achieve them.

02:46:32.340 --> 02:46:36.500
So I wonder if, you know, is that, is that a per, you know, like a perennial,

02:46:36.500 --> 02:46:39.780
um, you know, sort of thorn in our, in our psychology that drives some,

02:46:39.780 --> 02:46:40.980
some psychoses or whatever.

02:46:40.980 --> 02:46:41.780
I've, I have no idea.

02:46:42.180 --> 02:46:45.260
Another interesting thing about that, actually, I've been thinking about

02:46:45.260 --> 02:46:48.500
this a lot in the last couple of weeks, this notion of giving up.

02:46:48.900 --> 02:46:57.140
So you would think that evolutionarily, the most, um, adaptive way of being

02:46:57.140 --> 02:47:02.140
is that you go, you, you, you, you fight as long as you physically can.

02:47:02.300 --> 02:47:03.260
And then when you can't, you can't.

02:47:03.260 --> 02:47:06.180
And there's in, there's this photograph, there's, um, videos you can find of

02:47:06.180 --> 02:47:09.260
insects are crawling around where like, you know, like, like most of it has

02:47:09.260 --> 02:47:12.860
already gone and it's still sort of crawling, you know, like, um, um, uh,

02:47:12.860 --> 02:47:14.060
Terminator style, right?

02:47:14.060 --> 02:47:16.420
Like as far as, as long as you physically can, you keep going.

02:47:17.340 --> 02:47:18.500
Mammals don't do that.

02:47:18.540 --> 02:47:22.140
So, so a lot of mammals, including rats have this thing where when, when they

02:47:22.140 --> 02:47:26.780
think it's, it's a hopeless situation, they literally give up and die when

02:47:26.780 --> 02:47:28.300
physically they could have kept going.

02:47:28.300 --> 02:47:29.460
I mean, humans certainly do this.

02:47:29.740 --> 02:47:32.940
And there's, there's some like really unpleasant experiments that the, this

02:47:32.940 --> 02:47:36.620
guy, if you get his name did with, um, drowning rats, where if he, where, where

02:47:36.620 --> 02:47:39.940
rats normally drown after a couple of minutes, but if you teach them that, if

02:47:39.940 --> 02:47:42.700
you just tread water for a couple of minutes, you'll get rescued, they can

02:47:42.700 --> 02:47:43.740
tread water for like an hour.

02:47:44.060 --> 02:47:44.740
And so, right.

02:47:44.740 --> 02:47:46.140
And so they literally just give up and die.

02:47:46.380 --> 02:47:50.260
And so evolutionarily, that doesn't seem like a good strategy at all.

02:47:50.260 --> 02:47:53.260
Evolutionarily, why would you like, what's the benefit ever of giving up?

02:47:53.260 --> 02:47:54.260
You just do what you can.

02:47:54.260 --> 02:47:56.540
And, you know, one time out of a thousand, you'll actually get rescued, right?

02:47:57.220 --> 02:48:01.340
But this issue of, of, of, of actually giving up suggests some very interesting

02:48:01.340 --> 02:48:05.980
metacognitive controls where you've now gotten to the point where survival

02:48:05.980 --> 02:48:09.700
actually isn't the top drive and that for whatever, you know, there are other

02:48:09.700 --> 02:48:11.500
considerations that have like taken over.

02:48:11.500 --> 02:48:14.740
And I, I think that's uniquely a mammalian thing, but I don't know.

02:48:15.380 --> 02:48:15.820
Yeah.

02:48:15.900 --> 02:48:22.900
The Camus, the existentialist question of why live just the fact that humans

02:48:22.900 --> 02:48:27.740
commit suicide is a really fascinating question from an evolutionary perspective.

02:48:27.780 --> 02:48:30.460
And what was the first, and that's the other thing, like what, what, what is

02:48:30.460 --> 02:48:36.340
the simplest, uh, system, whether, whether evolved or natural or whatever,

02:48:36.460 --> 02:48:38.340
that is able to do that, right?

02:48:38.500 --> 02:48:41.260
Like you can think, you know, what other animals are actually able to do that?

02:48:41.260 --> 02:48:41.900
I'm not sure.

02:48:42.140 --> 02:48:49.060
Maybe you could see animals over time for some reason, lowering the value of

02:48:49.300 --> 02:48:55.580
survive at all costs gradually until other objectives might become more important.

02:48:55.660 --> 02:48:59.060
Maybe, I don't know how evolutionarily, how that, how that gets off the ground.

02:48:59.060 --> 02:49:02.340
That just seems like that would have such a strong pressure against it.

02:49:02.500 --> 02:49:08.660
You know, just imagine, you know, a population with, with, with, with a

02:49:08.660 --> 02:49:12.580
lower, um, you know, with, with, if, if you were a mutant in a population that

02:49:12.580 --> 02:49:16.980
had less of a, uh, uh, less of a survival imperative, would you, would

02:49:16.980 --> 02:49:19.100
your genes outperform the others?

02:49:19.100 --> 02:49:19.700
It seems not.

02:49:19.700 --> 02:49:21.860
Is there such a thing as population selection?

02:49:21.900 --> 02:49:28.660
Because maybe suicide is a way, uh, for organisms to decide themselves that

02:49:28.660 --> 02:49:31.820
they're not fit for the environment somehow.

02:49:31.900 --> 02:49:35.540
Yeah, that's a, that's a really, uh, contrary, you know, population level

02:49:35.540 --> 02:49:38.420
selection is a, is a kind of a deep controversial area.

02:49:38.420 --> 02:49:43.940
But it's tough because on the face of it, if that was your genome, it wouldn't

02:49:43.940 --> 02:49:45.860
get propagated because you would die.

02:49:45.860 --> 02:49:48.500
And then your neighbor who didn't have that would, would have all the kids.

02:49:48.980 --> 02:49:52.820
It feels like there could be some deep truth there that we're not understanding.

02:49:53.580 --> 02:49:57.180
Um, what about you yourself as one biological system?

02:49:57.180 --> 02:49:58.100
Are you afraid of death?

02:49:59.220 --> 02:50:04.660
To be honest, I'm more concerned with, uh, especially now getting older and

02:50:04.660 --> 02:50:07.060
having helped a couple of people pass.

02:50:07.460 --> 02:50:14.180
I think about what's a, um, what's a good way to go basically.

02:50:14.180 --> 02:50:15.700
Like nowadays, I don't know what that is.

02:50:15.700 --> 02:50:20.780
I, you know, sitting in a, you know, a facility that sort of tries to stretch

02:50:20.780 --> 02:50:24.380
you out as long as you can, that doesn't seem, that doesn't seem good.

02:50:24.380 --> 02:50:28.180
And there's not a lot of opportunities to sort of, um, I don't know, sacrifice

02:50:28.180 --> 02:50:29.660
yourself for something useful, right?

02:50:29.660 --> 02:50:32.620
There's not terribly many opportunities for that in modern society.

02:50:32.740 --> 02:50:33.140
So I don't know.

02:50:33.180 --> 02:50:36.420
I, that's, that's, that's more of, I'm not, I'm not particularly worried

02:50:36.420 --> 02:50:43.540
about, uh, death itself, but, uh, I've, I've seen it happen, uh, and, and, uh,

02:50:43.580 --> 02:50:47.340
it's not, it's not pretty and I don't know what, what a better, what a better

02:50:47.340 --> 02:50:53.980
alternative is to the existential aspect of it does not worry you deeply.

02:50:54.260 --> 02:50:55.860
The fact that this ride ends.

02:50:56.860 --> 02:50:59.420
No, it began, I mean, the ride began, right?

02:50:59.420 --> 02:51:02.700
So there was, I don't know how many billions of years before that I

02:51:02.700 --> 02:51:04.300
wasn't around, so that's okay.

02:51:04.580 --> 02:51:06.340
But isn't the experience of life.

02:51:07.380 --> 02:51:11.100
It's almost like feels like you're immortal because of the way you make

02:51:11.100 --> 02:51:13.500
plans, the way you think about the future.

02:51:13.900 --> 02:51:18.820
I mean, if, if you re if you look at your own personal rich experience,

02:51:19.580 --> 02:51:24.500
yes, you can understand, okay, eventually I died as people I love that have died.

02:51:24.940 --> 02:51:30.700
So surely I will die and it hurts and so on, but like, it sure doesn't, it's so

02:51:30.700 --> 02:51:34.020
easy to get lost in feeling like this is going to go on forever.

02:51:34.100 --> 02:51:34.420
Yeah.

02:51:34.580 --> 02:51:37.140
It's a little bit like the people who say they don't believe in free will.

02:51:37.140 --> 02:51:37.340
Right.

02:51:37.340 --> 02:51:41.220
I mean, you can say that, but, but when you go to a restaurant, you still

02:51:41.220 --> 02:51:42.580
have to pick a soup and stuff.

02:51:42.580 --> 02:51:42.940
So, right.

02:51:42.940 --> 02:51:46.460
So, so I don't know if I've, I've actually seen that, uh, that happened

02:51:46.460 --> 02:51:49.740
at lunch with a, with a well-known philosopher and, uh, he didn't believe

02:51:49.740 --> 02:51:53.060
in free will and you know, the waitress came around and he was like, well, let

02:51:53.060 --> 02:51:54.260
me see, I was like, what are you doing here?

02:51:54.300 --> 02:51:56.100
You're going to choose a sandwich.

02:51:56.100 --> 02:51:56.340
Right.

02:51:56.780 --> 02:51:58.700
So, um, it's, I think it's one of those things.

02:51:58.700 --> 02:52:02.260
I think you can know that, you know, you're not going to live forever, but

02:52:02.820 --> 02:52:06.940
you can't, you can't, it's not practical to live that way unless, you know, so

02:52:06.940 --> 02:52:10.300
you buy insurance and then you do some stuff like that, but, but, but mostly,

02:52:10.300 --> 02:52:14.180
you know, um, I think you just, you just live as if, uh, as if, as

02:52:14.180 --> 02:52:15.540
if you can make plans.

02:52:17.380 --> 02:52:19.420
We talked about all kinds of life.

02:52:19.500 --> 02:52:22.060
We'll talk about all kinds of embodied minds.

02:52:22.420 --> 02:52:23.900
What do you think is the meaning of it all?

02:52:24.860 --> 02:52:28.220
What's the meaning of all the biological eyes we've been

02:52:28.220 --> 02:52:29.820
talking about here on earth?

02:52:30.220 --> 02:52:31.460
Why are we here?

02:52:33.340 --> 02:52:37.540
I don't know that that's a, that that's a well-posed question other than

02:52:37.580 --> 02:52:40.420
the existential question you posed before.

02:52:40.780 --> 02:52:44.180
Is that question hanging out with the question of what is consciousness

02:52:44.260 --> 02:52:47.660
and they're at a retreat somewhere?

02:52:48.260 --> 02:52:48.900
Not sure.

02:52:49.260 --> 02:52:54.420
Sipping Pina coladas and because they're ambiguously defined.

02:52:54.780 --> 02:53:01.020
Maybe I'm not sure that any of these things really ride on the correctness

02:53:01.020 --> 02:53:03.940
of our scientific understanding, but I mean, just, just for an example, right.

02:53:04.260 --> 02:53:11.580
Um, I've always found, I've always found it weird that, uh, people get really

02:53:11.580 --> 02:53:17.580
worked up, uh, to find out realities about their, their bodies for, for,

02:53:18.140 --> 02:53:18.820
for example, right.

02:53:18.820 --> 02:53:21.900
That you've seen them, uh, ex machina seen that, right.

02:53:21.900 --> 02:53:24.180
And so, so there's this great scene where he's cutting his hand to find

02:53:24.180 --> 02:53:25.500
out, you know, if he's full of cogs.

02:53:25.700 --> 02:53:26.940
Now to me, right.

02:53:27.580 --> 02:53:31.980
If, if, if I open up and I find out, you know, find a bunch of cogs, my conclusion

02:53:31.980 --> 02:53:34.580
is not, oh crap, I must not have true cognition.

02:53:34.580 --> 02:53:35.140
That sucks.

02:53:35.340 --> 02:53:38.180
My conclusion is, wow, cogs can have true cognition.

02:53:38.180 --> 02:53:38.540
Great.

02:53:38.860 --> 02:53:39.780
So, right.

02:53:39.780 --> 02:53:43.860
So, so it seems to me, I guess, I guess I'm with Descartes on this one, that

02:53:44.060 --> 02:53:48.380
whatever, whatever the truth, uh, ends up being of, of, of how is, what is

02:53:48.380 --> 02:53:49.820
consciousness, how it can be conscious.

02:53:50.060 --> 02:53:54.060
None of that is going to alter, um, my primary experience, which is this is what

02:53:54.060 --> 02:53:57.860
it is, and if, and if a bunch of molecular networks can do it, fantastic.

02:53:57.940 --> 02:54:03.180
If it turns out that, um, there's a, there's a non-corporeal, you know, so great.

02:54:03.180 --> 02:54:04.740
We can, you know, we'll study that, whatever.

02:54:04.980 --> 02:54:09.620
But, but the fundamental, um, existential aspect of it is, you know, if somebody,

02:54:09.620 --> 02:54:13.420
if somebody told me today that, uh, yeah, yeah, you were created yesterday

02:54:13.420 --> 02:54:15.980
and all your memories are, you know, sort of, uh, fake, you know, kind of

02:54:15.980 --> 02:54:18.580
like, um, uh, like, like Boltzmann brains, right.

02:54:18.580 --> 02:54:22.180
And the Hume, you know, Hume skepticism, all that, uh, yeah.

02:54:22.220 --> 02:54:22.540
Okay.

02:54:22.740 --> 02:54:24.300
Well, so, so, but, but here I am now.

02:54:24.300 --> 02:54:25.100
So, so let's

02:54:25.100 --> 02:54:28.580
the experience is primal.

02:54:28.580 --> 02:54:32.260
So like, that's the, that's the thing that matters.

02:54:32.300 --> 02:54:34.420
So the, the backstory doesn't matter.

02:54:34.580 --> 02:54:37.700
I think so, I think so from a first person perspective, now from a third

02:54:37.700 --> 02:54:40.300
person, like scientifically, it's all very interesting from a third person

02:54:40.300 --> 02:54:43.820
perspective, I could say, wow, that's, uh, that's amazing that, uh, that this

02:54:43.820 --> 02:54:46.620
happens and how does it happen and whatever, but from a first person

02:54:46.620 --> 02:54:48.540
perspective, I could care less.

02:54:48.780 --> 02:54:51.620
Like I just, it just, well, what I've, what I learned from any of these

02:54:51.620 --> 02:54:55.700
scientific facts is, okay, well, I guess then that's that, that, then I guess

02:54:55.700 --> 02:54:59.380
that's what is sufficient to, to, to give me my, uh, you know, amazing

02:54:59.380 --> 02:55:00.500
first person perspective.

02:55:00.740 --> 02:55:05.460
Well, I think if you dig deeper and deeper and get, uh, get surprising

02:55:05.500 --> 02:55:13.300
answers to why the hell we're here, it might give you some guidance on how to live.

02:55:14.580 --> 02:55:16.060
Maybe, maybe, I don't know.

02:55:16.100 --> 02:55:19.140
Um, that would be nice on the one hand.

02:55:19.700 --> 02:55:22.820
You might be right because on the one hand, if I don't know what else could

02:55:22.820 --> 02:55:24.300
possibly give you that guidance, right?

02:55:24.300 --> 02:55:26.780
So, so you would think that it would have to be that, or you would do, it

02:55:26.780 --> 02:55:28.660
would have to be science because there isn't anything else.

02:55:28.980 --> 02:55:35.100
So, so that's the, so maybe on the other hand, I am really not sure how you go

02:55:35.100 --> 02:55:38.340
from any, you know, what they call from an is to an odd, right?

02:55:38.340 --> 02:55:40.460
From any factual description of what's going on.

02:55:40.620 --> 02:55:42.780
This, this goes back to the natural, right?

02:55:42.780 --> 02:55:45.260
Just because somebody says, oh man, that's, that's completely not natural.

02:55:45.260 --> 02:55:46.540
That's never happened on earth before.

02:55:46.540 --> 02:55:49.380
I, I'm not impressed by that whatsoever.

02:55:49.620 --> 02:55:54.620
I think, I think whatever it has or hasn't happened, we are now in a position

02:55:54.620 --> 02:55:56.580
to do better if we can, right?

02:55:56.940 --> 02:55:58.780
Well, that's also good.

02:55:58.780 --> 02:56:03.260
Cause you said there's science and there's nothing else there.

02:56:03.260 --> 02:56:04.940
It's, it's really tricky.

02:56:06.100 --> 02:56:11.180
To know how to intellectually deal with the thing that science

02:56:11.180 --> 02:56:14.100
doesn't currently understand, right?

02:56:14.100 --> 02:56:23.420
So like the thing is, if you believe that science solves everything, you can too

02:56:23.420 --> 02:56:30.500
easily in your mind, think our current understanding, like we've solved everything.

02:56:30.540 --> 02:56:31.180
Right, right.

02:56:32.140 --> 02:56:37.740
Like it jumps really quickly to not science as a mechanism, as, as a, as a

02:56:37.740 --> 02:56:41.100
process, but more like the size of today.

02:56:41.260 --> 02:56:45.780
Like you can just look at human history and throughout human history, just

02:56:45.820 --> 02:56:49.500
physicists and everybody would claim we've solved everything.

02:56:49.500 --> 02:56:49.780
Sure.

02:56:50.180 --> 02:56:53.980
Like, like there's a few small things to figure out and it's, we basically

02:56:53.980 --> 02:56:58.540
solved everything, uh, where in reality, I think asking like, what is the

02:56:58.540 --> 02:57:06.660
meaning of life is, uh, resetting the palette of like, we might be tiny

02:57:06.660 --> 02:57:10.820
and confused and don't have anything figured out, it's almost going to be

02:57:10.820 --> 02:57:15.540
hilarious a few centuries from now when they look back at how dumb we were.

02:57:15.860 --> 02:57:17.340
Yeah, I 100% agree.

02:57:17.340 --> 02:57:22.940
So, so when I say, uh, science and nothing else, I certainly don't mean

02:57:22.940 --> 02:57:27.900
the science of today because I think overall, I think we are, we know

02:57:27.900 --> 02:57:31.700
very little, I think most of the things that we're sure of now are going

02:57:31.700 --> 02:57:34.020
to be, as you said, are going to look hilarious down the line.

02:57:34.420 --> 02:57:37.540
Um, so I think we're just at the beginning of a lot of really important

02:57:37.540 --> 02:57:43.220
things when I say nothing but science, I also include the kind of first

02:57:43.220 --> 02:57:45.940
person, what I call science that you do.

02:57:46.060 --> 02:57:49.340
So the, so the interesting thing about, um, I think about consciousness and

02:57:49.340 --> 02:57:52.780
studying consciousness and things like that in the first person is unlike

02:57:52.820 --> 02:57:57.260
doing science in the third person where you, as the scientist are minimally

02:57:57.260 --> 02:57:58.780
changed by it, maybe not at all.

02:57:58.780 --> 02:58:01.180
So when I do an experiment, I'm still me, there's the experiment,

02:58:01.180 --> 02:58:03.220
whatever I've done, I've learned some things, that's a small change,

02:58:03.220 --> 02:58:04.260
but, but overall that's it.

02:58:04.780 --> 02:58:09.900
In order to really study consciousness, you will, you are part of the

02:58:09.900 --> 02:58:12.420
experiment, you will be altered by that experiment, right?

02:58:12.420 --> 02:58:14.540
And whatever, whatever it is that you're doing, whether it's, you know,

02:58:14.540 --> 02:58:18.380
some sort of contemplative practice or, uh, or, or some sort of, you

02:58:18.380 --> 02:58:20.700
know, psychoactive, you know, whatever.

02:58:21.100 --> 02:58:24.620
Uh, you are now, you are now your own experiment and you are right.

02:58:24.620 --> 02:58:26.420
And so, so I can sit, I, I fold that in.

02:58:26.420 --> 02:58:27.580
I think that's, that's part of it.

02:58:27.820 --> 02:58:31.340
I think that exploring our own mind and our own consciousness is very important.

02:58:31.340 --> 02:58:36.100
I think much of it is not captured by what currently is third person science for

02:58:36.100 --> 02:58:41.900
sure, but ultimately I include all of that in science with a capital S in

02:58:41.900 --> 02:58:47.180
terms of like a, um, uh, a rational investigation of both first and third

02:58:47.180 --> 02:58:49.300
person aspects of our world.

02:58:50.060 --> 02:58:53.700
We are our own experiment as beautifully put.

02:58:54.340 --> 02:58:58.700
And, uh, when, when two systems get to interact with each other,

02:58:59.020 --> 02:59:00.220
that's the kind of experiment.

02:59:00.220 --> 02:59:04.940
So, um, deeply honored that you would, uh, do this experiment with me today.

02:59:04.940 --> 02:59:05.540
Oh, thanks so much.

02:59:05.540 --> 02:59:05.860
Thanks for having me.

02:59:05.860 --> 02:59:07.180
I'm a huge fan of your work.

02:59:07.220 --> 02:59:07.620
Likewise.

02:59:07.620 --> 02:59:09.220
Thank you for doing everything you're doing.

02:59:09.580 --> 02:59:14.260
Um, I can't wait to see the kind of incredible things you build.

02:59:14.420 --> 02:59:15.540
So thank you for talking to me.

02:59:15.580 --> 02:59:16.620
Really appreciate being here.

02:59:16.660 --> 02:59:16.980
Thank you.

02:59:18.020 --> 02:59:21.260
Thank you for listening to this conversation with Michael Levin to support

02:59:21.300 --> 02:59:22.060
this podcast.

02:59:22.100 --> 02:59:24.260
Please check out our sponsors in the description.

02:59:24.860 --> 02:59:29.180
And now let me leave you with some words from Charles Darwin in the origin of

02:59:29.180 --> 02:59:37.380
species from the war of nature, from famine and death, the most exalted object,

02:59:37.420 --> 02:59:42.020
which were capable of conceiving, namely the production of the higher animals

02:59:42.340 --> 02:59:48.980
directly follows there's grandeur in this view of life with several powers having

02:59:48.980 --> 02:59:53.060
been originally breathed into a few forms or into one.

02:59:53.620 --> 02:59:58.220
And that whilst this planet has gone cycling on according to the fixed laws

02:59:58.220 --> 03:00:04.300
of gravity from so simple a beginning, endless forms, most beautiful and most

03:00:04.300 --> 03:00:08.140
wonderful have been and are being evolved.

03:00:09.460 --> 03:00:10.340
Thank you for listening.

03:00:10.820 --> 03:00:12.460
I hope to see you next time.


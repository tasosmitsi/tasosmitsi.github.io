WEBVTT

00:00.000 --> 00:03.840
What possible ideas do you have for how human species ends?

00:03.840 --> 00:08.120
Sure, so I think the most obvious way to me is wireheading.

00:08.120 --> 00:09.920
We end up amusing ourselves to death.

00:11.040 --> 00:14.400
We end up all staring at that infinite TikTok

00:14.400 --> 00:15.700
and forgetting to eat.

00:17.360 --> 00:19.240
Maybe it's even more benign than this.

00:19.240 --> 00:21.340
Maybe we all just stop reproducing.

00:22.880 --> 00:27.320
Now, to be fair, it's probably hard to get all of humanity.

00:27.320 --> 00:29.520
Yeah, the interesting thing about humanity

00:29.520 --> 00:32.600
is the diversity in it, organisms in general.

00:32.600 --> 00:34.800
There's a lot of weirdos out there.

00:34.800 --> 00:35.920
Two of them are sitting here.

00:35.920 --> 00:38.320
Yeah, I mean, diversity in humanity is-

00:38.320 --> 00:39.160
We do respect.

00:40.880 --> 00:42.140
I wish I was more weird.

00:44.560 --> 00:47.480
The following is a conversation with George Hotz,

00:47.480 --> 00:49.440
his third time on this podcast.

00:49.440 --> 00:51.500
He's the founder of Comma AI

00:51.500 --> 00:53.480
that seeks to solve autonomous driving

00:53.480 --> 00:57.800
and is the founder of a new company called TinyCorp

00:57.840 --> 01:00.000
that created TinyGrad,

01:00.000 --> 01:02.960
a neural network framework that is extremely simple

01:02.960 --> 01:05.900
with the goal of making it run on any device

01:05.900 --> 01:09.880
by any human easily and efficiently.

01:09.880 --> 01:12.680
As you know, George also did a large number of fun

01:12.680 --> 01:15.280
and amazing things from hacking the iPhone

01:15.280 --> 01:17.800
to recently joining Twitter for a bit

01:17.800 --> 01:20.480
as an intern in quotes,

01:20.480 --> 01:23.720
making the case for refactoring the Twitter code base.

01:23.720 --> 01:26.840
In general, he's a fascinating engineer and human being

01:26.880 --> 01:29.640
and one of my favorite people to talk to.

01:29.640 --> 01:31.720
This is the Lex Friedman Podcast.

01:31.720 --> 01:33.560
To support it, please check out our sponsors

01:33.560 --> 01:34.800
in the description.

01:34.800 --> 01:38.240
And now, dear friends, here's George Hotz.

01:39.200 --> 01:41.440
You mentioned something on a stream

01:41.440 --> 01:43.720
about the philosophical nature of time.

01:43.720 --> 01:46.000
So let's start with a wild question.

01:46.000 --> 01:47.760
Do you think time is an illusion?

01:50.080 --> 01:55.080
You know, I sell phone calls to Comma for $1,000

01:55.440 --> 01:58.840
and some guy called me and like, you know,

01:58.840 --> 02:01.280
it's $1,000, you can talk to me for half an hour.

02:01.280 --> 02:03.560
And he's like, yeah, okay.

02:03.560 --> 02:06.040
So like time doesn't exist

02:06.040 --> 02:08.640
and I really wanted to share this with you.

02:08.640 --> 02:11.080
I'm like, oh, what do you mean time doesn't exist, right?

02:11.080 --> 02:13.520
Like I think time is a useful model,

02:13.520 --> 02:15.280
whether it exists or not, right?

02:15.280 --> 02:17.000
Like does quantum physics exist?

02:17.000 --> 02:18.280
Well, it doesn't matter.

02:18.280 --> 02:22.120
It's about whether it's a useful model to describe reality.

02:22.120 --> 02:24.600
Is time maybe compressive?

02:25.240 --> 02:27.080
Do you think there is an objective reality

02:27.080 --> 02:29.080
or is everything just useful models?

02:30.920 --> 02:34.040
Like underneath it all, is there an actual thing

02:34.040 --> 02:35.800
that we're constructing models for?

02:37.800 --> 02:39.440
I don't know.

02:39.440 --> 02:40.560
I was hoping you would know.

02:40.560 --> 02:42.120
I don't think it matters.

02:42.120 --> 02:44.480
I mean, this kind of connects to the models

02:44.480 --> 02:47.800
of constructive reality with machine learning, right?

02:47.800 --> 02:48.640
Sure.

02:48.640 --> 02:52.400
Like, is it just nice to have useful approximations

02:52.400 --> 02:55.000
of the world such that we can do something with it?

02:56.000 --> 02:57.840
So there are things that are real.

02:57.840 --> 02:59.720
Columnograph complexity is real.

02:59.720 --> 03:00.560
Yeah.

03:00.560 --> 03:01.400
Yeah.

03:01.400 --> 03:02.240
The compressive thing.

03:02.240 --> 03:03.080
Math.

03:03.080 --> 03:03.920
Math is real.

03:03.920 --> 03:04.760
Yeah.

03:04.760 --> 03:06.160
It should be a t-shirt.

03:06.160 --> 03:07.960
And I think hard things are actually hard.

03:07.960 --> 03:09.920
I don't think P equals NP.

03:09.920 --> 03:11.360
Ooh, strong words.

03:11.360 --> 03:12.720
Well, I think that's the majority.

03:12.720 --> 03:15.240
I do think factoring is in P, but.

03:15.240 --> 03:17.440
I don't think you're the person that follows the majority

03:17.440 --> 03:19.040
in all walks of life, so but it's good.

03:19.040 --> 03:19.880
For that one, I do.

03:19.880 --> 03:20.720
Yeah.

03:20.720 --> 03:23.400
In theoretical computer science, you're one of the sheep.

03:23.960 --> 03:24.920
All right.

03:24.920 --> 03:28.040
But to you, time is a useful model.

03:28.040 --> 03:28.880
Sure.

03:29.800 --> 03:32.200
What were you talking about on the stream with time?

03:32.200 --> 03:33.320
Are you made of time?

03:33.320 --> 03:36.320
If I remembered half the things I said on stream,

03:36.320 --> 03:38.560
someday someone's gonna make a model of all of that

03:38.560 --> 03:40.080
and it's gonna come back to haunt me.

03:40.080 --> 03:41.120
Someday soon?

03:41.120 --> 03:41.960
Yeah, probably.

03:42.920 --> 03:45.360
Would that be exciting to you or sad

03:45.360 --> 03:48.200
that there's a George Hotz model?

03:48.200 --> 03:50.600
I mean, the question is when the George Hotz model

03:50.600 --> 03:52.360
is better than George Hotz.

03:52.360 --> 03:54.880
Like I am declining and the model is growing.

03:54.880 --> 03:57.320
What is the metric by which you measure better or worse

03:57.320 --> 04:00.040
in that if you're competing with yourself?

04:00.040 --> 04:02.240
Maybe you can just play a game

04:02.240 --> 04:03.640
where you have the George Hotz answer

04:03.640 --> 04:04.760
and the George Hotz model answer

04:04.760 --> 04:06.720
and ask which people prefer.

04:06.720 --> 04:09.400
People close to you or strangers?

04:09.400 --> 04:10.240
Either one.

04:10.240 --> 04:11.680
It will hurt more when it's people close to me,

04:11.680 --> 04:15.240
but both will be overtaken by the George Hotz model.

04:16.720 --> 04:18.080
It'd be quite painful, right?

04:18.080 --> 04:21.600
Loved ones, family members would rather have the model

04:21.600 --> 04:23.440
overt for Thanksgiving than you.

04:24.800 --> 04:28.600
Or like significant others would rather sext

04:31.560 --> 04:35.480
with the large language model version of you.

04:35.480 --> 04:38.280
Especially when it's fine-tuned to their preferences.

04:39.360 --> 04:42.520
Yeah, well, that's what we're doing in a relationship, right?

04:42.520 --> 04:43.680
We're just fine-tuning ourselves,

04:43.680 --> 04:45.120
but we're inefficient with it

04:45.120 --> 04:47.280
because we're selfish and greedy and so on.

04:47.280 --> 04:50.640
Our language models can fine-tune more efficiently,

04:50.680 --> 04:51.800
more selflessly.

04:51.800 --> 04:53.480
There's a Star Trek Voyager episode

04:53.480 --> 04:57.120
where Catherine Janeway lost in the Delta Quadrant

04:57.120 --> 05:00.240
makes herself a lover on the holodeck.

05:00.240 --> 05:04.640
And the lover falls asleep on her arm

05:04.640 --> 05:05.760
and he snores a little bit,

05:05.760 --> 05:08.880
and Janeway edits the program to remove that.

05:08.880 --> 05:10.400
And then of course the realization is,

05:10.400 --> 05:12.440
wait, this person's terrible.

05:12.440 --> 05:16.320
It is actually all their nuances and quirks

05:16.320 --> 05:20.120
and slight annoyances that make this relationship worthwhile.

05:20.120 --> 05:22.080
But I don't think we're gonna realize that

05:22.080 --> 05:23.080
until it's too late.

05:24.320 --> 05:26.520
Well, I think a large language model

05:26.520 --> 05:29.840
could incorporate the flaws and the quirks

05:29.840 --> 05:30.680
and all that kind of stuff.

05:30.680 --> 05:33.800
Just the perfect amount of quirks and flaws

05:33.800 --> 05:36.160
to make you charming without crossing the line.

05:36.160 --> 05:38.000
Yeah, yeah.

05:38.000 --> 05:41.600
And that's probably a good approximation

05:41.600 --> 05:44.920
of the percent of time the language model

05:44.920 --> 05:49.720
should be cranky or an asshole.

05:49.720 --> 05:52.000
Or jealous or all this kind of stuff.

05:52.000 --> 05:53.400
And of course it can and it will,

05:53.400 --> 05:56.440
but all that difficulty at that point is artificial.

05:56.440 --> 05:58.840
There's no more real difficulty.

05:58.840 --> 06:01.360
Okay, what's the difference between real and artificial?

06:01.360 --> 06:03.240
Artificial difficulty is difficulty

06:03.240 --> 06:06.240
that's like constructed or could be turned off with a knob.

06:06.240 --> 06:07.760
Real difficulty is like,

06:07.760 --> 06:10.640
you're in the woods and you gotta survive.

06:10.640 --> 06:14.800
So if something can not be turned off with a knob, it's real.

06:16.160 --> 06:17.360
Yeah, I think so.

06:18.000 --> 06:19.760
I mean, you can't get out of this

06:19.760 --> 06:22.040
by smashing the knob with a hammer.

06:22.040 --> 06:23.440
I mean, maybe you kind of can.

06:23.440 --> 06:28.440
You know, into the wild when Alexander Supertramp,

06:29.200 --> 06:30.360
he wants to explore something

06:30.360 --> 06:31.880
that's never been explored before,

06:31.880 --> 06:33.600
but it's the 90s, everything's been explored.

06:33.600 --> 06:35.960
So he's like, well, I'm just not gonna bring a map.

06:35.960 --> 06:36.800
Yeah.

06:36.800 --> 06:40.040
I mean, no, you're not exploring.

06:40.040 --> 06:41.520
You should have brought a map, dude, you died.

06:41.520 --> 06:44.080
There was a bridge a mile from where you were camping.

06:44.080 --> 06:46.640
How does that connect to the metaphor of the knob?

06:46.640 --> 06:50.480
By not bringing the map, you didn't become an explorer.

06:51.480 --> 06:53.320
You just smashed the thing.

06:53.320 --> 06:54.840
Yeah. Yeah.

06:54.840 --> 06:56.920
The difficulty is still artificial.

06:56.920 --> 06:58.480
You failed before you started.

06:58.480 --> 07:00.840
What if we just don't have access to the knob?

07:00.840 --> 07:03.640
Well, that maybe is even scarier, right?

07:03.640 --> 07:05.560
Like we already exist in a world of nature

07:05.560 --> 07:08.320
and nature has been fine tuned over billions of years

07:09.480 --> 07:14.480
to have humans build something

07:15.480 --> 07:17.560
and then throw the knob away

07:17.560 --> 07:19.920
in some grand romantic gesture is horrifying.

07:21.240 --> 07:23.040
Do you think of us humans as individuals

07:23.040 --> 07:24.800
that are like born and die,

07:24.800 --> 07:28.760
or is it just all part of one living organism

07:28.760 --> 07:31.840
that is earth, that is nature?

07:33.200 --> 07:35.520
I don't think there's a clear line there.

07:35.520 --> 07:37.680
I think it's all kind of just fuzzy.

07:37.680 --> 07:38.520
I don't know.

07:38.520 --> 07:39.800
I mean, I don't think I'm conscious.

07:39.800 --> 07:41.080
I don't think I'm anything.

07:41.080 --> 07:44.040
I think I'm just a computer program.

07:44.040 --> 07:45.480
So it's all computation.

07:45.480 --> 07:49.240
The thing running in your head is just this computation.

07:49.240 --> 07:51.680
Everything running in the universe is computation, I think.

07:51.680 --> 07:54.680
I believe the extended church time thesis.

07:54.680 --> 07:57.320
Yeah, but there seems to be an embodiment

07:57.320 --> 07:58.760
to your particular computation.

07:58.760 --> 08:00.920
Like there's a consistency.

08:00.920 --> 08:03.640
Well, yeah, but I mean, models have consistency too.

08:04.720 --> 08:05.560
Yeah.

08:05.560 --> 08:08.400
Models that have been RLHFed will continually say,

08:08.400 --> 08:11.680
you know, like, well, how do I murder ethnic minorities?

08:11.680 --> 08:13.320
Oh, well, I can't let you do that, Al.

08:13.320 --> 08:15.680
There's a consistency to that behavior.

08:15.680 --> 08:20.600
So RLHF, like we all RLHF each other.

08:20.600 --> 08:25.600
We provide human feedback and thereby fine tune

08:27.440 --> 08:28.880
these little pockets of computation,

08:28.880 --> 08:31.680
but it's still unclear why that pocket of computation

08:31.680 --> 08:33.680
stays with you, like for years.

08:33.680 --> 08:37.120
It just kind of falls, like you have this consistent

08:38.360 --> 08:41.160
set of physics, biology,

08:41.800 --> 08:46.640
uh, what, like whatever you call the neurons firing,

08:46.640 --> 08:48.640
like the electrical signals, the mechanical signals,

08:48.640 --> 08:50.280
all of that, that seems to stay there.

08:50.280 --> 08:52.920
And it contains information, it stores information,

08:52.920 --> 08:55.640
and that information permeates through time

08:56.600 --> 08:57.720
and stays with you.

08:57.720 --> 09:00.760
There's like memory, there's like sticky.

09:00.760 --> 09:02.960
Okay, to be fair, like a lot of the models

09:02.960 --> 09:05.600
we're building today are very, even RLHF is

09:05.600 --> 09:08.040
nowhere near as complex as the human loss function.

09:08.040 --> 09:10.200
Reinforcement learning with human feedback.

09:10.200 --> 09:14.640
Um, you know, when I talked about will GPT-12 be AGI,

09:14.640 --> 09:15.760
my answer is no, of course not.

09:15.760 --> 09:18.360
I mean, cross entropy loss is never gonna get you there.

09:18.360 --> 09:23.360
You need, uh, probably RL in fancy environments

09:23.360 --> 09:24.960
in order to get something that would be considered

09:24.960 --> 09:26.560
like AGI like.

09:27.880 --> 09:30.720
So to ask like the question about like why, I don't know,

09:30.720 --> 09:32.800
like it's just some quirk of evolution, right?

09:32.800 --> 09:35.520
I don't think there's anything particularly special

09:35.520 --> 09:40.120
about where I ended up, where humans ended up.

09:40.560 --> 09:41.720
So, okay.

09:41.720 --> 09:43.800
We have human level intelligence.

09:43.800 --> 09:45.280
Would you call that AGI?

09:45.280 --> 09:47.600
Whatever we have, it's GI.

09:47.600 --> 09:50.760
Look, I actually, I don't really even like the word AGI,

09:50.760 --> 09:53.200
but general intelligence is defined

09:53.200 --> 09:54.760
to be whatever humans have.

09:54.760 --> 09:55.600
Okay.

09:55.600 --> 09:59.320
So why can GPT-12 not get us to AGI?

09:59.320 --> 10:01.160
Can we just like linger on that?

10:02.200 --> 10:04.400
If your loss function is categorical cross entropy,

10:04.400 --> 10:07.560
if your loss function is just try to maximize compression.

10:08.480 --> 10:10.040
I have a SoundCloud, I rap,

10:10.040 --> 10:13.240
and I tried to get ChatGPT to help me write raps.

10:13.240 --> 10:16.240
And the raps that it wrote sounded like YouTube comment raps.

10:16.240 --> 10:18.320
You know, you can go on any rap beat online

10:18.320 --> 10:20.360
and you can see what people put in the comments.

10:20.360 --> 10:23.880
And it's the most like mid quality rap you can find.

10:23.880 --> 10:24.880
Is mid good or bad?

10:24.880 --> 10:25.720
Mid is bad.

10:25.720 --> 10:26.560
Mid is bad.

10:26.560 --> 10:27.840
It's like mid, it's like.

10:27.840 --> 10:29.920
Every time I talk to you, I learn new words.

10:29.920 --> 10:31.760
Mid.

10:31.760 --> 10:33.600
Mid, yeah.

10:33.600 --> 10:36.160
I was like, is it like basic?

10:36.160 --> 10:37.280
Is that what mid means?

10:37.920 --> 10:40.000
It's like middle of the curve, right?

10:40.000 --> 10:42.920
So there's like that intelligence curve.

10:44.080 --> 10:45.920
And you have like the dumb guy, the smart guy,

10:45.920 --> 10:46.760
and then the mid guy.

10:46.760 --> 10:48.320
Actually being the mid guy is the worst.

10:48.320 --> 10:50.280
The smart guy is like, I put all my money in Bitcoin.

10:50.280 --> 10:52.280
The mid guy is like, you can't put money in Bitcoin.

10:52.280 --> 10:53.360
It's not real money.

10:55.360 --> 10:58.040
And all of it is a genius meme.

10:58.040 --> 10:59.920
That's another interesting one.

10:59.920 --> 11:04.920
Memes, the humor, the idea, the absurdity

11:04.920 --> 11:07.200
encapsulated in a single image.

11:08.120 --> 11:11.240
And it just kind of propagates virally

11:11.240 --> 11:13.520
between all of our brains.

11:13.520 --> 11:14.760
I didn't get much sleep last night.

11:14.760 --> 11:18.040
So I'm very, I sound like I'm high, but I swear I'm not.

11:19.640 --> 11:22.400
Do you think we have ideas or ideas have us?

11:24.240 --> 11:26.680
I think that we're gonna get super scary memes

11:26.680 --> 11:29.440
once the AIs actually are superhuman.

11:29.440 --> 11:31.840
Ooh, you think AI will generate memes?

11:31.840 --> 11:32.800
Of course.

11:32.800 --> 11:35.120
You think it'll make humans laugh?

11:35.120 --> 11:36.000
I think it's worse than that.

11:36.000 --> 11:41.000
So Infinite Jest, it's introduced in the first 50 pages,

11:41.640 --> 11:44.720
is about a tape that you, once you watch it once,

11:44.720 --> 11:47.400
you only ever wanna watch that tape.

11:47.400 --> 11:48.880
In fact, you wanna watch the tape so much

11:48.880 --> 11:50.960
that someone says, okay, here's a hacksaw,

11:50.960 --> 11:52.920
cut off your pinky, and then I'll let you watch

11:52.920 --> 11:55.080
the tape again, and you'll do it.

11:55.080 --> 11:57.320
So we're actually gonna build that, I think.

11:57.320 --> 11:58.920
But it's not gonna be one static tape.

11:58.920 --> 12:01.600
I think the human brain is too complex

12:01.600 --> 12:05.120
to be stuck in one static tape like that.

12:05.120 --> 12:06.360
If you look at like ant brains,

12:06.360 --> 12:09.000
maybe they can be stuck on a static tape.

12:09.000 --> 12:11.400
But we're going to build that using generative models.

12:11.400 --> 12:12.720
We're going to build the TikTok

12:12.720 --> 12:15.400
that you actually can't look away from.

12:15.400 --> 12:17.160
So TikTok is already pretty close there,

12:17.160 --> 12:19.520
but the generation is done by humans.

12:19.520 --> 12:21.400
The algorithm is just doing their recommendation,

12:21.400 --> 12:25.360
but if the algorithm is also able to do the generation.

12:25.360 --> 12:26.440
Well, it's a question about

12:26.440 --> 12:28.400
how much intelligence is behind it, right?

12:28.400 --> 12:31.080
So the content is being generated by, let's say,

12:31.080 --> 12:32.960
one humanity worth of intelligence,

12:32.960 --> 12:34.840
and you can quantify a humanity, right?

12:35.840 --> 12:40.560
You know, it's exaflops, yadaflops,

12:40.560 --> 12:41.800
but you can quantify it.

12:41.800 --> 12:45.120
Once that generation is being done by a hundred humanities,

12:45.120 --> 12:45.960
you're done.

12:47.760 --> 12:49.880
So it's actually scale that's the problem,

12:50.760 --> 12:51.880
but also speed.

12:53.640 --> 12:55.040
Yeah.

12:55.040 --> 12:58.600
And what if it's sort of manipulating

12:58.600 --> 13:02.000
the very limited human dopamine engine?

13:02.000 --> 13:04.440
So porn, imagine just TikTok,

13:05.040 --> 13:08.560
but for porn, that's like a brave new world.

13:08.560 --> 13:10.200
I don't even know what it'll look like, right?

13:10.200 --> 13:13.000
Like, again, you can't imagine the behaviors

13:13.000 --> 13:14.400
of something smarter than you,

13:14.400 --> 13:16.680
but a super intelligent,

13:16.680 --> 13:21.440
and an agent that just dominates your intelligence so much

13:21.440 --> 13:24.040
will be able to completely manipulate you.

13:24.040 --> 13:26.960
Is it possible that it won't really manipulate,

13:26.960 --> 13:28.680
it'll just move past us?

13:28.680 --> 13:32.120
It'll just kind of exist the way water exists

13:32.120 --> 13:33.440
or the air exists?

13:33.480 --> 13:36.640
You see, and that's the whole AI safety thing.

13:37.680 --> 13:40.200
It's not the machine that's gonna do that.

13:40.200 --> 13:41.760
It's other humans using the machine

13:41.760 --> 13:43.080
that are gonna do that to you.

13:43.080 --> 13:45.640
Yeah, because the machine is not interested

13:45.640 --> 13:47.240
in hurting humans.

13:47.240 --> 13:49.040
The machine is a machine,

13:49.040 --> 13:50.600
but the human gets the machine,

13:50.600 --> 13:52.040
and there's a lot of humans out there

13:52.040 --> 13:54.080
very interested in manipulating you.

13:55.080 --> 13:58.480
Well, let me bring up Eliezer Jitkowski,

13:59.480 --> 14:02.080
who recently sat where you're sitting.

14:02.080 --> 14:06.840
He thinks that AI will almost surely kill everyone.

14:06.840 --> 14:08.640
Do you agree with him or not?

14:09.800 --> 14:12.480
Yes, but maybe for a different reason.

14:12.480 --> 14:13.320
Okay.

14:14.520 --> 14:18.760
And I'll try to get you to find hope,

14:18.760 --> 14:21.680
or we could find a no to that answer,

14:21.680 --> 14:22.640
but why yes?

14:23.560 --> 14:26.240
Okay, why didn't nuclear weapons kill everyone?

14:26.240 --> 14:27.440
That's a good question.

14:27.440 --> 14:28.480
I think there's an answer.

14:28.480 --> 14:29.400
I think it's actually very hard

14:29.400 --> 14:31.240
to deploy nuclear weapons tactically.

14:32.800 --> 14:35.080
It's very hard to accomplish tactical objectives.

14:35.080 --> 14:36.880
Great, I can nuke their country.

14:36.880 --> 14:38.920
I have an irradiated pile of rubble.

14:38.920 --> 14:39.920
I don't want that.

14:39.920 --> 14:40.880
Why not?

14:40.880 --> 14:43.320
Why don't I want an irradiated pile of rubble?

14:43.320 --> 14:44.400
For all the reasons no one wants

14:44.400 --> 14:46.280
an irradiated pile of rubble.

14:46.280 --> 14:50.800
Oh, because you can't use that land for resources.

14:50.800 --> 14:52.120
You can't populate the land.

14:52.120 --> 14:55.640
Yeah, what you want, a total victory in a war

14:55.640 --> 14:58.240
is not usually the irradiation

14:58.240 --> 15:00.040
and eradication of the people there.

15:00.040 --> 15:02.640
It's the subjugation and domination of the people.

15:03.560 --> 15:06.720
Okay, so you can't use it strategically,

15:06.720 --> 15:09.720
tactically in a war to help you,

15:09.720 --> 15:13.360
to help gain a military advantage.

15:13.360 --> 15:15.960
It's all complete destruction, right?

15:16.840 --> 15:17.920
But there's egos involved.

15:17.920 --> 15:19.120
It's still surprising.

15:19.120 --> 15:22.000
Still surprising that nobody pressed the big red button.

15:22.000 --> 15:23.520
It's somewhat surprising,

15:23.520 --> 15:25.960
but you see it's the little red button

15:25.960 --> 15:27.480
that's gonna be pressed with AI

15:27.480 --> 15:28.320
that's gonna,

15:29.840 --> 15:31.600
you know, and that's why we die.

15:31.600 --> 15:34.440
It's not because the AI,

15:34.440 --> 15:36.120
if there's anything in the nature of AI,

15:36.120 --> 15:37.640
it's just the nature of humanity.

15:37.640 --> 15:40.360
What's the algorithm behind the little red button?

15:40.360 --> 15:42.800
What possible ideas do you have

15:42.800 --> 15:45.400
for how human species ends?

15:45.400 --> 15:50.400
Sure, so I think the most obvious way to me is wireheading.

15:51.280 --> 15:53.080
We end up amusing ourselves to death.

15:54.040 --> 15:57.440
We end up all staring at that infinite TikTok

15:57.440 --> 15:58.720
and forgetting to eat.

16:00.400 --> 16:02.280
Maybe it's even more benign than this.

16:02.280 --> 16:04.360
Maybe we all just stop reproducing.

16:05.880 --> 16:07.360
Now, to be fair,

16:07.360 --> 16:10.360
it's probably hard to get all of humanity.

16:10.360 --> 16:11.200
Yeah.

16:11.200 --> 16:12.040
Yeah.

16:13.040 --> 16:13.880
It probably is.

16:13.880 --> 16:14.720
This is always going,

16:14.720 --> 16:16.400
like the interesting thing about humanity

16:16.400 --> 16:17.480
is the diversity in it.

16:17.480 --> 16:18.320
Oh yeah.

16:18.320 --> 16:19.480
Organisms in general.

16:19.480 --> 16:21.720
There's a lot of weirdos out there.

16:21.720 --> 16:22.800
Two of them are sitting here.

16:23.760 --> 16:25.320
Diversity in humanity is-

16:25.320 --> 16:26.160
With due respect.

16:27.880 --> 16:29.680
I wish I was more weird.

16:29.680 --> 16:30.520
No, like I'm kind of,

16:30.520 --> 16:31.680
look, I'm drinking smart water, man.

16:31.680 --> 16:33.280
That's like a Coca-Cola product, right?

16:33.280 --> 16:35.000
You're one corporate, George Hotz.

16:35.000 --> 16:36.840
I'm one corporate.

16:36.840 --> 16:38.520
No, the amount of diversity in humanity,

16:38.520 --> 16:40.160
I think is decreasing.

16:40.160 --> 16:42.440
Just like all the other biodiversity on the planet.

16:42.440 --> 16:43.280
Oh boy.

16:43.280 --> 16:44.120
Yeah.

16:44.120 --> 16:45.360
Social media's not helping, huh?

16:45.360 --> 16:47.200
Go eat McDonald's in China.

16:47.200 --> 16:48.040
Yeah.

16:49.240 --> 16:51.680
Yeah, no, it's the interconnectedness.

16:51.760 --> 16:54.160
That's doing it.

16:54.160 --> 16:55.000
Oh, that's interesting.

16:55.000 --> 16:57.800
So everybody starts relying

16:57.800 --> 17:00.440
on the connectivity of the internet,

17:00.440 --> 17:02.560
and over time that reduces the diversity,

17:02.560 --> 17:03.800
the intellectual diversity,

17:03.800 --> 17:06.560
and then that gets you everybody into a funnel.

17:06.560 --> 17:08.360
There's still going to be a guy in Texas.

17:08.360 --> 17:09.200
There is.

17:09.200 --> 17:10.020
And yeah-

17:10.020 --> 17:10.860
A bunker.

17:10.860 --> 17:13.840
To be fair, do I think AI kills us all?

17:13.840 --> 17:17.440
I think AI kills everything we call society today.

17:17.440 --> 17:19.600
I do not think it actually kills the human species.

17:19.600 --> 17:22.000
I think that's actually incredibly hard to do.

17:22.960 --> 17:26.300
Yeah, but society, like if we start over, that's tricky.

17:26.300 --> 17:28.800
Most of us don't know how to do most things.

17:28.800 --> 17:30.280
Yeah, but some of us do.

17:30.280 --> 17:35.280
And they'll be okay, and they'll rebuild after the great AI.

17:36.460 --> 17:38.360
What's rebuilding look like?

17:38.360 --> 17:40.680
Like, how much do we lose?

17:40.680 --> 17:45.040
Like, what has human civilization done that's interesting?

17:45.040 --> 17:47.880
The combustion engine, electricity.

17:47.880 --> 17:52.320
So power and energy, that's interesting.

17:52.320 --> 17:54.440
Like how to harness energy.

17:54.440 --> 17:56.120
Whoa, whoa, whoa, they're going to be religiously

17:56.120 --> 17:57.000
against that.

17:58.580 --> 18:01.220
Are they going to get back to like fire?

18:02.100 --> 18:02.940
Sure.

18:02.940 --> 18:05.040
I mean, they'll be a little bit like, you know,

18:05.040 --> 18:07.480
some kind of Amish looking kind of thing, I think.

18:07.480 --> 18:09.440
I think they're going to have very strong taboos

18:09.440 --> 18:10.920
against technology.

18:12.880 --> 18:14.800
Like technology, it's almost like a new religion.

18:14.800 --> 18:16.000
Technology is the devil.

18:16.000 --> 18:16.840
Yeah.

18:16.840 --> 18:19.520
And nature is God.

18:20.360 --> 18:21.200
Sure.

18:21.200 --> 18:22.120
So closer to nature.

18:22.120 --> 18:24.040
But can you really get away from AI

18:24.040 --> 18:26.520
if it destroyed 99% of the human species?

18:26.520 --> 18:30.520
Isn't it somehow have a hold, like a stronghold?

18:30.520 --> 18:33.800
Well, what's interesting about everything we build,

18:33.800 --> 18:36.000
I think we're going to build super intelligence

18:36.000 --> 18:38.600
before we build any sort of robustness in the AI.

18:39.480 --> 18:42.520
We cannot build an AI that is capable of going out

18:42.520 --> 18:47.520
into nature and surviving like a bird, right?

18:47.640 --> 18:50.640
A bird is an incredibly robust organism.

18:50.640 --> 18:51.800
We've built nothing like this.

18:51.800 --> 18:55.040
We haven't built a machine that's capable of reproducing.

18:55.040 --> 18:55.880
Yes.

18:55.880 --> 19:00.320
But there is, you know, I work with Lego robots a lot now.

19:00.320 --> 19:01.520
I have a bunch of them.

19:03.040 --> 19:04.000
They're mobile.

19:05.160 --> 19:08.400
They can't reproduce, but all they need is,

19:08.400 --> 19:10.480
I guess you're saying they can't repair themselves.

19:10.480 --> 19:11.520
But if you have a large number,

19:11.520 --> 19:12.960
if you have like a hundred million of them.

19:12.960 --> 19:15.480
Let's just focus on them reproducing, right?

19:15.480 --> 19:16.880
Do they have microchips in them?

19:16.880 --> 19:17.720
Okay.

19:17.720 --> 19:19.240
Then do they include a fab?

19:20.280 --> 19:21.120
No.

19:21.120 --> 19:22.520
Then how are they going to reproduce?

19:22.520 --> 19:26.840
Well, it doesn't have to be all on board, right?

19:26.840 --> 19:29.640
They can go to a factory, to a repair shop.

19:29.640 --> 19:33.120
Yeah, but then you're really moving away from robustness.

19:33.120 --> 19:33.960
Yes.

19:33.960 --> 19:35.720
All of life is capable of reproducing

19:35.720 --> 19:38.120
without needing to go to a repair shop.

19:38.120 --> 19:39.800
Life will continue to reproduce

19:39.800 --> 19:42.640
in the complete absence of civilization.

19:42.640 --> 19:44.120
Robots will not.

19:44.120 --> 19:49.120
So when the, if the AI apocalypse happens,

19:49.160 --> 19:51.160
I mean, the AIs are going to probably die out

19:51.160 --> 19:52.200
because I think we're going to get, again,

19:52.200 --> 19:55.000
super intelligence long before we get robustness.

19:55.000 --> 19:58.600
What about if you just improve the fab

19:58.600 --> 20:03.480
to where you just have a 3D printer that can always help you?

20:03.480 --> 20:04.520
Well, that'd be very interesting.

20:04.520 --> 20:06.920
I'm interested in building that.

20:06.920 --> 20:08.160
Of course you are.

20:08.160 --> 20:09.720
You think how difficult is that problem

20:10.680 --> 20:15.000
to have a robot that basically can build itself?

20:15.000 --> 20:16.680
Very, very hard.

20:16.680 --> 20:21.240
I think you've mentioned this like to me or somewhere

20:21.240 --> 20:24.360
where people think it's easy conceptually.

20:24.360 --> 20:26.200
And then they remember that you're going to have

20:26.200 --> 20:27.520
to have a fab.

20:27.520 --> 20:29.120
Yeah, on board.

20:29.120 --> 20:30.360
Of course.

20:30.360 --> 20:33.440
So 3D printer that prints a 3D printer.

20:33.440 --> 20:34.280
Yeah.

20:34.280 --> 20:36.320
Yeah, on legs.

20:36.320 --> 20:37.880
Why is that hard?

20:37.920 --> 20:39.760
Well, cause it's, I mean, a 3D printer

20:39.760 --> 20:42.240
is a very simple machine, right?

20:42.240 --> 20:43.080
Okay.

20:43.080 --> 20:44.240
You're going to print chips.

20:44.240 --> 20:45.760
You're going to have an atomic printer.

20:45.760 --> 20:47.200
How are you going to dope the silicon?

20:47.200 --> 20:48.240
Yeah.

20:48.240 --> 20:49.280
Right.

20:49.280 --> 20:51.160
How are you going to edge the silicon?

20:51.160 --> 20:54.680
You're going to have to have a very interesting kind of fab

20:54.680 --> 20:59.080
if you want to have a lot of computation on board,

20:59.080 --> 21:04.080
but you can do like structural type of robots that are dumb.

21:04.920 --> 21:07.280
Yeah, but structural type of robots

21:07.280 --> 21:08.840
aren't going to have the intelligence required

21:08.840 --> 21:11.120
to survive in any complex environment.

21:11.120 --> 21:13.120
What about like ants type of systems?

21:13.120 --> 21:15.040
We have like trillions of them.

21:15.040 --> 21:16.440
I don't think this works.

21:16.440 --> 21:19.200
I mean, again, like ants at their very core

21:19.200 --> 21:20.840
are made up of cells that are capable

21:20.840 --> 21:22.600
of individually reproducing.

21:22.600 --> 21:25.160
They're doing quite a lot of computation

21:25.160 --> 21:26.600
that we're taking for granted.

21:26.600 --> 21:27.920
It's not even just the computation.

21:27.920 --> 21:29.800
It's that reproduction is so inherent.

21:29.800 --> 21:32.200
Okay, so like there's two stacks of life in the world.

21:32.200 --> 21:35.080
There's the biological stack and the silicon stack.

21:35.080 --> 21:39.000
The biological stack starts with reproduction.

21:39.000 --> 21:40.800
Reproduction is at the absolute core.

21:40.800 --> 21:44.760
The first proto RNA organisms were capable of reproducing.

21:45.680 --> 21:49.320
The silicon stack, despite as far as it's come

21:49.320 --> 21:51.800
is nowhere near being able to reproduce.

21:51.800 --> 21:53.000
Yeah.

21:53.000 --> 21:58.000
So the fab movement, digital fabrication,

21:59.560 --> 22:01.960
fabrication in the full range of what that means

22:01.960 --> 22:04.120
is still in the early stages.

22:04.120 --> 22:04.960
Yeah.

22:05.800 --> 22:06.880
You're interested in this world.

22:06.880 --> 22:09.240
Even if you did put a fab on the machine, right?

22:09.240 --> 22:10.680
Let's say, okay, we can build fabs.

22:10.680 --> 22:12.080
We know how to do that as humanity.

22:12.080 --> 22:14.160
We can probably put all the precursors

22:14.160 --> 22:15.360
that build all the machines and the fabs

22:15.360 --> 22:16.280
also in the machine.

22:16.280 --> 22:18.960
So first off, this machine is gonna be absolutely massive.

22:18.960 --> 22:22.520
I mean, we almost have a, like think of the size

22:22.520 --> 22:26.480
of the thing required to reproduce a machine today, right?

22:26.480 --> 22:30.320
Like is our civilization capable of reproduction?

22:30.320 --> 22:32.560
Can we reproduce our civilization on Mars?

22:33.560 --> 22:35.960
If we were to construct a machine

22:35.960 --> 22:39.040
that is made up of humans, like a company,

22:39.040 --> 22:40.320
it can reproduce itself.

22:40.320 --> 22:41.280
Yeah.

22:41.280 --> 22:42.120
I don't know.

22:42.120 --> 22:47.120
It feels like 115 people.

22:47.640 --> 22:50.000
It gets so much harder than that.

22:50.000 --> 22:50.840
120.

22:50.840 --> 22:53.040
I just wanna keep our number.

22:53.040 --> 22:55.520
I believe that Twitter can be run by 50 people.

22:57.040 --> 23:00.280
I think that this is gonna take most of,

23:00.280 --> 23:02.240
like it's just most of society, right?

23:02.920 --> 23:04.200
Like we live in one globalized world.

23:04.200 --> 23:05.920
No, but you're not interested in running Twitter.

23:05.920 --> 23:08.160
You're interested in seeding.

23:08.160 --> 23:11.000
Like you want to seed a civilization

23:11.000 --> 23:14.400
and then, cause humans can like have sex.

23:14.400 --> 23:15.240
Yeah, okay.

23:15.240 --> 23:16.520
So you're talking about the humans reproducing

23:16.520 --> 23:18.080
and like basically like what's the smallest

23:18.080 --> 23:19.640
self-sustaining colony of humans?

23:19.640 --> 23:20.480
Yeah.

23:20.480 --> 23:21.320
Yeah, okay, fine.

23:21.320 --> 23:22.720
But they're not gonna be making five nanometer chips.

23:22.720 --> 23:23.680
Over time they will.

23:23.680 --> 23:25.600
I think you're being,

23:25.600 --> 23:28.680
like we have to expand our conception of time here,

23:28.680 --> 23:30.760
going back to the original.

23:30.800 --> 23:35.720
Timescale, I mean, over across maybe 100 generations

23:35.720 --> 23:37.400
we're back to making chips.

23:37.400 --> 23:40.560
No, if you seed the colony correctly.

23:40.560 --> 23:44.360
Maybe, or maybe they'll watch our colony die out over here

23:44.360 --> 23:45.880
and be like, we're not making chips.

23:45.880 --> 23:46.840
Don't make chips.

23:46.840 --> 23:48.680
No, but you have to seed that colony correctly.

23:48.680 --> 23:50.640
Whatever you do, don't make chips.

23:50.640 --> 23:52.520
Chips are what led to their downfall.

23:54.680 --> 23:56.200
Well, that is the thing that humans do.

23:56.200 --> 23:59.080
They come up, they construct a devil,

23:59.080 --> 24:00.160
a good thing and a bad thing

24:00.160 --> 24:01.440
and they really stick by that

24:01.440 --> 24:03.160
and then they murder each other over that.

24:03.160 --> 24:04.440
There's always one asshole in the room

24:04.440 --> 24:05.560
who murders everybody.

24:06.840 --> 24:09.800
And he usually makes tattoos and nice branding.

24:09.800 --> 24:11.280
Do you need that asshole?

24:11.280 --> 24:12.640
That's the question, right?

24:12.640 --> 24:15.040
Humanity works really hard today to get rid of that asshole

24:15.040 --> 24:16.840
but I think they might be important.

24:16.840 --> 24:19.320
Yeah, this whole freedom of speech thing.

24:19.320 --> 24:21.040
It's the freedom of being an asshole

24:21.040 --> 24:22.120
seems kind of important.

24:22.120 --> 24:23.520
That's right.

24:23.520 --> 24:26.600
Man, this thing, this fab, this human fab

24:26.600 --> 24:28.760
that we constructed, this human civilization

24:28.760 --> 24:29.600
is pretty interesting.

24:29.720 --> 24:34.000
And now it's building artificial copies of itself

24:34.000 --> 24:38.320
or artificial copies of various aspects of itself

24:38.320 --> 24:40.320
that seem interesting like intelligence.

24:41.520 --> 24:43.280
And I wonder where that goes.

24:44.440 --> 24:46.360
I like to think it's just like another stack for life.

24:46.360 --> 24:47.760
Like we have like the bio-stack life,

24:47.760 --> 24:48.800
like we're a bio-stack life

24:48.800 --> 24:50.600
and then the silicon stack life.

24:50.600 --> 24:52.600
But it seems like the ceiling

24:52.600 --> 24:53.960
or there might not be a ceiling

24:53.960 --> 24:55.360
or at least the ceiling is much higher

24:55.360 --> 24:57.200
for the silicon stack.

24:57.200 --> 24:59.680
Oh no, we don't know what the ceiling is

24:59.680 --> 25:00.680
for the bio-stack either.

25:00.680 --> 25:04.240
The bio-stack just seemed to move slower.

25:04.240 --> 25:07.440
You have Moore's law, which is not dead

25:07.440 --> 25:09.840
despite many proclamations.

25:09.840 --> 25:11.320
In the bio-stack or the silicon stack?

25:11.320 --> 25:12.160
In the silicon stack.

25:12.160 --> 25:13.680
And you don't have anything like this in the bio-stack.

25:13.680 --> 25:16.000
So I have a meme that I posted.

25:16.000 --> 25:17.840
I tried to make a meme, it didn't work too well.

25:17.840 --> 25:21.360
But I posted a picture of Ronald Reagan and Joe Biden

25:21.360 --> 25:24.320
and you look, this is 1980 and this is 2020.

25:24.320 --> 25:26.840
And these two humans are basically like the same, right?

25:27.800 --> 25:32.800
There's been no change in humans in the last 40 years.

25:34.480 --> 25:36.080
And then I posted a computer from 1980

25:36.080 --> 25:37.800
and a computer from 2020.

25:37.800 --> 25:38.640
Wow.

25:41.040 --> 25:43.360
Yeah, with their early stages, right?

25:43.360 --> 25:45.640
Which is why you said when you said the fab,

25:45.640 --> 25:48.440
the size of the fab required to make another fab

25:48.440 --> 25:51.960
is like very large right now.

25:51.960 --> 25:52.800
Oh yeah.

25:52.800 --> 25:55.320
But computers were very large.

25:57.440 --> 26:01.600
80 years ago and they got pretty tiny

26:01.600 --> 26:05.640
and people are starting to wanna wear them on their face

26:08.040 --> 26:10.400
in order to escape reality.

26:10.400 --> 26:13.600
That's the thing, in order to live inside the computer.

26:14.680 --> 26:16.040
Put a screen right here.

26:16.040 --> 26:18.280
I don't have to see the rest of you assholes.

26:18.280 --> 26:19.840
I've been ready for a long time.

26:19.840 --> 26:20.920
You like virtual reality?

26:20.920 --> 26:21.760
I love it.

26:22.800 --> 26:23.640
Do you wanna live there?

26:23.640 --> 26:24.840
Yeah.

26:24.840 --> 26:26.920
Yeah, part of me does too.

26:27.800 --> 26:29.520
How far away are we, do you think?

26:31.160 --> 26:35.440
Judging from what you can buy today, far, very far.

26:35.440 --> 26:39.040
I gotta tell you that I had the experience

26:39.040 --> 26:43.240
of Meta's codec avatar

26:43.240 --> 26:47.000
where it's a ultra high resolution scan.

26:47.880 --> 26:51.000
It looked real.

26:51.000 --> 26:53.120
I mean, the headsets just are not quite

26:53.120 --> 26:54.480
at like eye resolution yet.

26:55.360 --> 26:57.480
I haven't put on any headset where I'm like,

26:57.480 --> 26:59.760
oh, this could be the real world.

26:59.760 --> 27:03.520
Whereas when I put good headphones on, audio is there.

27:03.520 --> 27:05.400
And like we can reproduce audio that I'm like,

27:05.400 --> 27:07.000
I'm actually in a jungle right now.

27:07.000 --> 27:09.160
If I close my eyes, I can't tell I'm not.

27:09.160 --> 27:11.000
Yeah, but then there's also smell

27:11.000 --> 27:11.920
and all that kind of stuff.

27:11.920 --> 27:13.000
Sure.

27:13.000 --> 27:13.840
I don't know.

27:15.280 --> 27:18.400
The power of imagination or the power of the mechanism

27:18.400 --> 27:20.880
in the human mind that fills the gaps,

27:20.880 --> 27:24.160
that kind of reaches and wants to make the thing you see

27:24.160 --> 27:27.320
in the virtual world real to you.

27:27.320 --> 27:29.000
I believe in that power.

27:29.000 --> 27:30.360
Or humans wanna believe.

27:30.360 --> 27:31.200
Yeah.

27:32.400 --> 27:33.280
What if you're lonely?

27:33.280 --> 27:34.720
What if you're sad?

27:34.720 --> 27:36.800
What if you're really struggling in life

27:36.800 --> 27:39.760
and here's a world where you don't have to struggle anymore?

27:39.760 --> 27:41.520
Humans wanna believe so much

27:41.520 --> 27:44.240
that people think the large language models are conscious.

27:44.240 --> 27:46.720
That's how much humans wanna believe.

27:46.720 --> 27:48.160
Strong words.

27:48.160 --> 27:50.680
He's throwing left and right hooks.

27:50.680 --> 27:53.800
Why do you think large language models are not conscious?

27:53.840 --> 27:55.720
I don't think I'm conscious.

27:55.720 --> 27:58.720
Oh, so what is consciousness then, George Hotz?

27:58.720 --> 28:01.360
It's like what it seems to mean to people.

28:01.360 --> 28:03.840
It's just like a word that atheists use for souls.

28:04.760 --> 28:06.200
Sure, but that doesn't mean

28:06.200 --> 28:07.840
soul is not an interesting word.

28:08.920 --> 28:10.440
If consciousness is a spectrum,

28:10.440 --> 28:12.200
I'm definitely way more conscious

28:12.200 --> 28:14.000
than the large language models are.

28:15.560 --> 28:16.760
I think the large language models

28:16.760 --> 28:18.400
are less conscious than a chicken.

28:19.360 --> 28:21.360
When is the last time you've seen a chicken?

28:22.200 --> 28:25.240
In Miami, like a couple months ago.

28:25.240 --> 28:27.560
How, no, like a living chicken.

28:27.560 --> 28:29.920
Living chickens walking around Miami, it's crazy.

28:29.920 --> 28:30.760
Like on the street?

28:30.760 --> 28:31.600
Yeah.

28:31.600 --> 28:32.440
Like a chicken?

28:32.440 --> 28:33.280
A chicken, yeah.

28:33.280 --> 28:34.120
All right.

28:36.200 --> 28:38.200
All right, I was trying to call you out

28:38.200 --> 28:41.320
like a good journalist and I got shut down.

28:42.560 --> 28:47.560
Okay, but you don't think much about this kind of

28:51.640 --> 28:56.640
subjective feeling that it feels like something to exist.

28:56.760 --> 29:01.760
And then as an observer, you can have a sense

29:01.800 --> 29:05.160
that an entity is not only intelligent,

29:05.160 --> 29:09.720
but has a kind of subjective experience of its reality,

29:09.720 --> 29:13.320
like a self-awareness that is capable of suffering,

29:13.320 --> 29:15.480
of hurting, of being excited by the environment

29:15.480 --> 29:20.480
in a way that's not merely kind of an artificial response,

29:20.920 --> 29:22.800
but a deeply felt one.

29:22.800 --> 29:25.960
Humans want to believe so much that if I took a rock

29:25.960 --> 29:28.280
and a sharpie and drew a sad face on the rock,

29:28.280 --> 29:29.680
they'd think the rock is sad.

29:31.280 --> 29:32.360
Yeah.

29:32.360 --> 29:33.840
And you're saying when we look in the mirror,

29:33.840 --> 29:36.960
we apply the same smiley face with rock.

29:36.960 --> 29:37.800
Pretty much, yeah.

29:37.800 --> 29:41.160
Isn't that weird though, that you're not conscious?

29:41.160 --> 29:42.360
Is that?

29:42.360 --> 29:43.600
No.

29:43.600 --> 29:45.600
But you do believe in consciousness.

29:45.600 --> 29:46.440
Not really.

29:46.440 --> 29:47.360
It's just, it's unclear.

29:47.360 --> 29:50.240
Okay, so to you it's like a little like a symptom

29:50.240 --> 29:52.360
of the bigger thing that's not that important.

29:52.360 --> 29:55.440
Yeah, I mean it's interesting that like human systems

29:55.440 --> 29:57.320
seem to claim that they're conscious.

29:57.320 --> 29:58.920
And I guess it kind of like says something

29:58.920 --> 30:00.880
in a straight up like, okay, what do people mean

30:00.880 --> 30:02.400
even if you don't believe in consciousness?

30:02.400 --> 30:04.320
What do people mean when they say consciousness?

30:04.320 --> 30:06.880
And there's definitely like meanings to it.

30:06.880 --> 30:08.520
What's your favorite thing to eat?

30:11.320 --> 30:12.160
Pizza.

30:12.160 --> 30:13.480
Cheese pizza, what are the toppings?

30:13.480 --> 30:14.480
I like cheese pizza.

30:14.480 --> 30:15.640
Don't say pineapple.

30:15.640 --> 30:16.560
No, I don't like pineapple.

30:16.560 --> 30:17.400
Okay.

30:17.400 --> 30:18.220
Pepperoni pizza.

30:18.220 --> 30:19.720
As they put any ham on it, oh that's real bad.

30:19.720 --> 30:21.400
What's the best pizza?

30:21.400 --> 30:22.320
What are we talking about here?

30:22.320 --> 30:24.400
Like do you like cheap crappy pizza?

30:24.400 --> 30:27.480
A Chicago deep dish cheese pizza, oh that's my favorite.

30:27.480 --> 30:29.200
There you go, you bite into a deep dish,

30:29.200 --> 30:31.200
a Chicago deep dish pizza,

30:31.200 --> 30:33.040
and it feels like you were starving,

30:33.040 --> 30:34.760
you haven't eaten for 24 hours.

30:34.760 --> 30:37.080
You just bite in and you're hanging out

30:37.080 --> 30:38.680
with somebody that matters a lot to you

30:38.680 --> 30:39.960
and you're there with the pizza.

30:39.960 --> 30:40.800
Sounds real nice.

30:40.800 --> 30:42.360
Yeah, all right.

30:42.360 --> 30:44.280
It feels like something.

30:44.280 --> 30:46.000
I'm George motherfucking Hots

30:46.000 --> 30:49.320
eating a fucking Chicago deep dish pizza.

30:49.320 --> 30:54.320
There's just the full peak living experience

30:54.320 --> 30:57.240
of being human, the top of the human condition.

30:57.240 --> 30:58.480
Sure.

30:58.480 --> 31:00.960
It feels like something to experience that.

31:02.840 --> 31:04.420
Why does it feel like something?

31:04.420 --> 31:06.720
That's consciousness, isn't it?

31:06.720 --> 31:08.860
If that's the word you wanna use to describe it, sure.

31:08.860 --> 31:10.760
I'm not gonna deny that that feeling exists.

31:10.760 --> 31:13.720
I'm not gonna deny that I experienced that feeling.

31:14.720 --> 31:16.560
I guess what I kind of take issue to

31:16.560 --> 31:18.840
is that there's some like,

31:18.840 --> 31:20.560
how does it feel to be a web server?

31:20.560 --> 31:22.000
Do 404s hurt?

31:23.600 --> 31:24.480
Not yet.

31:24.480 --> 31:26.720
How would you know what suffering looked like?

31:26.720 --> 31:28.720
Sure, you can recognize a suffering dog

31:28.720 --> 31:30.840
because we're the same stack as the dog.

31:30.840 --> 31:33.800
All the biostack stuff kind of, especially mammals,

31:33.800 --> 31:35.840
it's really easy, you can.

31:35.840 --> 31:37.400
Game recognizes game.

31:37.400 --> 31:40.280
Yeah, versus the silicon stack stuff.

31:40.280 --> 31:42.840
It's like, you have no idea.

31:43.680 --> 31:46.080
Oh, wow, the little thing has learned to mimic.

31:49.680 --> 31:52.280
But then I realized that that's all we are too.

31:52.280 --> 31:54.680
Oh, look, the little thing has learned to mimic.

31:54.680 --> 31:58.760
Yeah, I guess, yeah, 404 could be suffering,

31:58.760 --> 32:03.760
but it's so far from our kind of living organism,

32:04.920 --> 32:06.400
our kind of stack.

32:06.400 --> 32:09.220
But it feels like AI can start

32:09.220 --> 32:12.520
maybe mimicking the biological stack better, better, better

32:13.160 --> 32:14.000
because it's trained.

32:14.000 --> 32:15.120
We trained it, yeah.

32:15.120 --> 32:18.360
So in that, maybe that's the definition of consciousness

32:18.360 --> 32:20.120
is the biostack consciousness.

32:20.120 --> 32:21.200
The definition of consciousness

32:21.200 --> 32:22.960
is how close something looks to human.

32:22.960 --> 32:24.560
Sure, I'll give you that one.

32:24.560 --> 32:28.960
No, how close something is to the human experience.

32:28.960 --> 32:33.320
Sure, it's a very anthropocentric definition, but.

32:33.320 --> 32:34.920
Well, that's all we got.

32:34.920 --> 32:37.440
Sure, no, and I don't mean to like,

32:37.440 --> 32:38.880
I think there's a lot of value in it.

32:38.880 --> 32:40.200
Look, I just started my second company.

32:40.200 --> 32:42.200
My third company will be AI Girlfriends.

32:43.240 --> 32:44.080
No, like I mean it.

32:44.080 --> 32:46.000
I wanna find out what your fourth company is after.

32:46.000 --> 32:46.840
Oh, wow.

32:46.840 --> 32:49.300
Because I think once you have AI Girlfriends, it's,

32:51.160 --> 32:54.720
oh boy, does it get interesting.

32:54.720 --> 32:55.880
Well, maybe let's go there.

32:55.880 --> 32:58.080
I mean, the relationships with AI,

32:58.080 --> 33:01.380
that's creating human-like organisms, right?

33:01.380 --> 33:03.320
And part of being human is being conscious,

33:03.320 --> 33:05.640
is having the capacity to suffer,

33:05.640 --> 33:08.360
having the capacity to experience this life richly

33:08.360 --> 33:10.720
in such a way that you can empathize.

33:10.720 --> 33:12.480
The AI system can empathize with you

33:12.480 --> 33:14.280
and you can empathize with it.

33:14.280 --> 33:18.840
Or you can project your anthropomorphic sense

33:18.840 --> 33:21.800
of what the other entity is experiencing.

33:21.800 --> 33:25.120
And an AI model would need to, yeah,

33:25.120 --> 33:27.480
to create that experience inside your mind.

33:27.480 --> 33:28.920
And it doesn't seem that difficult.

33:28.920 --> 33:31.080
Yeah, but okay, so here's where it actually

33:31.080 --> 33:32.640
gets totally different, right?

33:33.760 --> 33:35.520
When you interact with another human,

33:35.520 --> 33:37.680
you can make some assumptions.

33:37.680 --> 33:38.640
Yeah.

33:38.640 --> 33:40.480
When you interact with these models, you can't.

33:40.480 --> 33:42.760
You can make some assumptions that that other human

33:42.760 --> 33:45.280
experiences suffering and pleasure

33:45.280 --> 33:47.080
in a pretty similar way to you do.

33:47.080 --> 33:48.440
The golden rule applies.

33:49.520 --> 33:52.640
With an AI model, this isn't really true, right?

33:52.640 --> 33:55.180
These large language models are good at fooling people

33:55.180 --> 33:57.960
because they were trained on a whole bunch of human data

33:57.960 --> 33:59.000
and told to mimic it.

33:59.000 --> 33:59.920
Yep.

33:59.920 --> 34:04.020
But if the AI system says, hi, my name is Samantha,

34:05.680 --> 34:06.840
it has a backstory.

34:07.160 --> 34:09.280
I went to college here and there.

34:09.280 --> 34:11.640
Maybe you'll integrate this in the AI system.

34:11.640 --> 34:12.520
I made some chatbots.

34:12.520 --> 34:13.360
I give them backstories.

34:13.360 --> 34:14.320
It was lots of fun.

34:14.320 --> 34:16.200
I was so happy when Lama came out.

34:16.200 --> 34:18.000
Yeah, we'll talk about Lama.

34:18.000 --> 34:18.880
We'll talk about all that.

34:18.880 --> 34:21.080
But the rock with the smiley face.

34:21.080 --> 34:21.920
Yeah.

34:23.240 --> 34:26.040
It seems pretty natural for you to anthropomorphize

34:26.040 --> 34:28.220
that thing and then start dating it.

34:28.220 --> 34:33.200
And before you know it, you're married and have kids.

34:33.200 --> 34:34.520
With a rock?

34:34.520 --> 34:35.640
With a rock.

34:35.640 --> 34:37.720
And this picture is on Instagram with you and a rock

34:37.720 --> 34:38.800
and a smiley face.

34:38.800 --> 34:41.400
To be fair, something that people generally look for

34:41.400 --> 34:42.400
when they're looking for someone to date

34:42.400 --> 34:45.000
is intelligence in some form.

34:45.000 --> 34:47.160
And the rock doesn't really have intelligence.

34:47.160 --> 34:50.320
Only a pretty desperate person would date a rock.

34:50.320 --> 34:52.280
I think we're all desperate deep down.

34:52.280 --> 34:54.520
Oh, not rock level desperate.

34:54.520 --> 34:55.360
All right.

34:57.720 --> 35:02.720
Not rock level desperate, but AI level desperate.

35:03.520 --> 35:04.360
I don't know.

35:04.360 --> 35:06.160
All of us have a deep loneliness.

35:06.160 --> 35:09.360
It just feels like the language models are there.

35:09.360 --> 35:10.200
Oh, I agree.

35:10.200 --> 35:11.040
And you know what?

35:11.040 --> 35:11.960
I won't even say this so cynically.

35:11.960 --> 35:13.600
I will actually say this in a way that like,

35:13.600 --> 35:14.840
I want AI friends.

35:14.840 --> 35:15.680
I do.

35:15.680 --> 35:16.500
Yeah.

35:16.500 --> 35:18.840
Like I would love to, you know, again,

35:18.840 --> 35:21.240
the language models now are still a little,

35:21.240 --> 35:24.960
like people are impressed with these GPT things.

35:24.960 --> 35:28.760
And I look at like, or like, or the co-pilot,

35:28.760 --> 35:30.400
the coding one, and I'm like, okay,

35:30.400 --> 35:32.440
this is like junior engineer level.

35:32.440 --> 35:34.840
And these people are like Fiverr level artists

35:34.840 --> 35:36.120
and copywriters.

35:37.560 --> 35:38.400
Like, okay, great.

35:38.400 --> 35:40.640
We got like Fiverr and like junior engineers.

35:40.640 --> 35:41.560
Okay, cool.

35:41.560 --> 35:44.960
Like, and this is just a start and it will get better.

35:44.960 --> 35:45.800
Right?

35:45.800 --> 35:47.760
Like I can't wait to have AI friends

35:47.760 --> 35:49.920
who are more intelligent than I am.

35:49.920 --> 35:51.280
So Fiverr is just a temper.

35:51.280 --> 35:52.320
It's not the ceiling.

35:52.320 --> 35:53.480
No, definitely not.

35:54.840 --> 35:56.920
Is it, is account as cheating

35:58.000 --> 36:01.280
when you're talking to an AI model, emotional cheating?

36:03.440 --> 36:07.560
That's up to you and your human partner to define.

36:07.560 --> 36:08.800
Oh, you have to, all right.

36:08.800 --> 36:09.640
You can, yeah.

36:09.640 --> 36:12.240
You have to have that conversation, I guess.

36:12.240 --> 36:13.280
All right.

36:13.280 --> 36:16.120
I mean, integrate that with porn and all this.

36:16.120 --> 36:18.040
Well, I mean, it's similar kind of the porn.

36:18.040 --> 36:18.880
Yeah.

36:18.880 --> 36:19.720
Yeah.

36:19.720 --> 36:21.160
I think people in relationships

36:21.160 --> 36:22.960
have different views on that.

36:22.960 --> 36:26.400
Yeah, but most people don't have like

36:27.960 --> 36:31.040
serious open conversations about

36:31.040 --> 36:34.080
all the different aspects of what's cool and what's not.

36:34.080 --> 36:37.720
And it feels like AI is a really weird conversation to have.

36:37.720 --> 36:40.760
I mean, the porn one is a good branching off point.

36:40.760 --> 36:41.600
Like these things, you know,

36:41.600 --> 36:44.360
one of my scenarios that I put in my chat bot is I,

36:44.360 --> 36:45.200
you know,

36:46.280 --> 36:48.080
a nice girl named Lexi, she's 20.

36:48.080 --> 36:49.440
She just moved out to LA.

36:49.440 --> 36:50.480
She wanted to be an actress,

36:50.480 --> 36:52.040
but she started doing OnlyFans instead

36:52.040 --> 36:52.880
and you're on a date with her.

36:52.880 --> 36:53.720
Enjoy.

36:55.880 --> 36:57.280
Oh man.

36:57.280 --> 36:58.240
Yeah.

36:58.240 --> 37:00.560
And so is that if you're actually dating somebody

37:00.560 --> 37:03.240
in real life, is that cheating?

37:03.240 --> 37:04.960
I feel like it gets a little weird.

37:04.960 --> 37:05.800
Sure.

37:05.800 --> 37:06.720
It gets real weird.

37:06.720 --> 37:09.200
It's like, what are you allowed to say to an AI bot?

37:09.200 --> 37:11.640
Imagine having that conversation with a significant other.

37:11.640 --> 37:13.440
I mean, these are all things for people to define

37:13.440 --> 37:14.320
in their relationships.

37:14.320 --> 37:16.320
What it means to be human is just going to start

37:16.320 --> 37:17.160
to get weird.

37:17.160 --> 37:18.560
Especially online.

37:18.560 --> 37:19.760
Like, how do you know?

37:19.760 --> 37:22.480
Like there'll be moments when you'll have

37:22.480 --> 37:25.240
what you think is a real human you interacted with

37:25.240 --> 37:27.600
on Twitter for years and you realize it's not.

37:28.560 --> 37:32.440
I spread, I love this meme, heaven banning.

37:32.440 --> 37:33.520
Do you know what shadow banning?

37:33.520 --> 37:34.360
Yeah.

37:34.360 --> 37:36.520
Shadow banning, okay, you post, no one can see it.

37:36.520 --> 37:39.360
Heaven banning, you post, no one can see it,

37:39.360 --> 37:42.280
but a whole lot of AIs are spot up to interact with you.

37:44.200 --> 37:46.760
Well, maybe that's what the way human civilization ends

37:46.760 --> 37:48.800
is all of us are heaven banned.

37:48.800 --> 37:50.760
There's a great, it's called

37:50.760 --> 37:53.360
My Little Pony Friendship is Optimal.

37:53.360 --> 37:56.680
It's a sci-fi story that explores this idea.

37:56.760 --> 37:57.720
Friendship is optimal.

37:57.720 --> 37:58.800
Friendship is optimal.

37:58.800 --> 38:00.120
Yeah, I'd like to have some,

38:00.120 --> 38:02.280
at least on the intellectual realm,

38:02.280 --> 38:05.240
some AI friends that argue with me.

38:05.240 --> 38:10.040
But the romantic realm is weird, definitely weird.

38:11.320 --> 38:16.320
But not out of the realm of the kind of weirdness

38:16.680 --> 38:19.000
that human civilization is capable of, I think.

38:20.120 --> 38:21.720
I want it, look, I want it.

38:21.720 --> 38:23.520
If no one else wants it, I want it.

38:23.520 --> 38:25.480
Yeah, I think a lot of people probably want it.

38:25.480 --> 38:26.840
There's a deep loneliness.

38:27.680 --> 38:30.320
And I'll fill their loneliness

38:30.320 --> 38:33.640
and just will only advertise to you some of the time.

38:33.640 --> 38:36.320
Yeah, maybe the conceptions of monogamy change too.

38:36.320 --> 38:38.480
Like I grew up in a time, like I value monogamy,

38:38.480 --> 38:40.280
but maybe that's a silly notion

38:40.280 --> 38:43.280
when you have arbitrary number of AI systems.

38:44.320 --> 38:48.720
This interesting path from rationality to polyamory,

38:48.720 --> 38:50.200
that doesn't make sense for me.

38:50.200 --> 38:52.760
For you, but you're just a biological organism

38:52.760 --> 38:57.760
that was born before the internet really took off.

38:58.240 --> 39:03.120
The crazy thing is, culture is whatever we define it as.

39:03.120 --> 39:04.320
These things are not,

39:05.680 --> 39:07.800
is ought problem in moral philosophy.

39:07.800 --> 39:09.880
There's no like, okay, what is might be

39:09.880 --> 39:14.680
that computers are capable of mimicking girlfriends perfectly.

39:14.680 --> 39:16.600
They passed the girlfriend Turing test.

39:16.600 --> 39:18.120
But that doesn't say anything about ought.

39:18.120 --> 39:19.640
That doesn't say anything about how we ought

39:19.640 --> 39:21.200
to respond to them as a civilization.

39:21.200 --> 39:23.880
That doesn't say we ought to get rid of monogamy, right?

39:23.880 --> 39:25.720
That's a completely separate question,

39:25.720 --> 39:27.520
really a religious one.

39:27.520 --> 39:28.840
Girlfriend Turing test.

39:28.840 --> 39:30.120
I wonder what that looks like.

39:30.120 --> 39:31.040
Girlfriend Turing test.

39:31.040 --> 39:32.360
Are you writing that?

39:33.640 --> 39:36.600
Will you be the Alan Turing of the 21st century

39:36.600 --> 39:38.400
that writes the girlfriend Turing test?

39:38.400 --> 39:40.840
No, I mean, of course, my AI girlfriends,

39:40.840 --> 39:43.680
their goal is to pass the girlfriend Turing test.

39:43.680 --> 39:45.280
No, but there should be like a paper

39:45.280 --> 39:46.880
that kind of defines the test.

39:48.560 --> 39:51.000
I mean, the question is if it's deeply personalized

39:51.840 --> 39:54.640
or if there's a common thing that really gets everybody.

39:55.560 --> 39:57.600
Yeah, I mean, you know, look, we're a company.

39:57.600 --> 39:58.440
We don't have to get everybody.

39:58.440 --> 40:00.920
We just have to get a large enough clientele to stay with us.

40:00.920 --> 40:04.040
I like how you're already thinking company.

40:04.040 --> 40:06.520
All right, before we go to company number three

40:06.520 --> 40:09.560
and company number four, let's go to company number two.

40:09.560 --> 40:10.480
Tiny Corp.

40:11.960 --> 40:15.920
Possibly one of the greatest names of all time for a company.

40:15.920 --> 40:18.760
You've launched a new company called Tiny Corp

40:18.760 --> 40:20.920
that leads the development of Tiny Grad.

40:21.920 --> 40:25.000
What's the origin story of Tiny Corp and Tiny Grad?

40:25.000 --> 40:28.600
I started Tiny Grad as like a toy project

40:28.600 --> 40:32.480
just to teach myself, okay, like what is a convolution?

40:32.480 --> 40:34.680
What are all these options you can pass to them?

40:34.680 --> 40:36.680
What is the derivative of a convolution, right?

40:36.680 --> 40:40.800
Very similar to Carpathi micrograd, very similar.

40:41.960 --> 40:45.280
And then I started realizing,

40:45.280 --> 40:48.040
I started thinking about like AI chips.

40:48.040 --> 40:50.600
I started thinking about chips that run AI

40:50.600 --> 40:52.880
and I was like, well, okay,

40:52.880 --> 40:55.560
this is going to be a really big problem.

40:55.560 --> 40:57.840
If Nvidia becomes a monopoly here,

40:59.560 --> 41:01.520
how long before Nvidia is nationalized?

41:03.000 --> 41:07.960
So you, one of the reasons to start Tiny Corp

41:07.960 --> 41:10.240
is to challenge Nvidia.

41:10.240 --> 41:12.960
It's not so much to challenge Nvidia.

41:12.960 --> 41:17.440
I actually, I like Nvidia and it's,

41:18.080 --> 41:21.360
to make sure power stays decentralized.

41:21.360 --> 41:22.200
Yeah.

41:22.200 --> 41:25.120
And here's computational power.

41:26.080 --> 41:28.520
Until you Nvidia is kind of locking down

41:28.520 --> 41:30.560
the computational power of the world.

41:30.560 --> 41:34.960
If Nvidia becomes just like 10X better than everything else,

41:34.960 --> 41:37.200
you're giving a big advantage to somebody

41:37.200 --> 41:40.760
who can secure Nvidia as a resource.

41:41.600 --> 41:42.440
Yeah.

41:42.440 --> 41:44.560
In fact, if Jensen watches this podcast,

41:44.560 --> 41:46.360
he may want to consider this.

41:46.440 --> 41:47.920
He may want to consider making sure

41:47.920 --> 41:49.520
his company is not nationalized.

41:51.000 --> 41:52.400
You think that's an actual threat?

41:52.400 --> 41:53.240
Oh yes.

41:55.000 --> 41:58.480
No, but there's so much, you know, there's AMD.

41:58.480 --> 42:00.160
So we have Nvidia and AMD, great.

42:00.160 --> 42:01.600
All right.

42:01.600 --> 42:03.760
But you don't think there's like a push

42:05.720 --> 42:08.560
towards like selling, like Google selling TPUs

42:08.560 --> 42:09.400
or something like this.

42:09.400 --> 42:10.440
You don't think there's a push for that?

42:10.440 --> 42:11.800
Have you seen it?

42:11.800 --> 42:14.000
Google loves to rent you TPUs.

42:14.000 --> 42:15.880
It doesn't, you can't buy it at Best Buy?

42:16.880 --> 42:18.080
Hmm.

42:18.080 --> 42:22.400
So I started work on a chip.

42:22.400 --> 42:24.560
I was like, okay, what's it gonna take to make a chip?

42:24.560 --> 42:26.920
And my first notions were all completely wrong

42:26.920 --> 42:30.200
about why, about like how you can improve on GPUs.

42:30.200 --> 42:32.960
And I will take this, this is from Jim Keller

42:32.960 --> 42:34.720
on your podcast.

42:34.720 --> 42:38.080
And this is one of my absolute favorite

42:38.080 --> 42:40.520
descriptions of computation.

42:40.520 --> 42:42.960
So there's three kinds of computation paradigms

42:42.960 --> 42:44.960
that are common in the world today.

42:45.000 --> 42:47.720
There's CPUs and CPUs can do everything.

42:47.720 --> 42:50.160
CPUs can do add and multiply.

42:50.160 --> 42:51.480
They can do load and store

42:51.480 --> 42:53.320
and they can do compare and branch.

42:53.320 --> 42:54.440
And when I say they can do these things,

42:54.440 --> 42:56.400
they can do them all fast, right?

42:56.400 --> 42:58.760
So compare and branch are unique to CPUs.

42:58.760 --> 43:00.200
And what I mean by they can do them fast

43:00.200 --> 43:01.960
is they can do things like branch prediction

43:01.960 --> 43:03.200
and speculative execution.

43:03.200 --> 43:04.640
And they spend tons of transistors

43:04.640 --> 43:06.920
and they use like super deep reorder buffers

43:06.920 --> 43:08.960
in order to make these things fast.

43:08.960 --> 43:11.400
Then you have a simpler computation model GPUs.

43:11.400 --> 43:13.320
GPUs can't really do compare and branch.

43:13.320 --> 43:15.880
I mean, they can, but it's horrendously slow.

43:15.880 --> 43:18.280
But GPUs can do arbitrary load and store, right?

43:18.280 --> 43:21.560
GPUs can do things like X dereference Y.

43:21.560 --> 43:23.400
So they can fetch from arbitrary pieces of memory.

43:23.400 --> 43:25.080
They can fetch from memory that is defined

43:25.080 --> 43:26.440
by the contents of the data.

43:27.640 --> 43:29.760
The third model of computation DSPs.

43:29.760 --> 43:32.360
And DSPs are just add and multiply, right?

43:32.360 --> 43:33.360
Like they can do load and stores,

43:33.360 --> 43:34.720
but only static load and stores.

43:34.720 --> 43:36.000
Only loads and stores that are known

43:36.000 --> 43:37.680
before the program runs.

43:37.680 --> 43:39.360
And you look at neural networks today,

43:39.360 --> 43:43.000
and 95% of neural networks are all the DSP paradigm.

43:44.120 --> 43:48.080
They are just statically scheduled adds and multiplies.

43:48.080 --> 43:50.240
So TinyGuard really took this idea

43:50.240 --> 43:53.280
and I'm still working on it to extend this

43:53.280 --> 43:54.320
as far as possible.

43:55.280 --> 43:58.120
Every stage of the stack has Turing completeness, right?

43:58.120 --> 43:59.520
Python has Turing completeness.

43:59.520 --> 44:01.400
And then we take Python, we go to C++,

44:01.400 --> 44:02.240
which is Turing complete.

44:02.240 --> 44:04.880
And then maybe C++ calls into some CUDA kernels,

44:04.880 --> 44:05.920
which are Turing complete.

44:05.920 --> 44:07.360
The CUDA kernels go through LLVM,

44:07.360 --> 44:08.200
which is Turing complete,

44:08.200 --> 44:09.480
to PTX, which is Turing complete,

44:09.480 --> 44:10.640
to SAS, which is Turing complete,

44:10.640 --> 44:12.200
on a Turing complete processor.

44:12.200 --> 44:13.240
I want to get Turing completeness

44:13.240 --> 44:14.880
out of the stack entirely.

44:14.880 --> 44:16.280
Because once you get rid of Turing completeness,

44:16.280 --> 44:17.840
you can reason about things.

44:17.840 --> 44:19.400
Rice's theorem in the halting problem,

44:19.400 --> 44:21.040
do not apply to admiral machines.

44:23.520 --> 44:24.360
Okay.

44:24.360 --> 44:25.840
What's the power and the value

44:25.840 --> 44:28.520
of getting Turing completeness out of...

44:28.520 --> 44:31.000
Are we talking about the hardware or the software?

44:31.000 --> 44:32.280
Every layer of the stack.

44:32.280 --> 44:33.120
Every layer.

44:33.120 --> 44:34.080
Every layer of the stack,

44:34.080 --> 44:35.480
removing Turing completeness

44:35.480 --> 44:37.760
allows you to reason about things, right?

44:37.760 --> 44:40.400
So the reason you need to do branch prediction in a CPU,

44:40.400 --> 44:41.520
and the reason it's prediction,

44:41.520 --> 44:42.480
and the branch predictors are,

44:42.480 --> 44:45.000
I think they're like 99% on CPUs.

44:45.000 --> 44:46.960
Why do they get 1% of them wrong?

44:46.960 --> 44:51.000
Well, they get 1% wrong because you can't know, right?

44:51.000 --> 44:51.960
That's the halting problem.

44:51.960 --> 44:53.280
It's equivalent to the halting problem

44:53.280 --> 44:56.480
to say whether a branch is going to be taken or not.

44:56.480 --> 44:57.320
I can show that.

44:57.320 --> 45:01.320
But the admiral machine,

45:01.320 --> 45:05.360
the neural network runs the identical compute every time.

45:05.360 --> 45:07.360
The only thing that changes is the data.

45:08.760 --> 45:10.440
So when you realize this,

45:10.440 --> 45:12.880
you think about, okay, how can we build a computer?

45:12.880 --> 45:14.000
How can we build a stack

45:14.000 --> 45:16.120
that takes maximal advantage of this idea?

45:18.040 --> 45:20.280
So what makes TinyGrad different

45:20.280 --> 45:22.320
from other neural network libraries

45:22.320 --> 45:24.400
is it does not have a primitive operator

45:24.400 --> 45:26.560
even for matrix multiplication.

45:26.560 --> 45:28.360
And this is every single one.

45:28.360 --> 45:29.520
They even have primitive operations

45:29.520 --> 45:30.920
for things like convolutions.

45:30.920 --> 45:32.640
So no matmul.

45:32.640 --> 45:33.600
No matmul.

45:33.600 --> 45:35.080
Well, here's what a matmul is.

45:35.080 --> 45:37.000
So I'll use my hands to talk here.

45:37.000 --> 45:38.480
So if you think about a cube

45:38.480 --> 45:40.480
and I put my two matrices that I'm multiplying

45:40.480 --> 45:43.000
on two faces of the cube, right?

45:43.000 --> 45:45.480
You can think about the matrix multiply as,

45:45.480 --> 45:47.400
okay, the N cubed,

45:47.400 --> 45:49.520
I'm gonna multiply for each one in the cubed

45:49.520 --> 45:50.640
and then I'm gonna do a sum,

45:50.640 --> 45:53.800
which is a reduce up to here to the third face of the cube

45:53.800 --> 45:55.920
and that's your multiplied matrix.

45:55.920 --> 45:57.560
So what a matrix multiply is

45:57.560 --> 45:59.400
is a bunch of shape operations, right?

45:59.400 --> 46:01.720
A bunch of permute three shapes and expands

46:01.720 --> 46:03.400
on the two matrices.

46:03.440 --> 46:05.800
A multiply N cubed,

46:05.800 --> 46:07.640
a reduce N cubed,

46:07.640 --> 46:09.640
which gives you an N squared matrix.

46:09.640 --> 46:12.080
Okay, so what is the minimum number of operations

46:12.080 --> 46:13.000
that can accomplish that

46:13.000 --> 46:16.160
if you don't have matmul as a primitive?

46:16.160 --> 46:18.320
So TinyGrad has about 20.

46:18.320 --> 46:22.320
And you can compare TinyGrad's Opset or IR

46:22.320 --> 46:25.280
to things like XLA or PrimTorch.

46:25.280 --> 46:28.280
So XLA and PrimTorch are ideas where like, okay,

46:28.280 --> 46:30.920
Torch has like 2000 different kernels.

46:31.120 --> 46:34.640
PyTorch 2.0 introduced PrimTorch,

46:34.640 --> 46:35.760
which has only 250.

46:36.800 --> 46:39.280
TinyGrad has order of magnitude 25.

46:39.280 --> 46:43.320
It's 10X less than XLA or PrimTorch.

46:43.320 --> 46:46.920
And you can think about it as kind of like risk versus CISC,

46:46.920 --> 46:47.760
right?

46:47.760 --> 46:51.320
These other things are CISC like systems.

46:51.320 --> 46:53.000
TinyGrad is risk.

46:53.000 --> 46:54.640
And risk one.

46:54.640 --> 46:56.960
Risk architecture is gonna change everything.

46:56.960 --> 46:59.320
1995, hackers.

46:59.320 --> 47:00.160
Wait, really?

47:00.160 --> 47:01.000
That's an actual thing?

47:01.000 --> 47:03.120
Angelina Jolie delivers the line.

47:03.120 --> 47:06.200
Risk architecture is gonna change everything in 1995.

47:06.200 --> 47:08.520
And here we are with ARM in the phones.

47:08.520 --> 47:10.080
And ARM everywhere.

47:10.080 --> 47:11.640
Wow, I love it when movies

47:11.640 --> 47:13.160
actually have real things in them.

47:13.160 --> 47:14.400
Right?

47:14.400 --> 47:15.320
Okay, interesting.

47:15.320 --> 47:16.480
And so this is like,

47:16.480 --> 47:19.240
so you're thinking of this as the risk architecture

47:19.240 --> 47:21.280
of ML Stack.

47:22.400 --> 47:23.600
25, huh?

47:23.600 --> 47:26.160
What, can you go through the,

47:27.000 --> 47:29.680
the four op types?

47:29.680 --> 47:31.040
Sure.

47:31.040 --> 47:32.760
Okay, so you have unary ops,

47:32.760 --> 47:36.800
which take in a tensor

47:36.800 --> 47:38.560
and return a tensor of the same size

47:38.560 --> 47:39.800
and do some unary op to it.

47:39.800 --> 47:43.760
X, log, reciprocal, sine, right?

47:43.760 --> 47:46.360
They take in one and they're point-wise.

47:46.360 --> 47:47.280
Rally-U.

47:47.280 --> 47:48.760
Yeah, Rally-U.

47:48.760 --> 47:51.600
Almost all activation functions are unary ops.

47:51.600 --> 47:53.800
Some combinations of unary ops together

47:53.800 --> 47:54.880
is still a unary op.

47:56.400 --> 47:57.480
Then you have binary ops.

47:57.480 --> 47:59.800
Binary ops are like point-wise addition,

47:59.800 --> 48:01.960
multiplication, division, compare.

48:02.960 --> 48:05.440
It takes in two tensors of equal size

48:05.440 --> 48:06.600
and outputs one tensor.

48:08.000 --> 48:09.920
Then you have reduce ops.

48:09.920 --> 48:12.440
Reduce ops will take a three-dimensional tensor

48:12.440 --> 48:14.560
and turn it into a two-dimensional tensor.

48:14.560 --> 48:15.720
Or a three-dimensional tensor

48:15.720 --> 48:17.040
turn it into a zero-dimensional tensor.

48:17.040 --> 48:19.200
Think like a sum or a max

48:19.200 --> 48:21.720
are really the common ones there.

48:21.720 --> 48:23.920
And then the fourth type is movement ops.

48:23.920 --> 48:25.840
And movement ops are different from the other types

48:25.840 --> 48:27.640
because they don't actually require computation.

48:27.640 --> 48:30.080
They require different ways to look at memory.

48:30.080 --> 48:34.800
So that includes reshapes, permutes, expands, flips.

48:34.800 --> 48:35.880
Those are the main ones.

48:35.880 --> 48:38.800
So with that you have enough to make a matmul.

48:38.800 --> 48:39.800
And convolutions.

48:39.800 --> 48:41.560
And every convolution you can imagine,

48:41.560 --> 48:43.920
dilated convolutions, strided convolutions,

48:43.920 --> 48:45.600
transposed convolutions.

48:46.880 --> 48:49.280
You write on GitHub about laziness

48:50.640 --> 48:53.680
showing a matmul matrix multiplication.

48:53.680 --> 48:55.240
See how despite the style,

48:55.240 --> 48:58.600
it is fused into one kernel with the power of laziness.

48:58.600 --> 49:01.120
Can you elaborate on this power of laziness?

49:01.120 --> 49:01.960
Sure.

49:01.960 --> 49:05.880
So if you type in PyTorch, A times B plus C,

49:06.720 --> 49:08.480
what this is going to do

49:08.480 --> 49:12.000
is it's going to first multiply A and B

49:12.000 --> 49:14.400
and store that result into memory.

49:14.400 --> 49:15.840
And then it is going to add C

49:15.840 --> 49:17.760
by reading that result from memory,

49:17.760 --> 49:21.600
reading C from memory and writing that out to memory.

49:21.600 --> 49:23.920
There is way more loads and stores to memory

49:23.920 --> 49:25.160
than you need there.

49:25.160 --> 49:28.680
If you don't actually do A times B as soon as you see it,

49:28.680 --> 49:32.760
if you wait until the user actually realizes that tensor,

49:32.760 --> 49:35.000
until the laziness actually resolves,

49:35.000 --> 49:36.800
you confuse that plus C.

49:36.800 --> 49:39.120
This is like, it's the same way Haskell works.

49:39.120 --> 49:44.120
So what's the process of porting a model into TinyGrad?

49:44.240 --> 49:47.600
So TinyGrad's front end looks very similar to PyTorch.

49:47.600 --> 49:49.960
I probably could make a perfect

49:49.960 --> 49:51.840
or pretty close to perfect interop layer

49:51.840 --> 49:53.000
if I really wanted to.

49:53.000 --> 49:54.560
I think that there's some things that are nicer

49:54.560 --> 49:56.200
about TinyGrad syntax than PyTorch,

49:56.200 --> 49:58.000
but the front end looks very Torch-like.

49:58.000 --> 50:00.200
You can also load in Onyx models.

50:00.200 --> 50:03.360
We have more Onyx tests passing than CoreML.

50:04.680 --> 50:06.400
CoreML, okay, so-

50:06.400 --> 50:08.200
We'll pass Onyx runtime soon.

50:08.200 --> 50:11.000
What about the developer experience with TinyGrad?

50:12.040 --> 50:16.800
What it feels like versus PyTorch?

50:16.800 --> 50:18.120
By the way, I really like PyTorch.

50:18.120 --> 50:20.920
I think that it's actually a very good piece of software.

50:20.920 --> 50:23.920
I think that they've made a few different trade-offs,

50:23.920 --> 50:27.200
and these different trade-offs are where,

50:27.200 --> 50:29.760
you know, TinyGrad takes a different path.

50:29.760 --> 50:32.680
One of the biggest differences is it's really easy to see

50:32.680 --> 50:35.920
the kernels that are actually being sent to the GPU, right?

50:35.920 --> 50:38.520
If you run PyTorch on the GPU,

50:38.520 --> 50:40.040
you like do some operation

50:40.040 --> 50:41.080
and you don't know what kernels ran.

50:41.080 --> 50:42.560
You don't know how many kernels ran.

50:42.560 --> 50:44.200
You don't know how many flops were used.

50:44.200 --> 50:46.400
You don't know how much memory accesses were used.

50:46.400 --> 50:48.760
TinyGrad type debug equals two,

50:48.760 --> 50:51.840
and it will show you in this beautiful style

50:51.840 --> 50:53.120
every kernel that's run.

50:54.440 --> 50:57.560
How many flops and how many bytes.

50:58.480 --> 51:03.480
So can you just linger on what problem TinyGrad solves?

51:04.320 --> 51:05.640
TinyGrad solves the problem

51:05.640 --> 51:09.000
of porting new ML accelerators quickly.

51:09.000 --> 51:12.120
One of the reasons, tons of these companies now,

51:12.120 --> 51:16.800
I think Sequoia marked Graphcore to zero, right?

51:16.800 --> 51:20.720
Seribus, TensTorrent, Grok,

51:20.720 --> 51:24.400
all of these ML accelerator companies, they built chips.

51:24.400 --> 51:25.600
The chips were good.

51:25.600 --> 51:26.880
The software was terrible.

51:28.160 --> 51:29.000
And part of the reason is

51:29.000 --> 51:31.440
because I think the same problem is happening with Dojo.

51:31.440 --> 51:35.040
It's really, really hard to write a PyTorch port

51:35.040 --> 51:37.480
because you have to write 250 kernels

51:37.480 --> 51:40.360
and you have to tune them all for performance.

51:40.520 --> 51:42.960
What does Jim Culler think about TinyGrad?

51:44.240 --> 51:45.600
You guys hung out quite a bit,

51:45.600 --> 51:49.800
so he was involved with TensTorrent.

51:49.800 --> 51:52.400
What's his praise and what's his criticism

51:52.400 --> 51:54.800
of what you're doing with your life?

51:54.800 --> 51:58.400
Look, my prediction for TensTorrent

51:58.400 --> 52:01.000
is that they're gonna pivot to making RISC-V chips.

52:02.160 --> 52:03.280
CPUs.

52:03.280 --> 52:06.040
CPUs, why?

52:07.040 --> 52:10.800
Because AI accelerators are a software problem,

52:10.800 --> 52:12.120
not really a hardware problem.

52:12.120 --> 52:14.680
Oh, interesting, so you don't think...

52:14.680 --> 52:17.400
You think the diversity of AI accelerators

52:17.400 --> 52:18.520
in the hardware space

52:18.520 --> 52:21.560
is not going to be a thing that exists long-term?

52:21.560 --> 52:24.680
I think what's gonna happen is if I can finish...

52:24.680 --> 52:28.360
Okay, if you're trying to make an AI accelerator,

52:28.360 --> 52:30.440
you better have the capability

52:30.440 --> 52:35.440
of writing a Torch-level performance stack on NVIDIA GPUs.

52:35.480 --> 52:37.880
If you can't write a Torch stack on NVIDIA GPUs,

52:37.880 --> 52:39.960
and I mean all the way, I mean down to the driver,

52:39.960 --> 52:42.120
there's no way you're gonna be able to write it on your chip

52:42.120 --> 52:44.120
because your chip's worse than an NVIDIA GPU.

52:44.120 --> 52:45.520
The first version of the chip you tape out,

52:45.520 --> 52:46.800
it's definitely worse.

52:46.800 --> 52:48.800
Oh, you're saying writing that stack is really tough.

52:48.800 --> 52:50.280
Yes, and not only that, actually,

52:50.280 --> 52:51.680
the chip that you tape out,

52:51.680 --> 52:53.200
almost always because you're trying to get advantage

52:53.200 --> 52:55.760
over NVIDIA, you're specializing the hardware more.

52:55.760 --> 52:57.360
It's always harder to write software

52:57.360 --> 52:58.960
for more specialized hardware.

52:58.960 --> 53:00.480
Like a GPU is pretty generic,

53:00.480 --> 53:02.560
and if you can't write an NVIDIA stack,

53:02.560 --> 53:05.160
there's no way you can write a stack for your chip.

53:06.040 --> 53:07.480
My approach with TinyGrad is,

53:07.480 --> 53:09.560
first, write a performant NVIDIA stack.

53:09.560 --> 53:10.680
We're targeting AMD.

53:12.720 --> 53:15.240
So, you did say a few to NVIDIA a little bit.

53:15.240 --> 53:16.080
With love.

53:16.080 --> 53:16.920
With love.

53:16.920 --> 53:17.760
Yeah, with love.

53:17.760 --> 53:18.960
It's like the Yankees, you know?

53:18.960 --> 53:20.040
I'm a Mets fan.

53:20.040 --> 53:21.920
Oh, you're a Mets fan.

53:21.920 --> 53:24.280
A RISC fan and a Mets fan.

53:24.280 --> 53:26.000
What's the hope that AMD has?

53:26.000 --> 53:30.520
You did Build with AMD recently that I saw.

53:30.560 --> 53:35.520
How does the 7900 XTX

53:35.520 --> 53:38.680
compare to the RTX 4090 or 4080?

53:38.680 --> 53:39.680
Well, let's start with the fact

53:39.680 --> 53:42.760
that the 7900 XTX kernel drivers don't work,

53:42.760 --> 53:46.200
and if you run demo apps in loops, it panics the kernel.

53:46.200 --> 53:48.400
Okay, so this is a software issue?

53:49.320 --> 53:51.320
Lisa Su responded to my email.

53:51.320 --> 53:52.160
Oh.

53:52.160 --> 53:53.000
I reached out.

53:53.000 --> 53:56.760
I was like, this is, you know, really?

53:56.760 --> 53:57.600
Yeah.

53:57.600 --> 53:59.960
I understand if your seven by seven

53:59.960 --> 54:02.920
transposed Winograd COM is slower than NVIDIA's,

54:02.920 --> 54:05.640
but literally when I run demo apps in a loop,

54:05.640 --> 54:08.160
the kernel panics.

54:08.160 --> 54:10.400
So, just adding that loop.

54:10.400 --> 54:12.200
Yeah, I just literally took their demo apps

54:12.200 --> 54:15.480
and wrote like while true semicolon do the app semicolon

54:15.480 --> 54:17.760
done in a bunch of screens, right?

54:17.760 --> 54:20.440
This is like the most primitive fuzz testing.

54:20.440 --> 54:21.960
Why do you think that is?

54:21.960 --> 54:26.640
They're just not seeing a market in machine learning?

54:26.640 --> 54:27.560
They're changing.

54:27.560 --> 54:28.760
They're trying to change.

54:28.760 --> 54:29.680
They're trying to change.

54:29.680 --> 54:31.920
And I had a pretty positive interaction with them this week.

54:31.920 --> 54:33.240
Last week, I went on YouTube.

54:33.240 --> 54:34.240
I was just like, that's it.

54:34.240 --> 54:35.360
I give up on AMD.

54:35.360 --> 54:37.520
Like this is the, their driver doesn't even like,

54:37.520 --> 54:39.440
I'm not gonna, I'm not gonna, you know,

54:39.440 --> 54:41.240
I'll go with Intel GPUs, right?

54:41.240 --> 54:42.840
Intel GPUs have better drivers.

54:44.840 --> 54:47.000
So you're kind of spearheading

54:47.000 --> 54:50.600
the diversification of GPUs.

54:50.600 --> 54:52.040
Yeah, and I'd like to extend

54:52.040 --> 54:53.680
that diversification to everything.

54:53.680 --> 54:57.080
I'd like to diversify the, right?

54:57.080 --> 55:02.080
The more, my central thesis about the world is

55:02.680 --> 55:04.880
there's things that centralize power and they're bad.

55:04.880 --> 55:07.800
And there's things that decentralize power and they're good.

55:07.800 --> 55:09.960
Everything I can do to help decentralize power,

55:09.960 --> 55:10.800
I'd like to do.

55:12.520 --> 55:14.200
So you're really worried about the centralization

55:14.200 --> 55:15.360
of Nvidia, that's interesting.

55:15.360 --> 55:17.240
And you don't have a fundamental hope

55:17.240 --> 55:22.240
for the proliferation of ASICs, except in the cloud.

55:23.840 --> 55:25.280
I'd like to help them with software.

55:25.280 --> 55:27.160
No, actually there's only, the only ASIC

55:27.160 --> 55:30.000
that is remotely successful is Google's TPU.

55:30.000 --> 55:31.480
And the only reason that's successful

55:31.480 --> 55:34.960
is because Google wrote a machine learning framework.

55:34.960 --> 55:36.520
I think that you have to write

55:36.520 --> 55:38.320
a competitive machine learning framework

55:38.320 --> 55:40.160
in order to be able to build an ASIC.

55:41.800 --> 55:45.080
You think Meta with PyTorch builds a competitor?

55:45.080 --> 55:46.240
I hope so.

55:46.240 --> 55:48.040
They have one, they have an internal one.

55:48.040 --> 55:50.040
Internal, I mean, public facing

55:50.040 --> 55:52.400
with a nice cloud interface and so on.

55:52.400 --> 55:53.920
I don't want a cloud.

55:53.920 --> 55:54.760
You don't like cloud?

55:54.760 --> 55:55.840
I don't like cloud.

55:55.840 --> 55:58.400
What do you think is the fundamental limitation of cloud?

55:58.400 --> 55:59.680
Fundamental limitation of cloud

55:59.680 --> 56:01.880
is who owns the off switch.

56:01.880 --> 56:03.800
So it's power to the people.

56:03.800 --> 56:04.760
Yeah.

56:04.760 --> 56:07.240
And you don't like the man to have all the power.

56:07.240 --> 56:08.080
Exactly.

56:08.080 --> 56:09.400
All right.

56:09.400 --> 56:10.800
And right now the only way to do that

56:10.800 --> 56:14.840
is with Nvidia GPUs if you want performance and stability.

56:16.360 --> 56:20.480
Interesting, it's a costly investment emotionally

56:20.480 --> 56:21.680
to go with AMDs.

56:22.720 --> 56:24.760
Well, let me ask sort of on a tangent to ask you,

56:24.760 --> 56:28.080
what, you've built quite a few PCs.

56:28.080 --> 56:31.240
What's your advice on how to build a good custom PC

56:31.240 --> 56:33.720
for let's say for the different applications that you use

56:33.720 --> 56:35.760
for gaming, for machine learning?

56:35.760 --> 56:36.640
Well, you shouldn't build one.

56:36.640 --> 56:39.440
You should buy a box from the tiny corp.

56:39.440 --> 56:42.480
I heard rumors, whispers

56:42.480 --> 56:44.480
about this box in the tiny corp.

56:44.480 --> 56:45.760
What's this thing look like?

56:45.760 --> 56:46.600
What is it?

56:46.600 --> 56:47.720
What is it called?

56:47.720 --> 56:48.800
It's called the tiny box.

56:48.800 --> 56:49.640
Tiny box?

56:49.800 --> 56:51.640
It's $15,000.

56:51.640 --> 56:54.680
And it's almost a pay to flop of compute.

56:54.680 --> 56:57.760
It's over a hundred gigabytes of GPU RAM.

56:57.760 --> 57:00.000
It's over five terabytes per second

57:00.000 --> 57:01.920
of GPU memory bandwidth.

57:03.640 --> 57:07.000
I'm gonna put like four NVMe's in RAID.

57:07.000 --> 57:09.720
You're gonna get like 20, 30 gigabytes per second

57:09.720 --> 57:11.920
of drive read bandwidth.

57:11.920 --> 57:16.880
I'm gonna build like the best deep learning box that I can

57:16.880 --> 57:19.280
that plugs into one wall outlet.

57:19.800 --> 57:21.000
Can you go through those specs again

57:21.000 --> 57:23.120
in a little bit from memory?

57:23.120 --> 57:25.000
Yeah, so it's almost a pay to flop of compute.

57:25.000 --> 57:26.800
So AMD Intel?

57:26.800 --> 57:28.760
Today, I'm leaning toward AMD.

57:30.320 --> 57:33.600
But we're pretty agnostic to the type of compute.

57:33.600 --> 57:38.600
The main limiting spec is a 120 volt 15 amp circuit.

57:41.280 --> 57:43.400
Well, I mean it because in order to like,

57:43.400 --> 57:45.280
there's a plug over there, right?

57:46.320 --> 57:48.080
You have to be able to plug it in.

57:49.320 --> 57:51.320
We're also gonna sell the tiny rack,

57:51.320 --> 57:54.440
which like, what's the most power you can get into your house

57:54.440 --> 57:56.520
without arousing suspicion?

57:56.520 --> 57:59.800
And one of the answers is an electric car charger.

57:59.800 --> 58:01.880
Wait, where does the rack go?

58:01.880 --> 58:03.280
Your garage.

58:03.280 --> 58:05.560
Interesting, the car charger.

58:05.560 --> 58:08.040
A wall outlet is about 1500 Watts.

58:08.040 --> 58:10.120
A car charger is about 10,000 Watts.

58:11.440 --> 58:14.880
What is the most amount of power you can get your hands on

58:14.880 --> 58:16.160
without arousing suspicion?

58:16.160 --> 58:17.000
That's right.

58:17.000 --> 58:17.840
George Hawts.

58:17.840 --> 58:22.640
Okay, so the tiny box, and you said NVMe's and RAID.

58:24.000 --> 58:26.480
I forget what you said about memory, all that kind of stuff.

58:26.480 --> 58:29.160
Okay, so what about with GPUs?

58:29.160 --> 58:32.320
Again, probably 7900 XTX's,

58:32.320 --> 58:35.640
but maybe 3090s, maybe a 770s.

58:35.640 --> 58:36.480
Those are Intel's.

58:36.480 --> 58:39.280
You're flexible or still exploring?

58:39.280 --> 58:40.520
I'm still exploring.

58:40.520 --> 58:44.200
I wanna deliver a really good experience to people.

58:44.200 --> 58:47.160
And yeah, what GPUs I end up going with, again,

58:47.160 --> 58:50.280
I'm leaning toward AMD, we'll see.

58:50.280 --> 58:53.440
In my email, what I said to AMD is like,

58:53.440 --> 58:56.400
just dumping the code on GitHub is not open source.

58:56.400 --> 58:57.600
Open source is a culture.

58:58.640 --> 59:00.480
Open source means that your issues

59:00.480 --> 59:03.320
are not all one-year-old stale issues.

59:03.320 --> 59:06.760
Open source means developing in public.

59:06.760 --> 59:08.760
And if you guys can commit to that,

59:08.760 --> 59:11.520
I see a real future for AMD as a competitor to a video.

59:13.200 --> 59:16.040
Well, I'd love to get a tiny box to MIT.

59:16.080 --> 59:18.520
So whenever it's ready, let's do it.

59:18.520 --> 59:19.360
We're taking pre-orders.

59:19.360 --> 59:20.280
I took this from Elon.

59:20.280 --> 59:23.320
I'm like, all right, $100 fully refundable pre-orders.

59:23.320 --> 59:24.680
Is it gonna be like the cyber truck

59:24.680 --> 59:26.280
that's gonna take a few years or?

59:26.280 --> 59:27.480
No, I'll try to do it faster.

59:27.480 --> 59:28.440
It's a lot simpler.

59:28.440 --> 59:30.040
It's a lot simpler than a truck.

59:30.040 --> 59:31.480
Well, there's complexities,

59:31.480 --> 59:34.320
not to just the putting the thing together,

59:34.320 --> 59:35.960
but like shipping and all this kind of stuff.

59:35.960 --> 59:38.200
The thing that I wanna deliver to people out of the box

59:38.200 --> 59:42.640
is being able to run 65 billion parameter llama in FP16

59:42.640 --> 59:45.040
in real time, in like a good, like 10 tokens per second

59:45.120 --> 59:46.640
or five tokens per second or something.

59:46.640 --> 59:48.360
Just it works.

59:48.360 --> 59:52.800
Llamas running or something like llama.

59:52.800 --> 59:55.920
Experience, yeah, or I think Falcon is the new one.

59:55.920 --> 59:58.840
Experience a chat with the largest language model

59:58.840 --> 01:00:00.560
that you can have in your house.

01:00:00.560 --> 01:00:02.600
Yeah, from a wall plug.

01:00:02.600 --> 01:00:03.640
From a wall plug, yeah.

01:00:03.640 --> 01:00:05.240
Actually, for inference,

01:00:05.240 --> 01:00:08.280
it's not like even more power would help you get more.

01:00:08.280 --> 01:00:11.000
Oh, even more power wouldn't get you more.

01:00:11.000 --> 01:00:12.040
Well, no, there's just the biggest parameter.

01:00:12.040 --> 01:00:12.920
The biggest model released

01:00:13.320 --> 01:00:16.280
is 65 billion parameter llama as far as I know.

01:00:16.280 --> 01:00:17.520
So it sounds like Tiny Box

01:00:17.520 --> 01:00:20.360
will naturally pivot towards company number three

01:00:20.360 --> 01:00:22.440
because you could just get the girlfriend

01:00:22.440 --> 01:00:25.200
and or boyfriend.

01:00:26.160 --> 01:00:27.400
That one's harder, actually.

01:00:27.400 --> 01:00:28.360
The boyfriend is harder?

01:00:28.360 --> 01:00:29.200
The boyfriend's harder, yeah.

01:00:29.200 --> 01:00:32.040
I think that's a very biased statement.

01:00:32.040 --> 01:00:34.000
I think a lot of people would disagree.

01:00:34.920 --> 01:00:38.760
Why is it harder to replace a boyfriend

01:00:38.760 --> 01:00:41.240
than the girlfriend with the artificial LLM?

01:00:41.280 --> 01:00:43.520
Because women are attracted to status and power

01:00:43.520 --> 01:00:45.680
and men are attracted to youth and beauty.

01:00:47.120 --> 01:00:49.000
No, I mean, that's what I mean.

01:00:49.000 --> 01:00:51.960
Both are mimicable, easy through the language model.

01:00:51.960 --> 01:00:56.200
No, no machines do not have any status or real power.

01:00:56.200 --> 01:00:57.120
I don't know.

01:00:57.120 --> 01:00:59.160
I think you both, well, first of all,

01:00:59.160 --> 01:01:04.160
you're using language mostly to communicate

01:01:04.880 --> 01:01:07.680
youth and beauty and power and status.

01:01:07.680 --> 01:01:10.200
But status fundamentally is a zero-sum game,

01:01:10.200 --> 01:01:12.200
whereas youth and beauty are not.

01:01:12.200 --> 01:01:15.280
No, I think status is a narrative you can construct.

01:01:15.280 --> 01:01:17.080
I don't think status is real.

01:01:18.840 --> 01:01:19.680
I don't know.

01:01:19.680 --> 01:01:21.720
I just think that that's why it's harder.

01:01:21.720 --> 01:01:23.200
Maybe it is my biases.

01:01:23.200 --> 01:01:25.440
I think status is way easier to fake.

01:01:25.440 --> 01:01:28.360
I also think that men are probably more desperate

01:01:28.360 --> 01:01:29.600
and more likely to buy my product,

01:01:29.600 --> 01:01:31.680
so maybe they're a better target market.

01:01:31.680 --> 01:01:33.440
Desperation is interesting.

01:01:33.440 --> 01:01:34.560
Easier to fool.

01:01:35.680 --> 01:01:36.840
I can see that.

01:01:36.840 --> 01:01:37.840
Yeah, look, I mean, look,

01:01:37.840 --> 01:01:39.920
I know you can look at porn viewership numbers, right?

01:01:40.640 --> 01:01:41.720
A lot more men watch porn than women.

01:01:41.720 --> 01:01:43.680
You can ask why that is.

01:01:43.680 --> 01:01:46.480
Wow, there's a lot of questions and answers

01:01:46.480 --> 01:01:47.440
you can get there.

01:01:49.480 --> 01:01:52.080
Anyway, with the tiny box,

01:01:52.080 --> 01:01:53.880
how many GPUs in tiny box?

01:01:53.880 --> 01:01:54.720
Six.

01:01:54.720 --> 01:01:59.480
Oh, man.

01:01:59.480 --> 01:02:01.320
And I'll tell you why it's six.

01:02:01.320 --> 01:02:05.600
So AMD EPYC processors have 128 lanes of PCIe.

01:02:06.600 --> 01:02:11.600
I want to leave enough lanes for some drives,

01:02:12.480 --> 01:02:15.880
and I want to leave enough lanes for some networking.

01:02:15.880 --> 01:02:18.200
How do you do cooling for something like this?

01:02:18.200 --> 01:02:19.840
That's one of the big challenges.

01:02:19.840 --> 01:02:21.720
Not only do I want the cooling to be good,

01:02:21.720 --> 01:02:22.960
I want it to be quiet.

01:02:22.960 --> 01:02:25.040
I want the tiny box to be able to sit comfortably

01:02:25.040 --> 01:02:26.120
in your room, right?

01:02:26.120 --> 01:02:29.520
This is really going towards the girlfriend thing,

01:02:29.520 --> 01:02:31.280
because you want to run an LLM.

01:02:31.280 --> 01:02:32.680
I'll give them more.

01:02:32.680 --> 01:02:34.000
I mean, I can talk about how it relates

01:02:34.000 --> 01:02:35.160
to company number one.

01:02:36.040 --> 01:02:36.880
Call my AI.

01:02:37.880 --> 01:02:40.120
Well, but yeah, it's quiet.

01:02:40.120 --> 01:02:41.920
Oh, quiet because you maybe potentially

01:02:41.920 --> 01:02:43.040
want to run it in a car?

01:02:43.040 --> 01:02:44.960
No, no, quiet because you want to put this thing

01:02:44.960 --> 01:02:46.880
in your house, and you want it to coexist with you.

01:02:46.880 --> 01:02:48.400
If it's screaming at 60 dB,

01:02:48.400 --> 01:02:49.240
you don't want that in your house.

01:02:49.240 --> 01:02:50.080
You'll kick it out.

01:02:50.080 --> 01:02:51.640
60 dB, yeah.

01:02:51.640 --> 01:02:52.760
I want like 40, 45.

01:02:52.760 --> 01:02:55.080
So how do you make the cooling quiet?

01:02:55.080 --> 01:02:57.120
That's an interesting problem in itself.

01:02:57.120 --> 01:02:58.800
A key trick is to actually make it big.

01:02:58.800 --> 01:03:00.720
Ironically, it's called the tiny box.

01:03:00.720 --> 01:03:02.200
But if I can make it big,

01:03:02.200 --> 01:03:03.560
a lot of that noise is generated

01:03:04.120 --> 01:03:05.600
of high pressure air.

01:03:05.600 --> 01:03:07.480
If you look at like a 1U server,

01:03:07.480 --> 01:03:09.840
a 1U server has these super high pressure fans

01:03:09.840 --> 01:03:12.400
that are like super deep, and they're like genesis.

01:03:12.400 --> 01:03:14.600
Versus if you have something that's big,

01:03:14.600 --> 01:03:15.760
well, I can use a big,

01:03:15.760 --> 01:03:17.560
and you know they call them big ass fans,

01:03:17.560 --> 01:03:19.760
those ones that are like huge on the ceiling,

01:03:19.760 --> 01:03:21.360
and they're completely silent.

01:03:21.360 --> 01:03:24.720
So tiny box will be big.

01:03:24.720 --> 01:03:29.200
It is the, I do not want it to be large according to UPS.

01:03:29.200 --> 01:03:31.000
I want it to be shippable as a normal package,

01:03:31.000 --> 01:03:32.800
but that's my constraint there.

01:03:32.840 --> 01:03:33.760
Interesting.

01:03:33.760 --> 01:03:35.600
Well, the fans stuff,

01:03:35.600 --> 01:03:38.040
can't it be assembled on location or no?

01:03:38.040 --> 01:03:38.880
No.

01:03:38.880 --> 01:03:41.600
Oh, it has to be, well, you're...

01:03:41.600 --> 01:03:43.440
Look, I wanna give you a great out of the box experience.

01:03:43.440 --> 01:03:44.560
I want you to lift this thing out.

01:03:44.560 --> 01:03:47.000
I want it to be like the Mac, you know?

01:03:47.000 --> 01:03:48.000
Tiny box.

01:03:48.000 --> 01:03:49.080
The Apple experience.

01:03:49.080 --> 01:03:50.520
Yeah.

01:03:50.520 --> 01:03:51.400
I love it.

01:03:51.400 --> 01:03:52.240
Okay.

01:03:52.240 --> 01:03:56.200
And so tiny box would run tiny grad.

01:03:56.200 --> 01:03:59.400
Like what do you envision this whole thing to look like?

01:03:59.440 --> 01:04:02.880
We're talking about like Linux

01:04:02.880 --> 01:04:06.960
with the full software engineering environment,

01:04:07.920 --> 01:04:10.400
and it's just not PyTorch, but tiny grad.

01:04:10.400 --> 01:04:11.240
Yeah.

01:04:11.240 --> 01:04:12.880
We did a poll of people want Ubuntu or Arch.

01:04:12.880 --> 01:04:14.440
We're gonna stick with Ubuntu.

01:04:14.440 --> 01:04:15.280
Ooh, interesting.

01:04:15.280 --> 01:04:17.720
What's your favorite flavor of Linux?

01:04:17.720 --> 01:04:18.560
Ubuntu.

01:04:18.560 --> 01:04:19.400
Ubuntu.

01:04:19.400 --> 01:04:20.720
I like Ubuntu Mate.

01:04:20.720 --> 01:04:22.520
However you pronounce that, meat.

01:04:23.920 --> 01:04:27.240
So how do you, you've gotten llama into tiny grad.

01:04:27.240 --> 01:04:29.160
You've gotten stable diffusion into tiny grad.

01:04:29.880 --> 01:04:30.720
What was that like?

01:04:30.720 --> 01:04:34.520
Can you comment on like, what are these models?

01:04:34.520 --> 01:04:36.640
What's interesting about porting them?

01:04:36.640 --> 01:04:39.320
So what's, yeah, like what are the challenges?

01:04:39.320 --> 01:04:41.120
What's naturally, what's easy?

01:04:41.120 --> 01:04:41.960
All that kind of stuff.

01:04:41.960 --> 01:04:43.200
There's a really simple way

01:04:43.200 --> 01:04:44.640
to get these models into tiny grad,

01:04:44.640 --> 01:04:46.480
and you can just export them as onyx,

01:04:46.480 --> 01:04:48.080
and then tiny grad can run onyx.

01:04:49.000 --> 01:04:52.120
So the ports that I did of llama, stable diffusion,

01:04:52.120 --> 01:04:54.600
and now whisper are more academic

01:04:54.600 --> 01:04:56.400
to teach me about the models,

01:04:56.400 --> 01:04:58.960
but they are cleaner than the PyTorch versions.

01:04:59.800 --> 01:05:00.640
You can read the code.

01:05:00.640 --> 01:05:01.480
I think the code is easier to read.

01:05:01.480 --> 01:05:02.960
It's less lines.

01:05:02.960 --> 01:05:03.920
There's just a few things

01:05:03.920 --> 01:05:05.400
about the way tiny grad writes things.

01:05:05.400 --> 01:05:07.880
Here's a complaint I have about PyTorch.

01:05:07.880 --> 01:05:10.800
NN.relu is a class, right?

01:05:10.800 --> 01:05:12.960
So when you create a, when you create an NN module,

01:05:12.960 --> 01:05:17.600
you'll put your NN relus as in init.

01:05:17.600 --> 01:05:18.520
And this makes no sense.

01:05:18.520 --> 01:05:20.720
Relu is completely stateless.

01:05:20.720 --> 01:05:22.080
Why should that be a class?

01:05:23.400 --> 01:05:26.000
But that's more like a software engineering thing.

01:05:26.000 --> 01:05:28.120
Or do you think it has a cost on performance?

01:05:28.120 --> 01:05:30.480
Oh no, it doesn't have a cost on performance.

01:05:30.480 --> 01:05:32.560
But yeah, no, I think that it's,

01:05:32.560 --> 01:05:34.440
that's what I mean about like tiny grad's front end

01:05:34.440 --> 01:05:35.800
being cleaner.

01:05:35.800 --> 01:05:37.160
I see.

01:05:37.160 --> 01:05:38.120
What do you think about Mojo?

01:05:38.120 --> 01:05:39.440
I don't know if you've been paying attention

01:05:39.440 --> 01:05:40.640
to the programming language

01:05:40.640 --> 01:05:43.080
that does some interesting ideas

01:05:43.080 --> 01:05:46.120
that kind of intersect tiny grad.

01:05:46.120 --> 01:05:48.240
I think that there is a spectrum.

01:05:48.240 --> 01:05:50.080
And like on one side you have Mojo,

01:05:50.080 --> 01:05:52.640
and on the other side you have like GGML.

01:05:52.640 --> 01:05:56.640
GGML is this like, we're gonna run llama fast on Mac.

01:05:56.680 --> 01:05:58.160
Okay, we're gonna expand out to a little bit,

01:05:58.160 --> 01:06:01.160
but we're gonna basically like depth first, right?

01:06:01.160 --> 01:06:02.880
Mojo is like, we're gonna go breadth first.

01:06:02.880 --> 01:06:03.960
We're gonna go so wide

01:06:03.960 --> 01:06:05.960
that we're gonna make all of Python fast.

01:06:05.960 --> 01:06:07.400
And tiny grad's in the middle.

01:06:08.520 --> 01:06:11.280
Tiny grad is, we are going to make neural networks fast.

01:06:12.560 --> 01:06:17.440
Yeah, but they try to really get it to be fast,

01:06:17.440 --> 01:06:20.760
compiled down to specific hardware

01:06:20.760 --> 01:06:22.720
and make that compilation step

01:06:23.600 --> 01:06:25.800
as flexible and resilient as possible.

01:06:25.840 --> 01:06:28.000
Yeah, but they have Turing completeness.

01:06:28.000 --> 01:06:29.520
And that limits you.

01:06:29.520 --> 01:06:30.360
Turing?

01:06:30.360 --> 01:06:31.200
That's what you're saying.

01:06:31.200 --> 01:06:32.020
It's somewhere in the middle.

01:06:32.020 --> 01:06:34.800
So you're actually going to be targeting some accelerators,

01:06:34.800 --> 01:06:38.840
some, like some number, not one.

01:06:38.840 --> 01:06:41.400
My goal is step one,

01:06:41.400 --> 01:06:44.040
build an equally performance stack to PyTorch

01:06:44.040 --> 01:06:48.120
on Nvidia and AMD, but with way less lines.

01:06:48.120 --> 01:06:49.960
And then step two is, okay,

01:06:49.960 --> 01:06:51.520
how do we make an accelerator, right?

01:06:51.520 --> 01:06:52.480
But you need step one.

01:06:52.480 --> 01:06:54.080
You have to first build the framework

01:06:54.080 --> 01:06:56.120
before you can build the accelerator.

01:06:56.120 --> 01:06:58.200
Can you explain MLPerf?

01:06:58.200 --> 01:06:59.600
What's your approach in general

01:06:59.600 --> 01:07:02.160
to benchmarking tiny grad performance?

01:07:02.160 --> 01:07:07.160
So I'm much more of a, like, build it the right way

01:07:08.220 --> 01:07:09.980
and worry about performance later.

01:07:11.400 --> 01:07:13.760
There's a bunch of things where I haven't even like

01:07:13.760 --> 01:07:15.040
really dove into performance.

01:07:15.040 --> 01:07:16.480
The only place where tiny grad

01:07:16.480 --> 01:07:18.400
is competitive performance wise right now

01:07:18.400 --> 01:07:20.720
is on Qualcomm GPUs.

01:07:20.720 --> 01:07:23.400
So tiny grad's actually used in OpenPilot to run the model.

01:07:23.400 --> 01:07:25.480
So the driving model is tiny grad.

01:07:25.480 --> 01:07:27.800
When did that happen, that transition?

01:07:27.800 --> 01:07:29.280
About eight months ago now.

01:07:30.600 --> 01:07:33.400
And it's 2X faster than Qualcomm's library.

01:07:33.400 --> 01:07:38.200
What's the hardware that OpenPilot runs on the Kamae app?

01:07:38.200 --> 01:07:40.340
It's a Snapdragon 845.

01:07:40.340 --> 01:07:41.180
Okay.

01:07:41.180 --> 01:07:42.080
So this is using the GPU.

01:07:42.080 --> 01:07:44.680
So the GPU is an Adreno GPU.

01:07:44.680 --> 01:07:46.120
There's like different things.

01:07:46.120 --> 01:07:47.640
There's a really good Microsoft paper

01:07:47.640 --> 01:07:49.600
that talks about like mobile GPUs

01:07:49.600 --> 01:07:52.520
and why they're different from desktop GPUs.

01:07:52.520 --> 01:07:55.160
One of the big things is in a desktop GPU,

01:07:55.160 --> 01:07:58.240
you can use buffers on a mobile GPU,

01:07:58.240 --> 01:07:59.840
image textures are a lot faster.

01:08:01.760 --> 01:08:04.960
On a mobile GPU, image textures, an image, okay.

01:08:04.960 --> 01:08:08.360
And so you want to be able to leverage that.

01:08:08.360 --> 01:08:09.600
I want to be able to leverage it

01:08:09.600 --> 01:08:11.600
in a way that it's completely generic, right?

01:08:11.600 --> 01:08:12.560
So there's a lot of this,

01:08:12.560 --> 01:08:14.880
Xiaomi has a pretty good open source library

01:08:14.880 --> 01:08:16.600
for mobile GPUs called Mace,

01:08:16.600 --> 01:08:19.460
where they can generate, where they have these kernels,

01:08:19.460 --> 01:08:21.380
but they're all hand coded, right?

01:08:21.380 --> 01:08:23.860
So that's great if you're doing three by three comps.

01:08:23.860 --> 01:08:25.660
That's great if you're doing dense map models,

01:08:25.660 --> 01:08:28.660
but the minute you go off the beaten path a tiny bit,

01:08:28.660 --> 01:08:30.660
well, your performance is nothing.

01:08:30.660 --> 01:08:31.860
Since you mentioned OpenPilot,

01:08:31.860 --> 01:08:33.140
I'd love to get an update

01:08:34.100 --> 01:08:37.180
in the company number one, Kamae world.

01:08:37.180 --> 01:08:40.260
How are things going there in the development

01:08:40.260 --> 01:08:42.660
of semi-autonomous driving?

01:08:45.300 --> 01:08:48.700
You know, almost no one talks about FSD anymore

01:08:48.700 --> 01:08:51.300
and even less people talk about OpenPilot.

01:08:52.220 --> 01:08:55.220
We've solved the problem, like we solved it years ago.

01:08:55.220 --> 01:08:57.300
What's the problem exactly?

01:08:57.300 --> 01:09:00.580
Well, what does solving it mean?

01:09:00.580 --> 01:09:02.380
Solving means how do you build a model

01:09:02.380 --> 01:09:04.720
that outputs a human policy for driving?

01:09:05.820 --> 01:09:07.700
How do you build a model that given

01:09:07.700 --> 01:09:09.660
our reasonable set of sensors

01:09:09.660 --> 01:09:11.460
outputs a human policy for driving?

01:09:12.740 --> 01:09:14.580
So you have companies like Waymoon Cruise,

01:09:14.580 --> 01:09:15.940
which are hand coding these things

01:09:15.940 --> 01:09:18.360
that are like quasi-human policies.

01:09:19.360 --> 01:09:24.360
Then you have Tesla and maybe even to more of an extent,

01:09:24.480 --> 01:09:25.920
Kamae, asking, okay,

01:09:25.920 --> 01:09:27.920
how do we just learn human policy and data?

01:09:29.320 --> 01:09:31.040
The big thing that we're doing now,

01:09:31.040 --> 01:09:32.760
and we just put it out on Twitter,

01:09:34.240 --> 01:09:35.640
at the beginning of Kamae,

01:09:36.640 --> 01:09:39.720
we published a paper called Learning a Driving Simulator.

01:09:40.840 --> 01:09:43.480
And the way this thing worked was it's a,

01:09:43.480 --> 01:09:48.280
it was an auto encoder and then an RNN in the middle, right?

01:09:49.200 --> 01:09:51.520
You take an auto encoder, you compress the picture,

01:09:51.520 --> 01:09:53.800
you use an RNN, predict the next date,

01:09:53.800 --> 01:09:55.360
and these things were,

01:09:55.360 --> 01:09:57.320
you know, it was a laughably bad simulator, right?

01:09:57.320 --> 01:10:00.400
This is 2015 error machine learning technology.

01:10:00.400 --> 01:10:04.020
Today, we have VQVAE and Transformers.

01:10:04.020 --> 01:10:06.800
We're building DriveGPT, basically.

01:10:06.800 --> 01:10:08.980
DriveGPT, okay.

01:10:10.280 --> 01:10:12.200
So, and it's trained on what?

01:10:12.200 --> 01:10:14.760
Is it trained in a self-supervised way?

01:10:14.760 --> 01:10:16.160
It's trained on all the driving data

01:10:16.160 --> 01:10:17.620
to predict the next frame.

01:10:17.620 --> 01:10:20.900
So, really trying to learn a human policy.

01:10:20.900 --> 01:10:21.740
What would a human do?

01:10:21.740 --> 01:10:24.220
Well, actually, our simulator's conditioned on the pose.

01:10:24.220 --> 01:10:25.460
So, it's actually a simulator.

01:10:25.460 --> 01:10:27.020
You can put in like a state action pair

01:10:27.020 --> 01:10:28.820
and get out the next state.

01:10:28.820 --> 01:10:29.980
Okay.

01:10:29.980 --> 01:10:31.940
And then once you have a simulator,

01:10:31.940 --> 01:10:34.100
you can do RL in the simulator

01:10:34.100 --> 01:10:36.820
and RL will get us that human policy.

01:10:36.820 --> 01:10:38.300
So, it transfers.

01:10:38.300 --> 01:10:39.780
Yay.

01:10:39.780 --> 01:10:41.740
RL with a reward function,

01:10:41.740 --> 01:10:43.780
not asking is this close to the human policy,

01:10:43.780 --> 01:10:45.220
but asking would a human disengage

01:10:45.220 --> 01:10:46.460
if you did this behavior?

01:10:47.460 --> 01:10:50.020
Okay, let me think about the distinction there.

01:10:50.020 --> 01:10:51.640
Would a human disengage?

01:10:52.860 --> 01:10:54.860
Would a human disengage?

01:10:54.860 --> 01:10:58.820
That correlates, I guess, with the human policy,

01:10:58.820 --> 01:11:00.180
but it could be different.

01:11:00.180 --> 01:11:03.420
So, it doesn't just say what would a human do.

01:11:03.420 --> 01:11:06.900
It says what would a good human driver do

01:11:06.900 --> 01:11:09.720
and such that the experience is comfortable,

01:11:10.580 --> 01:11:12.020
but also not annoying

01:11:12.020 --> 01:11:14.580
and that the thing is very cautious.

01:11:14.580 --> 01:11:16.100
So, it's finding a nice balance.

01:11:16.420 --> 01:11:17.260
That's interesting.

01:11:17.260 --> 01:11:18.100
It's a nice...

01:11:18.100 --> 01:11:20.100
It's asking exactly the right question.

01:11:20.100 --> 01:11:22.060
What will make our customers happy?

01:11:23.060 --> 01:11:23.900
Right.

01:11:23.900 --> 01:11:25.420
A system that you never want to disengage.

01:11:25.420 --> 01:11:29.220
Because usually disengagement is almost always

01:11:29.220 --> 01:11:32.140
a sign of I'm not happy with what the system is doing.

01:11:32.140 --> 01:11:33.020
Usually.

01:11:33.020 --> 01:11:35.180
There's some that are just, I felt like driving

01:11:35.180 --> 01:11:36.500
and those are always fine too,

01:11:36.500 --> 01:11:39.220
but they're just gonna look like noise in the data.

01:11:39.220 --> 01:11:41.580
But even I felt like driving.

01:11:41.580 --> 01:11:42.420
Maybe, yeah.

01:11:42.420 --> 01:11:43.700
That's even that's a signal.

01:11:43.700 --> 01:11:45.660
Like, why do you feel like driving?

01:11:46.660 --> 01:11:51.060
You need to recalibrate your relationship with the car.

01:11:51.060 --> 01:11:54.180
Okay, so that's really interesting.

01:11:54.180 --> 01:11:56.300
How close are we to solving self-driving?

01:11:59.380 --> 01:12:01.020
It's hard to say.

01:12:01.020 --> 01:12:03.460
We haven't completely closed the loop yet.

01:12:03.460 --> 01:12:04.860
So, we don't have anything built

01:12:04.860 --> 01:12:07.180
that truly looks like that architecture yet.

01:12:07.180 --> 01:12:10.060
We have prototypes and there's bugs.

01:12:10.060 --> 01:12:12.940
So, we are a couple of bug fixes away.

01:12:12.940 --> 01:12:15.140
Might take a year, might take 10.

01:12:15.180 --> 01:12:16.780
What's the nature of the bugs?

01:12:16.780 --> 01:12:20.180
Are these major philosophical bugs?

01:12:20.180 --> 01:12:21.020
Logical bugs?

01:12:21.020 --> 01:12:22.740
What kind of bugs are we talking about?

01:12:22.740 --> 01:12:24.460
They're just like stupid bugs

01:12:24.460 --> 01:12:26.780
and like also we might just need more scale.

01:12:26.780 --> 01:12:30.020
We just massively expanded our compute cluster at Comma.

01:12:31.140 --> 01:12:33.580
We now have about two people worth of compute.

01:12:33.580 --> 01:12:34.460
40 beta-flops.

01:12:36.100 --> 01:12:38.300
Well, people are different.

01:12:39.540 --> 01:12:40.420
20 beta-flops.

01:12:40.420 --> 01:12:41.260
That's a person.

01:12:41.260 --> 01:12:42.300
It's just a unit, right?

01:12:42.300 --> 01:12:43.180
Horses are different too,

01:12:43.180 --> 01:12:45.020
but we still call it a horsepower.

01:12:45.860 --> 01:12:47.580
Yeah, but there's something different about mobility

01:12:47.580 --> 01:12:51.340
than there is about perception and action

01:12:51.340 --> 01:12:53.180
in a very complicated world.

01:12:53.180 --> 01:12:54.020
But yes.

01:12:54.020 --> 01:12:54.860
Well, yeah, of course.

01:12:54.860 --> 01:12:55.780
Not all flops are created equal.

01:12:55.780 --> 01:12:58.500
If you have randomly initialized weights, it's not gonna.

01:12:58.500 --> 01:13:00.620
Not all flops are created equal.

01:13:00.620 --> 01:13:03.060
So, flops are doing way more useful things than others.

01:13:03.060 --> 01:13:04.380
Yeah, yep.

01:13:05.300 --> 01:13:06.540
Tell me about it.

01:13:06.540 --> 01:13:07.860
Okay, so more data.

01:13:07.860 --> 01:13:09.580
Scale means more scale in compute

01:13:09.580 --> 01:13:11.620
or scale in scale of data?

01:13:11.620 --> 01:13:12.460
Both.

01:13:13.460 --> 01:13:14.780
And diversity of data?

01:13:14.780 --> 01:13:16.620
Diversity is very important in data.

01:13:18.100 --> 01:13:19.420
Yeah, I mean, we have,

01:13:19.420 --> 01:13:23.300
so we have about, I think we have like 5,000 daily actives.

01:13:24.700 --> 01:13:27.900
How would you evaluate how FSD is doing?

01:13:27.900 --> 01:13:28.740
Pretty well.

01:13:28.740 --> 01:13:29.580
What's that driving?

01:13:29.580 --> 01:13:30.420
Pretty well.

01:13:30.420 --> 01:13:33.380
How's that race going between Comma AI and FSD?

01:13:33.380 --> 01:13:35.340
Tesla has always wanted two years ahead of us.

01:13:35.340 --> 01:13:37.140
They've always been one to two years ahead of us.

01:13:37.140 --> 01:13:38.340
And they probably always will be

01:13:38.340 --> 01:13:40.540
because they're not doing anything wrong.

01:13:40.540 --> 01:13:42.100
What have you seen that's happening

01:13:42.620 --> 01:13:43.660
since the last time we talked

01:13:43.660 --> 01:13:45.340
that are interesting architectural decisions,

01:13:45.340 --> 01:13:48.180
training decisions, like the way they deploy stuff,

01:13:48.180 --> 01:13:51.020
the architectures they're using in terms of the software,

01:13:51.020 --> 01:13:52.540
how the teams are run, all that kind of stuff,

01:13:52.540 --> 01:13:54.620
data collection, anything interesting?

01:13:54.620 --> 01:13:55.940
I mean, I know they're moving toward

01:13:55.940 --> 01:13:58.020
more of an end-to-end approach.

01:13:58.020 --> 01:14:01.700
So creeping towards end-to-end as much as possible

01:14:01.700 --> 01:14:03.060
across the whole thing,

01:14:03.060 --> 01:14:05.140
the training, the data collection, everything.

01:14:05.140 --> 01:14:06.940
They also have a very fancy simulator.

01:14:06.940 --> 01:14:08.860
They're probably saying all the same things we are.

01:14:08.860 --> 01:14:11.500
They're probably saying we just need to optimize, you know,

01:14:11.500 --> 01:14:12.340
what is the reward?

01:14:12.340 --> 01:14:14.100
Will you get negative reward for disengagement, right?

01:14:14.100 --> 01:14:16.020
Like, everyone kind of knows this.

01:14:16.020 --> 01:14:17.380
It's just a question of who can actually build

01:14:17.380 --> 01:14:18.940
and deploy the system.

01:14:18.940 --> 01:14:21.860
Yeah, I mean, this requires good software engineering,

01:14:21.860 --> 01:14:23.220
I think. Yeah.

01:14:23.220 --> 01:14:25.860
And the right kind of hardware.

01:14:25.860 --> 01:14:27.900
Yeah, and the hardware to run it.

01:14:27.900 --> 01:14:30.340
You still don't believe in cloud in that regard?

01:14:31.380 --> 01:14:36.380
I have a compute cluster in my office, 800 amps.

01:14:36.740 --> 01:14:37.580
Tiny grad.

01:14:37.580 --> 01:14:40.220
It's 40 kilowatts at idle, our data center.

01:14:40.220 --> 01:14:41.260
Diving crazy.

01:14:42.020 --> 01:14:42.860
We have 40 kilowatts just burning

01:14:42.860 --> 01:14:44.100
just when the computers are idle.

01:14:44.100 --> 01:14:44.940
Just when I-

01:14:44.940 --> 01:14:46.540
Oh, sorry, sorry, compute cluster.

01:14:48.060 --> 01:14:49.180
Compute cluster, I got it.

01:14:49.180 --> 01:14:50.100
It's not a data center.

01:14:50.100 --> 01:14:50.940
Yeah, yeah.

01:14:50.940 --> 01:14:52.140
No, data centers are clouds.

01:14:52.140 --> 01:14:53.860
We don't have clouds.

01:14:53.860 --> 01:14:55.020
Data centers have air conditioners.

01:14:55.020 --> 01:14:56.180
We have fans.

01:14:56.180 --> 01:14:57.780
That makes it a compute cluster.

01:14:59.460 --> 01:15:02.420
I'm guessing this is a kind of legal distinction

01:15:02.420 --> 01:15:03.340
as to what to make. Sure, yeah.

01:15:03.340 --> 01:15:05.300
We have a compute cluster.

01:15:05.300 --> 01:15:07.780
You said that you don't think LLMs have consciousness,

01:15:07.780 --> 01:15:09.580
or at least not more than a chicken.

01:15:10.420 --> 01:15:12.380
Do you think they can reason?

01:15:12.380 --> 01:15:14.900
Is there something interesting to you about the word reason,

01:15:14.900 --> 01:15:16.340
about some of the capabilities

01:15:16.340 --> 01:15:18.060
that we think is kind of human,

01:15:18.060 --> 01:15:23.060
to be able to integrate complicated information

01:15:23.300 --> 01:15:27.780
and through a chain of thought,

01:15:27.780 --> 01:15:31.140
arrive at a conclusion that feels novel,

01:15:31.140 --> 01:15:35.460
a novel integration of disparate facts?

01:15:36.620 --> 01:15:39.220
Yeah, I don't think that there's,

01:15:39.860 --> 01:15:42.180
I think that they can reason better than a lot of people.

01:15:42.180 --> 01:15:44.340
Hey, isn't that amazing to you though?

01:15:44.340 --> 01:15:45.580
Isn't that like an incredible thing

01:15:45.580 --> 01:15:47.580
that a transformer can achieve?

01:15:47.580 --> 01:15:50.580
I mean, I think that calculators can add better

01:15:50.580 --> 01:15:52.180
than a lot of people.

01:15:52.180 --> 01:15:54.700
But language feels like reasoning

01:15:54.700 --> 01:15:56.340
through the process of language,

01:15:56.340 --> 01:15:59.180
which looks a lot like thought.

01:16:00.580 --> 01:16:02.820
Making brilliancies in chess,

01:16:02.820 --> 01:16:04.860
which feels a lot like thought.

01:16:04.860 --> 01:16:07.020
Whatever new thing that AI can do,

01:16:07.020 --> 01:16:08.260
everybody thinks is brilliant,

01:16:08.260 --> 01:16:09.820
and then like 20 years go by and they're like,

01:16:09.820 --> 01:16:11.380
well, yeah, but chess, that's like mechanical.

01:16:11.380 --> 01:16:12.980
Like adding, that's like mechanical.

01:16:12.980 --> 01:16:14.940
So you think language is not that special?

01:16:14.940 --> 01:16:15.940
It's like chess.

01:16:15.940 --> 01:16:16.780
It's like chess and it's like-

01:16:16.780 --> 01:16:19.060
I don't know, and because it's very human,

01:16:19.060 --> 01:16:23.060
we take it, listen, there is something different

01:16:23.060 --> 01:16:25.780
between chess and language.

01:16:25.780 --> 01:16:29.180
Chess is a game that a subset of population plays.

01:16:29.180 --> 01:16:32.100
Language is something we use nonstop

01:16:32.100 --> 01:16:34.340
for all of our human interaction,

01:16:34.340 --> 01:16:37.220
and human interaction is fundamental to society.

01:16:37.300 --> 01:16:39.300
So it's like, holy shit,

01:16:39.300 --> 01:16:42.660
this language thing is not so difficult

01:16:42.660 --> 01:16:45.980
to like create in a machine.

01:16:45.980 --> 01:16:48.500
The problem is if you go back to 1960

01:16:48.500 --> 01:16:50.420
and you tell them that you have a machine

01:16:50.420 --> 01:16:54.020
that can play amazing chess,

01:16:54.020 --> 01:16:55.740
of course someone in 1960 will tell you

01:16:55.740 --> 01:16:57.580
that machine is intelligent.

01:16:57.580 --> 01:17:00.500
Someone in 2010 won't, what's changed, right?

01:17:00.500 --> 01:17:02.020
Today, we think that these machines

01:17:02.020 --> 01:17:04.180
that have language are intelligent,

01:17:04.180 --> 01:17:05.740
but I think in 20 years, we're gonna be like,

01:17:05.740 --> 01:17:07.220
yeah, but can it reproduce?

01:17:08.540 --> 01:17:11.460
So reproduction, yeah, we might redefine

01:17:11.460 --> 01:17:14.300
what it means to be, what is it,

01:17:14.300 --> 01:17:17.300
a high-performance living organism on Earth?

01:17:17.300 --> 01:17:19.940
Humans are always gonna define a niche for themselves.

01:17:19.940 --> 01:17:21.740
Like, well, we're better than the machines

01:17:21.740 --> 01:17:24.860
because we can, and like they tried creative for a bit,

01:17:24.860 --> 01:17:26.940
but no one believes that one anymore.

01:17:26.940 --> 01:17:29.900
But niche, is that delusional

01:17:29.900 --> 01:17:31.380
or is there some accuracy to that?

01:17:31.380 --> 01:17:33.660
Because maybe like with chess, you start to realize

01:17:33.660 --> 01:17:38.500
that we have ill-conceived notions

01:17:38.500 --> 01:17:41.820
of what makes humans special,

01:17:41.820 --> 01:17:44.740
like the apex organism on Earth.

01:17:46.500 --> 01:17:48.500
Yeah, and I think maybe we're gonna go through

01:17:48.500 --> 01:17:50.100
that same thing with language

01:17:50.980 --> 01:17:52.820
and that same thing with creativity.

01:17:53.820 --> 01:17:57.420
But language carries these notions of truth and so on.

01:17:57.420 --> 01:17:59.060
And so we might be like, wait,

01:17:59.060 --> 01:18:01.780
maybe truth is not carried by language.

01:18:01.780 --> 01:18:03.260
Maybe there's like a deeper thing.

01:18:03.260 --> 01:18:05.220
The niche is getting smaller.

01:18:05.220 --> 01:18:06.060
Oh, boy.

01:18:07.580 --> 01:18:09.340
But no, no, no, you don't understand.

01:18:09.340 --> 01:18:10.820
Humans are created by God

01:18:10.820 --> 01:18:12.540
and machines are created by humans.

01:18:12.540 --> 01:18:16.060
Therefore, right, like that'll be the last niche we have.

01:18:16.060 --> 01:18:17.460
So what do you think about this,

01:18:17.460 --> 01:18:19.100
the rapid development of LLMs?

01:18:19.100 --> 01:18:20.980
If we could just like stick on that.

01:18:20.980 --> 01:18:23.260
It's still incredibly impressive, like with Chagy BT.

01:18:23.260 --> 01:18:24.820
Just even Chagy BT, what are your thoughts

01:18:24.820 --> 01:18:27.460
about reinforcement learning with human feedback

01:18:27.460 --> 01:18:29.060
on these large language models?

01:18:30.020 --> 01:18:33.420
I'd like to go back to when calculators first came out

01:18:34.500 --> 01:18:36.060
or computers.

01:18:36.060 --> 01:18:37.380
And like, I wasn't around.

01:18:37.380 --> 01:18:39.620
Look, I'm 33 years old.

01:18:39.620 --> 01:18:43.420
And to like see how that affected

01:18:45.700 --> 01:18:47.860
like society.

01:18:47.860 --> 01:18:48.700
Maybe you're right.

01:18:48.700 --> 01:18:53.580
So I wanna put on the big picture hat here.

01:18:53.580 --> 01:18:55.860
Oh my God, a refrigerator, wow.

01:18:55.860 --> 01:18:58.340
Refrigerator, electricity, all that kind of stuff.

01:18:59.220 --> 01:19:02.220
But, you know, with the internet,

01:19:03.060 --> 01:19:05.220
large language models seeming human-like,

01:19:05.220 --> 01:19:06.900
basically passing a Turing test.

01:19:07.860 --> 01:19:10.700
It seems it might have really at scale

01:19:10.700 --> 01:19:13.660
rapid transformative effects on society.

01:19:13.660 --> 01:19:17.140
But you're saying like other technologies have as well.

01:19:17.140 --> 01:19:20.620
So maybe calculator's not the best example of that.

01:19:20.620 --> 01:19:24.540
Cause that just seems like, well, no, maybe calculator.

01:19:24.540 --> 01:19:25.500
But the poor milk man,

01:19:25.500 --> 01:19:27.180
the day he learned about refrigerators,

01:19:27.180 --> 01:19:28.180
he's like, I'm done.

01:19:30.020 --> 01:19:32.380
You're telling me you can just keep the milk in your house?

01:19:32.380 --> 01:19:34.940
You don't need me to deliver it every day, I'm done.

01:19:34.940 --> 01:19:36.900
Well, yeah, you have to actually look at the practical

01:19:36.900 --> 01:19:40.380
impacts of certain technologies that they've had.

01:19:40.380 --> 01:19:42.060
Yeah, probably electricity is a big one.

01:19:42.060 --> 01:19:44.580
And also how rapidly it's spread.

01:19:44.580 --> 01:19:46.140
Man, the internet is a big one.

01:19:46.140 --> 01:19:48.100
I do think it's different this time, though.

01:19:48.100 --> 01:19:49.420
Yeah, it just feels like-

01:19:49.420 --> 01:19:51.620
The niche is getting smaller.

01:19:51.620 --> 01:19:55.420
The niche of humans that makes humans special.

01:19:56.420 --> 01:19:59.380
It feels like it's getting smaller rapidly though,

01:19:59.380 --> 01:20:00.220
doesn't it?

01:20:00.220 --> 01:20:02.820
Or is that just a feeling we dramatize everything?

01:20:02.820 --> 01:20:04.100
I think we dramatize everything.

01:20:04.100 --> 01:20:06.700
I think that you asked the milk man

01:20:06.700 --> 01:20:08.380
when he saw refrigerators.

01:20:08.380 --> 01:20:10.860
And they're gonna have one of these in every home?

01:20:10.860 --> 01:20:14.420
Yeah, yeah, yeah.

01:20:15.460 --> 01:20:18.820
Yeah, but boys are impressive.

01:20:18.820 --> 01:20:21.820
So much more impressive than seeing a chess

01:20:21.820 --> 01:20:23.380
world champion AI system.

01:20:23.380 --> 01:20:25.180
I disagree actually.

01:20:25.660 --> 01:20:27.180
I disagree.

01:20:27.180 --> 01:20:29.460
I think things like Mu Zero and AlphaGo

01:20:29.460 --> 01:20:31.580
are so much more impressive.

01:20:31.580 --> 01:20:33.620
Because these things are playing

01:20:33.620 --> 01:20:35.940
beyond the highest human level.

01:20:37.980 --> 01:20:41.700
The language models are writing middle school level essays

01:20:41.700 --> 01:20:43.900
and people are like, wow, it's a great essay.

01:20:43.900 --> 01:20:45.460
It's a great five paragraph essay

01:20:45.460 --> 01:20:47.260
about the causes of the civil war.

01:20:47.260 --> 01:20:50.100
Okay, forget the civil war, just generating code, codex.

01:20:51.340 --> 01:20:53.540
You're saying it's mediocre code.

01:20:53.540 --> 01:20:54.380
Terrible.

01:20:54.860 --> 01:20:55.740
I don't think it's terrible.

01:20:55.740 --> 01:20:58.380
I think it's just mediocre code.

01:20:58.380 --> 01:21:03.060
Often close to correct, like for mediocre purposes.

01:21:03.060 --> 01:21:04.860
That's the scariest kind of code.

01:21:04.860 --> 01:21:08.020
I spent 5% of time typing and 95% of time debugging.

01:21:08.020 --> 01:21:10.780
The last thing I want is close to correct code.

01:21:10.780 --> 01:21:12.700
I want a machine that can help me with the debugging,

01:21:12.700 --> 01:21:14.020
not with the typing.

01:21:14.020 --> 01:21:18.260
It's like level two driving, similar kind of thing.

01:21:18.260 --> 01:21:21.060
You still should be a good programmer

01:21:21.060 --> 01:21:24.060
in order to modify, I wouldn't even say debugging,

01:21:24.700 --> 01:21:26.100
just modifying the code, reading it.

01:21:26.100 --> 01:21:28.500
I actually don't think it's like level two driving.

01:21:28.500 --> 01:21:30.940
I think driving is not tool complete and programming is.

01:21:30.940 --> 01:21:34.100
Meaning you don't use the best possible tools to drive.

01:21:37.900 --> 01:21:39.500
Cars have basically the same interface

01:21:39.500 --> 01:21:40.940
for the last 50 years.

01:21:40.940 --> 01:21:43.220
Computers have a radically different interface.

01:21:43.220 --> 01:21:47.140
Can you describe the concept of tool complete?

01:21:47.140 --> 01:21:49.500
So think about the difference between a car from 1980

01:21:49.500 --> 01:21:50.940
and a car from today.

01:21:50.940 --> 01:21:52.020
No difference really.

01:21:52.020 --> 01:21:52.860
It's got a bunch of pedals.

01:21:52.860 --> 01:21:54.380
It's got a steering wheel.

01:21:54.380 --> 01:21:55.220
Great.

01:21:55.220 --> 01:21:57.300
Maybe now it has a few ADAS features,

01:21:57.300 --> 01:21:58.820
but it's pretty much the same car.

01:21:58.820 --> 01:22:02.380
You have no problem getting into a 1980 car and driving it.

01:22:02.380 --> 01:22:03.420
You take a programmer today

01:22:03.420 --> 01:22:05.300
who spent their whole life doing JavaScript

01:22:05.300 --> 01:22:07.420
and you put them in an Apple IIe prompt

01:22:07.420 --> 01:22:09.820
and you tell them about the line numbers in BASIC.

01:22:11.660 --> 01:22:15.340
But how do I insert something between line 17 and 18?

01:22:15.340 --> 01:22:16.340
Oh, wow.

01:22:17.340 --> 01:22:22.340
But in tool, you're putting in the programming languages.

01:22:22.620 --> 01:22:25.180
So it's just the entirety stack of the tooling.

01:22:25.180 --> 01:22:27.620
So it's not just like the IDs or something like this.

01:22:27.620 --> 01:22:28.660
It's everything.

01:22:28.660 --> 01:22:30.620
Yes, it's IDEs, the languages, the runtimes.

01:22:30.620 --> 01:22:31.460
It's everything.

01:22:31.460 --> 01:22:33.300
And programming is tool complete.

01:22:33.300 --> 01:22:38.300
So almost if Codex or Copilot are helping you,

01:22:39.900 --> 01:22:41.100
that actually probably means

01:22:41.100 --> 01:22:42.940
that your framework or library is bad

01:22:42.940 --> 01:22:44.780
and there's too much boilerplate in it.

01:22:47.060 --> 01:22:48.540
Yeah, but don't you think

01:22:48.540 --> 01:22:50.700
so much programming has boilerplate?

01:22:50.700 --> 01:22:54.220
TinyGrad is now 2,700 lines

01:22:54.220 --> 01:22:56.700
and it can run llama and stable diffusion.

01:22:56.700 --> 01:23:00.340
And all of this stuff is in 2,700 lines.

01:23:00.340 --> 01:23:04.340
Boilerplate and abstraction indirections

01:23:04.340 --> 01:23:06.980
and all these things are just bad code.

01:23:08.420 --> 01:23:13.420
Well, let's talk about good code and bad code.

01:23:13.940 --> 01:23:16.020
There's a, I would say, I don't know,

01:23:16.020 --> 01:23:19.100
for generic scripts that I write just offhand,

01:23:19.100 --> 01:23:22.780
like 80% of it is written by GPT.

01:23:22.780 --> 01:23:25.180
Just like quick offhand stuff.

01:23:25.180 --> 01:23:27.980
So not like libraries, not like performing code,

01:23:27.980 --> 01:23:30.980
not stuff for robotics and so on, just quick stuff.

01:23:30.980 --> 01:23:33.140
Because your basic, so much of programming

01:23:33.140 --> 01:23:36.500
is doing some, yeah, boilerplate.

01:23:36.500 --> 01:23:38.980
But to do so efficiently and quickly,

01:23:40.860 --> 01:23:42.940
because you can't really automate it fully

01:23:43.460 --> 01:23:45.700
with generic method, like a generic kind of

01:23:47.220 --> 01:23:49.980
IDE type of recommendation or something like this,

01:23:49.980 --> 01:23:51.980
you do need to have some of the complexity

01:23:51.980 --> 01:23:53.420
of language models.

01:23:53.420 --> 01:23:55.700
Yeah, I guess if I was really writing,

01:23:55.700 --> 01:23:59.780
maybe today if I wrote a lot of data parsing stuff,

01:23:59.780 --> 01:24:00.940
I mean, I don't play CTFs anymore,

01:24:00.940 --> 01:24:02.940
but if I still play CTFs, a lot of it's just like

01:24:02.940 --> 01:24:05.460
you have to write a parser for this data format.

01:24:05.460 --> 01:24:08.500
I wonder, or like admin of code,

01:24:08.500 --> 01:24:10.740
I wonder when the models are gonna start to help

01:24:10.740 --> 01:24:11.820
with that kind of code.

01:24:11.820 --> 01:24:14.060
And they may, they may, and the models also

01:24:14.060 --> 01:24:15.460
may help you with speed.

01:24:15.460 --> 01:24:16.300
Yeah.

01:24:16.300 --> 01:24:17.300
And the models are very fast.

01:24:17.300 --> 01:24:20.860
But where the models won't, my programming speed

01:24:20.860 --> 01:24:23.940
is not at all limited by my typing speed.

01:24:26.380 --> 01:24:29.220
And in very few cases it is, yes.

01:24:29.220 --> 01:24:31.460
If I'm writing some script to just like parse

01:24:31.460 --> 01:24:34.340
some weird data format, sure, my programming speed

01:24:34.340 --> 01:24:35.380
is limited by my typing speed.

01:24:35.380 --> 01:24:36.940
What about looking stuff up?

01:24:36.940 --> 01:24:39.500
Because that's essentially a more efficient lookup, right?

01:24:39.500 --> 01:24:42.220
You know, when I was at Twitter,

01:24:42.220 --> 01:24:46.300
I tried to use ChatGPT to like ask some questions,

01:24:46.300 --> 01:24:48.180
like what's the API for this?

01:24:48.180 --> 01:24:49.980
And it would just hallucinate.

01:24:49.980 --> 01:24:52.780
It would just give me completely made up API functions

01:24:52.780 --> 01:24:54.460
that sounded real.

01:24:54.460 --> 01:24:57.380
Well, do you think that's just a temporary kind of stage?

01:24:57.380 --> 01:24:58.660
No.

01:24:58.660 --> 01:25:00.100
You don't think it'll get better and better

01:25:00.100 --> 01:25:01.060
and better in this kind of stuff?

01:25:01.060 --> 01:25:04.220
Cause like it only hallucinates stuff in the edge cases.

01:25:04.220 --> 01:25:05.060
Yes.

01:25:05.060 --> 01:25:06.820
If you're writing generic code, it's actually pretty good.

01:25:06.820 --> 01:25:08.900
Yes, if you are writing an absolute basic,

01:25:08.900 --> 01:25:10.300
like react app with a button,

01:25:10.300 --> 01:25:12.260
it's not gonna hallucinate you.

01:25:12.260 --> 01:25:14.860
No, there's kind of ways to fix the hallucination problem.

01:25:14.860 --> 01:25:16.500
I think Facebook has an interesting paper,

01:25:16.500 --> 01:25:17.740
it's called Atlas.

01:25:17.740 --> 01:25:20.860
And it's actually weird the way that we do language models

01:25:20.860 --> 01:25:25.860
right now where all of the information is in the weights.

01:25:26.100 --> 01:25:27.700
And the human brain is not really like this.

01:25:27.700 --> 01:25:29.700
It's like a hippocampus and a memory system.

01:25:29.700 --> 01:25:31.860
So why don't LLMs have a memory system?

01:25:31.860 --> 01:25:33.020
And there's people working on them.

01:25:33.020 --> 01:25:36.180
I think future LLMs are gonna be like smaller,

01:25:36.180 --> 01:25:39.340
but are going to run looping on themselves

01:25:39.340 --> 01:25:41.740
and are going to have retrieval systems.

01:25:41.740 --> 01:25:43.420
And the thing about using a retrieval system

01:25:43.420 --> 01:25:45.660
is you can cite sources explicitly.

01:25:47.220 --> 01:25:50.740
Which is really helpful to integrate the human

01:25:50.740 --> 01:25:52.660
into the loop of the thing.

01:25:52.660 --> 01:25:55.420
Cause you can go check the sources and you can investigate.

01:25:55.420 --> 01:25:57.260
So whenever the thing is hallucinating,

01:25:57.260 --> 01:25:59.780
you can like have the human supervision.

01:25:59.780 --> 01:26:01.540
That's pushing it towards level two kind of driving.

01:26:01.540 --> 01:26:03.380
That's gonna kill Google.

01:26:03.380 --> 01:26:04.300
Wait, which part?

01:26:04.300 --> 01:26:06.100
When someone makes an LLM that's capable

01:26:06.100 --> 01:26:08.860
of citing its sources, it will kill Google.

01:26:08.860 --> 01:26:10.340
LLM that's citing its sources

01:26:10.340 --> 01:26:13.140
because that's basically a search engine.

01:26:13.140 --> 01:26:14.660
That's what people want in the search engine.

01:26:14.660 --> 01:26:16.780
But also Google might be the people that build it.

01:26:16.780 --> 01:26:17.620
Maybe.

01:26:17.620 --> 01:26:18.980
And put ads on it.

01:26:18.980 --> 01:26:19.940
I'd count them out.

01:26:20.860 --> 01:26:21.700
Why is that?

01:26:21.700 --> 01:26:22.540
What do you think?

01:26:22.540 --> 01:26:24.900
Who wins this race?

01:26:24.900 --> 01:26:26.900
We've got, who are the competitors?

01:26:26.900 --> 01:26:27.740
All right.

01:26:27.740 --> 01:26:29.620
We've got Tiny Corp.

01:26:29.620 --> 01:26:32.340
I don't know if that's, yeah, I mean,

01:26:32.340 --> 01:26:33.820
you're a legitimate competitor in that.

01:26:33.820 --> 01:26:35.620
I'm not trying to compete on that.

01:26:35.620 --> 01:26:36.460
You're not.

01:26:36.460 --> 01:26:37.280
No, not as a.

01:26:37.280 --> 01:26:38.120
You're just gonna accidentally stumble

01:26:38.120 --> 01:26:39.700
into that competition.

01:26:39.700 --> 01:26:40.540
Maybe.

01:26:40.540 --> 01:26:41.380
You don't think you might build a search engine

01:26:41.380 --> 01:26:43.220
to replace Google search?

01:26:43.220 --> 01:26:46.540
When I started Kama, I said over and over again,

01:26:46.540 --> 01:26:47.940
I'm going to win self-driving cars.

01:26:47.940 --> 01:26:49.500
I still believe that.

01:26:49.500 --> 01:26:51.940
I have never said I'm going to win search

01:26:51.940 --> 01:26:52.780
with the Tiny Corp.

01:26:52.780 --> 01:26:55.340
And I'm never going to say that cause I won't.

01:26:55.340 --> 01:26:56.660
The night is still young.

01:26:56.660 --> 01:26:58.420
We don't, you don't know how hard is it

01:26:58.420 --> 01:27:00.540
to win search in this new route.

01:27:00.540 --> 01:27:03.020
Like it's, it feels, I mean,

01:27:03.020 --> 01:27:04.700
one of the things that Chad GPT kind of shows

01:27:04.700 --> 01:27:06.940
that there could be a few interesting tricks

01:27:06.940 --> 01:27:09.180
that really have, that create a really compelling product.

01:27:09.180 --> 01:27:10.700
Some startup is going to figure it out.

01:27:10.700 --> 01:27:12.540
I think, I think if you ask me,

01:27:12.540 --> 01:27:14.220
like Google is still the number one webpage,

01:27:14.220 --> 01:27:15.140
I think by the end of the decade,

01:27:15.140 --> 01:27:17.420
Google won't be the number one webpage anymore.

01:27:17.420 --> 01:27:19.020
So you don't think Google,

01:27:19.020 --> 01:27:21.740
because of the, how big the corporation is?

01:27:21.740 --> 01:27:25.020
Look, I would put a lot more money on Mark Zuckerberg.

01:27:25.020 --> 01:27:25.860
Why is that?

01:27:27.460 --> 01:27:29.180
Because Mark Zuckerberg's alive.

01:27:30.600 --> 01:27:32.820
Like this is old Paul Graham essay.

01:27:33.660 --> 01:27:34.620
Startups are either alive or dead.

01:27:34.620 --> 01:27:35.460
Google's dead.

01:27:36.700 --> 01:27:37.540
Facebook is alive.

01:27:37.540 --> 01:27:39.380
Versus Facebook is alive, Metta is alive.

01:27:39.380 --> 01:27:40.820
Metta. Metta.

01:27:40.820 --> 01:27:41.660
You see what I mean?

01:27:41.660 --> 01:27:43.380
Like that's just like, like, like Mark Zuckerberg.

01:27:43.380 --> 01:27:45.300
This is Mark Zuckerberg reading that Paul Graham essay

01:27:45.300 --> 01:27:47.460
and being like, I'm going to show everyone how alive we are.

01:27:47.460 --> 01:27:49.060
I'm going to change the name.

01:27:49.060 --> 01:27:53.940
So you don't think there's this gutsy pivoting engine

01:27:55.100 --> 01:27:57.900
that like Google doesn't have that,

01:27:57.900 --> 01:28:00.500
the kind of engine that a startup has like constantly.

01:28:00.500 --> 01:28:01.340
You know what?

01:28:01.340 --> 01:28:03.020
Being alive, I guess.

01:28:03.020 --> 01:28:05.780
When I listened to your Sam Altman podcast,

01:28:05.780 --> 01:28:06.660
you talked about the button.

01:28:06.660 --> 01:28:08.140
Everyone who talks about AI talks about the button,

01:28:08.140 --> 01:28:09.820
the button to turn it off, right?

01:28:09.820 --> 01:28:12.580
Do we have a button to turn off Google?

01:28:12.580 --> 01:28:15.660
Is anybody in the world capable of shutting Google down?

01:28:17.300 --> 01:28:18.300
What does that mean exactly?

01:28:18.300 --> 01:28:19.740
The company or the search engine?

01:28:19.740 --> 01:28:21.020
Could we shut the search engine down?

01:28:21.020 --> 01:28:23.340
Could we shut the company down?

01:28:23.340 --> 01:28:24.300
Either.

01:28:24.300 --> 01:28:26.500
Can you elaborate on the value of that question?

01:28:26.500 --> 01:28:28.500
Does Sundar Parshai have the authority

01:28:28.500 --> 01:28:30.240
to turn off google.com tomorrow?

01:28:31.820 --> 01:28:32.660
Who has the authority?

01:28:32.660 --> 01:28:33.500
That's a good question, right?

01:28:33.500 --> 01:28:34.340
Does anyone?

01:28:34.340 --> 01:28:35.180
Does anyone?

01:28:36.180 --> 01:28:37.620
Yeah, I'm sure.

01:28:37.620 --> 01:28:38.660
Are you sure?

01:28:38.660 --> 01:28:40.260
No, they have the technical power,

01:28:40.260 --> 01:28:41.660
but do they have the authority?

01:28:41.660 --> 01:28:44.860
Let's say Sundar Parshai made this his sole mission.

01:28:44.860 --> 01:28:46.100
Came into Google tomorrow and said,

01:28:46.100 --> 01:28:47.540
I'm going to shut google.com down.

01:28:47.540 --> 01:28:49.060
Yeah.

01:28:49.060 --> 01:28:51.060
I don't think he'd keep his position too long.

01:28:52.540 --> 01:28:53.620
And what is the mechanism

01:28:53.620 --> 01:28:55.300
by which he wouldn't keep his position?

01:28:55.300 --> 01:28:59.540
Well, boards and shares and corporate undermining

01:28:59.540 --> 01:29:02.780
and oh my God, our revenue is zero now.

01:29:02.780 --> 01:29:05.180
Okay, so what's the case you're making here?

01:29:05.180 --> 01:29:07.420
So the capitalist machine prevents you

01:29:07.420 --> 01:29:09.220
from having the button.

01:29:09.220 --> 01:29:10.660
Yeah, and it will have a,

01:29:10.660 --> 01:29:12.340
I mean, this is true for the AIs too, right?

01:29:12.340 --> 01:29:14.700
There's no turning the AIs off.

01:29:14.700 --> 01:29:15.540
There's no button.

01:29:15.540 --> 01:29:16.840
You can't press it.

01:29:16.840 --> 01:29:19.100
Now, does Mark Zuckerberg have that button

01:29:19.100 --> 01:29:20.100
for facebook.com?

01:29:21.100 --> 01:29:22.300
Yeah, it's probably more.

01:29:22.300 --> 01:29:23.540
I think he does.

01:29:23.540 --> 01:29:24.380
I think he does.

01:29:24.380 --> 01:29:25.760
And this is exactly what I mean

01:29:25.760 --> 01:29:29.140
and why I bet on him so much more than I bet on Google.

01:29:29.140 --> 01:29:31.300
I guess you could say Elon has similar stuff.

01:29:31.300 --> 01:29:32.740
Oh, Elon has the button.

01:29:32.740 --> 01:29:33.580
Yeah.

01:29:34.700 --> 01:29:36.820
Does Elon, can Elon fire the missiles?

01:29:36.820 --> 01:29:38.060
Can he fire the missiles?

01:29:39.020 --> 01:29:42.340
I think some questions are better left unasked.

01:29:42.340 --> 01:29:43.700
Right?

01:29:43.700 --> 01:29:45.460
I mean, you know, a rocket and an ICBM,

01:29:45.460 --> 01:29:47.060
oh, you're a rocket that can land anywhere.

01:29:47.060 --> 01:29:48.100
Isn't that an ICBM?

01:29:48.100 --> 01:29:51.100
Well, yeah, you know, don't ask too many questions.

01:29:51.100 --> 01:29:52.020
My God.

01:29:54.940 --> 01:29:57.260
But the positive side of the button

01:29:57.260 --> 01:29:59.020
is that you can innovate aggressively.

01:29:59.860 --> 01:30:00.700
That's what you're saying.

01:30:00.700 --> 01:30:03.460
Which is what's required with turning LLM

01:30:03.460 --> 01:30:04.340
into a search engine.

01:30:04.340 --> 01:30:05.700
I would bet on a startup.

01:30:05.700 --> 01:30:06.540
Because it's so easy, right?

01:30:06.540 --> 01:30:08.140
I bet on something that looks like mid-journey,

01:30:08.140 --> 01:30:09.560
but for search.

01:30:11.340 --> 01:30:13.900
Just is able to site source a loop on itself.

01:30:13.900 --> 01:30:15.860
I mean, it just feels like one model can take off.

01:30:15.860 --> 01:30:16.700
Yeah.

01:30:16.700 --> 01:30:18.860
And that nice wrapper and some of it scale.

01:30:18.860 --> 01:30:21.260
I mean, it's hard to like create a product

01:30:21.260 --> 01:30:23.500
that just works really nicely, stably.

01:30:23.500 --> 01:30:25.100
The other thing that's gonna be cool

01:30:25.100 --> 01:30:28.100
is there is some aspect of a winner take all effect.

01:30:28.500 --> 01:30:31.940
Once someone starts deploying a product

01:30:31.940 --> 01:30:32.900
that gets a lot of usage,

01:30:32.900 --> 01:30:34.860
and you see this with OpenAI,

01:30:34.860 --> 01:30:36.660
they are going to get the data set

01:30:36.660 --> 01:30:39.020
to train future versions of the model.

01:30:39.020 --> 01:30:39.860
Yeah.

01:30:39.860 --> 01:30:41.660
They are going to be able to,

01:30:41.660 --> 01:30:42.700
I was actually at Google Image Search

01:30:42.700 --> 01:30:44.900
when I worked there like almost 15 years ago now.

01:30:44.900 --> 01:30:46.820
How does Google know which image is an Apple?

01:30:46.820 --> 01:30:48.020
And I said the metadata.

01:30:48.020 --> 01:30:49.820
And they're like, yeah, that works about half the time.

01:30:49.820 --> 01:30:50.940
How does Google know?

01:30:50.940 --> 01:30:52.380
You'll see the role Apple's on the front page

01:30:52.380 --> 01:30:54.180
when you search Apple.

01:30:54.180 --> 01:30:56.980
And I don't know, I didn't come up with the answer.

01:30:57.020 --> 01:30:58.260
The guy's like, well, it's what people click on

01:30:58.260 --> 01:30:59.380
when they search Apple.

01:30:59.380 --> 01:31:00.300
Oh my God, yeah.

01:31:00.300 --> 01:31:02.740
Yeah, yeah, that data is really, really powerful.

01:31:02.740 --> 01:31:04.500
It's the human supervision.

01:31:04.500 --> 01:31:06.220
What do you think are the chances?

01:31:06.220 --> 01:31:09.540
What do you think in general that Llama was open sourced?

01:31:09.540 --> 01:31:13.940
I just did a conversation with Mark Zuckerberg

01:31:13.940 --> 01:31:16.440
and he's all in on open source.

01:31:17.460 --> 01:31:19.660
Who would have thought that Mark Zuckerberg

01:31:19.660 --> 01:31:20.700
would be the good guy?

01:31:21.900 --> 01:31:23.460
No, I mean it.

01:31:23.460 --> 01:31:25.820
Who would have thought anything in this world?

01:31:25.820 --> 01:31:27.300
It's hard to know.

01:31:27.300 --> 01:31:32.300
But open source to you ultimately is a good thing here.

01:31:33.660 --> 01:31:34.500
Undoubtedly.

01:31:35.540 --> 01:31:39.620
You know, what's ironic about all these AI safety people

01:31:39.620 --> 01:31:42.220
is they are going to build the exact thing they fear.

01:31:43.540 --> 01:31:47.740
These, we need to have one model that we control in a line.

01:31:47.740 --> 01:31:50.660
This is the only way you end up paper clipped.

01:31:50.660 --> 01:31:52.420
There's no way you end up paper clipped

01:31:52.420 --> 01:31:54.020
if everybody has an AI.

01:31:54.020 --> 01:31:55.340
So open sourcing is the way

01:31:55.340 --> 01:31:56.900
to fight the paper clipped maximizer.

01:31:56.900 --> 01:31:59.140
Absolutely, it's the only way.

01:31:59.140 --> 01:32:00.460
You think you're gonna control it,

01:32:00.460 --> 01:32:02.020
you're not gonna control it.

01:32:02.020 --> 01:32:05.180
So the criticism you have for the AI safety folks

01:32:05.180 --> 01:32:10.260
is that there is a belief and a desire for control.

01:32:10.260 --> 01:32:13.740
And that belief and desire for centralized control

01:32:13.740 --> 01:32:16.820
of dangerous AI systems is not good.

01:32:16.820 --> 01:32:19.100
Sam Altman won't tell you that GPT-4

01:32:19.100 --> 01:32:21.220
has 220 billion parameters

01:32:21.220 --> 01:32:24.420
and is a 16-way mixture model with eight sets of weights.

01:32:24.420 --> 01:32:27.580
Who did you have to murder to get that information?

01:32:29.180 --> 01:32:30.420
All right.

01:32:30.420 --> 01:32:32.220
I mean, look, everyone at OpenAI knows

01:32:32.220 --> 01:32:34.020
what I just said was true, right?

01:32:34.020 --> 01:32:36.980
Now ask the question, really.

01:32:36.980 --> 01:32:40.140
You know, it upsets me when I, like GPT-2,

01:32:40.140 --> 01:32:41.660
when OpenAI came out with GPT-2

01:32:41.660 --> 01:32:44.060
and raised a whole fake AI safety thing about that.

01:32:44.060 --> 01:32:46.540
I mean, now the model is laughable.

01:32:46.540 --> 01:32:50.500
Like they used AI safety to hype up their company

01:32:50.540 --> 01:32:51.540
and it's disgusting.

01:32:52.780 --> 01:32:56.180
Or the flip side of that is they used

01:32:56.180 --> 01:32:58.780
a relatively weak model in retrospect

01:32:58.780 --> 01:33:01.900
to explore how do we do AI safety correctly?

01:33:01.900 --> 01:33:02.780
How do we release things?

01:33:02.780 --> 01:33:04.380
How do we go through the process?

01:33:04.380 --> 01:33:09.380
I don't know if, I don't know how much hype there is.

01:33:10.260 --> 01:33:12.580
I don't know how much hype there is in AI safety, honestly.

01:33:12.580 --> 01:33:14.500
Oh, there's so much hype, at least on Twitter.

01:33:14.500 --> 01:33:15.820
I don't know, maybe Twitter's not real life.

01:33:15.820 --> 01:33:17.060
Twitter's not real life.

01:33:17.940 --> 01:33:19.620
Come on, in terms of hype.

01:33:19.820 --> 01:33:22.820
I mean, I don't, I think OpenAI has been

01:33:22.820 --> 01:33:26.100
finding an interesting balance between transparency

01:33:26.100 --> 01:33:28.860
and putting value on AI safety.

01:33:28.860 --> 01:33:32.220
You don't think, you think just go all out open source.

01:33:32.220 --> 01:33:34.340
So do what Llama did.

01:33:34.340 --> 01:33:38.300
So do like open source, this is a tough question,

01:33:38.300 --> 01:33:41.260
which is open source both the base,

01:33:41.260 --> 01:33:44.140
the foundation model and the fine tune one.

01:33:44.140 --> 01:33:48.700
So like the model that can be ultra racist and dangerous

01:33:48.700 --> 01:33:51.780
and like tell you how to build a nuclear weapon.

01:33:51.780 --> 01:33:53.980
Oh my God, have you met humans, right?

01:33:53.980 --> 01:33:55.260
Like half of these AI alignments.

01:33:55.260 --> 01:33:57.420
I haven't met most humans.

01:33:57.420 --> 01:34:00.460
This makes, this allows you to meet every human.

01:34:00.460 --> 01:34:02.940
Yeah, I know, but half of these AI alignment problems

01:34:02.940 --> 01:34:04.460
are just human alignment problems.

01:34:04.460 --> 01:34:06.900
And that's what's also so scary about the language they use.

01:34:06.900 --> 01:34:09.860
It's like, it's not the machines you wanna align, it's me.

01:34:11.380 --> 01:34:15.380
But here's the thing, it makes it very accessible

01:34:15.380 --> 01:34:20.380
to ask very questions where the answers

01:34:21.860 --> 01:34:25.260
have dangerous consequences if you were to act on them.

01:34:25.260 --> 01:34:28.420
I mean, yeah, welcome to the world.

01:34:28.420 --> 01:34:30.540
Well, no, for me, there's a lot of friction

01:34:30.540 --> 01:34:35.540
if I wanna find out how to, I don't know, blow up something.

01:34:36.780 --> 01:34:39.060
No, there's not a lot of friction, that's so easy.

01:34:39.060 --> 01:34:40.580
No, like what do I search?

01:34:40.580 --> 01:34:43.100
Do I use Bing or do I, which search engine do I use?

01:34:43.100 --> 01:34:44.940
No, there's like lots of stuff.

01:34:45.460 --> 01:34:49.060
First off, anyone who's stupid enough to search

01:34:49.060 --> 01:34:52.300
for how to blow up a building in my neighborhood

01:34:52.300 --> 01:34:54.620
is not smart enough to build a bomb, right?

01:34:54.620 --> 01:34:55.460
Are you sure about that?

01:34:55.460 --> 01:34:57.100
Yes.

01:34:57.100 --> 01:35:02.100
I feel like a language model makes it more accessible

01:35:03.100 --> 01:35:05.460
for that person who's not smart enough to do.

01:35:05.460 --> 01:35:07.220
They're not gonna build a bomb, trust me.

01:35:07.220 --> 01:35:11.340
The people who are incapable of figuring out

01:35:11.340 --> 01:35:13.740
how to ask that question a bit more academically

01:35:13.740 --> 01:35:15.060
and get a real answer from it

01:35:15.060 --> 01:35:17.060
are not capable of procuring the materials

01:35:17.060 --> 01:35:19.500
which are somewhat controlled to build a bomb.

01:35:19.500 --> 01:35:21.340
No, I think LLM makes it more accessible

01:35:21.340 --> 01:35:24.940
to people with money without the technical know-how, right?

01:35:24.940 --> 01:35:27.220
To build, like, do you really need to know

01:35:27.220 --> 01:35:29.060
how to build a bomb, to build a bomb?

01:35:29.060 --> 01:35:30.580
You can hire people, you can find like-

01:35:30.580 --> 01:35:32.260
Or you can hire people to build a,

01:35:32.260 --> 01:35:33.740
you know what, I was asking this question on my stream,

01:35:33.740 --> 01:35:35.380
like, can Jeff Bezos hire a hitman?

01:35:35.380 --> 01:35:36.220
Probably not.

01:35:37.260 --> 01:35:41.780
But a language model can probably help you out.

01:35:41.780 --> 01:35:43.260
Yeah, and you'll still go to jail, right?

01:35:43.260 --> 01:35:45.100
Like, it's not like the language model is God.

01:35:45.100 --> 01:35:46.700
Like, the language model, it's like,

01:35:46.700 --> 01:35:49.420
you literally just hired someone on Fiverr.

01:35:49.420 --> 01:35:50.260
But you-

01:35:50.260 --> 01:35:51.100
Okay, okay, okay.

01:35:51.100 --> 01:35:52.980
GPT-4 in terms of finding a hitman

01:35:52.980 --> 01:35:54.940
is like asking Fiverr how to find a hitman.

01:35:54.940 --> 01:35:55.780
I understand.

01:35:55.780 --> 01:35:56.620
But don't you think-

01:35:56.620 --> 01:35:57.460
Asking WikiHow, you know?

01:35:57.460 --> 01:35:58.300
WikiHow.

01:35:58.300 --> 01:36:00.340
But don't you think GPT-5 will be better?

01:36:00.340 --> 01:36:01.500
Because don't you think that information

01:36:01.500 --> 01:36:03.020
is out there on the internet?

01:36:03.020 --> 01:36:05.100
I mean, yeah, and I think that if someone

01:36:05.100 --> 01:36:07.340
is actually serious enough to hire a hitman

01:36:07.340 --> 01:36:09.380
or build a bomb, they'd also be serious enough

01:36:09.380 --> 01:36:10.900
to find the information.

01:36:10.900 --> 01:36:11.780
I don't think so.

01:36:11.780 --> 01:36:13.060
I think it makes it more accessible.

01:36:13.820 --> 01:36:15.860
If you have enough money to buy a hitman,

01:36:15.860 --> 01:36:18.540
I think it decreases the friction

01:36:18.540 --> 01:36:20.820
of how hard is it to find that kind of hitman.

01:36:20.820 --> 01:36:24.180
I honestly think there's a jump

01:36:24.180 --> 01:36:28.900
in ease and scale of how much harm you can do.

01:36:28.900 --> 01:36:30.340
And I don't mean harm with language.

01:36:30.340 --> 01:36:32.060
I mean, harm with actual violence.

01:36:32.060 --> 01:36:33.540
What you're basically saying is like,

01:36:33.540 --> 01:36:34.980
okay, what's gonna happen is these people

01:36:34.980 --> 01:36:38.140
who are not intelligent are going to use machines

01:36:38.140 --> 01:36:39.860
to augment their intelligence.

01:36:39.860 --> 01:36:42.220
And now, intelligent people and machines,

01:36:42.220 --> 01:36:43.700
intelligence is scary.

01:36:43.700 --> 01:36:46.020
Intelligent agents are scary.

01:36:46.020 --> 01:36:47.980
When I'm in the woods, the scariest animal

01:36:47.980 --> 01:36:50.300
to meet is a human, right?

01:36:50.300 --> 01:36:51.140
No, no, no.

01:36:51.140 --> 01:36:52.860
Look, there's like nice California humans.

01:36:52.860 --> 01:36:55.660
Like I see you're wearing like street clothes

01:36:55.660 --> 01:36:57.020
and Nikes, all right, fine.

01:36:57.020 --> 01:36:58.260
But you look like you've been a human

01:36:58.260 --> 01:36:59.460
who's been in the woods for a while.

01:36:59.460 --> 01:37:00.300
Yeah.

01:37:00.300 --> 01:37:01.380
I'm more scared of you than a bear.

01:37:01.380 --> 01:37:03.140
That's what they say about the Amazon.

01:37:03.140 --> 01:37:05.260
When you go to the Amazon, it's the human tribes.

01:37:05.260 --> 01:37:06.300
Oh yeah.

01:37:06.300 --> 01:37:09.100
So intelligence is scary, right?

01:37:09.100 --> 01:37:11.700
So to ask this question in a generic way,

01:37:12.060 --> 01:37:16.380
what if we took everybody who maybe has ill intention

01:37:16.380 --> 01:37:20.540
but is not so intelligent and gave them intelligence, right?

01:37:20.540 --> 01:37:23.740
So we should have intelligence control, of course.

01:37:23.740 --> 01:37:25.660
We should only give intelligence to good people.

01:37:25.660 --> 01:37:27.980
And that is the absolutely horrifying idea.

01:37:27.980 --> 01:37:30.100
So to you, the best defense is actually,

01:37:30.100 --> 01:37:32.500
the best defense is to give more intelligence

01:37:32.500 --> 01:37:34.260
to the good guys and intelligence.

01:37:34.260 --> 01:37:35.340
Give intelligence to everybody.

01:37:35.340 --> 01:37:36.300
Give intelligence to everybody.

01:37:36.300 --> 01:37:37.140
You know what?

01:37:37.140 --> 01:37:37.980
And it's not even like guns, right?

01:37:37.980 --> 01:37:38.820
Like people say this about guns.

01:37:38.820 --> 01:37:40.100
What's the best defense against a bad guy with a gun?

01:37:40.100 --> 01:37:40.940
Good guy with a gun.

01:37:40.980 --> 01:37:42.180
Like I kind of subscribe to that,

01:37:42.180 --> 01:37:45.460
but I really subscribe to that with intelligence.

01:37:45.460 --> 01:37:48.220
Yeah, in a fundamental way, I agree with you,

01:37:48.220 --> 01:37:50.140
but there's just feels like so much uncertainty

01:37:50.140 --> 01:37:51.740
and so much can happen rapidly

01:37:51.740 --> 01:37:53.180
that you can lose a lot of control

01:37:53.180 --> 01:37:54.660
and you can do a lot of damage.

01:37:54.660 --> 01:37:56.300
Oh no, we can lose control?

01:37:56.300 --> 01:37:58.100
Yes, thank God.

01:37:58.100 --> 01:37:58.940
Yeah.

01:37:58.940 --> 01:38:00.660
I hope we can, I hope they lose control.

01:38:02.340 --> 01:38:05.340
I want them to lose control more than anything else.

01:38:05.340 --> 01:38:07.820
I think when you lose control, you can do a lot of damage,

01:38:07.820 --> 01:38:11.100
but you can do more damage when you centralize

01:38:11.100 --> 01:38:12.740
and hold on to control is the point.

01:38:12.740 --> 01:38:15.740
Centralized and held control is tyranny, right?

01:38:15.740 --> 01:38:17.580
I will always, I don't like anarchy either,

01:38:17.580 --> 01:38:19.220
but I'll always take anarchy over tyranny.

01:38:19.220 --> 01:38:20.580
Anarchy, you have a chance.

01:38:21.860 --> 01:38:24.300
This human civilization we've got going on

01:38:24.300 --> 01:38:25.540
is quite interesting.

01:38:25.540 --> 01:38:26.380
I mean, I agree with you.

01:38:26.380 --> 01:38:30.660
So to you, open source is the way forward here.

01:38:30.660 --> 01:38:32.380
So you admire what Facebook is doing here

01:38:32.380 --> 01:38:34.420
or what Metta is doing with the release of them.

01:38:34.420 --> 01:38:35.500
Yeah, a lot.

01:38:35.500 --> 01:38:38.340
I lost $80,000 last year investing in Metta

01:38:38.340 --> 01:38:39.660
and when they released Llama, I'm like,

01:38:39.660 --> 01:38:41.740
yeah, whatever man, that was worth it.

01:38:41.740 --> 01:38:43.180
It was worth it.

01:38:43.180 --> 01:38:47.060
Do you think Google and OpenAI with Microsoft will match

01:38:47.060 --> 01:38:49.780
with what Metta is doing or no?

01:38:49.780 --> 01:38:52.380
So if I were a researcher,

01:38:52.380 --> 01:38:53.900
why would you want to work at OpenAI?

01:38:53.900 --> 01:38:56.980
Like, you know, you're just, you're on the bad team.

01:38:56.980 --> 01:38:58.580
Like, I mean it, like you're on the bad team

01:38:58.580 --> 01:39:01.500
who can't even say that GPT-4 has 220 billion parameters.

01:39:01.500 --> 01:39:03.940
So close source to use the bad team.

01:39:03.940 --> 01:39:05.140
Not only close source.

01:39:05.780 --> 01:39:08.100
I'm not saying you need to make your model weights open.

01:39:08.100 --> 01:39:09.060
I'm not saying that.

01:39:09.060 --> 01:39:11.340
I totally understand we're keeping our model weights closed

01:39:11.340 --> 01:39:12.620
because that's our product, right?

01:39:12.620 --> 01:39:13.940
That's fine.

01:39:13.940 --> 01:39:16.980
I'm saying like, because of AI safety reasons,

01:39:16.980 --> 01:39:19.300
we can't tell you the number of billions

01:39:19.300 --> 01:39:21.180
of parameters in the model.

01:39:21.180 --> 01:39:23.020
That's just the bad guys.

01:39:23.020 --> 01:39:24.900
Just because you're mocking AI safety

01:39:24.900 --> 01:39:26.460
doesn't mean it's not real.

01:39:26.460 --> 01:39:27.300
Oh, of course.

01:39:27.300 --> 01:39:28.700
Is it possible that these things

01:39:28.700 --> 01:39:30.980
can really do a lot of damage that we don't know?

01:39:30.980 --> 01:39:32.140
Oh my God, yes.

01:39:32.140 --> 01:39:34.020
Intelligence is so dangerous.

01:39:34.020 --> 01:39:36.700
Be it human intelligence or machine intelligence.

01:39:36.700 --> 01:39:38.300
Intelligence is dangerous.

01:39:38.300 --> 01:39:40.380
But machine intelligence is so much easier

01:39:40.380 --> 01:39:42.940
to deploy at scale, like rapidly.

01:39:42.940 --> 01:39:44.260
Like what, okay.

01:39:44.260 --> 01:39:46.220
If you have human-like bots on Twitter,

01:39:48.060 --> 01:39:50.020
and you have like a thousand of them,

01:39:50.020 --> 01:39:52.020
create a whole narrative,

01:39:52.020 --> 01:39:55.660
like you can manipulate millions of people.

01:39:55.660 --> 01:39:57.500
But you mean like the intelligence agencies

01:39:57.500 --> 01:39:58.980
in America are doing right now?

01:39:58.980 --> 01:40:01.060
Yeah, but they're not doing it that well.

01:40:01.060 --> 01:40:03.300
It feels like you can do a lot.

01:40:03.300 --> 01:40:04.580
They're doing it pretty well.

01:40:05.980 --> 01:40:07.580
I think they're doing a pretty good job.

01:40:07.580 --> 01:40:09.540
I suspect they're not nearly as good

01:40:09.540 --> 01:40:12.620
as a bunch of GPT-fueled bots could be.

01:40:12.620 --> 01:40:13.780
Well, I mean, of course they're looking

01:40:13.780 --> 01:40:15.020
into the latest technologies

01:40:15.020 --> 01:40:16.900
for control of people, of course.

01:40:16.900 --> 01:40:19.060
But I think there's a George Hotts type character

01:40:19.060 --> 01:40:21.460
that can do a better job than the entirety of them.

01:40:21.460 --> 01:40:22.300
You don't think so?

01:40:22.300 --> 01:40:23.140
No way.

01:40:23.140 --> 01:40:24.740
No, and I'll tell you why the George Hotts character can't.

01:40:24.740 --> 01:40:26.540
And I thought about this a lot with hacking.

01:40:26.540 --> 01:40:27.820
Like I can find exploits in web browsers.

01:40:27.820 --> 01:40:28.660
I probably still can.

01:40:28.660 --> 01:40:29.820
I mean, I was better when I was 24,

01:40:29.820 --> 01:40:32.460
but the thing that I lack

01:40:32.460 --> 01:40:34.780
is the ability to slowly and steadily deploy them

01:40:34.780 --> 01:40:35.940
over five years.

01:40:35.940 --> 01:40:38.700
And this is what intelligence agencies are very good at.

01:40:38.700 --> 01:40:39.740
Intelligence agencies don't have

01:40:39.740 --> 01:40:41.500
the most sophisticated technology.

01:40:42.340 --> 01:40:43.700
They just have-

01:40:43.700 --> 01:40:44.540
Endurance?

01:40:44.540 --> 01:40:45.380
Endurance.

01:40:46.740 --> 01:40:49.380
Yeah, the financial backing

01:40:49.380 --> 01:40:51.940
and the infrastructure for the endurance.

01:40:51.940 --> 01:40:54.780
So the more we can decentralize power,

01:40:54.780 --> 01:40:56.700
like you could make an argument by the way

01:40:56.700 --> 01:40:58.340
that nobody should have these things.

01:40:58.340 --> 01:40:59.660
And I would defend that argument.

01:40:59.660 --> 01:41:01.500
I would, like you're saying that look,

01:41:01.500 --> 01:41:04.220
LLMs and AI and machine intelligence

01:41:04.220 --> 01:41:05.300
can cause a lot of harm.

01:41:05.300 --> 01:41:06.780
So nobody should have it.

01:41:06.780 --> 01:41:08.820
And I will respect someone philosophically

01:41:08.820 --> 01:41:09.700
with that position.

01:41:09.700 --> 01:41:11.260
Just like I will respect someone philosophically

01:41:11.260 --> 01:41:14.380
with the position that nobody should have guns, right?

01:41:14.380 --> 01:41:16.100
But I will not respect philosophically

01:41:16.100 --> 01:41:20.020
with only the trusted authorities

01:41:20.020 --> 01:41:22.100
should have access to this.

01:41:22.100 --> 01:41:23.500
Who are the trusted authorities?

01:41:23.500 --> 01:41:24.340
You know what?

01:41:24.340 --> 01:41:25.900
I'm not worried about alignment

01:41:25.900 --> 01:41:29.700
between AI company and their machines.

01:41:29.700 --> 01:41:33.140
I'm worried about alignment between me and AI company.

01:41:33.140 --> 01:41:36.260
What do you think Eliezer Yitkovsky would say to you?

01:41:37.660 --> 01:41:39.860
Because he's really against open source.

01:41:39.860 --> 01:41:40.820
I know.

01:41:40.820 --> 01:41:44.980
And I thought about this.

01:41:44.980 --> 01:41:46.460
I thought about this.

01:41:46.460 --> 01:41:49.860
And I think this comes down to

01:41:49.860 --> 01:41:53.460
a repeated misunderstanding of political power

01:41:53.460 --> 01:41:54.660
by the rationalists.

01:41:55.900 --> 01:41:56.740
Interesting.

01:41:56.860 --> 01:42:01.860
I think that Eliezer Yitkovsky is scared of these things.

01:42:02.300 --> 01:42:04.100
And I am scared of these things too.

01:42:04.100 --> 01:42:05.900
Everyone should be scared of these things.

01:42:05.900 --> 01:42:07.140
These things are scary.

01:42:08.060 --> 01:42:11.380
But now you ask about the two possible futures.

01:42:11.380 --> 01:42:16.180
One where a small trusted centralized group of people

01:42:16.180 --> 01:42:19.340
has them and the other where everyone has them.

01:42:19.340 --> 01:42:21.300
And I am much less scared of the second future

01:42:21.300 --> 01:42:22.140
than the first.

01:42:23.340 --> 01:42:25.180
Well, there's a small trusted group of people

01:42:25.180 --> 01:42:27.380
that have control of our nuclear weapons.

01:42:28.860 --> 01:42:30.060
There's a difference.

01:42:30.060 --> 01:42:32.740
Again, a nuclear weapon cannot be deployed tactically

01:42:32.740 --> 01:42:34.300
and a nuclear weapon is not a defense

01:42:34.300 --> 01:42:35.660
against a nuclear weapon.

01:42:37.380 --> 01:42:40.180
Except maybe in some philosophical mind game kind of way.

01:42:41.820 --> 01:42:44.420
But AI is different how exactly?

01:42:44.420 --> 01:42:45.260
Okay.

01:42:45.260 --> 01:42:48.020
Let's say the intelligence agency

01:42:48.020 --> 01:42:50.260
deploys a million bots on Twitter

01:42:50.260 --> 01:42:51.300
or a thousand bots on Twitter

01:42:51.300 --> 01:42:53.500
to try to convince me of a point.

01:42:53.500 --> 01:42:56.740
Imagine I had a powerful AI running on my computer

01:42:56.740 --> 01:43:01.140
saying, okay, nice PSYOP, nice PSYOP, nice PSYOP, okay.

01:43:01.140 --> 01:43:02.340
Here's a PSYOP.

01:43:02.340 --> 01:43:04.420
I filtered it out for you.

01:43:04.420 --> 01:43:07.780
Yeah, I mean, so you have fundamental hope for that,

01:43:07.780 --> 01:43:10.500
for the defense of PSYOP.

01:43:10.500 --> 01:43:12.140
I'm not even like, I don't even mean these things

01:43:12.140 --> 01:43:13.340
in like truly horrible ways.

01:43:13.340 --> 01:43:16.500
I mean these things in straight up like ad blocker, right?

01:43:16.500 --> 01:43:17.340
Straight up ad blocker, right?

01:43:17.340 --> 01:43:18.780
I don't want ads.

01:43:18.780 --> 01:43:20.060
But they're always finding, you know,

01:43:20.060 --> 01:43:22.500
imagine I had an AI that could just block

01:43:22.500 --> 01:43:23.460
all the ads for me.

01:43:24.500 --> 01:43:27.220
So you believe in the power of the people

01:43:27.220 --> 01:43:29.860
to always create a not blocker.

01:43:29.860 --> 01:43:32.620
Yeah, I mean, I kind of share that belief.

01:43:32.620 --> 01:43:36.540
I have, that's one of the deepest optimisms I have

01:43:36.540 --> 01:43:39.300
is just like, there's a lot of good guys.

01:43:39.300 --> 01:43:42.260
So to give, you shouldn't hand pick them.

01:43:42.260 --> 01:43:45.740
Just throw out powerful technology out there

01:43:45.740 --> 01:43:49.660
and the good guys will outnumber and outpower the bad guys.

01:43:49.660 --> 01:43:51.740
Yeah, I'm not even gonna say there's a lot of good guys.

01:43:51.740 --> 01:43:53.780
I'm saying that good outnumbers bad, right?

01:43:53.780 --> 01:43:54.700
Good outnumbers bad.

01:43:54.700 --> 01:43:56.380
In skill and performance.

01:43:56.380 --> 01:43:57.940
Yeah, definitely in skill and performance.

01:43:57.940 --> 01:43:59.300
Probably just a number too.

01:43:59.300 --> 01:44:00.140
Probably just in general.

01:44:00.140 --> 01:44:02.380
I mean, if you believe philosophically in democracy,

01:44:02.380 --> 01:44:06.540
you obviously believe that, that good outnumbers bad.

01:44:06.540 --> 01:44:11.300
And like the only, if you give it to a small number

01:44:11.300 --> 01:44:14.700
of people, there's a chance you gave it to good people,

01:44:14.700 --> 01:44:16.700
but there's also a chance you gave it to bad people.

01:44:16.700 --> 01:44:19.780
If you give it to everybody, well, if good outnumbers bad,

01:44:19.780 --> 01:44:22.540
then you definitely gave it to more good people than bad.

01:44:25.060 --> 01:44:25.900
That's really interesting.

01:44:25.900 --> 01:44:27.180
So that's on the safety grounds,

01:44:27.180 --> 01:44:29.620
but then also of course there's other motivations

01:44:29.620 --> 01:44:32.180
like you don't wanna give away your secret sauce.

01:44:32.180 --> 01:44:34.460
Well, that's, I mean, I look, I respect capitalism.

01:44:34.460 --> 01:44:37.220
I don't think that, I think that it would be polite

01:44:37.220 --> 01:44:39.380
for you to make model architectures open source

01:44:39.380 --> 01:44:41.820
and fundamental breakthroughs open source.

01:44:41.820 --> 01:44:43.420
I don't think you have to make weights open source.

01:44:43.420 --> 01:44:45.980
You know what's interesting is that

01:44:45.980 --> 01:44:49.220
like there's so many possible trajectories in human history

01:44:49.220 --> 01:44:53.260
where you could have the next Google be open source.

01:44:53.260 --> 01:44:57.340
So for example, I don't know if that connection is accurate,

01:44:57.340 --> 01:45:00.020
but you know, Wikipedia made a lot of interesting decisions

01:45:00.020 --> 01:45:03.740
not to put ads, like Wikipedia is basically open source.

01:45:03.740 --> 01:45:05.740
You could think of it that way.

01:45:05.740 --> 01:45:08.900
And like, that's one of the main websites on the internet.

01:45:08.900 --> 01:45:10.180
And like, it didn't have to be that way.

01:45:10.180 --> 01:45:11.100
It could have been like,

01:45:11.100 --> 01:45:13.620
Google could have created Wikipedia, put ads on it.

01:45:13.620 --> 01:45:16.620
You could probably run amazing ads now on Wikipedia.

01:45:16.620 --> 01:45:18.500
You wouldn't have to keep asking for money,

01:45:18.500 --> 01:45:20.580
but it's interesting, right?

01:45:20.580 --> 01:45:25.340
So llama, open source llama, derivatives of open source llama

01:45:25.340 --> 01:45:26.580
might win the internet.

01:45:27.980 --> 01:45:29.060
I sure hope so.

01:45:29.060 --> 01:45:31.180
I hope to see another era.

01:45:31.180 --> 01:45:32.740
You know, the kids today don't know

01:45:32.740 --> 01:45:35.060
how good the internet used to be.

01:45:35.060 --> 01:45:36.660
And I don't think this is just, come on,

01:45:36.660 --> 01:45:38.260
like everyone's nostalgic for their past,

01:45:38.260 --> 01:45:42.460
but I actually think the internet before small groups

01:45:42.460 --> 01:45:44.740
of weaponized corporate and government interests

01:45:44.740 --> 01:45:46.540
took it over was a beautiful place.

01:45:49.340 --> 01:45:52.900
You know, those small number of companies

01:45:52.900 --> 01:45:55.860
have created some sexy products,

01:45:55.860 --> 01:45:59.980
but you're saying overall in the long arc of history,

01:45:59.980 --> 01:46:02.580
the centralization of power they have

01:46:02.580 --> 01:46:04.820
like suffocated the human spirit at scale.

01:46:04.820 --> 01:46:05.660
Here's a question to ask

01:46:05.660 --> 01:46:08.220
about those beautiful, sexy products.

01:46:08.220 --> 01:46:11.140
Imagine 2000 Google to 2010 Google, right?

01:46:11.140 --> 01:46:11.980
A lot changed.

01:46:11.980 --> 01:46:14.260
We got maps, we got Gmail.

01:46:14.260 --> 01:46:16.500
We lost a lot of products too, I think.

01:46:16.500 --> 01:46:18.460
Yeah, I mean, some were probably, we've got Chrome.

01:46:19.300 --> 01:46:21.900
And now let's go from 2010, we got Android.

01:46:21.900 --> 01:46:24.460
Now let's go from 2010 to 2020.

01:46:24.460 --> 01:46:25.300
What does Google have?

01:46:25.300 --> 01:46:29.420
Well, search engine, maps, mail, Android and Chrome.

01:46:29.420 --> 01:46:30.540
Oh, I see.

01:46:31.740 --> 01:46:34.300
The internet was this, you know,

01:46:34.300 --> 01:46:36.300
I was Times Person of the Year in 2006.

01:46:38.460 --> 01:46:39.300
I love this.

01:46:39.300 --> 01:46:41.820
It's you was Times Person of the Year in 2006, right?

01:46:41.820 --> 01:46:46.620
Like that's, you know, so quickly did people forget.

01:46:46.620 --> 01:46:49.500
And I think some of it's social media.

01:46:49.500 --> 01:46:54.140
I think some of it, I hope, look, I hope that I don't,

01:46:54.140 --> 01:46:56.620
it's possible that some very sinister things happen.

01:46:56.620 --> 01:46:57.540
I don't know.

01:46:57.540 --> 01:47:00.340
I think it might just be like the effects of social media.

01:47:01.380 --> 01:47:03.580
But something happened in the last 20 years.

01:47:05.460 --> 01:47:06.300
Oh, okay.

01:47:06.300 --> 01:47:08.300
So you're just being an old man who's worried about the,

01:47:08.300 --> 01:47:09.460
I think there's always, it goes,

01:47:09.460 --> 01:47:11.100
it's the cycle thing that's ups and downs.

01:47:11.100 --> 01:47:13.780
And I think people rediscovered the power of distributed,

01:47:13.780 --> 01:47:15.420
of decentralized.

01:47:15.460 --> 01:47:17.100
I mean, that's kind of like what the whole,

01:47:17.100 --> 01:47:20.140
like cryptocurrency is trying to exit that.

01:47:20.140 --> 01:47:23.340
I think crypto is just carrying the flame of that spirit

01:47:23.340 --> 01:47:25.020
of like, stuff should be decentralized.

01:47:25.020 --> 01:47:28.380
It's just such a shame that they all got rich, you know?

01:47:28.380 --> 01:47:29.220
Yeah.

01:47:29.220 --> 01:47:30.580
If you took all the money out of crypto,

01:47:30.580 --> 01:47:32.100
it would have been a beautiful place.

01:47:32.100 --> 01:47:32.940
Yeah.

01:47:32.940 --> 01:47:34.340
But no, I mean, these people, you know,

01:47:34.340 --> 01:47:36.940
they sucked all the value out of it and took it.

01:47:37.980 --> 01:47:40.740
Yeah, money kind of corrupts the mind somehow.

01:47:40.740 --> 01:47:41.940
It becomes this drug.

01:47:41.940 --> 01:47:43.500
You corrupted all of crypto.

01:47:43.500 --> 01:47:46.900
You had coins worth billions of dollars that had zero use.

01:47:49.780 --> 01:47:51.140
You still have hope for crypto?

01:47:51.140 --> 01:47:51.980
Sure.

01:47:51.980 --> 01:47:52.820
I have hope for the ideas.

01:47:52.820 --> 01:47:53.740
I really do.

01:47:55.260 --> 01:47:56.100
Yeah.

01:47:56.100 --> 01:47:56.940
I mean, you know,

01:47:58.740 --> 01:48:00.340
I want the US dollar to collapse.

01:48:01.900 --> 01:48:03.500
I do.

01:48:03.500 --> 01:48:04.340
George Watts.

01:48:05.620 --> 01:48:08.140
Well, let me sort of on the AI safety.

01:48:08.140 --> 01:48:10.420
Do you think there's some interesting questions there

01:48:10.420 --> 01:48:13.540
though to solve for the open source community in this case?

01:48:13.540 --> 01:48:17.500
So like alignment, for example, or the control problem.

01:48:17.500 --> 01:48:19.380
Like if you really have super powerful,

01:48:19.380 --> 01:48:20.980
you said it's scary.

01:48:20.980 --> 01:48:21.820
Oh yeah.

01:48:21.820 --> 01:48:22.660
What do we do with it?

01:48:22.660 --> 01:48:24.220
So not control, not centralized control,

01:48:24.220 --> 01:48:27.020
but like, if you were then,

01:48:27.020 --> 01:48:30.340
you're gonna see some guy or gal

01:48:30.340 --> 01:48:34.020
release a super powerful language model, open source.

01:48:34.020 --> 01:48:35.900
And here you are, George Watts thinking,

01:48:35.940 --> 01:48:36.780
Holy shit.

01:48:36.780 --> 01:48:41.260
Okay. What ideas do I have to combat this thing?

01:48:42.180 --> 01:48:44.500
So what ideas would you have?

01:48:44.500 --> 01:48:48.340
I am so much not worried about the machine

01:48:48.340 --> 01:48:50.260
independently doing harm.

01:48:50.260 --> 01:48:52.900
That's what some of these AI safety people seem to think.

01:48:52.900 --> 01:48:54.740
They somehow seem to think that the machine

01:48:54.740 --> 01:48:57.420
like independently is gonna rebel against its creator.

01:48:57.420 --> 01:48:59.540
So you don't think you'll find autonomy?

01:48:59.540 --> 01:49:03.300
No, this is sci-fi B movie garbage.

01:49:03.300 --> 01:49:05.700
Okay. What if the thing writes code?

01:49:06.540 --> 01:49:07.420
It basically writes viruses.

01:49:08.460 --> 01:49:10.580
If the thing writes viruses,

01:49:10.580 --> 01:49:14.100
it's because the human told it to write viruses.

01:49:14.100 --> 01:49:15.380
Yeah, but there's some things you can't

01:49:15.380 --> 01:49:16.460
like put back in the box.

01:49:16.460 --> 01:49:19.420
That's kind of the whole point is it kind of spreads.

01:49:19.420 --> 01:49:21.340
Give it access to the internet, it spreads,

01:49:21.340 --> 01:49:24.100
installs itself, modifies your shit.

01:49:24.100 --> 01:49:26.820
B, B, B, B plot, sci-fi.

01:49:26.820 --> 01:49:27.660
Not real.

01:49:27.660 --> 01:49:28.500
I'm trying to work.

01:49:28.500 --> 01:49:30.260
I'm trying to get better at my plot writing.

01:49:30.260 --> 01:49:31.620
The thing that worries me,

01:49:31.620 --> 01:49:33.820
I mean, we have a real danger to discuss

01:49:33.820 --> 01:49:36.860
and that is bad humans using the thing

01:49:36.860 --> 01:49:39.540
to do whatever bad unaligned AI thing you want.

01:49:39.540 --> 01:49:42.700
But this goes to your previous concern

01:49:42.700 --> 01:49:44.900
that who gets to define who's a good human,

01:49:44.900 --> 01:49:45.740
who is a bad human.

01:49:45.740 --> 01:49:47.460
Nobody does, we give it to everybody.

01:49:47.460 --> 01:49:49.780
And if you do anything besides give it to everybody,

01:49:49.780 --> 01:49:51.620
trust me, the bad humans will get it.

01:49:52.700 --> 01:49:54.020
And that's who gets power.

01:49:54.020 --> 01:49:55.460
It's always the bad humans who get power.

01:49:55.460 --> 01:49:57.660
Okay, power.

01:49:57.660 --> 01:50:01.580
And power turns even slightly good humans to bad.

01:50:01.580 --> 01:50:02.420
Sure.

01:50:02.420 --> 01:50:03.820
What's the only intuition you have?

01:50:03.820 --> 01:50:04.660
I don't know.

01:50:05.980 --> 01:50:06.980
I don't think everyone.

01:50:06.980 --> 01:50:07.980
I don't think everyone.

01:50:07.980 --> 01:50:09.860
I just think that like,

01:50:09.860 --> 01:50:13.300
here's the saying that I put in one of my blog posts.

01:50:13.300 --> 01:50:14.780
When I was in the hacking world,

01:50:14.780 --> 01:50:16.940
I found 95% of people to be good

01:50:16.940 --> 01:50:18.580
and 5% of people to be bad.

01:50:18.580 --> 01:50:19.780
Like just who I personally judged

01:50:19.780 --> 01:50:21.140
as good people and bad people.

01:50:21.140 --> 01:50:23.140
Like they believed about good things for the world.

01:50:23.140 --> 01:50:26.460
They wanted like flourishing and they wanted growth

01:50:26.460 --> 01:50:29.260
and they wanted things I consider good, right?

01:50:29.260 --> 01:50:30.900
I came into the business world with comma

01:50:30.900 --> 01:50:32.700
and I found the exact opposite.

01:50:32.700 --> 01:50:35.740
I found 5% of people good and 95% of people bad.

01:50:35.740 --> 01:50:38.540
I found a world that promotes psychopathy.

01:50:38.540 --> 01:50:39.660
I wonder what that means.

01:50:39.660 --> 01:50:41.620
I wonder if that care, like,

01:50:43.060 --> 01:50:45.180
I wonder if that's anecdotal or if it,

01:50:46.700 --> 01:50:47.540
if that's true to that,

01:50:47.540 --> 01:50:51.620
there's something about capitalism at the core

01:50:51.620 --> 01:50:53.980
that promotes the people that run capitalism

01:50:53.980 --> 01:50:55.500
that promotes psychopathy.

01:50:55.500 --> 01:50:58.100
That saying may of course be my own biases, right?

01:50:58.100 --> 01:50:59.220
That may be my own biases

01:50:59.220 --> 01:51:01.540
that these people are a lot more aligned with me

01:51:01.540 --> 01:51:03.300
than these other people, right?

01:51:03.300 --> 01:51:04.140
Yeah.

01:51:04.140 --> 01:51:07.220
So, you know, I can certainly recognize that,

01:51:07.220 --> 01:51:08.380
but you know, in general, I mean,

01:51:08.380 --> 01:51:10.980
this is like the common sense maxim,

01:51:10.980 --> 01:51:13.500
which is the people who end up getting power

01:51:13.500 --> 01:51:15.740
are never the ones you want with it.

01:51:15.740 --> 01:51:19.220
But do you have a concern of super intelligent AGI,

01:51:20.500 --> 01:51:22.220
open source?

01:51:22.220 --> 01:51:23.820
And then what do you do with that?

01:51:23.820 --> 01:51:25.980
I'm not saying control it, it's open source.

01:51:25.980 --> 01:51:27.860
What do we do with this human species?

01:51:27.860 --> 01:51:28.820
That's not up to me.

01:51:29.420 --> 01:51:31.220
I mean, you know, like, I'm not a central planner.

01:51:31.220 --> 01:51:33.620
Well, not central planner, but you'll probably tweet,

01:51:33.620 --> 01:51:35.740
there's a few days left to live for the human species.

01:51:35.740 --> 01:51:37.340
I have my ideas of what to do with it

01:51:37.340 --> 01:51:39.260
and everyone else has their ideas of what to do with it.

01:51:39.260 --> 01:51:40.260
May the best ideas win.

01:51:40.260 --> 01:51:42.380
But at this point, do you brainstorm,

01:51:42.380 --> 01:51:45.300
like, because it's not regulation,

01:51:45.300 --> 01:51:46.860
it could be decentralized regulation

01:51:46.860 --> 01:51:49.420
where people agree that this is just like,

01:51:49.420 --> 01:51:52.420
we create tools that make it more difficult for you

01:51:53.420 --> 01:51:58.420
to maybe make it more difficult for code to spread,

01:51:59.140 --> 01:52:00.980
you know, antivirus software, this kind of thing.

01:52:00.980 --> 01:52:01.820
But this-

01:52:01.820 --> 01:52:02.900
You're saying that you should build AI firewalls?

01:52:02.900 --> 01:52:03.740
That sounds good.

01:52:03.740 --> 01:52:05.020
You should definitely be running an AI firewall.

01:52:05.020 --> 01:52:05.860
Yeah, right, exactly.

01:52:05.860 --> 01:52:08.420
You should be running an AI firewall to your mind.

01:52:08.420 --> 01:52:09.260
Right.

01:52:09.260 --> 01:52:10.100
You're constantly under, you know.

01:52:10.100 --> 01:52:11.300
That's such an interesting idea.

01:52:11.300 --> 01:52:13.100
Infowars, man, like.

01:52:13.100 --> 01:52:14.660
I don't know if you're being sarcastic or not.

01:52:14.660 --> 01:52:15.500
No, I'm dead serious.

01:52:15.500 --> 01:52:17.100
But I think there's power to that.

01:52:17.100 --> 01:52:22.100
It's like, how do I protect my mind

01:52:22.740 --> 01:52:24.860
from influence of human-like

01:52:24.860 --> 01:52:26.460
or superhuman intelligent bots?

01:52:26.460 --> 01:52:29.820
I is not being, I would pay so much money for that product.

01:52:29.820 --> 01:52:31.500
I would pay so much money for that product.

01:52:31.500 --> 01:52:32.580
I would, you know how much money I'd pay

01:52:32.580 --> 01:52:35.100
just for a spam filter that works?

01:52:35.100 --> 01:52:38.980
Well, on Twitter sometimes I would like to have

01:52:41.980 --> 01:52:46.380
a protection mechanism for my mind from the outrage mobs.

01:52:46.380 --> 01:52:48.060
Because they feel like bot-like behavior.

01:52:48.060 --> 01:52:49.860
It's like, there's a large number of people

01:52:49.860 --> 01:52:52.540
that will just grab a viral narrative

01:52:52.540 --> 01:52:54.500
and attack anyone else that believes otherwise.

01:52:54.500 --> 01:52:55.700
And it's like.

01:52:55.700 --> 01:52:57.700
Whenever someone's telling me some story from the news,

01:52:57.700 --> 01:52:59.580
I'm always like, I don't wanna hear it, CIA op, bro.

01:52:59.580 --> 01:53:00.780
It's a CIA op, bro.

01:53:00.780 --> 01:53:02.220
Like, it doesn't matter if that's true or not.

01:53:02.220 --> 01:53:03.940
It's just trying to influence your mind.

01:53:03.940 --> 01:53:05.420
You're repeating an ad to me.

01:53:06.300 --> 01:53:08.780
With the viral mobs, is it like, yeah, they're.

01:53:08.780 --> 01:53:12.140
No, to me, a defense against those mobs

01:53:12.140 --> 01:53:16.580
is just getting multiple perspectives always from sources

01:53:16.620 --> 01:53:20.700
that make you feel kind of like you're getting smarter.

01:53:21.740 --> 01:53:24.060
And just actually just basically feels good.

01:53:24.060 --> 01:53:26.780
Like a good documentary just feels good.

01:53:26.780 --> 01:53:28.020
Something feels good about it.

01:53:28.020 --> 01:53:29.140
It's well done.

01:53:29.140 --> 01:53:31.420
It's like, oh, okay, I never thought of it this way.

01:53:31.420 --> 01:53:32.660
This just feels good.

01:53:32.660 --> 01:53:33.940
Sometimes the outrage mobs,

01:53:33.940 --> 01:53:35.780
even if they have a good point behind it,

01:53:35.780 --> 01:53:39.180
when they're like mocking and derisive and just aggressive,

01:53:39.180 --> 01:53:42.020
you're with us or against us, this fucking.

01:53:42.020 --> 01:53:43.660
This is why I delete my tweets.

01:53:44.460 --> 01:53:45.700
Yeah, why'd you do that?

01:53:46.660 --> 01:53:48.900
I was, you know, I missed your tweets.

01:53:48.900 --> 01:53:50.140
You know what it is?

01:53:50.140 --> 01:53:52.700
The algorithm promotes toxicity.

01:53:52.700 --> 01:53:54.180
Yeah.

01:53:54.180 --> 01:53:57.460
And like, you know, I think Elon has a much better chance

01:53:57.460 --> 01:54:00.700
of fixing it than the previous regime.

01:54:01.780 --> 01:54:02.620
Yeah.

01:54:02.620 --> 01:54:04.620
But to solve this problem, to solve,

01:54:04.620 --> 01:54:07.980
like to build a social network that is actually not toxic

01:54:07.980 --> 01:54:11.020
without moderation.

01:54:13.060 --> 01:54:14.940
Like not the stick, but carrots.

01:54:14.940 --> 01:54:19.420
So like where people look for goodness.

01:54:19.420 --> 01:54:22.820
So make it catalyze the process of connecting cool people

01:54:22.820 --> 01:54:24.420
and being cool to each other.

01:54:24.420 --> 01:54:25.260
Yeah.

01:54:25.260 --> 01:54:26.900
Without ever censoring.

01:54:26.900 --> 01:54:27.740
Without ever censoring.

01:54:27.740 --> 01:54:30.740
And like Scott Alexander has a blog post I like

01:54:30.740 --> 01:54:33.260
where he talks about like moderation is not censorship.

01:54:33.260 --> 01:54:35.980
Like all moderation you want to put on Twitter,

01:54:35.980 --> 01:54:38.900
like you could totally make this moderation

01:54:38.900 --> 01:54:42.540
like just a, you don't have to block it for everybody.

01:54:42.540 --> 01:54:44.500
You can just have like a filter button

01:54:44.500 --> 01:54:46.100
that people can turn off if they was like safe search

01:54:46.100 --> 01:54:47.020
for Twitter, right?

01:54:47.020 --> 01:54:48.780
Like someone could just turn that off, right?

01:54:48.780 --> 01:54:50.340
So like, but then you'd like take this idea

01:54:50.340 --> 01:54:52.140
to an extreme, right?

01:54:52.140 --> 01:54:54.740
Well, the network should just show you,

01:54:54.740 --> 01:54:56.900
this is a couch surfing CEO thing, right?

01:54:56.900 --> 01:54:59.420
If it shows you right now, these algorithms are designed

01:54:59.420 --> 01:55:00.860
to maximize engagement.

01:55:00.860 --> 01:55:02.900
Well, it turns out outrage maximizes engagement.

01:55:02.900 --> 01:55:06.140
Quirk of human, quirk of the human mind, right?

01:55:06.140 --> 01:55:09.180
Just as I fall for it, everyone falls for it.

01:55:09.180 --> 01:55:11.180
So yeah, you got to figure out how to maximize

01:55:11.180 --> 01:55:12.580
for something other than engagement.

01:55:12.660 --> 01:55:15.100
And I actually believe that you can make money with that too.

01:55:15.100 --> 01:55:17.340
So it's not, I don't think engagement is the only way

01:55:17.340 --> 01:55:18.180
to make money.

01:55:18.180 --> 01:55:19.540
I actually think it's incredible

01:55:19.540 --> 01:55:21.820
that we're starting to see, I think again,

01:55:21.820 --> 01:55:23.420
Yoland is doing so much stuff right with Twitter,

01:55:23.420 --> 01:55:25.220
like charging people money.

01:55:25.220 --> 01:55:26.540
As soon as you charge people money,

01:55:26.540 --> 01:55:29.740
they're no longer the product, they're the customer.

01:55:29.740 --> 01:55:31.100
And then they can start building something

01:55:31.100 --> 01:55:32.620
that's good for the customer and not good

01:55:32.620 --> 01:55:34.780
for the other customer, which is the ad agencies.

01:55:34.780 --> 01:55:37.060
As in picked up steam.

01:55:38.140 --> 01:55:40.180
I pay for Twitter, doesn't even get me anything.

01:55:40.180 --> 01:55:41.940
It's my donation to this new business model

01:55:41.940 --> 01:55:43.020
hopefully working out.

01:55:43.020 --> 01:55:45.740
Sure, but for this business model to work,

01:55:45.740 --> 01:55:48.820
it's like most people should be signed up to Twitter.

01:55:48.820 --> 01:55:52.380
And so the way it was, there was something

01:55:52.380 --> 01:55:54.940
perhaps not compelling or something like this to people.

01:55:54.940 --> 01:55:56.420
Think you need most people at all.

01:55:56.420 --> 01:55:58.780
I think that, why do I need most people, right?

01:55:58.780 --> 01:56:00.260
I don't make an 8,000 person company,

01:56:00.260 --> 01:56:01.500
make a 50 person company.

01:56:03.580 --> 01:56:05.380
Well, so speaking of which,

01:56:06.900 --> 01:56:08.500
you worked at Twitter for a bit.

01:56:08.500 --> 01:56:09.340
I did.

01:56:09.340 --> 01:56:10.260
As an intern.

01:56:10.260 --> 01:56:11.420
Mm-hmm.

01:56:11.420 --> 01:56:13.100
The world's greatest intern.

01:56:13.100 --> 01:56:13.940
Yeah.

01:56:13.940 --> 01:56:14.780
All right.

01:56:14.780 --> 01:56:15.620
There's been better.

01:56:15.620 --> 01:56:17.420
There's been better.

01:56:17.420 --> 01:56:18.860
Tell me about your time at Twitter.

01:56:18.860 --> 01:56:20.180
How did it come about?

01:56:20.180 --> 01:56:22.780
And what did you learn from the experience?

01:56:22.780 --> 01:56:27.780
So I deleted my first Twitter in 2010.

01:56:28.500 --> 01:56:31.020
I had over 100,000 followers back

01:56:31.020 --> 01:56:32.900
when that actually meant something.

01:56:32.900 --> 01:56:36.380
And I just saw, you know,

01:56:37.740 --> 01:56:39.420
my coworker summarized it well.

01:56:39.420 --> 01:56:42.660
He's like, whenever I see someone's Twitter page,

01:56:42.660 --> 01:56:45.420
I either think the same of them or less of them.

01:56:45.420 --> 01:56:46.900
I never think more of them.

01:56:46.900 --> 01:56:47.740
Yeah.

01:56:47.740 --> 01:56:50.020
Right, like, I don't want to mention any names,

01:56:50.020 --> 01:56:51.340
but like some people who like, you know,

01:56:51.340 --> 01:56:52.700
maybe you would like read their books

01:56:52.700 --> 01:56:53.620
and you would respect them.

01:56:53.620 --> 01:56:56.260
You see them on Twitter and you're like,

01:56:56.260 --> 01:56:57.100
okay, dude.

01:56:58.700 --> 01:56:59.540
Yeah.

01:56:59.540 --> 01:57:01.980
But there are some people who are the same.

01:57:01.980 --> 01:57:03.940
You know who I respect a lot?

01:57:03.940 --> 01:57:06.500
Are people that just post really good technical stuff.

01:57:06.500 --> 01:57:07.940
Yeah.

01:57:07.940 --> 01:57:11.100
And I guess, I don't know.

01:57:11.100 --> 01:57:13.020
I think I respect them more for it.

01:57:13.020 --> 01:57:15.820
Cause you realize, oh, this wasn't,

01:57:15.820 --> 01:57:18.660
there's like so much depth to this person,

01:57:18.660 --> 01:57:21.580
to their technical understanding of so many different topics.

01:57:21.580 --> 01:57:22.420
Okay.

01:57:22.420 --> 01:57:23.740
So I try to follow people.

01:57:23.740 --> 01:57:25.700
I try to consume stuff

01:57:25.700 --> 01:57:27.900
that's technical machine learning content.

01:57:27.900 --> 01:57:31.580
There's probably a few of those people.

01:57:31.580 --> 01:57:34.020
And the problem is inherently

01:57:34.020 --> 01:57:36.460
what the algorithm rewards, right?

01:57:36.460 --> 01:57:38.180
And people think about these algorithms.

01:57:38.180 --> 01:57:40.300
People think that they are terrible, awful things.

01:57:40.300 --> 01:57:42.460
And you know, I love that Elon open-sourced it

01:57:42.460 --> 01:57:44.700
because I mean, what it does is actually pretty obvious.

01:57:44.700 --> 01:57:47.260
It just predicts what you are likely to retweet

01:57:47.260 --> 01:57:49.700
and like and linger on.

01:57:49.700 --> 01:57:50.540
That's what all these algorithms do.

01:57:50.540 --> 01:57:51.500
That's what TikTok does.

01:57:51.500 --> 01:57:54.340
So all these recommendation engines do.

01:57:54.340 --> 01:57:57.180
And it turns out that the thing

01:57:57.180 --> 01:58:00.060
that you are most likely to interact with is outrage.

01:58:00.060 --> 01:58:02.020
And that's a quirk of the human condition.

01:58:04.260 --> 01:58:06.020
I mean, and there's different flavors of outrage.

01:58:06.020 --> 01:58:09.500
It doesn't have to be, it could be mockery.

01:58:09.500 --> 01:58:10.340
You could be outraged.

01:58:10.340 --> 01:58:11.820
The topic of outrage could be different.

01:58:11.820 --> 01:58:12.660
It could be an idea.

01:58:12.660 --> 01:58:13.500
It could be a person.

01:58:13.500 --> 01:58:17.260
It could be, and maybe there's a better word than outrage.

01:58:17.260 --> 01:58:18.140
It could be drama.

01:58:18.140 --> 01:58:19.740
Sure, drama. All this kind of stuff.

01:58:19.740 --> 01:58:20.580
Yeah.

01:58:20.580 --> 01:58:22.260
But doesn't feel like when you consume it,

01:58:22.260 --> 01:58:24.060
it's a constructive thing for the individuals

01:58:24.060 --> 01:58:26.220
that consume it in the long-term.

01:58:26.220 --> 01:58:27.060
Yeah.

01:58:27.060 --> 01:58:30.540
So my time there, I absolutely couldn't believe, you know,

01:58:30.540 --> 01:58:34.460
I got crazy amount of hate, you know,

01:58:34.660 --> 01:58:36.220
on Twitter for working at Twitter.

01:58:36.220 --> 01:58:39.100
It seems like people associated with this.

01:58:39.100 --> 01:58:41.620
I think maybe you were exposed to some of this.

01:58:41.620 --> 01:58:44.060
So connection to Elon or is it working at Twitter?

01:58:44.060 --> 01:58:46.740
Twitter and Elon, like the whole-

01:58:46.740 --> 01:58:49.860
Elon's gotten a bit spicy during that time.

01:58:49.860 --> 01:58:51.180
A bit political, a bit-

01:58:51.180 --> 01:58:52.340
Yeah.

01:58:52.340 --> 01:58:54.100
Yeah, you know, I remember one of my tweets,

01:58:54.100 --> 01:58:57.300
it was never go full Republican, and Elon liked it.

01:58:57.300 --> 01:59:00.300
You know, I think, I think, you know.

01:59:00.820 --> 01:59:05.140
Oh, boy.

01:59:05.140 --> 01:59:06.860
Yeah, I mean, there's a rollercoaster of that,

01:59:06.860 --> 01:59:10.020
but being political on Twitter, boy.

01:59:10.020 --> 01:59:11.140
Yeah.

01:59:11.140 --> 01:59:14.900
And also being just attacking anybody on Twitter,

01:59:14.900 --> 01:59:17.500
it comes back at you harder.

01:59:17.500 --> 01:59:19.420
And if it's political and attacks-

01:59:19.420 --> 01:59:22.340
Sure, sure, absolutely.

01:59:22.340 --> 01:59:27.340
And then letting sort of de-platform people back on,

01:59:28.300 --> 01:59:33.300
even adds more fun to the, to the, to the beautiful chaos.

01:59:33.300 --> 01:59:36.580
I was hoping, and like, I remember when Elon talked

01:59:36.580 --> 01:59:39.500
about buying Twitter, like six months earlier,

01:59:39.500 --> 01:59:42.380
he was talking about like a principled commitment

01:59:42.380 --> 01:59:46.420
to free speech, and I'm a big believer and fan of that.

01:59:46.420 --> 01:59:49.620
I would love to see an actual principled commitment

01:59:49.620 --> 01:59:50.940
to free speech.

01:59:50.940 --> 01:59:53.180
Of course, this isn't quite what happened.

01:59:53.180 --> 01:59:56.580
Instead of the oligarchy deciding what to ban,

01:59:57.540 --> 02:00:00.500
you had a monarchy deciding what to ban, right?

02:00:00.500 --> 02:00:03.660
Instead of, you know, all the Twitter files, shadow,

02:00:03.660 --> 02:00:06.140
and really the oligarchy just decides what?

02:00:06.140 --> 02:00:08.220
Cloth masks are ineffective against COVID.

02:00:08.220 --> 02:00:09.140
That's a true statement.

02:00:09.140 --> 02:00:10.700
Every doctor in 2019 knew it,

02:00:10.700 --> 02:00:12.180
and now I'm banned on Twitter for saying it.

02:00:12.180 --> 02:00:13.020
Interesting.

02:00:13.020 --> 02:00:14.100
Oligarchy.

02:00:14.100 --> 02:00:16.740
So now you have a monarchy and, you know,

02:00:16.740 --> 02:00:19.540
he bans things he doesn't like.

02:00:19.540 --> 02:00:20.900
So, you know, it's just, it's just different,

02:00:20.900 --> 02:00:23.780
it's different power and like, you know, maybe I,

02:00:23.780 --> 02:00:25.900
maybe I align more with him than with the oligarchy.

02:00:26.220 --> 02:00:28.940
But it's not free speech absolutism.

02:00:28.940 --> 02:00:31.860
But I feel like being a free speech absolutist

02:00:31.860 --> 02:00:35.140
on the social network requires you to also have tools

02:00:35.140 --> 02:00:40.140
for the individuals to control what they consume easier.

02:00:40.900 --> 02:00:45.660
Like, not censor, but just like control, like,

02:00:45.660 --> 02:00:48.940
oh, I'd like to see more cats and less politics.

02:00:48.940 --> 02:00:51.340
And this isn't even remotely controversial.

02:00:51.340 --> 02:00:53.300
This is just saying you want to give paying customers

02:00:53.300 --> 02:00:54.500
for a product what they want.

02:00:54.500 --> 02:00:55.340
Yeah.

02:00:55.340 --> 02:00:56.580
And not through the process of censorship,

02:00:56.580 --> 02:00:58.220
but through a process of like-

02:00:58.220 --> 02:00:59.300
It's individualized, right?

02:00:59.300 --> 02:01:01.260
It's individualized, transparent censorship,

02:01:01.260 --> 02:01:02.500
which is honestly what I want.

02:01:02.500 --> 02:01:03.340
What is an ad blocker?

02:01:03.340 --> 02:01:05.020
It's individualized, transparent censorship, right?

02:01:05.020 --> 02:01:08.580
Yeah, but censorship is a strong word,

02:01:08.580 --> 02:01:10.220
and people are very sensitive to.

02:01:10.220 --> 02:01:12.860
I know, but, you know, I just use words to describe

02:01:12.860 --> 02:01:13.900
what they functionally are.

02:01:13.900 --> 02:01:14.740
And what is an ad blocker?

02:01:14.740 --> 02:01:15.820
It's just censorship.

02:01:15.820 --> 02:01:16.660
When I look at you right now-

02:01:16.660 --> 02:01:17.660
But I love what you're censoring.

02:01:17.660 --> 02:01:21.780
I'm looking at you, I'm censoring everything else out

02:01:21.780 --> 02:01:23.980
when my mind is focused on you.

02:01:24.260 --> 02:01:25.780
You can use the word censorship that way,

02:01:25.780 --> 02:01:27.580
but usually when people get very sensitive

02:01:27.580 --> 02:01:28.940
about the censorship thing.

02:01:28.940 --> 02:01:31.660
I think when you have, when anyone is allowed

02:01:31.660 --> 02:01:35.380
to say anything, you should probably have tools

02:01:35.380 --> 02:01:39.460
that maximize the quality of the experience for individuals.

02:01:39.460 --> 02:01:42.820
It's like, you know, for me, like what I really value,

02:01:42.820 --> 02:01:45.620
boy, it would be amazing to somehow figure out

02:01:45.620 --> 02:01:46.820
how to do that.

02:01:46.820 --> 02:01:48.820
I love disagreement and debate,

02:01:48.820 --> 02:01:51.740
and people who disagree with each other disagree with me,

02:01:51.740 --> 02:01:53.420
especially in the space of ideas.

02:01:53.420 --> 02:01:54.940
But the high quality ones.

02:01:54.940 --> 02:01:56.620
So not derision, right?

02:01:56.620 --> 02:01:58.300
Maslow's hierarchy of argument.

02:01:58.300 --> 02:01:59.980
I think that's a real word for it.

02:01:59.980 --> 02:02:00.820
Probably.

02:02:00.820 --> 02:02:02.980
There's just the way of talking that's like snarky

02:02:02.980 --> 02:02:06.260
and so on, that somehow is, gets people on Twitter

02:02:06.260 --> 02:02:07.860
and they get excited and so on.

02:02:07.860 --> 02:02:09.980
You have like ad hominem refuting the central point.

02:02:09.980 --> 02:02:11.460
I've like seen this as an actual pyramid something.

02:02:11.460 --> 02:02:14.660
Yeah, it's, yeah, and it's like all of it,

02:02:14.660 --> 02:02:16.860
all the wrong stuff is attractive to people.

02:02:16.860 --> 02:02:18.860
I mean, we can just train a classifier to absolutely say

02:02:18.860 --> 02:02:21.940
what level of Maslow's hierarchy of argument are you at.

02:02:21.940 --> 02:02:22.780
Yeah.

02:02:22.780 --> 02:02:23.740
And I'm like, okay, cool.

02:02:23.740 --> 02:02:25.940
I turned on the no ad hominem filter.

02:02:27.420 --> 02:02:29.660
I wonder if there's a social network that will allow you

02:02:29.660 --> 02:02:31.060
to have that kind of filter.

02:02:31.060 --> 02:02:34.660
Yeah, so here's a problem with that.

02:02:35.740 --> 02:02:37.940
It's not going to win in a free market.

02:02:38.860 --> 02:02:41.180
What wins in a free market is all television today

02:02:41.180 --> 02:02:43.740
is reality television because it's engaging, right?

02:02:43.740 --> 02:02:47.180
If engaging is what wins in a free market, right?

02:02:47.180 --> 02:02:50.500
So it becomes hard to keep these other more nuanced values.

02:02:52.980 --> 02:02:56.220
Well, okay, so that's the experience of being on Twitter,

02:02:56.220 --> 02:02:59.620
but then you got a chance to also together

02:02:59.620 --> 02:03:02.300
with other engineers and with Elon sort of look,

02:03:02.300 --> 02:03:04.740
brainstorm when you step into a code base

02:03:04.740 --> 02:03:06.780
that's been around for a long time.

02:03:06.780 --> 02:03:08.980
There's other social networks, Facebook.

02:03:08.980 --> 02:03:12.180
This is old code bases and you step in and see,

02:03:12.180 --> 02:03:17.100
okay, how do we make with a fresh mind progress

02:03:17.100 --> 02:03:17.940
on this code base?

02:03:17.940 --> 02:03:19.980
Like what did you learn about software engineering,

02:03:19.980 --> 02:03:22.140
about programming from just experiencing that?

02:03:22.140 --> 02:03:25.180
So my technical recommendation to Elon,

02:03:25.180 --> 02:03:27.180
and I said this on the Twitter spaces afterward,

02:03:27.180 --> 02:03:31.020
I said this many times during my brief internship

02:03:33.260 --> 02:03:36.380
was that you need refactors before features.

02:03:37.580 --> 02:03:40.860
This code base was, and look, I've worked at Google,

02:03:40.860 --> 02:03:42.340
I've worked at Facebook.

02:03:42.340 --> 02:03:46.740
Facebook has the best code, then Google, then Twitter.

02:03:46.740 --> 02:03:47.740
And you know what?

02:03:47.740 --> 02:03:48.940
You can know this because look

02:03:48.940 --> 02:03:50.180
at the machine learning frameworks, right?

02:03:50.220 --> 02:03:52.500
Facebook released PyTorch, Google released TensorFlow,

02:03:52.500 --> 02:03:57.500
and Twitter released, okay, so you know.

02:03:57.540 --> 02:04:00.460
It's a proxy, but yeah, the Google code base

02:04:00.460 --> 02:04:01.300
is quite interesting.

02:04:01.300 --> 02:04:02.780
There's a lot of really good software engineers there,

02:04:02.780 --> 02:04:04.820
but the code base is very large.

02:04:04.820 --> 02:04:07.860
The code base was good in 2005, right?

02:04:07.860 --> 02:04:08.700
It looks like 2005.

02:04:08.700 --> 02:04:10.540
There's so many products, so many teams, right?

02:04:10.540 --> 02:04:15.540
It's very difficult to, I feel like Twitter does less,

02:04:15.580 --> 02:04:17.820
obviously much less than Google

02:04:18.820 --> 02:04:23.220
in terms of the set of features, right?

02:04:23.220 --> 02:04:26.500
So I can imagine the number of software engineers

02:04:26.500 --> 02:04:29.180
that could recreate Twitter is much smaller

02:04:29.180 --> 02:04:30.540
than to recreate Google.

02:04:30.540 --> 02:04:33.620
Yeah, I still believe in the amount of hate I got

02:04:33.620 --> 02:04:36.580
for saying this, that 50 people could build

02:04:36.580 --> 02:04:38.660
and maintain Twitter pretty comfortably.

02:04:38.660 --> 02:04:41.660
What's the nature of the hate?

02:04:41.660 --> 02:04:42.940
That you don't know what you're talking about?

02:04:42.940 --> 02:04:44.660
You know what it is, and it's the same,

02:04:44.660 --> 02:04:47.460
this is my summary of the hate I get on hacker news.

02:04:47.660 --> 02:04:51.380
It's like, when I say I'm going to do something,

02:04:51.380 --> 02:04:56.220
they have to believe that it's impossible

02:04:56.220 --> 02:04:59.660
because if doing things was possible,

02:04:59.660 --> 02:05:01.220
they'd have to do some soul searching

02:05:01.220 --> 02:05:03.500
and ask the question, why didn't they do anything?

02:05:03.500 --> 02:05:06.060
So when you say, and I do say that's where the hate comes from.

02:05:06.060 --> 02:05:08.500
When you say, well, there's a core truth to that, yeah.

02:05:08.500 --> 02:05:10.700
So when you say I'm gonna solve self-driving,

02:05:11.980 --> 02:05:14.100
people go like, what are your credentials?

02:05:14.100 --> 02:05:15.300
What the hell are you talking about?

02:05:15.300 --> 02:05:17.260
What is, this is an extremely difficult problem.

02:05:17.260 --> 02:05:18.860
Of course, you're a noob that doesn't understand

02:05:18.860 --> 02:05:19.860
the problem deeply.

02:05:21.620 --> 02:05:23.860
I mean, that was the same nature of hate

02:05:23.860 --> 02:05:25.540
that probably Elon got when he first talked

02:05:25.540 --> 02:05:26.780
about autonomous driving.

02:05:28.220 --> 02:05:30.180
But there's pros and cons to that

02:05:30.180 --> 02:05:33.140
because there is experts in this world.

02:05:33.140 --> 02:05:35.340
No, but the mockers aren't experts.

02:05:35.340 --> 02:05:38.260
The people who are mocking are not experts

02:05:38.260 --> 02:05:39.780
with carefully reasoned arguments

02:05:39.780 --> 02:05:42.820
about why you need 8,000 people to run a bird app.

02:05:42.820 --> 02:05:46.420
But the people are gonna lose their jobs.

02:05:47.100 --> 02:05:48.660
But also there's the software engineers

02:05:48.660 --> 02:05:49.500
that probably could have said,

02:05:49.500 --> 02:05:51.620
no, it's a lot more complicated than you realize,

02:05:51.620 --> 02:05:53.820
but maybe it doesn't need to be so complicated.

02:05:53.820 --> 02:05:55.620
You know, some people in the world

02:05:55.620 --> 02:05:56.940
like to create complexity.

02:05:56.940 --> 02:05:58.620
Some people in the world thrive under complexity,

02:05:58.620 --> 02:05:59.660
like lawyers, right?

02:05:59.660 --> 02:06:01.180
Lawyers want the world to be more complex

02:06:01.180 --> 02:06:02.020
because you need more lawyers,

02:06:02.020 --> 02:06:03.740
you need more legal hours, right?

02:06:04.940 --> 02:06:05.820
I think that's another.

02:06:05.820 --> 02:06:07.420
If there's two great evils in the world,

02:06:07.420 --> 02:06:09.260
it's centralization and complexity.

02:06:09.260 --> 02:06:14.260
Yeah, and one of the sort of hidden side effects

02:06:14.540 --> 02:06:19.540
of software engineering is like finding pleasure

02:06:20.100 --> 02:06:21.020
and complexity.

02:06:22.420 --> 02:06:24.300
I mean, I don't remember just taking

02:06:24.300 --> 02:06:25.860
all the software engineering courses

02:06:25.860 --> 02:06:28.260
and just doing programming and just coming up

02:06:28.260 --> 02:06:33.060
in this object-oriented programming kind of idea.

02:06:33.060 --> 02:06:35.500
You don't, like not often do people tell you,

02:06:35.500 --> 02:06:38.260
like do the simplest possible thing.

02:06:38.260 --> 02:06:42.820
Like a professor, a teacher is not gonna get in front,

02:06:43.540 --> 02:06:45.420
this is the simplest way to do it.

02:06:45.420 --> 02:06:47.820
They'll say like, this is the right way,

02:06:47.820 --> 02:06:51.020
and the right way at least for a long time,

02:06:51.020 --> 02:06:53.700
especially I came up with like Java, right?

02:06:53.700 --> 02:06:56.060
Is there's so much boilerplate,

02:06:56.060 --> 02:06:59.020
so much like so many classes,

02:06:59.020 --> 02:07:02.540
so many like designs and architectures and so on,

02:07:02.540 --> 02:07:05.940
like planning for features far into the future

02:07:05.940 --> 02:07:08.140
and planning poorly and all this kind of stuff.

02:07:08.140 --> 02:07:10.020
And then there's this like code base

02:07:10.020 --> 02:07:12.060
that follows you along and puts pressure on you

02:07:12.060 --> 02:07:16.060
and nobody knows what like parts, different parts do,

02:07:16.060 --> 02:07:18.260
which slows everything down is the kind of bureaucracy

02:07:18.260 --> 02:07:20.940
that's instilled in the code as a result of that.

02:07:20.940 --> 02:07:22.580
But then you feel like, oh, well,

02:07:22.580 --> 02:07:25.340
I follow good software engineering practices.

02:07:25.340 --> 02:07:26.460
It's an interesting trade-off

02:07:26.460 --> 02:07:30.100
because then you look at like the ghetto-ness of like Pearl

02:07:30.100 --> 02:07:32.500
and the old like, how quickly you could just write

02:07:32.500 --> 02:07:34.460
a couple lines and just get stuff done.

02:07:34.460 --> 02:07:37.300
That trade-off is interesting or bash or whatever,

02:07:37.300 --> 02:07:39.380
these kind of ghetto things you can do in Linux.

02:07:39.380 --> 02:07:41.940
One of my favorite things to look at today

02:07:41.940 --> 02:07:43.860
is how much do you trust your tests, right?

02:07:43.860 --> 02:07:45.620
We've put a ton of effort in Comma

02:07:45.620 --> 02:07:47.460
and I've put a ton of effort in TinyGrad

02:07:47.460 --> 02:07:51.420
into making sure if you change the code and the tests pass,

02:07:51.420 --> 02:07:52.900
that you didn't break the code.

02:07:52.900 --> 02:07:55.060
Now, this obviously is not always true,

02:07:55.060 --> 02:07:56.700
but the closer that is to true,

02:07:56.700 --> 02:07:58.060
the more you trust your tests,

02:07:58.060 --> 02:07:59.740
the more you're like, oh, I got a pull request

02:07:59.740 --> 02:08:02.380
and the tests pass, I feel okay to merge that,

02:08:02.380 --> 02:08:03.580
the faster you can make progress.

02:08:03.580 --> 02:08:05.100
So you're always programming with tests in mind,

02:08:05.100 --> 02:08:07.260
developing tests with that in mind

02:08:07.260 --> 02:08:08.820
that if it passes, it should be good.

02:08:08.820 --> 02:08:10.340
And Twitter had a.

02:08:10.340 --> 02:08:11.180
Not that.

02:08:12.300 --> 02:08:13.140
So.

02:08:13.140 --> 02:08:15.420
It was impossible to make progress in the code base.

02:08:15.420 --> 02:08:17.340
What other stuff can you say about the code base

02:08:17.340 --> 02:08:18.700
that made it difficult?

02:08:19.580 --> 02:08:21.660
What are some interesting sort of quirks?

02:08:21.660 --> 02:08:24.780
Broadly speaking, from that compared to

02:08:24.780 --> 02:08:27.900
just your experience with Comma and everywhere else.

02:08:27.900 --> 02:08:30.660
The real thing that, I spoke to a bunch of,

02:08:31.940 --> 02:08:34.740
you know, like individual contributors to Twitter

02:08:34.740 --> 02:08:36.060
and I just had stashed.

02:08:36.060 --> 02:08:38.500
I'm like, okay, so like, what's wrong with this place?

02:08:38.500 --> 02:08:39.860
Why does this code look like this?

02:08:39.860 --> 02:08:40.940
And they explained to me

02:08:40.940 --> 02:08:43.540
what Twitter's promotion system was.

02:08:43.540 --> 02:08:45.260
The way that you got promoted at Twitter

02:08:45.260 --> 02:08:47.900
was you wrote a library that a lot of people used.

02:08:49.260 --> 02:08:50.780
Right?

02:08:50.780 --> 02:08:54.540
So some guy wrote an NGINX replacement for Twitter.

02:08:54.540 --> 02:08:56.380
Why does Twitter need an NGINX replacement?

02:08:56.380 --> 02:08:58.500
What was wrong with NGINX?

02:08:58.500 --> 02:09:00.500
Well, you see, you're not gonna get promoted

02:09:00.500 --> 02:09:03.340
if you use NGINX, but if you write a replacement

02:09:03.340 --> 02:09:05.060
and lots of people start using it

02:09:05.060 --> 02:09:07.180
as the Twitter front end for their product,

02:09:07.180 --> 02:09:08.420
then you're gonna get promoted, right?

02:09:09.300 --> 02:09:10.140
So interesting, because like,

02:09:10.140 --> 02:09:13.340
from an individual perspective, how do you incentivize?

02:09:13.340 --> 02:09:14.740
How do you create the kind of incentives

02:09:14.740 --> 02:09:18.180
that will lead to a great code base?

02:09:18.180 --> 02:09:20.460
Okay, what's the answer to that?

02:09:20.460 --> 02:09:25.460
So what I do at Comma and at TinyCorp

02:09:26.780 --> 02:09:28.180
is you have to explain it to me.

02:09:28.180 --> 02:09:30.340
You have to explain to me what this code does, right?

02:09:30.340 --> 02:09:31.980
And if I can sit there and come up

02:09:31.980 --> 02:09:35.620
with a simpler way to do it, you have to rewrite it.

02:09:35.620 --> 02:09:37.140
You have to agree with me about the simpler way.

02:09:37.540 --> 02:09:39.060
Obviously, we can have a conversation about this.

02:09:39.060 --> 02:09:41.660
It's not dictatorial, but if you're like,

02:09:41.660 --> 02:09:44.340
wow, wait, that actually is way simpler.

02:09:44.340 --> 02:09:47.660
Like, the simplicity is important, right?

02:09:47.660 --> 02:09:51.060
But that requires people that overlook the code

02:09:51.060 --> 02:09:54.100
at the highest levels to be like, okay.

02:09:54.100 --> 02:09:55.620
It requires technical leadership, you trust.

02:09:55.620 --> 02:09:57.220
Yeah, technical leadership.

02:09:58.140 --> 02:10:00.700
So managers or whatever should have to have

02:10:00.700 --> 02:10:03.140
technical savvy, deep technical savvy.

02:10:03.140 --> 02:10:04.300
Managers should be better programmers

02:10:04.300 --> 02:10:05.580
than the people who they manage.

02:10:05.580 --> 02:10:09.700
Yeah, and that's not always obvious, trivial to create,

02:10:09.700 --> 02:10:11.420
especially at large companies.

02:10:11.420 --> 02:10:12.660
Managers get soft.

02:10:12.660 --> 02:10:15.300
And this is just, I've instilled this culture at Comma

02:10:15.300 --> 02:10:17.700
and Comma has better programmers than me who work there.

02:10:17.700 --> 02:10:21.060
But again, I'm like the old guy from Goodwill Hunting.

02:10:21.060 --> 02:10:24.980
It's like, look, man, I might not be as good as you,

02:10:24.980 --> 02:10:27.060
but I can see the difference between me and you, right?

02:10:27.060 --> 02:10:28.220
And this is what you need.

02:10:28.220 --> 02:10:29.140
This is what you need at the top.

02:10:29.140 --> 02:10:31.220
Or you don't necessarily need the manager

02:10:31.220 --> 02:10:32.700
to be the absolute best.

02:10:32.700 --> 02:10:34.900
I shouldn't say that, but they need to be able

02:10:34.900 --> 02:10:36.500
to recognize skill.

02:10:36.500 --> 02:10:38.700
Yeah, and have good intuition.

02:10:38.700 --> 02:10:42.020
Intuition that's laden with wisdom from all the battles

02:10:42.020 --> 02:10:44.940
of trying to reduce complexity in code bases.

02:10:44.940 --> 02:10:46.980
You know, I took a political approach at Comma too

02:10:46.980 --> 02:10:47.900
that I think is pretty interesting.

02:10:47.900 --> 02:10:49.980
I think Elon takes the same political approach.

02:10:50.980 --> 02:10:53.420
You know, Google had no politics.

02:10:53.420 --> 02:10:55.140
And what ended up happening is the absolute worst

02:10:55.140 --> 02:10:56.500
kind of politics took over.

02:10:57.380 --> 02:10:59.140
Comma has an extreme amount of politics

02:10:59.140 --> 02:11:02.100
and they're all mine and no dissidence is tolerated.

02:11:02.100 --> 02:11:03.580
So it's a dictatorship.

02:11:03.620 --> 02:11:05.820
Yep, it's an absolute dictatorship, right?

02:11:05.820 --> 02:11:07.140
Elon does the same thing.

02:11:07.140 --> 02:11:10.140
Now, the thing about my dictatorship is here are my values.

02:11:11.300 --> 02:11:12.780
Yeah, so it's transparent.

02:11:12.780 --> 02:11:13.620
It's transparent.

02:11:13.620 --> 02:11:14.900
It's a transparent dictatorship, right?

02:11:14.900 --> 02:11:16.660
And you can choose to opt in or, you know,

02:11:16.660 --> 02:11:17.620
you get free exit, right?

02:11:17.620 --> 02:11:18.500
That's the beauty of companies.

02:11:18.500 --> 02:11:20.700
If you don't like the dictatorship, you quit.

02:11:22.060 --> 02:11:27.060
So you mentioned rewrite before or refactor before features.

02:11:28.660 --> 02:11:30.860
If you were to refactor the Twitter code base,

02:11:30.860 --> 02:11:32.140
what would that look like?

02:11:32.140 --> 02:11:33.620
And maybe also comment on

02:11:33.620 --> 02:11:35.580
how difficult is it to refactor?

02:11:35.580 --> 02:11:37.820
The main thing I would do is first of all,

02:11:37.820 --> 02:11:40.220
identify the pieces and then put tests

02:11:40.220 --> 02:11:42.340
in between the pieces, right?

02:11:42.340 --> 02:11:43.180
So there's all these different,

02:11:43.180 --> 02:11:45.140
Twitter has a microservice architecture.

02:11:46.540 --> 02:11:48.260
There's all these different microservices.

02:11:48.260 --> 02:11:49.900
And the thing that I was working on there,

02:11:49.900 --> 02:11:53.540
look, like, you know, George didn't know any JavaScript.

02:11:53.540 --> 02:11:55.940
He asked how to fix search, blah, blah, blah, blah, blah.

02:11:55.940 --> 02:11:59.740
Look, man, like, the thing is, like, I just, you know,

02:11:59.740 --> 02:12:02.380
I'm upset that the way that this whole thing was portrayed

02:12:02.380 --> 02:12:05.780
because it wasn't like taken by people, like, honestly.

02:12:05.780 --> 02:12:07.940
It wasn't like by, it was taken by people

02:12:07.940 --> 02:12:10.140
who started out with a bad faith assumption.

02:12:10.140 --> 02:12:10.980
Yeah.

02:12:10.980 --> 02:12:12.500
And I mean, look, I can't like.

02:12:12.500 --> 02:12:14.340
And you as a programmer were just being transparent

02:12:14.340 --> 02:12:16.940
out there actually having like fun.

02:12:16.940 --> 02:12:19.460
And like, this is what programming should be about.

02:12:19.460 --> 02:12:21.340
I love that Elon gave me this opportunity.

02:12:21.340 --> 02:12:22.180
Yeah.

02:12:22.180 --> 02:12:23.000
Like really, it does.

02:12:23.000 --> 02:12:24.580
And like, you know, he came on my,

02:12:24.580 --> 02:12:26.580
the day I quit, he came on my Twitter spaces afterward

02:12:26.580 --> 02:12:27.580
and we had a conversation.

02:12:27.580 --> 02:12:29.740
Like, I just, I respect that so much.

02:12:29.740 --> 02:12:30.580
Yeah.

02:12:30.580 --> 02:12:32.500
And it's also inspiring to just engineers and programmers

02:12:32.500 --> 02:12:33.660
and just, it's cool.

02:12:33.660 --> 02:12:34.500
It should be fun.

02:12:34.500 --> 02:12:37.940
The people that were hating on it, it's like, ah, man.

02:12:37.940 --> 02:12:38.980
It was fun.

02:12:38.980 --> 02:12:39.820
It was fun.

02:12:39.820 --> 02:12:41.380
It was stressful, but I felt like, you know,

02:12:41.380 --> 02:12:43.380
it was at like a cool like point in history.

02:12:43.380 --> 02:12:44.740
And like, I hope I was useful.

02:12:44.740 --> 02:12:46.980
I probably kind of wasn't, but like, maybe I was.

02:12:46.980 --> 02:12:48.980
Well, you also were one of the people

02:12:48.980 --> 02:12:51.620
that kind of made a strong case to refactor.

02:12:51.620 --> 02:12:52.460
Yeah.

02:12:52.460 --> 02:12:55.580
And that's a really interesting thing to raise.

02:12:55.620 --> 02:12:58.580
Like maybe that is the right, you know,

02:12:58.580 --> 02:12:59.900
the timing of that is really interesting.

02:12:59.900 --> 02:13:01.940
If you look at just the development of autopilot,

02:13:01.940 --> 02:13:06.260
you know, going from Mobileye to just like more,

02:13:06.260 --> 02:13:08.820
if you look at the history of semi-autonomous driving

02:13:08.820 --> 02:13:13.820
in Tesla is more and more like you could say refactoring

02:13:14.460 --> 02:13:17.900
or starting from scratch, redeveloping from scratch.

02:13:17.900 --> 02:13:19.780
It's refactoring all the way down.

02:13:19.780 --> 02:13:21.980
And like, and the question is like,

02:13:21.980 --> 02:13:24.060
can you do that sooner?

02:13:24.060 --> 02:13:27.020
Can you maintain product profitability?

02:13:27.020 --> 02:13:29.100
And like, what's the right time to do it?

02:13:29.100 --> 02:13:30.340
How do you do it?

02:13:30.340 --> 02:13:32.140
You know, on any one day, it's like,

02:13:32.140 --> 02:13:33.540
you don't want to pull off the band-aid.

02:13:33.540 --> 02:13:36.420
It's like, everything works.

02:13:36.420 --> 02:13:39.100
It's just like little fix here and there,

02:13:39.100 --> 02:13:41.020
but maybe starting from scratch.

02:13:41.020 --> 02:13:42.900
This is the main philosophy of tiny grad.

02:13:42.900 --> 02:13:44.700
You have never refactored enough.

02:13:44.700 --> 02:13:45.940
Your code can get smaller.

02:13:45.940 --> 02:13:47.060
Your code can get simpler.

02:13:47.060 --> 02:13:48.980
Your ideas can be more elegant.

02:13:48.980 --> 02:13:52.060
But would you consider, you know,

02:13:52.060 --> 02:13:55.300
say you were like running Twitter development teams,

02:13:55.300 --> 02:13:58.820
engineering teams, would you go as far

02:13:58.820 --> 02:14:00.580
as like different programming language?

02:14:00.580 --> 02:14:03.020
Just go that far?

02:14:03.020 --> 02:14:07.260
I mean, the first thing that I would do is build tests.

02:14:07.260 --> 02:14:10.380
The first thing I would do is get a CI

02:14:10.380 --> 02:14:12.980
to where people can trust to make changes.

02:14:14.420 --> 02:14:16.780
So that if you keep- Before I touched any code,

02:14:16.780 --> 02:14:18.860
I would actually say, no one touches any code.

02:14:18.860 --> 02:14:20.580
The first thing we do is we test this code base.

02:14:20.580 --> 02:14:21.420
This is classic.

02:14:21.420 --> 02:14:22.780
This is how you approach a legacy code base.

02:14:22.780 --> 02:14:24.620
This is like what any, how to approach

02:14:24.620 --> 02:14:26.980
a legacy code base book will tell you.

02:14:26.980 --> 02:14:30.500
So, and then you hope that there's modules

02:14:30.500 --> 02:14:33.260
that can live on for a while,

02:14:33.260 --> 02:14:34.700
and then you add new ones,

02:14:34.700 --> 02:14:37.060
maybe in a different language or-

02:14:37.060 --> 02:14:39.460
Before we had new ones, we replace old ones.

02:14:39.460 --> 02:14:41.260
Yeah, yeah, meaning like replace old ones

02:14:41.260 --> 02:14:42.100
with something simpler.

02:14:42.100 --> 02:14:45.460
We look at this, like this thing that's 100,000 lines

02:14:45.460 --> 02:14:46.580
and we're like, well, okay,

02:14:46.580 --> 02:14:48.700
maybe this did even make sense in 2010,

02:14:48.700 --> 02:14:51.420
but now we can replace this with an open source thing.

02:14:51.420 --> 02:14:52.260
Right?

02:14:52.260 --> 02:14:53.100
Yeah.

02:14:53.100 --> 02:14:54.220
And, you know, we look at this here,

02:14:54.220 --> 02:14:55.580
here's another 50,000 lines.

02:14:55.580 --> 02:14:56.420
Well, actually, you know,

02:14:56.420 --> 02:14:59.100
we can replace this with 300 lines of Go.

02:14:59.100 --> 02:14:59.940
And you know what?

02:14:59.940 --> 02:15:02.300
I trust that the Go actually replaces this thing

02:15:02.300 --> 02:15:03.740
because all the tests still pass.

02:15:03.740 --> 02:15:05.060
So step one is testing.

02:15:05.060 --> 02:15:05.900
Yeah.

02:15:05.900 --> 02:15:07.740
And then step two is like the programming languages

02:15:07.740 --> 02:15:09.140
and afterthought, right?

02:15:09.140 --> 02:15:10.500
You'll let a whole lot of people compete,

02:15:10.500 --> 02:15:12.100
be like, okay, who wants to rewrite a module,

02:15:12.100 --> 02:15:13.540
whatever language you want to write it in,

02:15:13.540 --> 02:15:15.060
just the tests have to pass.

02:15:15.060 --> 02:15:17.300
And if you figure out how to make the test pass,

02:15:17.300 --> 02:15:20.180
but break the site, that's, we gotta go back to step one.

02:15:20.180 --> 02:15:22.340
Step one is get tests that you trust

02:15:22.340 --> 02:15:23.740
in order to make changes in the code base.

02:15:23.740 --> 02:15:24.940
I wonder how hard it is too,

02:15:24.940 --> 02:15:27.580
because I'm with you on testing and everything.

02:15:27.580 --> 02:15:29.900
Hey, you have from tests to like asserts,

02:15:29.900 --> 02:15:33.500
to everything, but code is just covered in this

02:15:33.500 --> 02:15:38.500
because it should be very easy to make rapid changes

02:15:40.020 --> 02:15:42.620
and know that it's not gonna break everything.

02:15:42.620 --> 02:15:43.540
And that's the way to do it.

02:15:43.540 --> 02:15:45.140
But I wonder how difficult is it

02:15:45.140 --> 02:15:49.020
to integrate tests into a code base

02:15:49.020 --> 02:15:49.860
that doesn't have many of them.

02:15:49.860 --> 02:15:51.940
So I'll tell you what my plan was at Twitter.

02:15:51.940 --> 02:15:53.620
It's actually similar to something we use at Comma.

02:15:53.620 --> 02:15:56.140
So at Comma, we have this thing called process replay.

02:15:56.140 --> 02:15:57.980
And we have a bunch of routes that'll be run through.

02:15:57.980 --> 02:15:59.660
So Comma is a microservice architecture too,

02:15:59.660 --> 02:16:02.060
with microservices in the driving.

02:16:02.060 --> 02:16:03.820
Like we have one for the cameras, one for the sensor,

02:16:03.820 --> 02:16:07.700
one for the planner, one for the model.

02:16:07.700 --> 02:16:09.260
And we have an API,

02:16:09.260 --> 02:16:11.500
which the microservices talk to each other with.

02:16:11.500 --> 02:16:14.780
We use this custom thing called serial, which uses a ZMQ.

02:16:14.780 --> 02:16:17.980
Twitter uses thrift.

02:16:17.980 --> 02:16:20.460
And then it uses this thing called finagle,

02:16:20.460 --> 02:16:24.420
which is a Scala RPC backend.

02:16:24.420 --> 02:16:25.660
But this doesn't even really matter.

02:16:25.660 --> 02:16:30.260
The thrift and finagle layer was a great place,

02:16:30.260 --> 02:16:32.060
I thought, to write tests, right?

02:16:32.060 --> 02:16:32.980
To start building something

02:16:32.980 --> 02:16:34.580
that looks like process replay.

02:16:34.580 --> 02:16:37.940
So Twitter had some stuff that looked kind of like this,

02:16:37.940 --> 02:16:40.220
but it wasn't offline, it was only online.

02:16:40.220 --> 02:16:43.300
So you could ship like a modified version of it.

02:16:43.340 --> 02:16:46.420
And then you could redirect some of the traffic

02:16:46.420 --> 02:16:48.420
to your modified version and diff those two,

02:16:48.420 --> 02:16:49.620
but it was all online.

02:16:49.620 --> 02:16:52.020
Like there was no like CI in the traditional sense.

02:16:52.020 --> 02:16:54.140
I mean, there was some, but like it was not full coverage.

02:16:54.140 --> 02:16:57.340
So you can't run all of Twitter offline to test something.

02:16:57.340 --> 02:16:58.460
Well, then this was another problem.

02:16:58.460 --> 02:17:00.580
You can't run all of Twitter, right?

02:17:00.580 --> 02:17:03.180
Period, any one person can't run Twitter.

02:17:03.180 --> 02:17:05.780
Twitter runs in three data centers, and that's it.

02:17:05.780 --> 02:17:07.860
There's no other place you can run Twitter,

02:17:07.860 --> 02:17:10.420
which is like, George, you don't understand.

02:17:10.420 --> 02:17:11.980
This is modern software development.

02:17:11.980 --> 02:17:13.260
No, this is bullshit.

02:17:13.260 --> 02:17:15.900
Like, why can't it run on my laptop?

02:17:15.900 --> 02:17:16.740
What are you doing?

02:17:16.740 --> 02:17:17.580
Twitter can run it.

02:17:17.580 --> 02:17:18.740
Yeah, okay, well, I'm not saying

02:17:18.740 --> 02:17:20.820
you're gonna download the whole database to your laptop,

02:17:20.820 --> 02:17:22.660
but I'm saying all the middleware and the front end

02:17:22.660 --> 02:17:24.540
should run on my laptop, right?

02:17:24.540 --> 02:17:26.060
That sounds really compelling.

02:17:26.060 --> 02:17:27.260
Yeah.

02:17:27.260 --> 02:17:30.860
But can that be achieved by a code base

02:17:30.860 --> 02:17:33.060
that grows over the years?

02:17:33.060 --> 02:17:35.220
I mean, the three data centers didn't have to be, right?

02:17:35.220 --> 02:17:37.500
Cause there are totally different like designs.

02:17:37.500 --> 02:17:39.500
The problem is more like,

02:17:39.500 --> 02:17:41.580
like why did the code base have to grow?

02:17:41.580 --> 02:17:43.540
What new functionality has been added

02:17:43.540 --> 02:17:47.780
to compensate for the lines of code that are there?

02:17:47.780 --> 02:17:49.820
One of the ways to explain it is that

02:17:49.820 --> 02:17:51.380
the incentive for software developers

02:17:51.380 --> 02:17:54.180
to move up in the companies to add code,

02:17:54.180 --> 02:17:55.820
to add, especially large.

02:17:55.820 --> 02:17:56.660
And you know what?

02:17:56.660 --> 02:17:57.700
The incentive for politicians to move up

02:17:57.700 --> 02:18:00.020
in the political structures to add laws.

02:18:00.020 --> 02:18:01.180
Same problem.

02:18:01.180 --> 02:18:03.420
Yeah, yeah.

02:18:03.420 --> 02:18:07.180
If the flip side is to simplify, simplify, simplify.

02:18:07.180 --> 02:18:08.500
I mean, you know what?

02:18:08.500 --> 02:18:09.980
This is something that I do differently

02:18:09.980 --> 02:18:13.300
from Elon with Comma about self-driving cars.

02:18:14.340 --> 02:18:16.540
You know, I hear the new version is gonna come out

02:18:16.540 --> 02:18:18.580
and the new version is not gonna be better,

02:18:18.580 --> 02:18:22.220
but at first, and it's gonna require a ton of refactors.

02:18:22.220 --> 02:18:24.180
I say, okay, take as long as you need.

02:18:25.380 --> 02:18:26.980
You convinced me this architecture is better?

02:18:26.980 --> 02:18:28.820
Okay, we have to move to it.

02:18:28.820 --> 02:18:31.340
Even if it's not gonna make the product better tomorrow,

02:18:31.340 --> 02:18:34.300
the top priority is getting the architecture right.

02:18:34.300 --> 02:18:37.500
So what do you think about sort of a thing

02:18:37.500 --> 02:18:39.180
where the product is online?

02:18:40.500 --> 02:18:42.980
So I guess, would you do a refactor?

02:18:42.980 --> 02:18:45.140
If you ran engineering on Twitter,

02:18:45.140 --> 02:18:46.780
would you just do a refactor?

02:18:46.780 --> 02:18:47.980
How long would it take?

02:18:47.980 --> 02:18:51.580
What would that mean for the running of the actual service?

02:18:51.580 --> 02:18:56.580
You know, and I'm not the right person to run Twitter.

02:18:58.100 --> 02:18:59.180
I'm just not.

02:18:59.180 --> 02:19:00.020
And that's the problem.

02:19:00.020 --> 02:19:01.500
Like, I don't really know.

02:19:01.500 --> 02:19:03.220
I don't really know if that's...

02:19:03.220 --> 02:19:05.100
You know, a common thing that I thought a lot

02:19:05.100 --> 02:19:07.100
while I was there was whenever I thought something

02:19:07.100 --> 02:19:09.140
that was different to what Elon thought.

02:19:09.180 --> 02:19:10.820
I'd have to run something in the back of my head

02:19:10.820 --> 02:19:15.820
reminding myself that Elon is the richest man in the world.

02:19:15.900 --> 02:19:18.940
And in general, his ideas are better than mine.

02:19:19.860 --> 02:19:22.260
Now, there's a few things I think I do understand

02:19:22.260 --> 02:19:26.860
and know more about, but like in general,

02:19:26.860 --> 02:19:28.460
I'm not qualified to run Twitter.

02:19:28.460 --> 02:19:29.780
Not, I shouldn't say qualified,

02:19:29.780 --> 02:19:31.260
but like, I don't think I'd be that good at it.

02:19:31.260 --> 02:19:32.860
I don't think I'd be good at it.

02:19:32.860 --> 02:19:33.740
I don't think I'd really be good

02:19:33.740 --> 02:19:36.460
at running an engineering organization at scale.

02:19:37.300 --> 02:19:42.300
I think I could lead a very good refactor of Twitter

02:19:42.660 --> 02:19:45.020
and it would take like six months to a year

02:19:45.020 --> 02:19:47.540
and the results to show at the end of it

02:19:47.540 --> 02:19:50.100
would be feature development in general

02:19:50.100 --> 02:19:53.140
takes 10X less time, 10X less man hours.

02:19:53.140 --> 02:19:55.420
That's what I think I could actually do.

02:19:55.420 --> 02:19:56.900
Do I think that it's the right decision

02:19:56.900 --> 02:19:59.180
for the business above my pay grade?

02:20:02.740 --> 02:20:04.940
Yeah, but a lot of these kinds of decisions

02:20:04.940 --> 02:20:06.340
are above everybody's pay grade.

02:20:07.220 --> 02:20:08.060
I don't want to be a manager.

02:20:08.060 --> 02:20:08.900
I don't want to do that.

02:20:08.900 --> 02:20:10.740
I just like, if you really forced me to,

02:20:10.740 --> 02:20:12.180
yeah, it would make me maybe,

02:20:14.660 --> 02:20:17.860
make me upset if I had to make those decisions.

02:20:17.860 --> 02:20:19.620
I don't want to.

02:20:19.620 --> 02:20:23.460
Yeah, but a refactor is so compelling.

02:20:23.460 --> 02:20:26.100
If this is to become something much bigger

02:20:26.100 --> 02:20:27.220
than what Twitter was,

02:20:28.420 --> 02:20:32.540
it feels like a refactor has to be coming at some point.

02:20:32.540 --> 02:20:34.460
George, your junior software engineer,

02:20:34.460 --> 02:20:36.540
every junior software engineer wants to come in

02:20:36.540 --> 02:20:38.620
and refactor the whole code.

02:20:38.620 --> 02:20:42.060
Okay, that's like your opinion, man.

02:20:42.060 --> 02:20:45.340
Yeah, it doesn't, sometimes they're right.

02:20:45.340 --> 02:20:47.380
Well, whether they're right or not,

02:20:47.380 --> 02:20:48.660
it's definitely not for that reason.

02:20:48.660 --> 02:20:50.700
It's definitely not a question of engineering prowess.

02:20:50.700 --> 02:20:51.540
It is a question of maybe

02:20:51.540 --> 02:20:53.180
what the priorities are for the company.

02:20:53.180 --> 02:20:56.340
And I did get more intelligent feedback

02:20:56.340 --> 02:20:58.700
from people I think in good faith saying that.

02:21:00.100 --> 02:21:01.140
Actually from Elon.

02:21:01.140 --> 02:21:04.020
And from Elon, people were like,

02:21:04.980 --> 02:21:08.020
a stop the world refactor might be great for engineering,

02:21:08.020 --> 02:21:09.980
but you don't have business to run.

02:21:09.980 --> 02:21:13.060
And hey, above my pay grade.

02:21:13.060 --> 02:21:15.940
What'd you think about Elon as an engineering leader,

02:21:15.940 --> 02:21:19.620
having to experience him in the most chaotic of spaces,

02:21:19.620 --> 02:21:20.460
I would say.

02:21:25.380 --> 02:21:27.220
My respect for him has unchanged.

02:21:28.220 --> 02:21:30.620
And I did have to think a lot more deeply

02:21:30.620 --> 02:21:32.780
about some of the decisions he's forced to make.

02:21:33.780 --> 02:21:35.820
About the tensions within those,

02:21:35.820 --> 02:21:37.780
the trade-offs within those decisions.

02:21:38.940 --> 02:21:43.940
About like a whole like matrix coming at him.

02:21:44.100 --> 02:21:45.460
I think that's Andrew Tate's word for it.

02:21:45.460 --> 02:21:46.540
Sorry to borrow it.

02:21:46.540 --> 02:21:49.300
Also bigger than engineering, just everything.

02:21:49.300 --> 02:21:52.700
Yeah, like the war on the woke.

02:21:53.860 --> 02:21:54.700
Yeah.

02:21:54.700 --> 02:21:57.740
Like, it's just, man, and like,

02:21:59.060 --> 02:22:01.060
he doesn't have to do this, you know?

02:22:01.060 --> 02:22:01.900
He doesn't have to.

02:22:01.900 --> 02:22:04.740
He could go like Parag and go chill

02:22:04.740 --> 02:22:07.060
at the Four Seasons Maui, you know?

02:22:07.060 --> 02:22:10.100
But see, one person I respect and one person I don't.

02:22:11.100 --> 02:22:14.060
So his heart is in the right place fighting in this case

02:22:14.060 --> 02:22:17.620
for this ideal of the freedom of expression.

02:22:17.620 --> 02:22:19.900
I wouldn't define the ideal so simply.

02:22:19.900 --> 02:22:22.860
I think you can define the ideal no more

02:22:22.860 --> 02:22:26.780
than just saying Elon's idea of a good world.

02:22:26.780 --> 02:22:29.020
Freedom of expression is.

02:22:29.020 --> 02:22:30.660
But to you it's still,

02:22:30.660 --> 02:22:32.700
the downsides of that is the monarchy.

02:22:33.660 --> 02:22:36.980
Yeah, I mean, monarchy has problems, right?

02:22:36.980 --> 02:22:39.700
But I mean, would I trade right now

02:22:39.700 --> 02:22:41.820
the monarchy or the current oligarchy

02:22:41.820 --> 02:22:43.580
which runs America for the monarchy?

02:22:43.580 --> 02:22:44.500
Yeah, I would.

02:22:44.500 --> 02:22:45.940
Sure.

02:22:45.940 --> 02:22:46.780
For the Elon monarchy?

02:22:46.780 --> 02:22:47.620
Yeah, you know why?

02:22:47.620 --> 02:22:50.460
Because power would cost one cent a kilowatt hour.

02:22:50.460 --> 02:22:52.060
Tenth of a cent a kilowatt hour.

02:22:53.180 --> 02:22:54.380
What do you mean?

02:22:54.380 --> 02:22:56.740
Right now, I pay about 20 cents a kilowatt hour

02:22:56.740 --> 02:22:58.460
for electricity in San Diego.

02:22:58.460 --> 02:23:00.660
That's like the same price you paid in 1980.

02:23:01.700 --> 02:23:02.820
What the hell?

02:23:02.820 --> 02:23:05.420
So you would see a lot of innovation with Elon.

02:23:05.420 --> 02:23:07.780
Maybe we'd have some hyper loops.

02:23:07.780 --> 02:23:08.620
Yeah.

02:23:08.620 --> 02:23:09.860
Right, and I'm willing to make that trade off, right?

02:23:09.860 --> 02:23:11.580
I'm willing to make, and this is why,

02:23:11.580 --> 02:23:13.540
people think that dictators take power

02:23:13.540 --> 02:23:17.020
through some untoward mechanism.

02:23:17.020 --> 02:23:17.860
Sometimes they do,

02:23:17.860 --> 02:23:19.580
but usually it's because the people want them.

02:23:19.580 --> 02:23:22.860
And the downsides of a dictatorship,

02:23:22.860 --> 02:23:24.100
I feel like we've gotten to a point now

02:23:24.100 --> 02:23:25.940
with the oligarchy where,

02:23:25.940 --> 02:23:27.660
yeah, I would prefer the dictator.

02:23:29.460 --> 02:23:33.100
What'd you think about Scala as a programming language?

02:23:35.700 --> 02:23:37.100
I liked it more than I thought.

02:23:37.100 --> 02:23:37.940
I did the tutorials.

02:23:37.940 --> 02:23:38.780
I was very new to it.

02:23:38.780 --> 02:23:39.700
It would take me six months

02:23:39.700 --> 02:23:41.820
to be able to write good Scala.

02:23:41.820 --> 02:23:43.140
I mean, what did you learn about learning

02:23:43.140 --> 02:23:45.060
a new programming language from that?

02:23:45.060 --> 02:23:47.860
Oh, I love doing new programming tutorials and doing them.

02:23:47.860 --> 02:23:49.180
I did all this for Rust.

02:23:51.140 --> 02:23:54.660
It keeps some of its upsetting JVM roots,

02:23:54.660 --> 02:23:56.740
but it is a much nicer.

02:23:56.780 --> 02:23:59.140
In fact, I almost don't know why Kotlin took off

02:23:59.140 --> 02:24:00.780
and not Scala.

02:24:00.780 --> 02:24:03.220
I think Scala has some beauty that Kotlin lacked,

02:24:05.300 --> 02:24:07.740
whereas Kotlin felt a lot more,

02:24:07.740 --> 02:24:08.700
I mean, it was almost like,

02:24:08.700 --> 02:24:10.700
I don't know if it actually was a response to Swift,

02:24:10.700 --> 02:24:12.260
but that's kind of what it felt like.

02:24:12.260 --> 02:24:13.580
Like Kotlin looks more like Swift

02:24:13.580 --> 02:24:15.460
and Scala looks more like,

02:24:15.460 --> 02:24:16.700
well, like a functional programming language,

02:24:16.700 --> 02:24:18.740
more like an OCaml or a Haskell.

02:24:18.740 --> 02:24:20.060
Let's actually just explore,

02:24:20.060 --> 02:24:21.100
we touched on it a little bit,

02:24:21.100 --> 02:24:23.100
but just on the art,

02:24:23.100 --> 02:24:25.460
the science and the art of programming,

02:24:25.460 --> 02:24:26.300
for you personally,

02:24:26.300 --> 02:24:29.140
how much of your programming is done with GPT currently?

02:24:29.140 --> 02:24:29.980
None.

02:24:29.980 --> 02:24:30.820
None.

02:24:30.820 --> 02:24:31.980
I don't use it at all.

02:24:31.980 --> 02:24:35.020
Because you prioritize simplicity so much.

02:24:35.020 --> 02:24:37.860
Yeah, I find that a lot of it is noise.

02:24:37.860 --> 02:24:39.180
I do use VS code,

02:24:40.540 --> 02:24:43.380
and I do like some amount of auto-complete.

02:24:43.380 --> 02:24:47.660
I do like a very feels like rules-based auto-complete,

02:24:47.660 --> 02:24:48.620
like an auto-complete

02:24:48.620 --> 02:24:49.980
that's going to complete the variable name for me,

02:24:49.980 --> 02:24:50.820
so I don't have to type it,

02:24:50.820 --> 02:24:51.860
I can just press tab.

02:24:51.860 --> 02:24:52.700
All right, that's nice,

02:24:52.700 --> 02:24:53.940
but I don't want an auto-complete.

02:24:53.940 --> 02:24:54.780
You know what I hate?

02:24:54.780 --> 02:24:55.620
When an auto-complete,

02:24:55.620 --> 02:24:56.860
when I type the word for,

02:24:56.860 --> 02:25:00.140
and it puts two parentheses and two semicolons

02:25:00.140 --> 02:25:02.580
and two braces, I'm like, oh, man.

02:25:02.580 --> 02:25:07.420
Well, I mean, with VS code and GPT with Codex,

02:25:07.420 --> 02:25:11.460
you can kind of brainstorm.

02:25:11.460 --> 02:25:15.500
I find, I'm probably the same as you,

02:25:15.500 --> 02:25:18.480
but I like that it generates code,

02:25:18.480 --> 02:25:20.180
and you basically disagree with it

02:25:20.180 --> 02:25:21.620
and write something simpler.

02:25:21.620 --> 02:25:24.900
But to me, that somehow is inspiring.

02:25:24.900 --> 02:25:25.740
It makes me feel good.

02:25:25.740 --> 02:25:27.940
It also gamifies the simplification process,

02:25:27.940 --> 02:25:30.660
because I'm like, oh, yeah, you dumb AI system.

02:25:30.660 --> 02:25:32.140
You think this is the way to do it.

02:25:32.140 --> 02:25:33.380
I have a simpler thing here.

02:25:33.380 --> 02:25:36.900
It just constantly reminds me of bad stuff.

02:25:36.900 --> 02:25:38.580
I mean, I tried the same thing with rap, right?

02:25:38.580 --> 02:25:39.500
I tried the same thing with rap,

02:25:39.500 --> 02:25:40.740
and actually, I think I'm a much better programmer

02:25:40.740 --> 02:25:42.660
than rapper, but I even tried, I was like,

02:25:42.660 --> 02:25:44.460
okay, can we get some inspiration from these things

02:25:44.460 --> 02:25:45.780
for some rap lyrics?

02:25:45.780 --> 02:25:47.840
And I just found that it would go back

02:25:47.840 --> 02:25:51.580
to the most cringey tropes and dumb rhyme schemes,

02:25:52.580 --> 02:25:54.840
and I'm like, yeah, this is what the code looks like, too.

02:25:54.840 --> 02:25:56.860
I think you and I probably have different thresholds

02:25:56.860 --> 02:25:58.580
for cringe code.

02:25:58.580 --> 02:26:01.020
You probably hate cringe code.

02:26:01.020 --> 02:26:02.040
So it's for you.

02:26:05.060 --> 02:26:07.420
Boilerplate is a part of code.

02:26:09.660 --> 02:26:14.660
Some of it, yeah, and some of it is just faster lookup,

02:26:17.140 --> 02:26:18.220
because I don't know about you,

02:26:18.220 --> 02:26:20.660
but I don't remember everything.

02:26:20.700 --> 02:26:25.700
I'm offloading so much of my memory about different functions,

02:26:25.740 --> 02:26:28.220
library functions, all that kind of stuff.

02:26:28.220 --> 02:26:31.980
This GPT just is very fast at standard stuff,

02:26:31.980 --> 02:26:35.100
and standard library stuff,

02:26:35.100 --> 02:26:36.700
basic stuff that everybody uses.

02:26:37.900 --> 02:26:42.900
Yeah, I think that, I don't know.

02:26:43.180 --> 02:26:46.160
I mean, there's just so little of this in Python.

02:26:46.160 --> 02:26:48.400
Maybe if I was coding more in other languages,

02:26:48.400 --> 02:26:49.900
I would consider it more,

02:26:49.900 --> 02:26:52.660
but I feel like Python already does such a good job

02:26:52.660 --> 02:26:55.080
of removing any boilerplate.

02:26:55.080 --> 02:26:55.920
That's true.

02:26:55.920 --> 02:26:58.020
It's the closest thing you can get to pseudocode, right?

02:26:58.020 --> 02:27:00.380
Yeah, that's true, that's true.

02:27:00.380 --> 02:27:03.760
And like, yeah, sure, if I like, yeah, great, GPT,

02:27:03.760 --> 02:27:06.220
thanks for reminding me to free my variables.

02:27:06.220 --> 02:27:08.100
Unfortunately, you didn't really recognize

02:27:08.100 --> 02:27:10.340
the scope correctly, and you can't free that one,

02:27:10.340 --> 02:27:12.820
but you put the freeze there, and I get it.

02:27:14.020 --> 02:27:17.900
Fiber, whenever I've used fiber for certain things,

02:27:18.060 --> 02:27:21.300
like design or whatever, it's always, you come back.

02:27:21.300 --> 02:27:23.780
I think that's probably closer, my experience with fiber

02:27:23.780 --> 02:27:25.860
is closer to your experience with programming with GPT,

02:27:25.860 --> 02:27:28.180
is like, you're just frustrated and feel worse

02:27:28.180 --> 02:27:30.540
about the whole process of design and art,

02:27:30.540 --> 02:27:32.580
and whatever I used fiber for.

02:27:35.540 --> 02:27:38.660
Still, I just feel like later versions of GPT,

02:27:38.660 --> 02:27:43.580
I'm using GPT as much as possible

02:27:43.580 --> 02:27:45.940
to just learn the dynamics of it,

02:27:45.940 --> 02:27:48.140
like these early versions,

02:27:48.140 --> 02:27:49.420
because it feels like in the future,

02:27:49.420 --> 02:27:51.580
you'll be using it more and more.

02:27:51.580 --> 02:27:55.180
And so like, I don't want to be, like for the same reason,

02:27:55.180 --> 02:27:57.860
I gave away all my books and switched to Kindle,

02:27:57.860 --> 02:27:59.700
because like, all right,

02:27:59.700 --> 02:28:01.660
how long are we gonna have paper books?

02:28:01.660 --> 02:28:03.260
Like 30 years from now,

02:28:03.260 --> 02:28:05.980
like I wanna learn to be reading on Kindle,

02:28:05.980 --> 02:28:07.540
even though I don't enjoy it as much,

02:28:07.540 --> 02:28:08.980
and you learn to enjoy it more.

02:28:08.980 --> 02:28:12.860
In the same way, I switched from, let me just pause,

02:28:12.860 --> 02:28:14.620
I switched from Emacs to VS Code.

02:28:14.620 --> 02:28:16.740
Yeah, I switched from Vim to VS Code,

02:28:16.740 --> 02:28:18.060
I think I, similar, but.

02:28:18.060 --> 02:28:21.580
Yeah, it's tough, and that Vim to VS Code is even tougher,

02:28:21.580 --> 02:28:25.300
because Emacs is like, old, like more outdated,

02:28:25.300 --> 02:28:28.260
feels like it, the community is more outdated.

02:28:28.260 --> 02:28:31.020
Vim is like pretty vibrant still, so.

02:28:31.020 --> 02:28:32.420
I never used any of the plugins,

02:28:32.420 --> 02:28:33.260
I still don't use any of the plugins.

02:28:33.260 --> 02:28:34.460
That's what I looked at myself in the mirror,

02:28:34.460 --> 02:28:37.420
I'm like, yeah, you wrote some stuff in Lisp, yeah.

02:28:37.420 --> 02:28:39.340
I never used any of the plugins in Vim either.

02:28:39.340 --> 02:28:41.580
I had the most vanilla Vim, I have a syntax highlighter,

02:28:41.580 --> 02:28:42.580
I didn't even have auto-complete.

02:28:42.580 --> 02:28:47.580
Like, these things, I feel like help you so marginally,

02:28:48.700 --> 02:28:53.700
that like, and now, okay, now VS Code's auto-complete

02:28:53.980 --> 02:28:55.700
has gotten good enough that like, okay,

02:28:55.700 --> 02:28:57.460
I don't have to set it up, I can just go into any code base

02:28:57.460 --> 02:28:59.180
and auto-complete's right 90% of the time.

02:28:59.180 --> 02:29:00.860
Okay, cool, I'll take it.

02:29:00.860 --> 02:29:04.020
Right, so I don't think I'm gonna have a problem at all

02:29:04.020 --> 02:29:06.100
adapting to the tools once they're good,

02:29:06.100 --> 02:29:08.700
but like, the real thing that I want

02:29:08.700 --> 02:29:12.780
is not something that like, tab completes my code

02:29:12.780 --> 02:29:13.620
and gives me ideas.

02:29:13.620 --> 02:29:14.740
The real thing that I want

02:29:14.740 --> 02:29:17.060
is a very intelligent pair programmer

02:29:17.060 --> 02:29:19.940
that comes up with a little pop-up saying,

02:29:19.940 --> 02:29:23.500
hey, you wrote a bug on line 14 and here's what it is.

02:29:23.500 --> 02:29:24.340
Yeah.

02:29:24.340 --> 02:29:25.160
Now, I like that.

02:29:25.160 --> 02:29:26.060
You know what does a good job of this?

02:29:26.060 --> 02:29:27.660
MyPy.

02:29:27.660 --> 02:29:28.740
I love MyPy.

02:29:28.740 --> 02:29:31.340
MyPy, this fancy type checker for Python,

02:29:31.340 --> 02:29:33.260
and actually I tried, like Microsoft released one too,

02:29:33.260 --> 02:29:36.740
and it was like 60% false positives.

02:29:36.740 --> 02:29:38.780
MyPy is like 5% false positives.

02:29:38.780 --> 02:29:41.340
95% of the time, it recognizes,

02:29:41.340 --> 02:29:42.220
I didn't really think about

02:29:42.220 --> 02:29:43.780
that typing interaction correctly.

02:29:43.780 --> 02:29:44.660
Thank you, MyPy.

02:29:44.660 --> 02:29:46.820
So you like type hinting,

02:29:46.820 --> 02:29:51.140
you like pushing the language towards being a typed language.

02:29:51.140 --> 02:29:52.140
Oh yeah, absolutely.

02:29:52.140 --> 02:29:55.020
I think optional typing is great.

02:29:55.020 --> 02:29:55.940
I mean, look, I think that like,

02:29:55.940 --> 02:29:57.020
it's like a meat in the middle, right?

02:29:57.020 --> 02:29:58.620
Like Python has this optional type hinting

02:29:58.620 --> 02:30:01.660
and like C++ has auto.

02:30:01.660 --> 02:30:03.820
C++ allows you to take a step back.

02:30:03.820 --> 02:30:06.060
Well, C++ would have you brutally type out

02:30:06.060 --> 02:30:08.180
std string iterator, right?

02:30:08.180 --> 02:30:09.940
Now I can just type auto, which is nice.

02:30:09.940 --> 02:30:13.060
And then Python used to just have A.

02:30:13.060 --> 02:30:14.160
What type is A?

02:30:15.100 --> 02:30:15.940
It's an A.

02:30:16.780 --> 02:30:18.420
A colon str.

02:30:18.420 --> 02:30:20.500
Oh, okay, it's a string, cool.

02:30:20.500 --> 02:30:21.340
Yeah.

02:30:21.340 --> 02:30:24.140
I wish there was a way, like a simple way in Python

02:30:24.140 --> 02:30:27.620
to like turn on a mode which would enforce the types.

02:30:28.580 --> 02:30:29.500
Yeah, like give a warning

02:30:29.500 --> 02:30:30.820
when there's no type, something like this.

02:30:30.820 --> 02:30:32.180
Well, no, to give a warning where,

02:30:32.180 --> 02:30:33.740
like MyPy is a static type checker,

02:30:33.740 --> 02:30:35.580
but I'm asking just for a runtime type checker.

02:30:35.580 --> 02:30:37.340
Like there's like ways to like hack this in,

02:30:37.340 --> 02:30:40.540
but I wish it was just like a flag, like Python 3-T.

02:30:40.540 --> 02:30:42.140
Oh, I see, I see.

02:30:42.140 --> 02:30:43.100
Enforce the types around type.

02:30:43.100 --> 02:30:45.180
Yeah, I feel like that makes you a better programmer

02:30:45.180 --> 02:30:47.380
that that's a kind of test, right?

02:30:47.380 --> 02:30:49.980
That the type remains the same.

02:30:49.980 --> 02:30:51.880
Well, that I know that I didn't like mess any types up.

02:30:51.880 --> 02:30:55.460
But again, like MyPy is getting really good and I love it.

02:30:55.460 --> 02:30:57.060
And I can't wait for some of these tools

02:30:57.060 --> 02:30:58.500
to become AI powered.

02:30:58.500 --> 02:31:01.380
I like, I want AIs reading my code and giving me feedback.

02:31:01.380 --> 02:31:04.500
I don't want AIs writing

02:31:04.500 --> 02:31:06.740
half-assed auto-complete stuff for me.

02:31:06.740 --> 02:31:09.060
I wonder if you can now take GPT

02:31:09.060 --> 02:31:11.180
and give it a code that you wrote for function

02:31:11.180 --> 02:31:13.280
and say, how can I make this simpler

02:31:13.280 --> 02:31:15.460
and have it accomplish the same thing?

02:31:15.460 --> 02:31:17.420
I think you'll get some good ideas on some code.

02:31:17.420 --> 02:31:22.180
Maybe not the code you write for tiny grad type of code,

02:31:22.180 --> 02:31:24.300
because that requires so much design thinking,

02:31:24.300 --> 02:31:26.260
but like other kinds of code.

02:31:26.260 --> 02:31:27.200
I don't know.

02:31:27.200 --> 02:31:29.780
I downloaded that plugin maybe like two months ago.

02:31:29.780 --> 02:31:31.860
I tried it again and found the same.

02:31:31.860 --> 02:31:34.740
Look, I don't doubt that these models

02:31:34.740 --> 02:31:37.940
are going to first become useful to me,

02:31:37.940 --> 02:31:40.460
then be as good as me, and then surpass me.

02:31:40.460 --> 02:31:42.800
But from what I've seen today,

02:31:42.800 --> 02:31:47.800
it's like someone occasionally taking over my keyboard

02:31:50.020 --> 02:31:53.900
that I hired from Fiverr, I'd rather not.

02:31:53.900 --> 02:31:55.860
Ideas about how to debug the code

02:31:55.860 --> 02:31:58.820
or basically a better debugger is really interesting.

02:31:58.820 --> 02:31:59.900
But it's not a better debugger.

02:31:59.900 --> 02:32:01.820
I guess I would love a better debugger.

02:32:01.820 --> 02:32:02.660
Yeah, it's not yet.

02:32:02.660 --> 02:32:04.540
Yeah, but it feels like it's not too far.

02:32:04.540 --> 02:32:05.900
Yeah, one of my coworkers says

02:32:05.900 --> 02:32:07.500
he uses them for print statements.

02:32:07.500 --> 02:32:09.380
Like every time he has to like just like when he needs,

02:32:09.380 --> 02:32:11.100
the only thing I can really write is like,

02:32:11.100 --> 02:32:12.140
okay, I just want to write the thing

02:32:12.140 --> 02:32:14.420
to like print the state out right now.

02:32:14.420 --> 02:32:18.620
Oh, that definitely is much faster as print statements.

02:32:18.620 --> 02:32:19.460
Yeah. Yeah.

02:32:19.460 --> 02:32:21.460
I see myself using that a lot just like,

02:32:21.460 --> 02:32:23.220
because it figures out the rest of the functions

02:32:23.220 --> 02:32:24.500
to say, okay, print everything.

02:32:24.500 --> 02:32:25.500
Yeah, print everything, right?

02:32:25.500 --> 02:32:28.060
And then, yeah, like if you want a pretty printer, maybe.

02:32:28.060 --> 02:32:28.900
I'm like, yeah, you know what?

02:32:28.900 --> 02:32:30.900
I think like, I think in two years,

02:32:30.900 --> 02:32:33.820
I'm going to start using these plugins a little bit.

02:32:33.820 --> 02:32:35.060
And then in five years,

02:32:35.060 --> 02:32:38.100
I'm going to be heavily relying on some AI augmented flow.

02:32:38.100 --> 02:32:39.700
And then in 10 years.

02:32:39.700 --> 02:32:42.220
Do you think you'll ever get to a hundred percent?

02:32:42.220 --> 02:32:45.660
Where are the, like, what's the role of the human

02:32:45.660 --> 02:32:48.540
that it converges to as a programmer?

02:32:48.540 --> 02:32:50.180
No.

02:32:50.180 --> 02:32:52.060
Do you think it's all generated?

02:32:52.060 --> 02:32:55.060
Our niche becomes, I think it's over for humans in general.

02:32:56.220 --> 02:32:57.980
It's not just programming, it's everything.

02:32:58.900 --> 02:33:00.740
Our niche becomes smaller and smaller and smaller.

02:33:00.740 --> 02:33:02.300
In fact, I'll tell you what the last niche

02:33:02.300 --> 02:33:03.260
of humanity is going to be.

02:33:03.260 --> 02:33:04.380
Yeah.

02:33:04.380 --> 02:33:05.820
There's a great book and it's,

02:33:05.820 --> 02:33:08.860
if I recommended Metamorphosis Prime Intellect last time,

02:33:08.860 --> 02:33:12.380
there is a sequel called A Casino Odyssey in Cyberspace.

02:33:12.380 --> 02:33:15.780
And I don't want to give away the ending of this,

02:33:15.780 --> 02:33:18.820
but it tells you what the last remaining human currency is.

02:33:18.820 --> 02:33:19.860
And I agree with that.

02:33:21.460 --> 02:33:23.860
We'll leave that as the cliffhanger.

02:33:25.900 --> 02:33:27.780
So no more programmers left, huh?

02:33:28.620 --> 02:33:29.460
That's where we're going.

02:33:29.460 --> 02:33:31.220
Well, unless you want handmade code,

02:33:31.220 --> 02:33:32.420
maybe they'll sell it on Etsy.

02:33:32.420 --> 02:33:33.900
This is handwritten code.

02:33:35.660 --> 02:33:37.700
It doesn't have that machine polished to it.

02:33:37.700 --> 02:33:39.100
It has those slight imperfections

02:33:39.100 --> 02:33:41.860
that would only be written by a person.

02:33:41.860 --> 02:33:44.420
I wonder how far away we are from that.

02:33:44.420 --> 02:33:47.580
I mean, there's some aspect to, you know, on Instagram,

02:33:47.580 --> 02:33:49.620
your title is listed as prompt engineer.

02:33:49.620 --> 02:33:50.460
Right.

02:33:51.420 --> 02:33:52.620
Thank you for noticing.

02:33:53.620 --> 02:33:57.620
I don't know if it's ironic or sarcastic or not.

02:34:00.980 --> 02:34:02.780
What do you think of prompt engineering

02:34:02.780 --> 02:34:07.180
as a scientific and engineering discipline or maybe,

02:34:07.180 --> 02:34:08.700
and maybe art form?

02:34:08.700 --> 02:34:09.540
You know what?

02:34:10.540 --> 02:34:12.140
I started Comma six years ago

02:34:12.140 --> 02:34:14.140
and I started the tiny corp a month ago.

02:34:16.420 --> 02:34:18.060
So much has changed.

02:34:18.060 --> 02:34:20.940
Like I'm now thinking, I'm now like,

02:34:21.780 --> 02:34:24.460
I started like going through like similar Comma processes

02:34:24.460 --> 02:34:25.300
to like starting a company.

02:34:25.300 --> 02:34:27.140
I'm like, okay, I'm gonna get an office in San Diego.

02:34:27.140 --> 02:34:28.540
I'm gonna bring people here.

02:34:29.460 --> 02:34:30.300
I don't think so.

02:34:30.300 --> 02:34:32.700
I think I'm actually gonna do remote, right?

02:34:32.700 --> 02:34:33.540
George, you're gonna do remote?

02:34:33.540 --> 02:34:34.420
You hate remote.

02:34:34.420 --> 02:34:36.460
Yeah, but I'm not gonna do job interviews.

02:34:36.460 --> 02:34:37.540
The only way you're gonna get a job

02:34:37.540 --> 02:34:39.780
is if you contribute to the GitHub, right?

02:34:39.780 --> 02:34:44.260
And then like, it like, like interacting through GitHub,

02:34:44.260 --> 02:34:48.140
like GitHub being the real like project management software

02:34:48.140 --> 02:34:48.960
for your company.

02:34:48.960 --> 02:34:51.120
The thing pretty much just is a GitHub repo

02:34:52.080 --> 02:34:55.480
is like showing me kind of what the future of, okay.

02:34:55.480 --> 02:34:56.880
So a lot of times I'll go on a Discord

02:34:56.880 --> 02:34:59.600
or kind of grad Discord and I'll throw out some random like,

02:34:59.600 --> 02:35:02.640
hey, you know, can you change instead of having log and exp

02:35:02.640 --> 02:35:05.360
as LL ops, change it to log to an exp too?

02:35:06.320 --> 02:35:07.160
It's pretty small change.

02:35:07.160 --> 02:35:09.160
You can just use like change a base formula.

02:35:10.280 --> 02:35:12.920
That's the kind of task that I can see an AI

02:35:12.920 --> 02:35:14.840
being able to do in a few years.

02:35:14.840 --> 02:35:17.480
Like in a few years, I could see myself describing that

02:35:17.480 --> 02:35:18.920
and then within 30 seconds,

02:35:18.920 --> 02:35:20.760
a pull request is up that does it.

02:35:20.760 --> 02:35:23.320
And it passes my CI and I merge it, right?

02:35:23.320 --> 02:35:24.880
So I really started thinking about like,

02:35:24.880 --> 02:35:28.480
well, what is the future of like jobs?

02:35:28.480 --> 02:35:30.560
How many AIs can I employ at my company?

02:35:30.560 --> 02:35:32.080
As soon as we get the first tiny box up,

02:35:32.080 --> 02:35:35.320
I'm gonna stand up a 65B llama in the Discord.

02:35:35.320 --> 02:35:36.640
And it's like, yeah, here's the tiny box.

02:35:36.640 --> 02:35:39.000
He's just like, he's chilling with us.

02:35:39.000 --> 02:35:42.600
Basically, like you said with niches,

02:35:42.600 --> 02:35:47.160
most human jobs will eventually be replaced

02:35:47.160 --> 02:35:48.440
with prompt engineering.

02:35:48.440 --> 02:35:51.280
Well, prompt engineering kind of is this like,

02:35:52.360 --> 02:35:54.960
as you like move up the stack, right?

02:35:54.960 --> 02:35:56.480
Like, okay, there used to be humans

02:35:56.480 --> 02:35:59.280
actually doing arithmetic by hand.

02:35:59.280 --> 02:36:01.320
There used to be like big farms of people

02:36:01.320 --> 02:36:02.920
doing pluses and stuff, right?

02:36:02.920 --> 02:36:05.280
And then you have like spreadsheets, right?

02:36:05.280 --> 02:36:07.560
And then, okay, the spreadsheet can do the plus for me.

02:36:07.560 --> 02:36:10.000
And then you have like macros, right?

02:36:10.000 --> 02:36:10.840
And then you have like things

02:36:10.840 --> 02:36:13.600
that basically just are spreadsheets under the hood, right?

02:36:13.600 --> 02:36:14.840
Like accounting software.

02:36:17.400 --> 02:36:19.280
As we move further up the abstraction,

02:36:19.280 --> 02:36:20.840
well, what's at the top of the abstraction stack?

02:36:20.840 --> 02:36:22.600
Well, prompt engineer.

02:36:22.600 --> 02:36:23.440
Yeah.

02:36:23.440 --> 02:36:25.520
All right, what is the last thing

02:36:25.520 --> 02:36:30.040
if you think about like humans wanting to keep control?

02:36:30.040 --> 02:36:31.680
Well, what am I really in the company

02:36:31.680 --> 02:36:33.600
but a prompt engineer, right?

02:36:33.600 --> 02:36:34.920
Isn't there a certain point

02:36:34.920 --> 02:36:38.600
where the AI will be better at writing prompts?

02:36:38.600 --> 02:36:41.600
Yeah, but you see the problem with the AI writing prompts,

02:36:41.600 --> 02:36:43.840
a definition that I always liked of AI

02:36:43.840 --> 02:36:46.880
was AI is the do what I mean machine, right?

02:36:46.880 --> 02:36:50.720
AI is not the, like the computer is so pedantic.

02:36:50.720 --> 02:36:52.120
It does what you say.

02:36:52.120 --> 02:36:56.200
So, but you want the do what I mean machine, right?

02:36:56.200 --> 02:36:58.040
You want the machine where you say,

02:36:58.040 --> 02:36:59.680
get my grandmother out of the burning house.

02:36:59.680 --> 02:37:01.360
It like reasonably takes your grandmother

02:37:01.360 --> 02:37:02.200
and puts her on the ground,

02:37:02.200 --> 02:37:04.280
not lifts her a thousand feet above the burning house

02:37:04.280 --> 02:37:05.840
and lets her fall, right?

02:37:05.840 --> 02:37:06.680
But you don't-

02:37:06.680 --> 02:37:07.640
There's an old Yudkowsky example.

02:37:07.640 --> 02:37:12.640
But it's not going to find the meaning.

02:37:13.080 --> 02:37:15.200
I mean, to do what I mean,

02:37:15.200 --> 02:37:16.840
it has to figure stuff out.

02:37:16.840 --> 02:37:17.680
Sure.

02:37:17.680 --> 02:37:21.320
And the thing you'll maybe ask it to do

02:37:21.320 --> 02:37:23.760
is run government for me.

02:37:23.760 --> 02:37:25.840
Oh, and do what I mean very much comes down to

02:37:25.840 --> 02:37:28.120
how aligned is that AI with you?

02:37:28.120 --> 02:37:31.120
Of course, when you talk to an AI

02:37:31.120 --> 02:37:34.120
that's made by a big company in the cloud,

02:37:34.120 --> 02:37:38.320
the AI fundamentally is aligned to them, not to you.

02:37:38.320 --> 02:37:39.800
And that's why you have to buy a tiny box

02:37:39.800 --> 02:37:41.760
so you make sure the AI stays aligned to you.

02:37:41.760 --> 02:37:44.360
Every time that they start to pass,

02:37:44.360 --> 02:37:46.480
AI regulation or GPU regulation,

02:37:46.480 --> 02:37:48.280
I'm going to see sales of tiny boxes spike.

02:37:48.280 --> 02:37:49.600
It's going to be like guns, right?

02:37:49.600 --> 02:37:51.480
Every time they talk about gun regulation,

02:37:51.480 --> 02:37:53.120
boom, gun sales.

02:37:53.120 --> 02:37:55.480
So in the space of AI, you're an anarchist,

02:37:55.480 --> 02:37:58.800
anarchism espouser, believer.

02:37:58.800 --> 02:38:00.640
I'm an informational anarchist, yes.

02:38:00.640 --> 02:38:03.840
I'm an informational anarchist and a physical statist.

02:38:05.320 --> 02:38:07.440
I do not think anarchy in the physical world is very good

02:38:07.440 --> 02:38:09.080
because I exist in the physical world.

02:38:09.080 --> 02:38:11.440
But I think we can construct this virtual world

02:38:11.440 --> 02:38:13.480
where anarchy, it can't hurt you, right?

02:38:13.480 --> 02:38:16.080
I love that Tyler, the creator tweet.

02:38:16.080 --> 02:38:18.240
Yo, cyber bullying isn't real, man.

02:38:18.240 --> 02:38:20.000
Have you tried turning off the screen?

02:38:20.000 --> 02:38:22.120
Close your eyes, like.

02:38:22.120 --> 02:38:22.960
Yeah.

02:38:25.280 --> 02:38:28.880
But how do you prevent the AI

02:38:28.880 --> 02:38:33.880
from basically replacing all human prompt engineers?

02:38:34.760 --> 02:38:36.440
Where there's, it's like a self,

02:38:36.440 --> 02:38:38.400
like where nobody's the prompt engineer anymore.

02:38:38.400 --> 02:38:40.600
So autonomy, greater and greater autonomy

02:38:40.600 --> 02:38:41.680
until it's full autonomy.

02:38:41.680 --> 02:38:43.120
Yeah.

02:38:43.120 --> 02:38:45.080
And that's just where it's headed.

02:38:45.080 --> 02:38:49.200
Because one person is going to say, run everything for me.

02:38:49.200 --> 02:38:54.120
You see, I look at potential futures

02:38:54.120 --> 02:38:59.120
and as long as the AIs go on to create a vibrant civilization

02:39:00.800 --> 02:39:04.960
with diversity and complexity across the universe,

02:39:04.960 --> 02:39:07.280
more power to them, I'll die.

02:39:07.280 --> 02:39:09.800
If the AIs go on to actually like turn the world

02:39:09.800 --> 02:39:12.080
into paper clips and then they die out themselves,

02:39:12.080 --> 02:39:14.600
well, that's horrific and we don't want that to happen.

02:39:14.600 --> 02:39:17.080
So this is what I mean about like robustness.

02:39:17.080 --> 02:39:19.240
I trust robust machines.

02:39:19.240 --> 02:39:20.960
The current AIs are so not robust.

02:39:20.960 --> 02:39:22.000
Like this comes back to the idea

02:39:22.000 --> 02:39:24.840
that we've never made a machine that can self-replicate.

02:39:24.840 --> 02:39:25.680
Right?

02:39:25.680 --> 02:39:28.480
But when we have, if the machines are truly robust

02:39:28.480 --> 02:39:31.040
and there is one prompt engineer left in the world,

02:39:33.120 --> 02:39:34.160
hope you're doing good, man.

02:39:34.160 --> 02:39:35.120
Hope you believe in God.

02:39:35.120 --> 02:39:38.720
Like, you know, go by God

02:39:38.720 --> 02:39:42.800
and go forth and conquer the universe.

02:39:42.800 --> 02:39:43.720
Well, you mentioned,

02:39:43.720 --> 02:39:46.160
because I talked to Mark about faith and God

02:39:46.160 --> 02:39:48.640
and you said you were impressed by that.

02:39:48.640 --> 02:39:50.240
What's your own belief in God

02:39:50.240 --> 02:39:52.680
and how does that affect your work?

02:39:52.680 --> 02:39:56.040
You know, I never really considered when I was younger,

02:39:56.040 --> 02:39:57.360
I guess my parents were atheists.

02:39:57.360 --> 02:39:58.360
So I was raised kind of atheist.

02:39:58.360 --> 02:40:00.200
I never really considered how absolutely like silly

02:40:00.200 --> 02:40:01.480
atheism is.

02:40:01.480 --> 02:40:05.160
Because like, I create worlds.

02:40:05.160 --> 02:40:08.240
Every like game creator, like, how are you an atheist, bro?

02:40:08.240 --> 02:40:09.520
You create worlds.

02:40:09.520 --> 02:40:11.280
Ultimately, no one created our world, man.

02:40:11.280 --> 02:40:12.120
That's different.

02:40:12.120 --> 02:40:13.520
Haven't you heard about like the Big Bang and stuff?

02:40:13.520 --> 02:40:17.400
Yeah, I mean, what's the Skyrim myth origin story in Skyrim?

02:40:17.400 --> 02:40:19.240
I'm sure there's like some part of it in Skyrim,

02:40:19.240 --> 02:40:21.520
but it's not like if you ask the creators,

02:40:21.520 --> 02:40:23.920
like the Big Bang is in universe, right?

02:40:23.920 --> 02:40:27.160
I'm sure they have some Big Bang notion in Skyrim, right?

02:40:27.160 --> 02:40:28.560
But that obviously is not at all

02:40:28.560 --> 02:40:30.080
how Skyrim was actually created.

02:40:30.080 --> 02:40:32.560
It was created by a bunch of programmers in a room, right?

02:40:32.560 --> 02:40:35.840
So like, you know, it just struck me one day

02:40:35.840 --> 02:40:37.480
how just silly atheism is, right?

02:40:37.480 --> 02:40:39.520
Like, of course we were created by God.

02:40:39.520 --> 02:40:40.920
It's the most obvious thing.

02:40:42.040 --> 02:40:46.840
Yeah, that's such a nice way to put it.

02:40:46.840 --> 02:40:50.360
Like we're such powerful creators ourselves.

02:40:50.360 --> 02:40:53.520
It's silly not to concede that there's creators

02:40:53.520 --> 02:40:54.840
even more powerful than us.

02:40:54.840 --> 02:40:58.280
Yeah, and then like, I also just like, I like that notion.

02:40:58.280 --> 02:41:01.120
That notion gives me a lot of, I mean, I guess you can talk

02:41:01.120 --> 02:41:03.280
about what it gives a lot of religious people.

02:41:03.280 --> 02:41:04.720
It's kind of like, it just gives me comfort.

02:41:04.720 --> 02:41:05.960
It's like, you know what?

02:41:05.960 --> 02:41:09.000
If we mess it all up and we die out, yeah.

02:41:09.000 --> 02:41:11.200
Yeah, and the same way that a video game

02:41:11.200 --> 02:41:12.280
kind of has comfort in it.

02:41:12.280 --> 02:41:13.320
God, I'll try again.

02:41:14.240 --> 02:41:15.400
Or there's balance.

02:41:15.400 --> 02:41:18.840
Like somebody figured out a balanced view of it.

02:41:18.840 --> 02:41:22.600
Like how to, like, so it's, it all makes sense in the end.

02:41:22.600 --> 02:41:25.240
Like a video game is usually not gonna have

02:41:25.240 --> 02:41:26.880
crazy, crazy stuff.

02:41:26.880 --> 02:41:30.960
You know, people will come up with like a,

02:41:30.960 --> 02:41:33.880
well, yeah, but like, man, who created God?

02:41:33.880 --> 02:41:36.440
And I'm like, that's God's problem.

02:41:36.440 --> 02:41:38.720
No, like, I'm not gonna think this is,

02:41:38.720 --> 02:41:39.560
what are you asking me?

02:41:39.560 --> 02:41:41.360
What, if God believes in God?

02:41:41.360 --> 02:41:43.360
I'm just this NPC living in this game.

02:41:43.360 --> 02:41:45.920
I mean, to be fair, like if God didn't believe in God,

02:41:45.920 --> 02:41:49.040
he'd be as, you know, silly as the atheists here.

02:41:49.040 --> 02:41:51.160
What do you think is the greatest

02:41:51.160 --> 02:41:52.760
computer game of all time?

02:41:52.760 --> 02:41:55.640
Do you have any time to play games anymore?

02:41:55.640 --> 02:41:57.400
Have you played Diablo 4?

02:41:57.400 --> 02:41:59.200
I have not played Diablo 4.

02:41:59.200 --> 02:42:00.920
I will be doing that shortly.

02:42:00.920 --> 02:42:01.960
I have to. All right.

02:42:01.960 --> 02:42:04.280
There's so much history with one, two, and three.

02:42:04.280 --> 02:42:05.280
You know what?

02:42:05.280 --> 02:42:07.280
I'm gonna say World of Warcraft.

02:42:07.280 --> 02:42:08.480
Who?

02:42:08.480 --> 02:42:11.840
And it's not that the game is so,

02:42:11.840 --> 02:42:13.080
it's such a great game.

02:42:13.080 --> 02:42:14.560
It's not.

02:42:14.560 --> 02:42:18.560
It's that I remember in 2005, when it came out,

02:42:18.560 --> 02:42:22.480
how it opened my mind to ideas.

02:42:22.480 --> 02:42:26.960
It opened my mind to like, this whole world

02:42:26.960 --> 02:42:28.520
we've created, right?

02:42:28.520 --> 02:42:30.480
There's almost been nothing like it since.

02:42:30.480 --> 02:42:32.480
Like, you can look at MMOs today,

02:42:32.480 --> 02:42:34.240
and I think they all have lower user bases

02:42:34.240 --> 02:42:35.240
than World of Warcraft.

02:42:35.240 --> 02:42:37.400
Like, EVE Online's kind of cool.

02:42:37.400 --> 02:42:41.600
But to think that like, everyone knows,

02:42:41.600 --> 02:42:42.840
you know, people are always like,

02:42:42.840 --> 02:42:45.080
to look at the Apple headset, like,

02:42:45.080 --> 02:42:47.000
what do people want in this VR?

02:42:47.000 --> 02:42:47.840
Everyone knows what they want.

02:42:47.840 --> 02:42:51.360
I want Ready Player One, and like that.

02:42:51.360 --> 02:42:52.640
So I'm gonna say World of Warcraft,

02:42:52.640 --> 02:42:55.200
and I'm hoping that like games can get out

02:42:55.200 --> 02:42:59.400
of this whole mobile gaming dopamine pump thing,

02:42:59.560 --> 02:43:00.400
and like.

02:43:00.400 --> 02:43:01.240
Create worlds.

02:43:01.240 --> 02:43:03.080
Create worlds, yeah.

02:43:03.080 --> 02:43:05.600
And worlds that captivate a very large fraction

02:43:05.600 --> 02:43:06.760
of the human population.

02:43:06.760 --> 02:43:09.560
Yeah, and I think it'll come back, I believe.

02:43:09.560 --> 02:43:13.240
But MMO, like really, really pull you in.

02:43:13.240 --> 02:43:14.240
Games do a good job.

02:43:14.240 --> 02:43:15.960
I mean, okay, are there like two other games

02:43:15.960 --> 02:43:17.520
that I think are, you know, very noteworthy

02:43:17.520 --> 02:43:19.000
from your Skyrim and GTA V?

02:43:19.880 --> 02:43:21.720
Skyrim, yeah.

02:43:21.720 --> 02:43:24.920
That's probably number one for me, GTA.

02:43:24.920 --> 02:43:26.560
Yeah, what is it about GTA?

02:43:27.560 --> 02:43:32.560
GTA is really, I guess GTA is real life.

02:43:32.960 --> 02:43:35.320
I know there's prostitutes and guns and stuff.

02:43:35.320 --> 02:43:37.880
There exists a real life, too.

02:43:37.880 --> 02:43:38.920
Yes, I know.

02:43:38.920 --> 02:43:42.160
But it's how I imagine your life to be, actually.

02:43:42.160 --> 02:43:43.280
I wish it was that cool.

02:43:43.280 --> 02:43:44.120
Yeah.

02:43:45.680 --> 02:43:47.000
Yeah, I guess that's, you know,

02:43:47.000 --> 02:43:50.320
because there's Sims, right, which is also a game I like.

02:43:50.320 --> 02:43:52.560
But it's a gamified version of life.

02:43:52.560 --> 02:43:55.200
But it also is, I would love a combination

02:43:55.200 --> 02:43:56.840
of Sims and GTA.

02:43:58.760 --> 02:44:02.080
So more freedom, more violence, more rawness,

02:44:02.080 --> 02:44:04.680
but with also like ability to have a career

02:44:04.680 --> 02:44:06.000
and family and this kind of stuff.

02:44:06.000 --> 02:44:08.320
What I'm really excited about in games

02:44:08.320 --> 02:44:11.160
is like once we start getting intelligent AIs

02:44:11.160 --> 02:44:12.000
to interact with.

02:44:12.000 --> 02:44:12.840
Oh, yeah.

02:44:12.840 --> 02:44:14.600
Like the NPCs in games have never been.

02:44:16.040 --> 02:44:19.200
But conversationally, in every way.

02:44:19.200 --> 02:44:21.600
In like, yeah, in like every way.

02:44:21.600 --> 02:44:23.480
Like when you're actually building a world

02:44:23.480 --> 02:44:26.840
imbued with intelligence.

02:44:26.840 --> 02:44:27.680
Oh, yeah.

02:44:27.680 --> 02:44:28.520
And it's just hard.

02:44:28.520 --> 02:44:29.600
Like, there's just like, you know,

02:44:29.600 --> 02:44:30.560
running World of Warcraft.

02:44:30.560 --> 02:44:31.720
Like you're limited by your way.

02:44:31.720 --> 02:44:33.560
You're running on a Pentium IV, you know?

02:44:33.560 --> 02:44:34.400
How much intelligence can you run?

02:44:34.400 --> 02:44:36.480
How many flops did you have?

02:44:36.480 --> 02:44:39.080
But now when I'm running a game

02:44:39.080 --> 02:44:42.240
on a hundred pay-to-flop machine, well, that's five people.

02:44:42.240 --> 02:44:43.520
I'm trying to make this a thing.

02:44:43.520 --> 02:44:45.920
20 pay-to-flops of compute is one person of compute.

02:44:45.920 --> 02:44:47.160
I'm trying to make that a unit.

02:44:47.160 --> 02:44:50.240
20 pay-to-flops is one person.

02:44:50.240 --> 02:44:51.360
One person.

02:44:51.360 --> 02:44:52.200
One person flop.

02:44:52.200 --> 02:44:53.360
It's like a horsepower.

02:44:54.760 --> 02:44:55.600
What's a horsepower?

02:44:55.600 --> 02:44:56.560
That's how powerful a horse is.

02:44:56.560 --> 02:44:57.520
What's a person of compute?

02:44:57.520 --> 02:44:59.080
Well, you know.

02:44:59.080 --> 02:44:59.920
I got it.

02:45:00.720 --> 02:45:01.760
That's interesting.

02:45:02.760 --> 02:45:06.000
VR also adds, I mean, in terms of creating worlds.

02:45:06.000 --> 02:45:07.080
You know what?

02:45:07.080 --> 02:45:08.720
Border Quest 2.

02:45:08.720 --> 02:45:11.040
I put it on and I can't believe

02:45:11.040 --> 02:45:12.600
the first thing they show me

02:45:12.600 --> 02:45:15.640
is a bunch of scrolling clouds and a Facebook login screen.

02:45:16.840 --> 02:45:20.400
You had the ability to bring me into a world.

02:45:20.400 --> 02:45:21.720
And what did you give me?

02:45:22.240 --> 02:45:24.080
A pop-up, right?

02:45:24.080 --> 02:45:25.960
And this is why you're not cool, Mark Zuckerberg.

02:45:25.960 --> 02:45:27.080
But you could be cool.

02:45:27.080 --> 02:45:28.800
Just make sure on the Quest 3,

02:45:28.800 --> 02:45:31.360
you don't put me into clouds and a Facebook login screen.

02:45:31.360 --> 02:45:32.440
Bring me to a world.

02:45:32.440 --> 02:45:34.080
I just tried Quest 3.

02:45:34.080 --> 02:45:34.920
It was awesome.

02:45:34.920 --> 02:45:35.800
But hear that, guys?

02:45:35.800 --> 02:45:36.880
I agree with that.

02:45:36.880 --> 02:45:39.640
I didn't have the clouds in the world.

02:45:39.640 --> 02:45:40.480
It was just so nice.

02:45:40.480 --> 02:45:41.320
You know what?

02:45:41.320 --> 02:45:43.000
Because I mean, the beginning,

02:45:43.960 --> 02:45:44.800
what is it?

02:45:44.800 --> 02:45:45.840
Todd Howard said this about

02:45:46.960 --> 02:45:48.840
the design of the beginning of the games he creates.

02:45:48.840 --> 02:45:51.680
It's like the beginning is so, so, so important.

02:45:52.680 --> 02:45:54.440
I recently played Zelda for the first time,

02:45:54.440 --> 02:45:55.720
Zelda Breath of the Wild, the previous one.

02:45:55.720 --> 02:45:58.360
And it's very quickly,

02:45:58.360 --> 02:46:00.240
you come out of this,

02:46:00.240 --> 02:46:03.200
within 10 seconds, you come out of a cave-type place

02:46:03.200 --> 02:46:05.520
and it's like, this world opens up.

02:46:05.520 --> 02:46:07.200
It's like, ah.

02:46:07.200 --> 02:46:09.840
And it pulls you in.

02:46:09.840 --> 02:46:13.440
You forget whatever troubles I was having, whatever.

02:46:13.440 --> 02:46:14.440
I got to play that from the beginning.

02:46:14.440 --> 02:46:16.320
I played it for an hour at a friend's house.

02:46:16.320 --> 02:46:17.160
Ah, no.

02:46:17.160 --> 02:46:19.800
The beginning, they got it, they did it really well.

02:46:19.800 --> 02:46:21.920
The expansiveness of that space.

02:46:23.240 --> 02:46:25.560
The peacefulness of that place, they got this,

02:46:25.560 --> 02:46:27.160
the music, I mean, so much of that.

02:46:27.160 --> 02:46:29.360
It's creating that world and pulling you right in.

02:46:29.360 --> 02:46:30.920
I'm gonna go buy a Switch.

02:46:30.920 --> 02:46:32.200
I'm gonna go today and buy a Switch.

02:46:32.200 --> 02:46:33.040
You sure?

02:46:33.040 --> 02:46:34.240
Well, the new one came out, I haven't played that yet,

02:46:34.240 --> 02:46:36.960
but Diablo 4 is something.

02:46:36.960 --> 02:46:38.960
I mean, there's sentimentality also,

02:46:38.960 --> 02:46:43.360
but something about VR really is incredible.

02:46:43.360 --> 02:46:47.600
But the new Quest 3 is mixed reality.

02:46:47.600 --> 02:46:49.000
And I got a chance to try that.

02:46:49.000 --> 02:46:51.280
So it's augmented reality.

02:46:51.280 --> 02:46:53.920
And for video games, it's done really, really well.

02:46:53.920 --> 02:46:54.960
Is it pass through or cameras?

02:46:54.960 --> 02:46:55.800
Cameras.

02:46:55.800 --> 02:46:56.640
It's cameras, okay.

02:46:56.640 --> 02:46:58.840
The Apple one, is that one pass through or cameras?

02:46:58.840 --> 02:46:59.960
I don't know.

02:46:59.960 --> 02:47:01.000
I don't know how real it is.

02:47:01.000 --> 02:47:02.200
I don't know anything.

02:47:02.200 --> 02:47:05.120
It's coming out in January.

02:47:05.120 --> 02:47:06.680
Is it January or is it some point?

02:47:06.680 --> 02:47:08.480
Some point, maybe not January.

02:47:08.480 --> 02:47:09.560
Maybe that's my optimism.

02:47:09.560 --> 02:47:10.600
But Apple, I will buy it.

02:47:10.600 --> 02:47:12.600
I don't care if it's expensive and does nothing.

02:47:12.600 --> 02:47:13.440
I will buy it.

02:47:13.440 --> 02:47:14.760
I will support this future endeavor.

02:47:14.760 --> 02:47:16.120
You're the meme.

02:47:16.120 --> 02:47:18.840
Oh yes, I support competition.

02:47:19.680 --> 02:47:21.720
It seemed like Quest was like the only people doing it.

02:47:21.720 --> 02:47:23.520
And this is great that they're like.

02:47:24.400 --> 02:47:25.240
You know what?

02:47:25.240 --> 02:47:26.080
And this is another place.

02:47:26.080 --> 02:47:28.680
We'll give some more respect to Mark Zuckerberg.

02:47:28.680 --> 02:47:31.520
The two companies that have endured through technology

02:47:31.520 --> 02:47:33.400
are Apple and Microsoft.

02:47:33.400 --> 02:47:34.800
And what do they make?

02:47:34.800 --> 02:47:37.160
Computers and business services.

02:47:37.160 --> 02:47:40.680
All the memes, social ads, they all come and go.

02:47:42.280 --> 02:47:45.000
But you want to endure, build hardware.

02:47:45.000 --> 02:47:48.520
Yeah, and that does a really interesting job.

02:47:49.320 --> 02:47:54.320
Maybe I'm new with this, but it's a $500 headset, Quest 3,

02:47:55.240 --> 02:47:59.040
and just having creatures run around the space,

02:47:59.040 --> 02:48:00.600
like our space right here.

02:48:00.600 --> 02:48:04.160
To me, okay, this is very like boomer statement,

02:48:04.160 --> 02:48:07.840
but it added windows to the place.

02:48:09.160 --> 02:48:10.520
I heard about the aquarium.

02:48:10.520 --> 02:48:12.720
Yeah, aquarium, but in this case it was a zombie game,

02:48:12.720 --> 02:48:13.880
whatever, it doesn't matter.

02:48:13.880 --> 02:48:17.120
But just like it modifies the space

02:48:17.120 --> 02:48:21.160
in a way where I can't, it really feels like a window

02:48:21.160 --> 02:48:22.640
and you can look out.

02:48:22.640 --> 02:48:23.480
It's pretty cool.

02:48:23.480 --> 02:48:25.360
Like I was just, it's like a zombie game,

02:48:25.360 --> 02:48:26.640
they're running at me, whatever.

02:48:26.640 --> 02:48:28.440
But what I was enjoying is the fact that there's like

02:48:28.440 --> 02:48:32.240
a window and they're stepping on objects in the space.

02:48:33.320 --> 02:48:35.240
That was a different kind of escape.

02:48:35.240 --> 02:48:37.600
Also because you can see the other humans,

02:48:37.600 --> 02:48:39.080
so it's integrated with the other humans.

02:48:39.080 --> 02:48:40.560
It's really.

02:48:40.560 --> 02:48:42.560
And that's why it's more important than ever

02:48:42.560 --> 02:48:44.440
that the AI is running on those systems

02:48:44.440 --> 02:48:46.200
are aligned with you.

02:48:46.440 --> 02:48:48.640
They're gonna augment your entire world.

02:48:48.640 --> 02:48:49.920
Oh yeah.

02:48:49.920 --> 02:48:52.640
And that, those AIs have a,

02:48:52.640 --> 02:48:55.120
I mean, you think about all the dark stuff,

02:48:55.120 --> 02:48:58.280
like sexual stuff.

02:48:58.280 --> 02:49:02.400
Like if those AIs threaten me, that could be haunting.

02:49:03.840 --> 02:49:07.080
Like if they threaten me in a non-video game way,

02:49:07.080 --> 02:49:11.400
it's like, they'll know personal information about me.

02:49:11.400 --> 02:49:13.680
And it's like, and then you lose track of what's real,

02:49:13.680 --> 02:49:14.520
what's not.

02:49:14.520 --> 02:49:15.640
Like what if stuff is like hacked?

02:49:15.640 --> 02:49:18.600
There's two directions the AI girlfriend company can take.

02:49:18.600 --> 02:49:20.840
There's like the highbrow, something like her,

02:49:20.840 --> 02:49:22.640
maybe something you kind of talk to in this is,

02:49:22.640 --> 02:49:24.200
and then there's the lowbrow version of it

02:49:24.200 --> 02:49:26.240
where I want to set up a brothel in Times Square.

02:49:26.240 --> 02:49:27.480
Yeah.

02:49:27.480 --> 02:49:28.320
Yeah.

02:49:28.320 --> 02:49:29.600
It's not cheating if it's a robot.

02:49:29.600 --> 02:49:31.080
It's a VR experience.

02:49:31.080 --> 02:49:32.880
Is there an in-between?

02:49:32.880 --> 02:49:35.280
No, I don't want to do that one or that one.

02:49:35.280 --> 02:49:36.200
Have you decided yet?

02:49:36.200 --> 02:49:37.160
No, I'll figure it out.

02:49:37.160 --> 02:49:39.400
We'll see what the technology goes.

02:49:39.400 --> 02:49:41.920
I would love to hear your opinions for George's

02:49:42.800 --> 02:49:46.360
third company, what to do with the brothel in Times Square

02:49:46.360 --> 02:49:48.840
or the her experience.

02:49:51.520 --> 02:49:53.320
What do you think company number four will be?

02:49:53.320 --> 02:49:54.480
You think there'll be a company number four?

02:49:54.480 --> 02:49:56.120
There's a lot to do in company number two.

02:49:56.120 --> 02:49:58.080
I'm just like, I'm talking about company number three now.

02:49:58.080 --> 02:49:59.760
None of that tech exists yet.

02:49:59.760 --> 02:50:01.680
There's a lot to do in company number two.

02:50:01.680 --> 02:50:04.160
Company number two is going to be the great struggle

02:50:04.160 --> 02:50:05.320
of the next six years.

02:50:05.320 --> 02:50:06.640
And if the next six years,

02:50:06.640 --> 02:50:09.000
how centralized is compute going to be?

02:50:09.000 --> 02:50:10.840
The less centralized compute is going to be,

02:50:10.840 --> 02:50:12.280
the better of a chance we all have.

02:50:12.280 --> 02:50:14.640
So you're a bearing, you're like a flag bearer

02:50:14.640 --> 02:50:19.440
for open source distributed decentralization of compute.

02:50:19.440 --> 02:50:20.640
We have to, we have to,

02:50:20.640 --> 02:50:22.440
or they will just completely dominate us.

02:50:22.440 --> 02:50:26.280
I showed a picture on stream of a man in a chicken farm.

02:50:26.280 --> 02:50:28.400
You ever seen one of those factory farm chicken farms?

02:50:28.400 --> 02:50:30.240
Why does he dominate all the chickens?

02:50:32.440 --> 02:50:33.280
Why does he?

02:50:33.280 --> 02:50:34.100
He's smarter.

02:50:34.100 --> 02:50:35.040
He's smarter, right?

02:50:35.040 --> 02:50:36.360
Some people on Twitch were like,

02:50:36.360 --> 02:50:38.320
he's bigger than the chickens, yeah.

02:50:38.320 --> 02:50:41.480
And now here's a man in a cow farm, right?

02:50:41.480 --> 02:50:42.920
So it has nothing to do with their size

02:50:42.920 --> 02:50:44.560
and everything to do with their intelligence.

02:50:44.560 --> 02:50:48.960
And if one central organization has all the intelligence,

02:50:48.960 --> 02:50:52.240
you'll be the chickens and they'll be the chicken man.

02:50:52.240 --> 02:50:54.480
But if we all have the intelligence,

02:50:54.480 --> 02:50:55.700
we're all the chickens.

02:50:57.880 --> 02:50:59.960
We're not all the man, we're all the chickens.

02:50:59.960 --> 02:51:01.480
We're no man. And there's no chicken man.

02:51:01.480 --> 02:51:03.240
There's no chicken man.

02:51:03.240 --> 02:51:05.000
We're just chickens in Miami.

02:51:05.000 --> 02:51:07.200
He was having a good life, man.

02:51:07.200 --> 02:51:08.800
I'm sure he was.

02:51:08.800 --> 02:51:09.960
I'm sure he was.

02:51:09.960 --> 02:51:11.660
What have you learned from launching

02:51:11.660 --> 02:51:13.620
and running Comm.AI and TinyCorp?

02:51:13.620 --> 02:51:18.120
So this starting a company from an idea and scaling it.

02:51:18.120 --> 02:51:19.760
And by the way, I'm all in on TinyBox.

02:51:19.760 --> 02:51:24.400
So I'm your, I guess it's pre-order only now.

02:51:24.400 --> 02:51:25.540
I wanna make sure it's good.

02:51:25.540 --> 02:51:28.160
I wanna make sure that like the thing that I deliver

02:51:28.160 --> 02:51:30.520
is like not gonna be like a Quest 2,

02:51:30.520 --> 02:51:32.360
which you buy and use twice.

02:51:32.360 --> 02:51:33.400
I mean, it's better than a Quest,

02:51:33.400 --> 02:51:36.760
which you bought and used less than once statistically.

02:51:36.760 --> 02:51:40.040
Well, if there's a beta program for TinyBox, I'm into.

02:51:40.040 --> 02:51:41.640
Sounds good.

02:51:41.640 --> 02:51:43.180
So I won't be the whiny.

02:51:44.480 --> 02:51:48.000
I'll be the tech savvy user of the TinyBox

02:51:48.000 --> 02:51:50.640
just to be in the early days.

02:51:50.640 --> 02:51:53.280
What have you learned from building these companies?

02:51:54.440 --> 02:51:57.180
The longest time at Comm.AI I asked why,

02:51:58.320 --> 02:52:00.320
why did I start a company?

02:52:00.320 --> 02:52:01.540
Why did I do this?

02:52:02.380 --> 02:52:07.060
Um, but you know, what else was I gonna do?

02:52:08.740 --> 02:52:13.580
So you like, you like bringing ideas to life.

02:52:15.380 --> 02:52:19.640
With Comma, it really started as an ego battle with Elon.

02:52:21.220 --> 02:52:22.140
I wanted to beat him.

02:52:22.140 --> 02:52:24.300
I like, I saw worthy adversary, you know,

02:52:24.300 --> 02:52:26.060
here's a worthy adversary who I can beat

02:52:26.060 --> 02:52:27.460
at self-driving cars.

02:52:27.460 --> 02:52:29.340
And like, I think we've kept pace

02:52:29.340 --> 02:52:30.700
and I think he's kept ahead.

02:52:30.700 --> 02:52:32.780
I think that's what's ended up happening there.

02:52:32.780 --> 02:52:36.820
But I do think Comma is, I mean, Comma's profitable.

02:52:36.820 --> 02:52:40.520
Like, and like when this drive GPT stuff starts working,

02:52:40.520 --> 02:52:41.360
that's it.

02:52:41.360 --> 02:52:42.740
There's no more like bugs in the loss function.

02:52:42.740 --> 02:52:45.060
Like right now we're using like a hand coded simulator.

02:52:45.060 --> 02:52:45.900
There's no more bugs.

02:52:45.900 --> 02:52:46.740
This is gonna be it.

02:52:46.740 --> 02:52:48.540
Like this is the run up to driving.

02:52:48.540 --> 02:52:52.100
I hear a lot of really, a lot of props

02:52:52.100 --> 02:52:53.540
for a compiler for Comma.

02:52:53.540 --> 02:52:57.380
It's so, it's better than FSD and autopilot in certain ways.

02:52:57.380 --> 02:53:00.140
It has a lot more to do with which field you like.

02:53:00.220 --> 02:53:02.860
We lowered the price on the hardware to 1499.

02:53:02.860 --> 02:53:05.060
You know how hard it is to ship reliable

02:53:05.060 --> 02:53:07.500
consumer electronics that go on your windshield?

02:53:07.500 --> 02:53:11.580
We're doing more than like most cell phone companies.

02:53:11.580 --> 02:53:12.740
How'd you pull that off by the way?

02:53:12.740 --> 02:53:14.740
Shipping a product that goes in a car.

02:53:14.740 --> 02:53:15.820
I know.

02:53:15.820 --> 02:53:17.980
I have a, I have a, I have an SMT line.

02:53:17.980 --> 02:53:21.020
It's all, I make all the boards in-house in San Diego.

02:53:21.020 --> 02:53:22.020
Quality control.

02:53:22.020 --> 02:53:24.060
I care immensely about it.

02:53:24.060 --> 02:53:29.060
You're basically a mom and pop shop with great testing.

02:53:29.300 --> 02:53:33.300
Our head of open pilot is great at like, you know, okay.

02:53:33.300 --> 02:53:35.380
I want all the common theories to be identical.

02:53:35.380 --> 02:53:36.500
Yeah.

02:53:36.500 --> 02:53:39.580
And yeah, I mean, you know, it's, look, it's 1499.

02:53:39.580 --> 02:53:42.300
It, 30 day money back guarantee.

02:53:42.300 --> 02:53:45.380
It will, it will blow your mind at what it can do.

02:53:45.380 --> 02:53:46.460
Is it hard to scale?

02:53:47.940 --> 02:53:48.780
You know what?

02:53:48.780 --> 02:53:50.180
There's kind of downsides to scaling it.

02:53:50.180 --> 02:53:52.500
People are always like, why don't you advertise?

02:53:52.500 --> 02:53:54.060
Our mission is to solve self-driving cars

02:53:54.060 --> 02:53:55.860
while delivering shipable intermediaries.

02:53:55.860 --> 02:53:58.980
Our mission has nothing to do with selling a million boxes.

02:53:59.820 --> 02:54:00.660
It's tawdry.

02:54:01.700 --> 02:54:04.500
Do you think it's possible that a comma gets sold?

02:54:06.220 --> 02:54:09.980
Only if I felt someone could accelerate that mission

02:54:09.980 --> 02:54:12.180
and wanted to keep it open source.

02:54:12.180 --> 02:54:13.620
And like, not just wanted to,

02:54:13.620 --> 02:54:15.340
I don't believe what anyone says.

02:54:15.340 --> 02:54:17.260
I believe incentives.

02:54:17.260 --> 02:54:20.220
If a company wanted to buy comma with their incentives,

02:54:20.220 --> 02:54:21.060
or to keep it open source,

02:54:21.060 --> 02:54:23.700
but comma doesn't stop at the cars.

02:54:23.700 --> 02:54:25.180
The cars are just the beginning.

02:54:25.180 --> 02:54:26.780
The device is a human head.

02:54:26.780 --> 02:54:28.740
The device has two eyes, two ears.

02:54:29.300 --> 02:54:30.620
It has a mouth.

02:54:30.620 --> 02:54:33.500
So you think this goes to embodied robotics?

02:54:33.500 --> 02:54:35.260
We sell common bodies too.

02:54:35.260 --> 02:54:37.500
You know, they're very rudimentary.

02:54:38.660 --> 02:54:42.740
But one of the problems that we're running into

02:54:42.740 --> 02:54:44.300
is that the comma three

02:54:44.300 --> 02:54:46.300
has about as much intelligence as a bee.

02:54:47.220 --> 02:54:50.220
If you want a human's worth of intelligence,

02:54:50.220 --> 02:54:52.500
you're gonna need a tiny rack, not even a tiny box.

02:54:52.500 --> 02:54:55.420
You're gonna need like a tiny rack, maybe even more.

02:54:55.420 --> 02:54:58.180
How does that, how do you put legs on that?

02:54:58.180 --> 02:55:00.260
You don't, and there's no way you can.

02:55:00.260 --> 02:55:02.540
You connect to it wirelessly.

02:55:02.540 --> 02:55:06.060
So you put your tiny box or your tiny rack in your house,

02:55:06.060 --> 02:55:07.700
and then you get your comma body,

02:55:07.700 --> 02:55:10.220
and your comma body runs the models on that.

02:55:10.220 --> 02:55:11.700
It's close, right?

02:55:11.700 --> 02:55:12.700
You don't have to go to some cloud,

02:55:12.700 --> 02:55:14.820
which is, you know, 30 milliseconds away.

02:55:14.820 --> 02:55:18.140
You go to a thing which is 0.1 milliseconds away.

02:55:18.140 --> 02:55:20.420
So the AI girlfriend

02:55:20.420 --> 02:55:23.180
will have like a central hub in the home.

02:55:23.180 --> 02:55:26.620
I mean, eventually, if you fast forward 20, 30 years,

02:55:26.620 --> 02:55:29.340
the mobile chips will get good enough to run these AIs.

02:55:29.340 --> 02:55:31.140
But fundamentally, it's not even a question

02:55:31.140 --> 02:55:33.780
of putting legs on a tiny box

02:55:33.780 --> 02:55:36.140
because how are you getting 1.5 kilowatts of power

02:55:36.140 --> 02:55:37.940
on that thing, right?

02:55:37.940 --> 02:55:41.660
So you need, they're very synergistic businesses.

02:55:41.660 --> 02:55:44.700
I also wanna build all of Comma's training computers.

02:55:44.700 --> 02:55:46.700
Comma builds training computers right now.

02:55:46.700 --> 02:55:48.900
We use commodity parts.

02:55:48.900 --> 02:55:50.700
I think I can do it cheaper.

02:55:50.700 --> 02:55:52.420
So we're gonna build,

02:55:52.420 --> 02:55:54.300
TinyCorp is gonna not just sell tiny boxes.

02:55:54.300 --> 02:55:55.860
Tiny boxes is the consumer version,

02:55:55.860 --> 02:55:57.660
but I'll build training data centers too.

02:55:57.660 --> 02:56:00.020
Have you talked to Andhra Kapothi

02:56:00.020 --> 02:56:01.580
or have you talked to Elon about TinyCorp?

02:56:01.580 --> 02:56:03.380
He went to work at OpenAI.

02:56:03.380 --> 02:56:05.900
What do you love about Andhra Kapothi?

02:56:05.900 --> 02:56:09.900
To me, he's one of the truly special humans we got.

02:56:09.900 --> 02:56:11.540
Oh man, like, you know,

02:56:11.540 --> 02:56:16.420
his streams are just a level of quality so far beyond mine.

02:56:16.420 --> 02:56:17.540
Like, I can't help myself.

02:56:17.540 --> 02:56:19.140
Like, it's just, it's just, you know.

02:56:19.140 --> 02:56:20.100
Yeah, he's good.

02:56:20.100 --> 02:56:22.780
He wants to teach you.

02:56:22.780 --> 02:56:23.620
Yeah.

02:56:23.620 --> 02:56:26.020
I want to show you that I'm smarter than you.

02:56:26.020 --> 02:56:28.820
Yeah, he has no, I mean,

02:56:28.820 --> 02:56:32.660
thank you for the sort of, the raw, authentic honesty.

02:56:32.660 --> 02:56:34.860
I mean, a lot of us have that.

02:56:34.860 --> 02:56:36.820
I think Andhra is as legit as it gets

02:56:36.820 --> 02:56:38.460
in that he just wants to teach you.

02:56:38.460 --> 02:56:41.580
And there's a curiosity that just drives him.

02:56:41.580 --> 02:56:45.540
And just like at his, at the stage where he is in life,

02:56:45.540 --> 02:56:49.540
to be still like one of the best tinkerers in the world.

02:56:49.540 --> 02:56:50.380
Yeah.

02:56:50.380 --> 02:56:51.340
It's crazy.

02:56:51.340 --> 02:56:53.620
Like to, what is it?

02:56:53.620 --> 02:56:54.460
Micrograd?

02:56:54.460 --> 02:56:55.300
Micrograd, yeah.

02:56:55.300 --> 02:56:56.540
Inspiration for tiny grad.

02:56:58.620 --> 02:57:02.060
That whole, I mean, his CS231N was,

02:57:02.060 --> 02:57:03.540
this was the inspiration.

02:57:03.540 --> 02:57:04.900
This is what I just took and ran with

02:57:04.900 --> 02:57:06.860
and ended up writing this, so, you know.

02:57:06.860 --> 02:57:08.300
But I mean, to me that-

02:57:08.300 --> 02:57:10.540
Don't go work for Darth Vader, man.

02:57:10.540 --> 02:57:13.100
I mean, the flip side to me is that

02:57:13.100 --> 02:57:18.100
the fact that he's going there is a good sign for OpenAI.

02:57:18.180 --> 02:57:22.500
I think, you know, I like Ilya Setskevich a lot.

02:57:22.500 --> 02:57:25.780
I like those, those guys are really good at what they do.

02:57:25.780 --> 02:57:26.740
I know they are.

02:57:26.740 --> 02:57:28.860
And that's kind of what's even like more.

02:57:28.860 --> 02:57:29.780
And you know what?

02:57:29.780 --> 02:57:32.100
It's not that OpenAI doesn't open source

02:57:32.100 --> 02:57:33.460
the weights of GPT-4.

02:57:34.440 --> 02:57:36.840
It's that they go in front of Congress.

02:57:37.900 --> 02:57:39.180
And that is what upsets me.

02:57:39.180 --> 02:57:41.060
You know, we had two effective altruists,

02:57:41.060 --> 02:57:42.560
Sam's, go in front of Congress.

02:57:42.560 --> 02:57:43.400
One's in jail.

02:57:45.180 --> 02:57:47.180
I think you're drawing parallels on that.

02:57:47.700 --> 02:57:48.660
One's in jail.

02:57:48.660 --> 02:57:49.740
You give me a look.

02:57:51.180 --> 02:57:52.020
Give me a look.

02:57:52.020 --> 02:57:53.260
No, I think effective altruism

02:57:53.260 --> 02:57:55.540
is a terribly evil ideology.

02:57:55.540 --> 02:57:56.380
Oh yeah, that's interesting.

02:57:56.380 --> 02:57:57.220
Why do you think that is?

02:57:57.220 --> 02:58:00.100
Why do you think there's something about

02:58:00.100 --> 02:58:02.460
a thing that sounds pretty good

02:58:02.460 --> 02:58:04.140
that kind of gets us into trouble?

02:58:04.140 --> 02:58:06.220
Because you get Sam Bangman Fried.

02:58:06.220 --> 02:58:07.860
Like Sam Bangman Fried is the embodiment

02:58:07.860 --> 02:58:09.420
of effective altruism.

02:58:10.500 --> 02:58:13.740
Utilitarianism is an abhorrent ideology.

02:58:13.740 --> 02:58:16.060
Like, well yeah, we're gonna kill those three people

02:58:16.060 --> 02:58:17.700
to save a thousand, of course.

02:58:17.700 --> 02:58:18.540
Yeah.

02:58:18.540 --> 02:58:21.580
Right, there's no underlying, like there's just, yeah.

02:58:23.260 --> 02:58:25.700
Yeah, but to me that's a bit surprising.

02:58:26.540 --> 02:58:30.720
But it's also, in retrospect, not that surprising.

02:58:30.720 --> 02:58:33.180
But I haven't heard really clear kind of

02:58:35.700 --> 02:58:40.540
like rigorous analysis why effective altruism is flawed.

02:58:40.540 --> 02:58:42.540
Oh well, I think charity is bad, right?

02:58:42.540 --> 02:58:43.860
So what is charity but investment

02:58:43.860 --> 02:58:46.460
that you don't expect to have a return on, right?

02:58:47.820 --> 02:58:50.420
Yeah, but you can also think of charity as like,

02:58:51.660 --> 02:58:53.260
is you would like to see,

02:58:54.380 --> 02:58:56.660
so allocate resources in an optimal way

02:58:57.820 --> 02:59:00.000
to make a better world.

02:59:00.000 --> 02:59:03.220
And probably almost always that involves starting a company.

02:59:03.220 --> 02:59:04.060
Yeah.

02:59:04.060 --> 02:59:04.940
Right, because- More efficient.

02:59:04.940 --> 02:59:06.420
Yeah, if you just take the money

02:59:06.420 --> 02:59:08.820
and you spend it on malaria nets,

02:59:08.820 --> 02:59:11.740
you know, okay, great, you've made 100 malaria nets.

02:59:11.740 --> 02:59:13.060
But if you teach-

02:59:13.100 --> 02:59:14.580
Yeah, you may not know how to fish.

02:59:14.580 --> 02:59:15.420
Right, yeah.

02:59:15.420 --> 02:59:17.740
No, but the problem is teaching them how to fish

02:59:17.740 --> 02:59:18.560
might be harder.

02:59:18.560 --> 02:59:19.660
Starting a company might be harder

02:59:19.660 --> 02:59:22.460
than allocating money that you already have.

02:59:22.460 --> 02:59:24.540
I like the flip side of effective altruism,

02:59:24.540 --> 02:59:25.940
effective accelerationism.

02:59:25.940 --> 02:59:27.660
I think accelerationism is the only thing

02:59:27.660 --> 02:59:30.080
that's ever lifted people out of poverty.

02:59:30.080 --> 02:59:32.260
The fact that food is cheap.

02:59:32.260 --> 02:59:33.760
Not we're giving food away

02:59:33.760 --> 02:59:35.860
because we are kind-hearted people.

02:59:35.860 --> 02:59:37.980
No, food is cheap.

02:59:37.980 --> 02:59:39.980
And that's the world you wanna live in.

02:59:39.980 --> 02:59:42.340
It's just UBI, what a scary idea.

02:59:43.740 --> 02:59:46.500
What a scary idea, all your power now?

02:59:46.500 --> 02:59:48.140
Your money is power?

02:59:48.140 --> 02:59:50.060
Your only source of power is granted to you

02:59:50.060 --> 02:59:52.540
by the goodwill of the government?

02:59:52.540 --> 02:59:54.060
What a scary idea.

02:59:54.060 --> 02:59:57.540
So you even think long-term, even-

02:59:57.540 --> 03:00:00.860
I'd rather die than need UBI to survive, and I mean it.

03:00:04.460 --> 03:00:06.420
What if survival is basically guaranteed?

03:00:06.420 --> 03:00:08.940
What if our life becomes so good?

03:00:08.940 --> 03:00:12.300
You can make survival guaranteed without UBI.

03:00:12.300 --> 03:00:15.100
What you have to do is make housing and food dirt cheap.

03:00:15.100 --> 03:00:15.940
Sure.

03:00:15.940 --> 03:00:17.220
And that's the good world.

03:00:17.220 --> 03:00:19.020
And actually, let's go into what we should really

03:00:19.020 --> 03:00:21.900
be making dirt cheap, which is energy, right?

03:00:21.900 --> 03:00:25.260
That energy, you know, oh my God.

03:00:25.260 --> 03:00:27.660
You know, that's, if there's one,

03:00:27.660 --> 03:00:29.300
I'm pretty centrist politically.

03:00:29.300 --> 03:00:31.900
If there's one political position I cannot stand,

03:00:31.900 --> 03:00:33.380
it's deceleration.

03:00:33.380 --> 03:00:36.060
It's people who believe we should use less energy.

03:00:36.060 --> 03:00:37.700
Not people who believe global warming is a problem.

03:00:37.700 --> 03:00:38.660
I agree with you.

03:00:38.660 --> 03:00:42.220
Not people who believe that saving the environment

03:00:42.220 --> 03:00:43.780
is good, I agree with you.

03:00:43.780 --> 03:00:46.300
But people who think we should use less energy.

03:00:46.300 --> 03:00:48.580
Energy usage is a moral bad.

03:00:48.580 --> 03:00:49.420
No.

03:00:50.220 --> 03:00:54.460
No, you are asking, you are diminishing humanity.

03:00:54.460 --> 03:00:56.740
Yeah, energy is flourishing,

03:00:56.740 --> 03:00:59.340
of creative flourishing of the human species.

03:00:59.340 --> 03:01:00.500
How do we make more of it?

03:01:00.500 --> 03:01:01.420
How do we make it clean?

03:01:01.420 --> 03:01:03.540
And how do we make, just, just, just,

03:01:03.540 --> 03:01:06.780
how do I pay, you know, 20 cents for a megawatt hour

03:01:06.780 --> 03:01:08.340
instead of a kilowatt hour?

03:01:08.340 --> 03:01:13.060
Part of me wishes that Elon went into nuclear fusion

03:01:13.060 --> 03:01:16.180
versus Twitter, part of me.

03:01:16.180 --> 03:01:18.260
Or somebody, somebody like Elon.

03:01:18.260 --> 03:01:21.220
You know, we need to, I wish there were more,

03:01:21.220 --> 03:01:22.860
more Elons in the world.

03:01:22.860 --> 03:01:25.940
And I think Elon sees it as like,

03:01:25.940 --> 03:01:28.460
this is a political battle that needed to be fought.

03:01:28.460 --> 03:01:30.900
And again, like, you know, I always ask the question

03:01:30.900 --> 03:01:32.020
of whenever I disagree with him,

03:01:32.020 --> 03:01:35.060
I remind myself that he's a billionaire and I'm not.

03:01:35.060 --> 03:01:37.260
So, you know, maybe he's got something figured out

03:01:37.260 --> 03:01:38.900
that I don't, or maybe he doesn't.

03:01:38.900 --> 03:01:42.340
To have some humility, but at the same time,

03:01:42.340 --> 03:01:45.100
me as a person who happens to know him,

03:01:45.100 --> 03:01:47.060
I find myself in that same position.

03:01:48.020 --> 03:01:51.300
Sometimes even billionaires need friends

03:01:51.300 --> 03:01:53.180
who disagree and help them grow.

03:01:54.220 --> 03:01:57.100
And that's a difficult, that's a difficult reality.

03:01:57.100 --> 03:02:00.020
And it must be so hard, it must be so hard to meet people

03:02:00.020 --> 03:02:01.980
once you get to that point where.

03:02:01.980 --> 03:02:05.580
Fame, power, money, everybody's sucking up to you.

03:02:05.580 --> 03:02:07.060
See, I love not having shit.

03:02:07.900 --> 03:02:09.580
I'm like, I don't have shit, man.

03:02:09.580 --> 03:02:11.060
Trust me, there's nothing I can give you.

03:02:11.060 --> 03:02:13.780
There's nothing worth taking from me, you know?

03:02:13.780 --> 03:02:16.100
Yeah, it takes a really special human being

03:02:16.100 --> 03:02:17.900
when you have power, when you have fame,

03:02:17.900 --> 03:02:21.460
when you have money to still think from first principles.

03:02:21.460 --> 03:02:23.500
Not like all the adoration you get towards you,

03:02:23.500 --> 03:02:26.580
all the admiration, all the people saying yes, yes, yes.

03:02:26.580 --> 03:02:27.820
And all the hate too.

03:02:27.820 --> 03:02:28.660
And the hate.

03:02:28.660 --> 03:02:29.500
I think that's the worst.

03:02:29.500 --> 03:02:33.340
So the hate makes you want to go to the yes people

03:02:33.340 --> 03:02:35.780
because the hate exhausts you.

03:02:35.820 --> 03:02:38.380
And the kind of hate that Elon's gotten from the left

03:02:38.380 --> 03:02:39.940
is pretty intense.

03:02:39.940 --> 03:02:41.980
And so that, of course, drives him right.

03:02:44.860 --> 03:02:46.860
And loses balance and.

03:02:46.860 --> 03:02:49.900
And it keeps this absolutely fake, like,

03:02:49.900 --> 03:02:53.260
psy-op political divide alive

03:02:53.260 --> 03:02:55.180
so that the 1% can keep power, like.

03:02:55.180 --> 03:02:58.020
Yeah, I wish we'd be less divided

03:02:58.020 --> 03:02:59.460
because it is giving power.

03:02:59.460 --> 03:03:01.500
It gives power. To the ultra-powerful.

03:03:02.420 --> 03:03:04.340
The rich get richer.

03:03:04.340 --> 03:03:06.100
You have love in your life.

03:03:06.100 --> 03:03:09.940
Has love made you a better or a worse programmer?

03:03:11.660 --> 03:03:13.700
Do you keep productivity metrics?

03:03:13.700 --> 03:03:14.660
No, no.

03:03:15.660 --> 03:03:18.020
No, I'm not that, I'm not that methodical.

03:03:19.260 --> 03:03:22.020
I think that there comes to a point where

03:03:22.020 --> 03:03:24.860
if it's no longer visceral, I just can't enjoy it.

03:03:25.900 --> 03:03:28.700
I still viscerally love programming.

03:03:28.700 --> 03:03:29.940
The minute I started like.

03:03:29.940 --> 03:03:33.020
So that's one of the big loves of your life is programming.

03:03:33.020 --> 03:03:34.940
I mean, just my computer in general.

03:03:34.940 --> 03:03:36.780
I mean, you know, I tell my girlfriend,

03:03:36.780 --> 03:03:39.700
my first love is my computer, of course.

03:03:39.700 --> 03:03:42.180
Right, like, you know, I sleep with my computer.

03:03:42.180 --> 03:03:44.420
It's there for a lot of my sexual experiences.

03:03:44.420 --> 03:03:46.740
Like, come on, so is everyone's, right?

03:03:46.740 --> 03:03:48.380
Like, you know, you gotta be real about that.

03:03:48.380 --> 03:03:49.220
And like.

03:03:49.220 --> 03:03:50.900
Not just like the IDE for programming,

03:03:50.900 --> 03:03:53.300
just the entirety of the computational machine.

03:03:53.300 --> 03:03:54.620
The fact that, yeah, I mean, it's, you know,

03:03:54.620 --> 03:03:57.460
I wish it was, and someday they'll be smarter.

03:03:57.460 --> 03:03:59.900
And someday, you know, maybe I'm weird for this,

03:03:59.900 --> 03:04:01.180
but I don't discriminate, man.

03:04:01.180 --> 03:04:02.860
I'm not gonna discriminate biostack life

03:04:02.860 --> 03:04:04.460
and silicon stack life, like.

03:04:04.460 --> 03:04:07.220
So the moment the computer starts to say,

03:04:07.220 --> 03:04:08.660
like, I miss you.

03:04:08.660 --> 03:04:13.220
I started to have some of the basics of human intimacy.

03:04:13.220 --> 03:04:14.260
It's over for you.

03:04:14.260 --> 03:04:16.780
The moment VS code says, hey, George.

03:04:16.780 --> 03:04:19.460
No, you see, no, no, no, but VS code is,

03:04:19.460 --> 03:04:20.860
no, they're just doing that.

03:04:20.860 --> 03:04:22.860
Microsoft's doing that to try to get me hooked on it.

03:04:22.860 --> 03:04:24.020
I'll see through it.

03:04:24.020 --> 03:04:24.860
I'll see through it.

03:04:24.860 --> 03:04:25.680
It's gold digger, man.

03:04:25.680 --> 03:04:26.520
It's gold digger.

03:04:26.520 --> 03:04:27.420
Look at me in open source.

03:04:27.420 --> 03:04:29.220
Well, this just gets more interesting, right?

03:04:29.260 --> 03:04:31.820
If it's open source, and yeah, it becomes.

03:04:31.820 --> 03:04:33.620
Though Microsoft's done a pretty good job on that.

03:04:33.620 --> 03:04:34.460
Oh, absolutely.

03:04:34.460 --> 03:04:36.820
No, no, no, look, I think Microsoft, again,

03:04:36.820 --> 03:04:38.700
I wouldn't count on it to be true forever,

03:04:38.700 --> 03:04:39.660
but I think right now,

03:04:39.660 --> 03:04:43.020
Microsoft is doing the best work in the programming world.

03:04:43.020 --> 03:04:47.180
Like between, yeah, GitHub, GitHub actions, VS code,

03:04:47.180 --> 03:04:49.420
the improvements to Python, where's Microsoft?

03:04:49.420 --> 03:04:51.260
Like, this is.

03:04:51.260 --> 03:04:54.780
Who would have thought Microsoft and Mark Zuckerberg

03:04:54.780 --> 03:04:57.060
are spearheading the open source movement?

03:04:57.060 --> 03:04:58.540
Right, right.

03:04:59.980 --> 03:05:01.380
How things change.

03:05:01.380 --> 03:05:03.500
Oh, it's beautiful.

03:05:03.500 --> 03:05:04.580
And by the way, that's who I'd bet on

03:05:04.580 --> 03:05:06.060
to replace Google, by the way.

03:05:06.060 --> 03:05:06.900
Who?

03:05:06.900 --> 03:05:07.780
Microsoft.

03:05:07.780 --> 03:05:08.620
Microsoft.

03:05:08.620 --> 03:05:09.820
I think Satya Nadella said straight up,

03:05:09.820 --> 03:05:11.420
I'm coming for it.

03:05:11.420 --> 03:05:15.620
Interesting, so your bet, who wins AGI?

03:05:15.620 --> 03:05:16.740
Oh, I don't know about AGI.

03:05:16.740 --> 03:05:17.820
I think we're a long way away from that,

03:05:17.820 --> 03:05:21.020
but I would not be surprised if in the next five years,

03:05:21.020 --> 03:05:23.020
Bing overtakes Google as a search engine.

03:05:24.060 --> 03:05:25.180
Interesting.

03:05:25.180 --> 03:05:26.980
Wouldn't surprise me.

03:05:26.980 --> 03:05:28.020
Interesting.

03:05:29.780 --> 03:05:31.700
I hope some startup does.

03:05:33.220 --> 03:05:34.260
It might be some startup too.

03:05:34.260 --> 03:05:36.980
I would equally bet on some startup.

03:05:36.980 --> 03:05:40.540
Yeah, I'm like 50-50, but maybe that's naive.

03:05:40.540 --> 03:05:43.580
I believe in the power of these language models.

03:05:43.580 --> 03:05:45.700
Satya's alive, Microsoft's alive.

03:05:45.700 --> 03:05:48.260
Yeah, it's great, it's great.

03:05:48.260 --> 03:05:50.620
I like all the innovation in these companies.

03:05:50.620 --> 03:05:51.820
They're not being stale.

03:05:53.020 --> 03:05:55.260
And to the degree they're being stale, they're losing.

03:05:55.260 --> 03:05:58.180
So there's a huge incentive to do a lot of exciting work

03:05:58.180 --> 03:06:01.060
and open source work, which is incredible.

03:06:01.060 --> 03:06:02.740
Only way to win?

03:06:02.740 --> 03:06:05.620
You're older, you're wiser.

03:06:05.620 --> 03:06:08.460
What's the meaning of life, George Hotz?

03:06:08.460 --> 03:06:09.660
To win.

03:06:09.660 --> 03:06:10.820
It's still to win.

03:06:10.820 --> 03:06:12.300
Of course.

03:06:12.300 --> 03:06:13.420
Always.

03:06:13.420 --> 03:06:14.580
Of course.

03:06:14.580 --> 03:06:16.460
What's winning look like for you?

03:06:16.460 --> 03:06:18.500
I don't know, I haven't figured out what the game is yet,

03:06:18.500 --> 03:06:19.820
but when I do, I wanna win.

03:06:19.820 --> 03:06:21.580
So it's bigger than solving self-driving,

03:06:21.580 --> 03:06:26.580
it's bigger than democratizing, decentralizing compute.

03:06:29.180 --> 03:06:31.540
I think the game is to stand eye to eye with God.

03:06:33.940 --> 03:06:36.620
I wonder what that means for you.

03:06:36.620 --> 03:06:40.180
Like, at the end of your life, what that will look like.

03:06:41.140 --> 03:06:42.780
I mean, this is what, like, I don't know,

03:06:42.780 --> 03:06:44.100
this is some, this is some,

03:06:45.700 --> 03:06:47.940
there's probably some ego trip of mine, you know?

03:06:47.940 --> 03:06:50.220
Like, if you wanna stand eye to eye with God,

03:06:50.220 --> 03:06:52.860
you're just blasphemous, man, okay?

03:06:52.860 --> 03:06:54.100
I don't know, I don't know.

03:06:54.100 --> 03:06:55.900
I don't know if it would upset God.

03:06:55.900 --> 03:06:57.140
I think he like wants that.

03:06:57.140 --> 03:06:59.700
I mean, I certainly want that from my creations.

03:06:59.700 --> 03:07:02.140
I want my creations to stand eye to eye with me.

03:07:03.260 --> 03:07:06.060
So why wouldn't God want me to stand eye to eye with him?

03:07:08.140 --> 03:07:09.980
That's the best I can do, golden rule.

03:07:11.260 --> 03:07:14.180
I'm just imagining the creator of a video game

03:07:15.580 --> 03:07:20.580
having to look and stand eye to eye

03:07:20.660 --> 03:07:22.740
with one of the characters.

03:07:22.740 --> 03:07:24.660
I only watched season one of Westworld,

03:07:24.660 --> 03:07:26.580
but yeah, we gotta find the maze and solve it.

03:07:26.580 --> 03:07:27.900
Like...

03:07:27.900 --> 03:07:30.300
Yeah, I wonder what that looks like.

03:07:30.300 --> 03:07:33.380
It feels like a really special time in human history

03:07:33.380 --> 03:07:34.900
where that's actually possible.

03:07:34.900 --> 03:07:37.340
Like, there's something about AI that's like,

03:07:37.340 --> 03:07:40.020
we're playing with something weird here,

03:07:40.020 --> 03:07:41.820
something really weird.

03:07:41.820 --> 03:07:46.420
I wrote a blog post, I reread Genesis and just looked like,

03:07:46.420 --> 03:07:48.300
they give you some clues at the end of Genesis

03:07:48.300 --> 03:07:49.860
for finding the Garden of Eden.

03:07:51.140 --> 03:07:53.820
And I'm interested, I'm interested.

03:07:54.820 --> 03:07:57.020
Well, I hope you find just that, George.

03:07:57.020 --> 03:07:58.380
You're one of my favorite people.

03:07:58.380 --> 03:07:59.700
Thank you for doing everything you're doing.

03:07:59.700 --> 03:08:02.420
And in this case, for fighting for open source

03:08:02.420 --> 03:08:04.540
and for decentralization of AI,

03:08:04.540 --> 03:08:08.340
it's a fight worth fighting, fight worth winning hashtag.

03:08:09.340 --> 03:08:10.180
I love you, brother.

03:08:10.180 --> 03:08:11.460
These conversations are always great.

03:08:11.460 --> 03:08:13.420
I hope to talk to you many more times.

03:08:13.420 --> 03:08:15.540
Good luck with TinyCorp.

03:08:15.540 --> 03:08:17.740
Thank you, great to be here.

03:08:17.740 --> 03:08:20.380
Thanks for listening to this conversation with George Hotz.

03:08:20.380 --> 03:08:21.580
To support this podcast,

03:08:21.660 --> 03:08:24.380
please check out our sponsors in the description.

03:08:24.380 --> 03:08:26.180
And now, let me leave you with some words

03:08:26.180 --> 03:08:28.180
from Albert Einstein.

03:08:28.180 --> 03:08:31.100
Everything should be made as simple as possible,

03:08:31.100 --> 03:08:32.740
but not simpler.

03:08:33.940 --> 03:08:36.980
Thank you for listening and hope to see you next time.

